[
    {
        "paper id": "2407.05082",
        "abstract url": "https://arxiv.org/abs/2407.05082",
        "title": "DMTG: One-Shot Differentiable Multi-Task Grouping",
        "rating": "2.5",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "We aim to address Multi-Task Learning (MTL) with a large number of tasks by Multi-Task Grouping (MTG). Given N tasks, we propose to simultaneously identify the best task groups from 2^N candidates and train the model weights simultaneously in one-shot, with the high-order task-affinity fully exploited. This is distinct from the pioneering methods which sequentially identify the groups and train the model weights, where the group identification often relies on heuristics. As a result, our method not only improves the training efficiency, but also mitigates the objective bias introduced by the sequential procedures that potentially lead to a suboptimal solution. Specifically, we formulate MTG as a fully differentiable pruning problem on an adaptive network architecture determined by an underlying Categorical distribution. To categorize N tasks into K groups (represented by K encoder branches), we initially set up KN task heads, where each branch connects to all N task heads to exploit the high-order task-affinity. Then, we gradually prune the KN heads down to N by learning a relaxed differentiable Categorical distribution, ensuring that each task is exclusively and uniquely categorized into only one branch. Extensive experiments on CelebA and Taskonomy datasets with detailed ablations show the promising performance and efficiency of our method. The codes are available at https://github.com/ethanygao/DMTG.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "stat.ML"
        ],
        "comment": "Accepted to ICML 2024"
    },
    {
        "paper id": "2407.04998",
        "abstract url": "https://arxiv.org/abs/2407.04998",
        "title": "The Solution for the 5th GCAIAC Zero-shot Referring Expression Comprehension Challenge",
        "rating": "2",
        "keywords": [
            [
                "Visual-language"
            ],
            [
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "This report presents a solution for the zero-shot referring expression comprehension task. Visual-language multimodal base models (such as CLIP, SAM) have gained significant attention in recent years as a cornerstone of mainstream research. One of the key applications of multimodal base models lies in their ability to generalize to zero-shot downstream tasks. Unlike traditional referring expression comprehension, zero-shot referring expression comprehension aims to apply pre-trained visual-language models directly to the task without specific training. Recent studies have enhanced the zero-shot performance of multimodal base models in referring expression comprehension tasks by introducing visual prompts. To address the zero-shot referring expression comprehension challenge, we introduced a combination of visual prompts and considered the influence of textual prompts, employing joint prediction tailored to the data characteristics. Ultimately, our approach achieved accuracy rates of 84.825 on the A leaderboard and 71.460 on the B leaderboard, securing the first position.",
        "subjects": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05000",
        "abstract url": "https://arxiv.org/abs/2407.05000",
        "title": "LoRA-GA: Low-Rank Adaptation with Gradient Approximation",
        "rating": "2",
        "keywords": [
            [
                "Parameter-Efficient",
                "PEFT",
                "Efficient Fine-Tuning"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Fine-tuning large-scale pretrained models is prohibitively expensive in terms of computational and memory costs. LoRA, as one of the most popular Parameter-Efficient Fine-Tuning (PEFT) methods, offers a cost-effective alternative by fine-tuning an auxiliary low-rank model that has significantly fewer parameters. Although LoRA reduces the computational and memory requirements significantly at each iteration, extensive empirical evidence indicates that it converges at a considerably slower rate compared to full fine-tuning, ultimately leading to increased overall compute and often worse test performance. In our paper, we perform an in-depth investigation of the initialization method of LoRA and show that careful initialization (without any change of the architecture and the training algorithm) can significantly enhance both efficiency and performance. In particular, we introduce a novel initialization method, LoRA-GA (Low Rank Adaptation with Gradient Approximation), which aligns the gradients of low-rank matrix product with those of full fine-tuning at the first step. Our extensive experiments demonstrate that LoRA-GA achieves a convergence rate comparable to that of full fine-tuning (hence being significantly faster than vanilla LoRA as well as various recent improvements) while simultaneously attaining comparable or even better performance. For example, on the subset of the GLUE dataset with T5-Base, LoRA-GA outperforms LoRA by 5.69% on average. On larger models such as Llama 2-7B, LoRA-GA shows performance improvements of 0.34, 11.52%, and 5.05% on MT-bench, GSM8K, and Human-eval, respectively. Additionally, we observe up to 2-4 times convergence speed improvement compared to vanilla LoRA, validating its effectiveness in accelerating convergence and enhancing model performance. Code is available at https://github.com/Outsider565/LoRA-GA.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05061",
        "abstract url": "https://arxiv.org/abs/2407.05061",
        "title": "A Study of Test-time Contrastive Concepts for Open-world, Open-vocabulary Semantic Segmentation",
        "rating": "2",
        "keywords": [
            [
                "VLMs"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent VLMs, pre-trained on large amounts of image-text pairs to align both modalities, have opened the way to open-vocabulary semantic segmentation. Given an arbitrary set of textual queries, image regions are assigned the closest query in feature space. However, the usual setup expects the user to list all possible visual concepts that may occur in the image, typically all classes of benchmark datasets, that act as negatives to each other. We consider here the more challenging scenario of segmenting a single concept, given a textual prompt and nothing else. To achieve good results, besides contrasting with the generic 'background' text, we study different ways to generate query-specific test-time contrastive textual concepts, which leverage either the distribution of text in the VLM's training set or crafted LLM prompts. We show the relevance of our approach using a new, specific metric.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05183",
        "abstract url": "https://arxiv.org/abs/2407.05183",
        "title": "FlowLearn: Evaluating Large Vision-Language Models on Flowchart Understanding",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Flowcharts are graphical tools for representing complex concepts in concise visual representations. This paper introduces the FlowLearn dataset, a resource tailored to enhance the understanding of flowcharts. FlowLearn contains complex scientific flowcharts and simulated flowcharts. The scientific subset contains 3,858 flowcharts sourced from scientific literature and the simulated subset contains 10,000 flowcharts created using a customizable script. The dataset is enriched with annotations for visual components, OCR, Mermaid code representation, and VQA question-answer pairs. Despite the proven capabilities of Large Vision-Language Models (LVLMs) in various visual understanding tasks, their effectiveness in decoding flowcharts - a crucial element of scientific communication - has yet to be thoroughly investigated. The FlowLearn test set is crafted to assess the performance of LVLMs in flowchart comprehension. Our study thoroughly evaluates state-of-the-art LVLMs, identifying existing limitations and establishing a foundation for future enhancements in this relatively underexplored domain. For instance, in tasks involving simulated flowcharts, GPT-4V achieved the highest accuracy (58%) in counting the number of nodes, while Claude recorded the highest accuracy (83%) in OCR tasks. Notably, no single model excels in all tasks within the FlowLearn framework, highlighting significant opportunities for further development.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "ECAI 2024"
    },
    {
        "paper id": "2407.05229",
        "abstract url": "https://arxiv.org/abs/2407.05229",
        "title": "HiDe-PET: Continual Learning via Hierarchical Decomposition of Parameter-Efficient Tuning",
        "rating": "2",
        "keywords": [
            [
                "Parameter-Efficient"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "The deployment of pre-trained models (PTMs) has greatly advanced the field of continual learning (CL), enabling positive knowledge transfer and resilience to catastrophic forgetting. To sustain these advantages for sequentially arriving tasks, a promising direction involves keeping the pre-trained backbone frozen while employing parameter-efficient tuning (PET) techniques to instruct representation learning. Despite the popularity of Prompt-based PET for CL, its empirical design often leads to sub-optimal performance in our evaluation of different PTMs and target tasks. To this end, we propose a unified framework for CL with PTMs and PET that provides both theoretical and empirical advancements. We first perform an in-depth theoretical analysis of the CL objective in a pre-training context, decomposing it into hierarchical components namely within-task prediction, task-identity inference and task-adaptive prediction. We then present Hierarchical Decomposition PET (HiDe-PET), an innovative approach that explicitly optimizes the decomposed objective through incorporating task-specific and task-shared knowledge via mainstream PET techniques along with efficient recovery of pre-trained representations. Leveraging this framework, we delve into the distinct impacts of implementation strategy, PET technique and PET architecture, as well as adaptive knowledge accumulation amidst pronounced distribution changes. Finally, across various CL scenarios, our approach demonstrates remarkably superior performance over a broad spectrum of recent strong baselines.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "This is a generalized version of our HiDe-Prompt (NeurIPS 2023, Spotlight)"
    },
    {
        "paper id": "2407.05118",
        "abstract url": "https://arxiv.org/abs/2407.05118",
        "title": "SHINE: Saliency-aware HIerarchical NEgative Ranking for Compositional Temporal Grounding",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Temporal grounding, a.k.a video moment retrieval, aims at locating video segments corresponding to a given query sentence. The compositional nature of natural language enables the localization beyond predefined events, posing a certain challenge to the compositional generalizability of existing methods. Recent studies establish the correspondence between videos and queries through a decompose-reconstruct manner to achieve compositional generalization. However, they only consider dominant primitives and build negative queries through random sampling and recombination, resulting in semantically implausible negatives that hinder the models from learning rational compositions. In addition, recent DETR-based methods still underperform in compositional temporal grounding, showing irrational saliency responses when given negative queries that have subtle differences from positive queries. To address these limitations, we first propose a large language model-driven method for negative query construction, utilizing GPT-3.5-Turbo to generate semantically plausible hard negative queries. Subsequently, we introduce a coarse-to-fine saliency ranking strategy, which encourages the model to learn the multi-granularity semantic relationships between videos and hierarchical negative queries to boost compositional generalization. Extensive experiments on two challenging benchmarks validate the effectiveness and generalizability of our proposed method. Our code is available at https://github.com/zxccade/SHINE.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to ECCV 2024"
    },
    {
        "paper id": "2407.04965",
        "abstract url": "https://arxiv.org/abs/2407.04965",
        "title": "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are increasingly deployed in real-world scenarios with the help of recent model compression techniques. Such momentum towards local deployment means the use of compressed LLMs will widely impact a large population. However, prior analysis works often prioritize on preserving perplexity which is a direct analogy to training loss. The impact of compression method on other critical aspects of model behavior, particularly safety, still calls for a systematic assessment. To this end, we investigate the impact of model compression on four dimensions: 1) degeneration harm, i.e., bias and toxicity in generation; 2) representational harm, i.e., biases in discriminative tasks; 3) dialect bias; 4) language modeling and downstream task performance. We cover a wide spectrum of LLM compression techniques, including structured pruning, un/semi-structured ones, and quantization. Our analyses reveal that compression can lead to unexpected consequences. Although compression may unintentionally remedy LLMs' degeneration harm, it can still exacerbate on the representational harm axis. Moreover, there is a divergent impact on different protected groups as the compression rate grows. Finally, different compression methods have drastically different safety impacts, e.g., quantization mostly preserves bias while pruning degrades quickly. Our findings underscore the importance of integrating safety assessments into the development of compressed LLMs to ensure their reliability across real-world applications. Our full results are available here: \\url{https://github.com/zhichaoxu-shufe/Beyond-Perplexity-Compression-Safety-Eval}",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2407.04966",
        "abstract url": "https://arxiv.org/abs/2407.04966",
        "title": "A Layer-Anchoring Strategy for Enhancing Cross-Lingual Speech Emotion Recognition",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Cross-lingual speech emotion recognition (SER) is important for a wide range of everyday applications. While recent SER research relies heavily on large pretrained models for emotion training, existing studies often concentrate solely on the final transformer layer of these models. However, given the task-specific nature and hierarchical architecture of these models, each transformer layer encapsulates different levels of information. Leveraging this hierarchical structure, our study focuses on the information embedded across different layers. Through an examination of layer feature similarity across different languages, we propose a novel strategy called a layer-anchoring mechanism to facilitate emotion transfer in cross-lingual SER tasks. Our approach is evaluated using two distinct language affective corpora (MSP-Podcast and BIIC-Podcast), achieving a best UAR performance of 60.21% on the BIIC-podcast corpus. The analysis uncovers interesting insights into the behavior of popular pretrained models.",
        "subjects": [
            "cs.SD",
            "cs.LG",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2407.04981",
        "abstract url": "https://arxiv.org/abs/2407.04981",
        "title": "TRACE: TRansformer-based Attribution using Contrastive Embeddings in LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The rapid evolution of large language models (LLMs) represents a substantial leap forward in natural language understanding and generation. However, alongside these advancements come significant challenges related to the accountability and transparency of LLM responses. Reliable source attribution is essential to adhering to stringent legal and regulatory standards, including those set forth by the General Data Protection Regulation. Despite the well-established methods in source attribution within the computer vision domain, the application of robust attribution frameworks to natural language processing remains underexplored. To bridge this gap, we propose a novel and versatile TRansformer-based Attribution framework using Contrastive Embeddings called TRACE that, in particular, exploits contrastive learning for source attribution. We perform an extensive empirical evaluation to demonstrate the performance and efficiency of TRACE in various settings and show that TRACE significantly improves the ability to attribute sources accurately, making it a valuable tool for enhancing the reliability and trustworthiness of LLMs.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2407.04986",
        "abstract url": "https://arxiv.org/abs/2407.04986",
        "title": "Calorie Burn Estimation in Community Parks Through DLICP: A Mathematical Modelling Approach",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Community parks play a crucial role in promoting physical activity and overall well-being. This study introduces DLICP (Deep Learning Integrated Community Parks), an innovative approach that combines deep learning techniques specifically, face recognition technology with a novel walking activity measurement algorithm to enhance user experience in community parks. The DLICP utilizes a camera with face recognition software to accurately identify and track park users. Simultaneously, a walking activity measurement algorithm calculates parameters such as the average pace and calories burned, tailored to individual attributes. Extensive evaluations confirm the precision of DLICP, with a Mean Absolute Error (MAE) of 5.64 calories and a Mean Percentage Error (MPE) of 1.96%, benchmarked against widely available fitness measurement devices, such as the Apple Watch Series 6. This study contributes significantly to the development of intelligent smart park systems, enabling real-time updates on burned calories and personalized fitness tracking.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted and to be presented at Intellisys 2024 , Also Part of the Indian Patent: 202441050325"
    },
    {
        "paper id": "2407.04990",
        "abstract url": "https://arxiv.org/abs/2407.04990",
        "title": "Conditional Semi-Supervised Data Augmentation for Spam Message Detection with Low Resource Data",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Several machine learning schemes have attempted to perform the detection of spam messages. However, those schemes mostly require a huge amount of labeled data. The existing techniques addressing the lack of data availability have issues with effectiveness and robustness. Therefore, this paper proposes a conditional semi-supervised data augmentation (CSSDA) for a spam detection model lacking the availability of data. The main architecture of CSSDA comprises feature extraction and enhanced generative network. Here, we exploit unlabeled data for data augmentation to extend training data. The enhanced generative in our proposed scheme produces latent variables as fake samples from unlabeled data through a conditional scheme. Latent variables can come from labeled and unlabeled data as the input for the final classifier in our spam detection model. The experimental results indicate that our proposed CSSDA achieves excellent results compared to several related methods both exploiting unlabeled data and not. In the experiment stage with various amounts of unlabeled data, CSSDA is the only robust model that obtains a balanced accuracy of about 85% when the availability of labeled data is large. We also conduct several ablation studies to investigate our proposed scheme in detail. The result also shows that several ablation studies strengthen our proposed innovations. These experiments indicate that unlabeled data has a significant contribution to data augmentation using the conditional semi-supervised scheme for spam detection.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2407.04991",
        "abstract url": "https://arxiv.org/abs/2407.04991",
        "title": "The Solution for the AIGC Inference Performance Optimization Competition",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "In recent years, the rapid advancement of large-scale pre-trained language models based on transformer architectures has revolutionized natural language processing tasks. Among these, ChatGPT has gained widespread popularity, demonstrating human-level conversational abilities and attracting over 100 million monthly users by late 2022. Concurrently, Baidu's commercial deployment of the Ernie Wenxin model has significantly enhanced marketing effectiveness through AI-driven technologies. This paper focuses on optimizing high-performance inference for Ernie models, emphasizing GPU acceleration and leveraging the Paddle inference framework. We employ techniques such as Faster Transformer for efficient model processing, embedding layer pruning to reduce computational overhead, and FP16 half-precision inference for enhanced computational efficiency. Additionally, our approach integrates efficient data handling strategies using multi-process parallel processing to minimize latency. Experimental results demonstrate that our optimized solution achieves up to an 8.96x improvement in inference speed compared to standard methods, while maintaining competitive performance.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2407.04994",
        "abstract url": "https://arxiv.org/abs/2407.04994",
        "title": "The Solution for Language-Enhanced Image New Category Discovery",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Treating texts as images, combining prompts with textual labels for prompt tuning, and leveraging the alignment properties of CLIP have been successfully applied in zero-shot multi-label image recognition. Nonetheless, relying solely on textual labels to store visual information is insufficient for representing the diversity of visual objects. In this paper, we propose reversing the training process of CLIP and introducing the concept of Pseudo Visual Prompts. These prompts are initialized for each object category and pre-trained on large-scale, low-cost sentence data generated by large language models. This process mines the aligned visual information in CLIP and stores it in class-specific visual prompts. We then employ contrastive learning to transfer the stored visual information to the textual labels, enhancing their visual representation capacity. Additionally, we introduce a dual-adapter module that simultaneously leverages knowledge from the original CLIP and new learning knowledge derived from downstream datasets. Benefiting from the pseudo visual prompts, our method surpasses the state-of-the-art not only on clean annotated text data but also on pseudo text data generated by large language models.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2407.04996",
        "abstract url": "https://arxiv.org/abs/2407.04996",
        "title": "The Solution for the sequential task continual learning track of the 2nd Greater Bay Area International Algorithm Competition",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "This paper presents a data-free, parameter-isolation-based continual learning algorithm we developed for the sequential task continual learning track of the 2nd Greater Bay Area International Algorithm Competition. The method learns an independent parameter subspace for each task within the network's convolutional and linear layers and freezes the batch normalization layers after the first task. Specifically, for domain incremental setting where all domains share a classification head, we freeze the shared classification head after first task is completed, effectively solving the issue of catastrophic forgetting. Additionally, facing the challenge of domain incremental settings without providing a task identity, we designed an inference task identity strategy, selecting an appropriate mask matrix for each sample. Furthermore, we introduced a gradient supplementation strategy to enhance the importance of unselected parameters for the current task, facilitating learning for new tasks. We also implemented an adaptive importance scoring strategy that dynamically adjusts the amount of parameters to optimize single-task performance while reducing parameter usage. Moreover, considering the limitations of storage space and inference time, we designed a mask matrix compression strategy to save storage space and improve the speed of encryption and decryption of the mask matrix. Our approach does not require expanding the core network or using external auxiliary networks or data, and performs well under both task incremental and domain incremental settings. This solution ultimately won a second-place prize in the competition.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05006",
        "abstract url": "https://arxiv.org/abs/2407.05006",
        "title": "Recent Advancements and Challenges of Turkic Central Asian Language Processing",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Research in the NLP sphere of the Turkic counterparts of Central Asian languages, namely Kazakh, Uzbek, Kyrgyz, and Turkmen, comes with the typical challenges of low-resource languages, like data scarcity and a general lack of linguistic resources. However, in the recent years research has greatly advanced via collection of language-specific datasets and development of downstream task technologies. Aiming to summarize this research up until May 2024, this paper also seeks to identify potential areas of future work. To achieve this, the paper gives a broad, high-level overview of the linguistic properties of the languages, the current coverage and performance of already developed technology, application of transfer learning techniques from higher-resource languages, and availability of labeled and unlabeled data for each language. Providing a summary of the current state of affairs, we hope that further research will be facilitated with the considerations we provide in the current paper.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05010",
        "abstract url": "https://arxiv.org/abs/2407.05010",
        "title": "PRANCE: Joint Token-Optimization and Structural Channel-Pruning for Adaptive ViT Inference",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We introduce PRANCE, a Vision Transformer compression framework that jointly optimizes the activated channels and reduces tokens, based on the characteristics of inputs. Specifically, PRANCE~ leverages adaptive token optimization strategies for a certain computational budget, aiming to accelerate ViTs' inference from a unified data and architectural perspective. However, the joint framework poses challenges to both architectural and decision-making aspects. Firstly, while ViTs inherently support variable-token inference, they do not facilitate dynamic computations for variable channels. To overcome this limitation, we propose a meta-network using weight-sharing techniques to support arbitrary channels of the Multi-head Self-Attention and Multi-layer Perceptron layers, serving as a foundational model for architectural decision-making. Second, simultaneously optimizing the structure of the meta-network and input data constitutes a combinatorial optimization problem with an extremely large decision space, reaching up to around $10^{14}$, making supervised learning infeasible. To this end, we design a lightweight selector employing Proximal Policy Optimization for efficient decision-making. Furthermore, we introduce a novel \"Result-to-Go\" training mechanism that models ViTs' inference process as a Markov decision process, significantly reducing action space and mitigating delayed-reward issues during training. Extensive experiments demonstrate the effectiveness of PRANCE~ in reducing FLOPs by approximately 50\\%, retaining only about 10\\% of tokens while achieving lossless Top-1 accuracy. Additionally, our framework is shown to be compatible with various token optimization techniques such as pruning, merging, and sequential pruning-merging strategies. The code is available at \\href{https://github.com/ChildTang/PRANCE}{https://github.com/ChildTang/PRANCE}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05013",
        "abstract url": "https://arxiv.org/abs/2407.05013",
        "title": "Progress or Regress? Self-Improvement Reversal in Post-training",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Self-improvement through post-training methods such as iterative preference learning has been acclaimed for enhancing the problem-solving capabilities (e.g., mathematical reasoning) of Large Language Models (LLMs) without human intervention. However, as exploration deepens, it becomes crucial to assess whether these improvements genuinely signify progress in solving more challenging problems or if they could lead to unintended regressions. To address this, we propose a comprehensive evaluative framework that goes beyond the superficial pass@1 metric to scrutinize the underlying enhancements of post-training paradigms for self-improvement. Through rigorous experimentation and analysis across diverse problem-solving tasks, the empirical results point out the phenomenon of \\emph{self-improvement reversal}, where models showing improved performance across benchmarks will paradoxically exhibit declines in broader, essential capabilities, like output diversity and out-of-distribution (OOD) generalization. These findings indicate that current self-improvement practices through post-training are inadequate for equipping models to tackle more complex problems. Furthermore, they underscore the necessity of our critical evaluation metrics in discerning the \\emph{progress or regress} dichotomy for self-improving LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05022",
        "abstract url": "https://arxiv.org/abs/2407.05022",
        "title": "A Principled Framework for Evaluating on Typologically Diverse Languages",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Beyond individual languages, multilingual natural language processing (NLP) research increasingly aims to develop models that perform well across languages generally. However, evaluating these systems on all the world's languages is practically infeasible. To attain generalizability, representative language sampling is essential. Previous work argues that generalizable multilingual evaluation sets should contain languages with diverse typological properties. However, 'typologically diverse' language samples have been found to vary considerably in this regard, and popular sampling methods are flawed and inconsistent. We present a language sampling framework for selecting highly typologically diverse languages given a sampling frame, informed by language typology. We compare sampling methods with a range of metrics and find that our systematic methods consistently retrieve more typologically diverse language selections than previous methods in NLP. Moreover, we provide evidence that this affects generalizability in multilingual model evaluation, emphasizing the importance of diverse language sampling in NLP evaluation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05036",
        "abstract url": "https://arxiv.org/abs/2407.05036",
        "title": "Enhance the Robustness of Text-Centric Multimodal Alignments",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Converting different modalities into general text, serving as input prompts for large language models (LLMs), is a common method to align multimodal models when there is limited pairwise data. This text-centric approach leverages the unique properties of text as a modality space, transforming diverse inputs into a unified textual representation. This enables downstream models to effectively interpret various modal inputs. This study assesses the quality and robustness of multimodal representations in the presence of missing entries, noise, or absent modalities, revealing that current text-centric alignment methods compromise downstream robustness. To address this issue, we propose a new text-centric approach that achieves superior robustness compared to previous methods across various modalities in different settings. Our findings highlight the potential of this approach to enhance the robustness and adaptability of multimodal representations, offering a promising solution for dynamic and real-world applications.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05054",
        "abstract url": "https://arxiv.org/abs/2407.05054",
        "title": "Cross-Lingual Word Alignment for ASEAN Languages with Contrastive Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Cross-lingual word alignment plays a crucial role in various natural language processing tasks, particularly for low-resource languages. Recent study proposes a BiLSTM-based encoder-decoder model that outperforms pre-trained language models in low-resource settings. However, their model only considers the similarity of word embedding spaces and does not explicitly model the differences between word embeddings. To address this limitation, we propose incorporating contrastive learning into the BiLSTM-based encoder-decoder framework. Our approach introduces a multi-view negative sampling strategy to learn the differences between word pairs in the shared cross-lingual embedding space. We evaluate our model on five bilingual aligned datasets spanning four ASEAN languages: Lao, Vietnamese, Thai, and Indonesian. Experimental results demonstrate that integrating contrastive learning consistently improves word alignment accuracy across all datasets, confirming the effectiveness of the proposed method in low-resource scenarios. We will release our data set and code to support future research on ASEAN or more low-resource word alignment.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05092",
        "abstract url": "https://arxiv.org/abs/2407.05092",
        "title": "Exploring Sound Change Over Time: A Review of Computational and Human Perception",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Computational and human perception are often considered separate approaches for studying sound changes over time; few works have touched on the intersection of both. To fill this research gap, we provide a pioneering review contrasting computational with human perception from the perspectives of methods and tasks. Overall, computational approaches rely on computer-driven models to perceive historical sound changes on etymological datasets, while human approaches use listener-driven models to perceive ongoing sound changes on recording corpora. Despite their differences, both approaches complement each other on phonetic and acoustic levels, showing the potential to achieve a more comprehensive perception of sound change. Moreover, we call for a comparative study on the datasets used by both approaches to investigate the influence of historical sound changes on ongoing changes. Lastly, we discuss the applications of sound change in computational linguistics, and point out that perceiving sound change alone is insufficient, as many processes of language change are complex, with entangled changes at syntactic, semantic, and phonetic levels.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "LChange24 Camera Ready"
    },
    {
        "paper id": "2407.05119",
        "abstract url": "https://arxiv.org/abs/2407.05119",
        "title": "Open-Event Procedure Planning in Instructional Videos",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Given the current visual observations, the traditional procedure planning task in instructional videos requires a model to generate goal-directed plans within a given action space. All previous methods for this task conduct training and inference under the same action space, and they can only plan for pre-defined events in the training set. We argue this setting is not applicable for human assistance in real lives and aim to propose a more general and practical planning paradigm. Specifically, in this paper, we introduce a new task named Open-event Procedure Planning (OEPP), which extends the traditional procedure planning to the open-event setting. OEPP aims to verify whether a planner can transfer the learned knowledge to similar events that have not been seen during training. We rebuild a new benchmark of OpenEvent for this task based on existing datasets and divide the events involved into base and novel parts. During the data collection process, we carefully ensure the transfer ability of procedural knowledge for base and novel events by evaluating the similarity between the descriptions of different event steps with multiple stages. Based on the collected data, we further propose a simple and general framework specifically designed for OEPP, and conduct extensive study with various baseline methods, providing a detailed and insightful analysis on the results for this task.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "9 pages(main text), 6 figures, 10 tables"
    },
    {
        "paper id": "2407.05128",
        "abstract url": "https://arxiv.org/abs/2407.05128",
        "title": "SCSA: Exploring the Synergistic Effects Between Spatial and Channel Attention",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Channel and spatial attentions have respectively brought significant improvements in extracting feature dependencies and spatial structure relations for various downstream vision tasks. While their combination is more beneficial for leveraging their individual strengths, the synergy between channel and spatial attentions has not been fully explored, lacking in fully harness the synergistic potential of multi-semantic information for feature guidance and mitigation of semantic disparities. Our study attempts to reveal the synergistic relationship between spatial and channel attention at multiple semantic levels, proposing a novel Spatial and Channel Synergistic Attention module (SCSA). Our SCSA consists of two parts: the Shareable Multi-Semantic Spatial Attention (SMSA) and the Progressive Channel-wise Self-Attention (PCSA). SMSA integrates multi-semantic information and utilizes a progressive compression strategy to inject discriminative spatial priors into PCSA's channel self-attention, effectively guiding channel recalibration. Additionally, the robust feature interactions based on the self-attention mechanism in PCSA further mitigate the disparities in multi-semantic information among different sub-features within SMSA. We conduct extensive experiments on seven benchmark datasets, including classification on ImageNet-1K, object detection on MSCOCO 2017, segmentation on ADE20K, and four other complex scene detection datasets. Our results demonstrate that our proposed SCSA not only surpasses the current state-of-the-art attention but also exhibits enhanced generalization capabilities across various task scenarios. The code and models are available at: https://github.com/HZAI-ZJNU/SCSA.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05134",
        "abstract url": "https://arxiv.org/abs/2407.05134",
        "title": "Solving for X and Beyond: Can Large Language Models Solve Complex Math Problems with More-Than-Two Unknowns?",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in solving math problems, a hallmark of human intelligence. Despite high success rates on current benchmarks; however, these often feature simple problems with only one or two unknowns, which do not sufficiently challenge their reasoning capacities. This paper introduces a novel benchmark, BeyondX, designed to address these limitations by incorporating problems with multiple unknowns. Recognizing the challenges in proposing multi-unknown problems from scratch, we developed BeyondX using an innovative automated pipeline that progressively increases complexity by expanding the number of unknowns in simpler problems. Empirical study on BeyondX reveals that the performance of existing LLMs, even those fine-tuned specifically on math tasks, significantly decreases as the number of unknowns increases - with a performance drop of up to 70\\% observed in GPT-4. To tackle these challenges, we propose the Formulate-and-Solve strategy, a generalized prompting approach that effectively handles problems with an arbitrary number of unknowns. Our findings reveal that this strategy not only enhances LLM performance on the BeyondX benchmark but also provides deeper insights into the computational limits of LLMs when faced with more complex mathematical challenges.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05154",
        "abstract url": "https://arxiv.org/abs/2407.05154",
        "title": "Identifying Intensity of the Structure and Content in Tweets and the Discriminative Power of Attributes in Context with Referential Translation Machines",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "We use referential translation machines (RTMs) to identify the similarity between an attribute and two words in English by casting the task as machine translation performance prediction (MTPP) between the words and the attribute word and the distance between their similarities for Task 10 with stacked RTM models. RTMs are also used to predict the intensity of the structure and content in tweets in English, Arabic, and Spanish in Task 1 where MTPP is between the tweets and the set of words for the emotion selected from WordNet affect emotion lists. Stacked RTM models obtain encouraging results in both.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "11 pages, 3 figures, 12 tables"
    },
    {
        "paper id": "2407.05189",
        "abstract url": "https://arxiv.org/abs/2407.05189",
        "title": "Enhancing Language Learning through Technology: Introducing a New English-Azerbaijani (Arabic Script) Parallel Corpus",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces a pioneering English-Azerbaijani (Arabic Script) parallel corpus, designed to bridge the technological gap in language learning and machine translation (MT) for under-resourced languages. Consisting of 548,000 parallel sentences and approximately 9 million words per language, this dataset is derived from diverse sources such as news articles and holy texts, aiming to enhance natural language processing (NLP) applications and language education technology. This corpus marks a significant step forward in the realm of linguistic resources, particularly for Turkic languages, which have lagged in the neural machine translation (NMT) revolution. By presenting the first comprehensive case study for the English-Azerbaijani (Arabic Script) language pair, this work underscores the transformative potential of NMT in low-resource contexts. The development and utilization of this corpus not only facilitate the advancement of machine translation systems tailored for specific linguistic needs but also promote inclusive language learning through technology. The findings demonstrate the corpus's effectiveness in training deep learning MT systems and underscore its role as an essential asset for researchers and educators aiming to foster bilingual education and multilingual communication. This research covers the way for future explorations into NMT applications for languages lacking substantial digital resources, thereby enhancing global language education frameworks. The Python package of our code is available at https://pypi.org/project/chevir-kartalol/, and we also have a website accessible at https://translate.kartalol.com/.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "This paper is accepted and published at NeTTT 2024 Conf"
    },
    {
        "paper id": "2407.05193",
        "abstract url": "https://arxiv.org/abs/2407.05193",
        "title": "CBM: Curriculum by Masking",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We propose Curriculum by Masking (CBM), a novel state-of-the-art curriculum learning strategy that effectively creates an easy-to-hard training schedule via patch (token) masking, offering significant accuracy improvements over the conventional training regime and previous curriculum learning (CL) methods. CBM leverages gradient magnitudes to prioritize the masking of salient image regions via a novel masking algorithm and a novel masking block. Our approach enables controlling sample difficulty via the patch masking ratio, generating an effective easy-to-hard curriculum by gradually introducing harder samples as training progresses. CBM operates with two easily configurable parameters, i.e. the number of patches and the curriculum schedule, making it a versatile curriculum learning approach for object recognition and detection. We conduct experiments with various neural architectures, ranging from convolutional networks to vision transformers, on five benchmark data sets (CIFAR-10, CIFAR-100, ImageNet, Food-101 and PASCAL VOC), to compare CBM with conventional as well as curriculum-based training regimes. Our results reveal the superiority of our strategy compared with the state-of-the-art curriculum learning regimes. We also observe improvements in transfer learning contexts, where CBM surpasses previous work by considerable margins in terms of accuracy. We release our code for free non-commercial use at https://github.com/CroitoruAlin/CBM.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted at ECAI 2024"
    },
    {
        "paper id": "2407.05216",
        "abstract url": "https://arxiv.org/abs/2407.05216",
        "title": "Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Using large language models (LLMs) for automatic evaluation has become an important evaluation method in NLP research. However, it is unclear whether these LLM-based evaluators can be applied in real-world classrooms to assess student assignments. This empirical report shares how we use GPT-4 as an automatic assignment evaluator in a university course with 1,028 students. Based on student responses, we find that LLM-based assignment evaluators are generally acceptable to students when students have free access to these LLM-based evaluators. However, students also noted that the LLM sometimes fails to adhere to the evaluation instructions. Additionally, we observe that students can easily manipulate the LLM-based evaluator to output specific strings, allowing them to achieve high scores without meeting the assignment rubric. Based on student feedback and our experience, we provide several recommendations for integrating LLM-based evaluators into future classrooms.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "An empirical report of our course: Introduction to Generative AI 2024 Spring (https://speech.ee.ntu.edu.tw/~hylee/genai/2024-spring.php)"
    },
    {
        "paper id": "2407.05233",
        "abstract url": "https://arxiv.org/abs/2407.05233",
        "title": "Advancing Prompt Recovery in NLP: A Deep Dive into the Integration of Gemma-2b-it and Phi2 Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Prompt recovery, a crucial task in natural language processing, entails the reconstruction of prompts or instructions that language models use to convert input text into a specific output. Although pivotal, the design and effectiveness of prompts represent a challenging and relatively untapped field within NLP research. This paper delves into an exhaustive investigation of prompt recovery methodologies, employing a spectrum of pre-trained language models and strategies. Our study is a comparative analysis aimed at gauging the efficacy of various models on a benchmark dataset, with the goal of pinpointing the most proficient approach for prompt recovery. Through meticulous experimentation and detailed analysis, we elucidate the outstanding performance of the Gemma-2b-it + Phi2 model + Pretrain. This model surpasses its counterparts, showcasing its exceptional capability in accurately reconstructing prompts for text transformation tasks. Our findings offer a significant contribution to the existing knowledge on prompt recovery, shedding light on the intricacies of prompt design and offering insightful perspectives for future innovations in text rewriting and the broader field of natural language processing.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05244",
        "abstract url": "https://arxiv.org/abs/2407.05244",
        "title": "Some Issues in Predictive Ethics Modeling: An Annotated Contrast Set of \"Moral Stories\"",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Models like Delphi have been able to label ethical dilemmas as moral or immoral with astonishing accuracy. This paper challenges accuracy as a holistic metric for ethics modeling by identifying issues with translating moral dilemmas into text-based input. It demonstrates these issues with contrast sets that substantially reduce the performance of classifiers trained on the dataset Moral Stories. Ultimately, we obtain concrete estimates for how much specific forms of data misrepresentation harm classifier accuracy. Specifically, label-changing tweaks to the descriptive content of a situation (as small as 3-5 words) can reduce classifier accuracy to as low as 51%, almost half the initial accuracy of 99.8%. Associating situations with a misleading social norm lowers accuracy to 98.8%, while adding textual bias (i.e. an implication that a situation already fits a certain label) lowers accuracy to 77%. These results suggest not only that many ethics models have substantially overfit, but that several precautions are required to ensure that input accurately captures a moral dilemma. This paper recommends re-examining the structure of a social norm, training models to ask for context with defeasible reasoning, and filtering input for textual bias. Doing so not only gives us the first concrete estimates of the average cost to accuracy of misrepresenting ethics data, but gives researchers practical tips for considering these estimates in research.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": "This project was a runner-up to the Novel Research prize for the BlueDot Impact course AI Safety Fundamentals. View my contrast set as JSONL, the UI used to generate it, and Emelin et. al.\"s initial paper and code at https://github.com/bfitzgerald3132/MoralStoriesContrastSet"
    },
    {
        "paper id": "2407.05246",
        "abstract url": "https://arxiv.org/abs/2407.05246",
        "title": "Deep Probability Aggregation Clustering",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Combining machine clustering with deep models has shown remarkable superiority in deep clustering. It modifies the data processing pipeline into two alternating phases: feature clustering and model training. However, such alternating schedule may lead to instability and computational burden issues. We propose a centerless clustering algorithm called Probability Aggregation Clustering (PAC) to proactively adapt deep learning technologies, enabling easy deployment in online deep clustering. PAC circumvents the cluster center and aligns the probability space and distribution space by formulating clustering as an optimization problem with a novel objective function. Based on the computation mechanism of the PAC, we propose a general online probability aggregation module to perform stable and flexible feature clustering over mini-batch data and further construct a deep visual clustering framework deep PAC (DPAC). Extensive experiments demonstrate that PAC has superior clustering robustness and performance and DPAC remarkably outperforms the state-of-the-art deep clustering methods.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": "19 pages,2 figures, conference"
    },
    {
        "paper id": "2407.05256",
        "abstract url": "https://arxiv.org/abs/2407.05256",
        "title": "Unlocking Textual and Visual Wisdom: Open-Vocabulary 3D Object Detection Enhanced by Comprehensive Guidance from Text and Image",
        "rating": "1",
        "keywords": [
            [
                "vision-language",
                "VLM"
            ],
            [
                "3D"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Open-vocabulary 3D object detection (OV-3DDet) aims to localize and recognize both seen and previously unseen object categories within any new 3D scene. While language and vision foundation models have achieved success in handling various open-vocabulary tasks with abundant training data, OV-3DDet faces a significant challenge due to the limited availability of training data. Although some pioneering efforts have integrated vision-language models (VLM) knowledge into OV-3DDet learning, the full potential of these foundational models has yet to be fully exploited. In this paper, we unlock the textual and visual wisdom to tackle the open-vocabulary 3D detection task by leveraging the language and vision foundation models. We leverage a vision foundation model to provide image-wise guidance for discovering novel classes in 3D scenes. Specifically, we utilize a object detection vision foundation model to enable the zero-shot discovery of objects in images, which serves as the initial seeds and filtering guidance to identify novel 3D objects. Additionally, to align the 3D space with the powerful vision-language space, we introduce a hierarchical alignment approach, where the 3D feature space is aligned with the vision-language feature space using a pre-trained VLM at the instance, category, and scene levels. Through extensive experimentation, we demonstrate significant improvements in accuracy and generalization, highlighting the potential of foundation models in advancing open-vocabulary 3D object detection in real-world scenarios.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.04964",
        "abstract url": "https://arxiv.org/abs/2407.04964",
        "title": "ZOBNN: Zero-Overhead Dependable Design of Binary Neural Networks with Deliberately Quantized Parameters",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Low-precision weights and activations in deep neural networks (DNNs) outperform their full-precision counterparts in terms of hardware efficiency. When implemented with low-precision operations, specifically in the extreme case where network parameters are binarized (i.e. BNNs), the two most frequently mentioned benefits of quantization are reduced memory consumption and a faster inference process. In this paper, we introduce a third advantage of very low-precision neural networks: improved fault-tolerance attribute. We investigate the impact of memory faults on state-of-the-art binary neural networks (BNNs) through comprehensive analysis. Despite the inclusion of floating-point parameters in BNN architectures to improve accuracy, our findings reveal that BNNs are highly sensitive to deviations in these parameters caused by memory faults. In light of this crucial finding, we propose a technique to improve BNN dependability by restricting the range of float parameters through a novel deliberately uniform quantization. The introduced quantization technique results in a reduction in the proportion of floating-point parameters utilized in the BNN, without incurring any additional computational overheads during the inference stage. The extensive experimental fault simulation on the proposed BNN architecture (i.e. ZOBNN) reveal a remarkable 5X enhancement in robustness compared to conventional floating-point DNN. Notably, this improvement is achieved without incurring any computational overhead. Crucially, this enhancement comes without computational overhead. \\ToolName~excels in critical edge applications characterized by limited computational resources, prioritizing both dependability and real-time performance.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.04974",
        "abstract url": "https://arxiv.org/abs/2407.04974",
        "title": "Multi-agent Off-policy Actor-Critic Reinforcement Learning for Partially Observable Environments",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This study proposes the use of a social learning method to estimate a global state within a multi-agent off-policy actor-critic algorithm for reinforcement learning (RL) operating in a partially observable environment. We assume that the network of agents operates in a fully-decentralized manner, possessing the capability to exchange variables with their immediate neighbors. The proposed design methodology is supported by an analysis demonstrating that the difference between final outcomes, obtained when the global state is fully observed versus estimated through the social learning method, is $\\varepsilon$-bounded when an appropriate number of iterations of social learning updates are implemented. Unlike many existing dec-POMDP-based RL approaches, the proposed algorithm is suitable for model-free multi-agent reinforcement learning as it does not require knowledge of a transition model. Furthermore, experimental results illustrate the efficacy of the algorithm and demonstrate its superiority over the current state-of-the-art methods.",
        "subjects": [
            "cs.LG",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2407.04980",
        "abstract url": "https://arxiv.org/abs/2407.04980",
        "title": "Enabling Causal Discovery in Post-Nonlinear Models with Normalizing Flows",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Post-nonlinear (PNL) causal models stand out as a versatile and adaptable framework for modeling intricate causal relationships. However, accurately capturing the invertibility constraint required in PNL models remains challenging in existing studies. To address this problem, we introduce CAF-PoNo (Causal discovery via Normalizing Flows for Post-Nonlinear models), harnessing the power of the normalizing flows architecture to enforce the crucial invertibility constraint in PNL models. Through normalizing flows, our method precisely reconstructs the hidden noise, which plays a vital role in cause-effect identification through statistical independence testing. Furthermore, the proposed approach exhibits remarkable extensibility, as it can be seamlessly expanded to facilitate multivariate causal discovery via causal order identification, empowering us to efficiently unravel complex causal relationships. Extensive experimental evaluations on both simulated and real datasets consistently demonstrate that the proposed method outperforms several state-of-the-art approaches in both bivariate and multivariate causal discovery tasks.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ME"
        ],
        "comment": "Acepted at ECAI 2024"
    },
    {
        "paper id": "2407.04985",
        "abstract url": "https://arxiv.org/abs/2407.04985",
        "title": "Combining Neuroevolution with the Search for Novelty to Improve the Generation of Test Inputs for Games",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "As games challenge traditional automated white-box test generators, the Neatest approach generates test suites consisting of neural networks that exercise the source code by playing the games. Neatest generates these neural networks using an evolutionary algorithm that is guided by an objective function targeting individual source code statements. This approach works well if the objective function provides sufficient guidance, but deceiving or complex fitness landscapes may inhibit the search. In this paper, we investigate whether the issue of challenging fitness landscapes can be addressed by promoting novel behaviours during the search. Our case study on two Scratch games demonstrates that rewarding novel behaviours is a promising approach for overcoming challenging fitness landscapes, thus enabling future research on how to adapt the search algorithms to best use this information.",
        "subjects": [
            "cs.SE",
            "cs.LG",
            "cs.NE"
        ],
        "comment": "6 pages, 5 figures, Foundations of Applied Software Engineering for Games Workshop (FaSE4Games 2024)"
    },
    {
        "paper id": "2407.04997",
        "abstract url": "https://arxiv.org/abs/2407.04997",
        "title": "Achieving Tool Calling Functionality in LLMs Using Only Prompt Engineering Without Fine-Tuning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Currently, the vast majority of locally deployed open-source large language models (LLMs) and some commercial model interfaces do not support stable tool calling functionality. The existing solution involves fine-tuning LLMs, which results in significant time and computational resource consumption. This paper proposes a method that enables LLMs to achieve stable tool calling capabilities using only prompt engineering and some ingenious code design. We conducted experiments on multiple LLMs that lack tool calling capabilities across various tool calling tasks, achieving a success rate of 100%.",
        "subjects": [
            "cs.SE",
            "cs.AI",
            "cs.HC"
        ],
        "comment": "5 pages, 2 figures,review comments welcome"
    },
    {
        "paper id": "2407.05005",
        "abstract url": "https://arxiv.org/abs/2407.05005",
        "title": "Personalized Federated Domain-Incremental Learning based on Adaptive Knowledge Matching",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper focuses on Federated Domain-Incremental Learning (FDIL) where each client continues to learn incremental tasks where their domain shifts from each other. We propose a novel adaptive knowledge matching-based personalized FDIL approach (pFedDIL) which allows each client to alternatively utilize appropriate incremental task learning strategy on the correlation with the knowledge from previous tasks. More specifically, when a new task arrives, each client first calculates its local correlations with previous tasks. Then, the client can choose to adopt a new initial model or a previous model with similar knowledge to train the new task and simultaneously migrate knowledge from previous tasks based on these correlations. Furthermore, to identify the correlations between the new task and previous tasks for each client, we separately employ an auxiliary classifier to each target classification model and propose sharing partial parameters between the target classification model and the auxiliary classifier to condense model parameters. We conduct extensive experiments on several datasets of which results demonstrate that pFedDIL outperforms state-of-the-art methods by up to 14.35\\% in terms of average accuracy of all tasks.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05040",
        "abstract url": "https://arxiv.org/abs/2407.05040",
        "title": "Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data Pruning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent work targeting large language models (LLMs) for code generation demonstrated that increasing the amount of training data through synthetic code generation often leads to exceptional performance. In this paper we explore data pruning methods aimed at enhancing the efficiency of model training specifically for code LLMs. We present techniques that integrate various clustering and pruning metrics to selectively reduce training data without compromising the accuracy and functionality of the generated code. We observe significant redundancies in synthetic training data generation, where our experiments demonstrate that benchmark performance can be largely preserved by training on only 10% of the data. Moreover, we observe consistent improvements in benchmark results through moderate pruning of the training data. Our experiments show that these pruning strategies not only reduce the computational resources needed but also enhance the overall quality code generation.",
        "subjects": [
            "cs.SE",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05047",
        "abstract url": "https://arxiv.org/abs/2407.05047",
        "title": "MFE-ETP: A Comprehensive Evaluation Benchmark for Multi-modal Foundation Models on Embodied Task Planning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In recent years, Multi-modal Foundation Models (MFMs) and Embodied Artificial Intelligence (EAI) have been advancing side by side at an unprecedented pace. The integration of the two has garnered significant attention from the AI research community. In this work, we attempt to provide an in-depth and comprehensive evaluation of the performance of MFM s on embodied task planning, aiming to shed light on their capabilities and limitations in this domain. To this end, based on the characteristics of embodied task planning, we first develop a systematic evaluation framework, which encapsulates four crucial capabilities of MFMs: object understanding, spatio-temporal perception, task understanding, and embodied reasoning. Following this, we propose a new benchmark, named MFE-ETP, characterized its complex and variable task scenarios, typical yet diverse task types, task instances of varying difficulties, and rich test case types ranging from multiple embodied question answering to embodied task reasoning. Finally, we offer a simple and easy-to-use automatic evaluation platform that enables the automated testing of multiple MFMs on the proposed benchmark. Using the benchmark and evaluation platform, we evaluated several state-of-the-art MFMs and found that they significantly lag behind human-level performance. The MFE-ETP is a high-quality, large-scale, and challenging benchmark relevant to real-world tasks.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05058",
        "abstract url": "https://arxiv.org/abs/2407.05058",
        "title": "Advancing Algorithmic Approaches to Probabilistic Argumentation under the Constellation Approach",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Reasoning with defeasible and conflicting knowledge in an argumentative form is a key research field in computational argumentation. Reasoning under various forms of uncertainty is both a key feature and a challenging barrier for automated argumentative reasoning. It was shown that argumentative reasoning using probabilities faces in general high computational complexity, in particular for the so-called constellation approach. In this paper, we develop an algorithmic approach to overcome this obstacle. We refine existing complexity results and show that two main reasoning tasks, that of computing the probability of a given set being an extension and an argument being acceptable, diverge in their complexity: the former is #P-complete and the latter is #-dot-NP-complete when considering their underlying counting problems. We present an algorithm for the complex task of computing the probability of a set of arguments being a complete extension by using dynamic programming operating on tree-decompositions. An experimental evaluation shows promise of our approach.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05083",
        "abstract url": "https://arxiv.org/abs/2407.05083",
        "title": "Exploring agent interaction patterns in the comment sections of fake and real news",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "User comments on social media have been recognized as a crucial factor in distinguishing between fake and real news, with many studies focusing on the textual content of user reactions. However, the interactions among agents in the comment sections for fake and real news have not been fully explored. In this study, we analyze a dataset comprising both fake and real news from Reddit to investigate agent interaction patterns, considering both the network structure and the sentiment of the nodes. Our findings reveal that (i) comments on fake news are more likely to form groups, (ii) compared to fake news, where users generate more negative sentiment, real news tend to elicit more neutral and positive sentiments. Additionally, nodes with similar sentiments cluster together more tightly than anticipated. From a dynamic perspective, we found that the sentiment distribution among nodes stabilizes early and remains stable over time. These findings have both theoretical and practical implications, particularly for the early detection of real and fake news within social networks.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05104",
        "abstract url": "https://arxiv.org/abs/2407.05104",
        "title": "Crowdsourced reviews reveal substantial disparities in public perceptions of parking",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Due to increased reliance on private vehicles and growing travel demand, parking remains a longstanding urban challenge globally. Quantifying parking perceptions is paramount as it enables decision-makers to identify problematic areas and make informed decisions on parking management. This study introduces a cost-effective and widely accessible data source, crowdsourced online reviews, to investigate public perceptions of parking across the U.S. Specifically, we examine 4,987,483 parking-related reviews for 1,129,460 points of interest (POIs) across 911 core-based statistical areas (CBSAs) sourced from Google Maps. We employ the Bidirectional Encoder Representations from Transformers (BERT) model to classify the parking sentiment and conduct regression analyses to explore its relationships with socio-spatial factors. Findings reveal significant variations in parking sentiment across POI types and CBSAs, with Restaurant POIs showing the most negative. Regression results further indicate that denser urban areas with higher proportions of African Americans and Hispanics and lower socioeconomic status are more likely to exhibit negative parking sentiment. Interestingly, an opposite relationship between parking supply and sentiment is observed, indicating increasing supply does not necessarily improve parking experiences. Finally, our textual analysis identifies keywords associated with positive or negative sentiments and highlights disparities between urban and rural areas. Overall, this study demonstrates the potential of a novel data source and methodological framework in measuring parking sentiment, offering valuable insights that help identify hyperlocal parking issues and guide targeted parking management strategies.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05106",
        "abstract url": "https://arxiv.org/abs/2407.05106",
        "title": "DailyDVS-200: A Comprehensive Benchmark Dataset for Event-Based Action Recognition",
        "rating": "0.5",
        "keywords": [
            [
                "event cameras"
            ],
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Neuromorphic sensors, specifically event cameras, revolutionize visual data acquisition by capturing pixel intensity changes with exceptional dynamic range, minimal latency, and energy efficiency, setting them apart from conventional frame-based cameras. The distinctive capabilities of event cameras have ignited significant interest in the domain of event-based action recognition, recognizing their vast potential for advancement. However, the development in this field is currently slowed by the lack of comprehensive, large-scale datasets, which are critical for developing robust recognition frameworks. To bridge this gap, we introduces DailyDVS-200, a meticulously curated benchmark dataset tailored for the event-based action recognition community. DailyDVS-200 is extensive, covering 200 action categories across real-world scenarios, recorded by 47 participants, and comprises more than 22,000 event sequences. This dataset is designed to reflect a broad spectrum of action types, scene complexities, and data acquisition diversity. Each sequence in the dataset is annotated with 14 attributes, ensuring a detailed characterization of the recorded actions. Moreover, DailyDVS-200 is structured to facilitate a wide range of research paths, offering a solid foundation for both validating existing approaches and inspiring novel methodologies. By setting a new benchmark in the field, we challenge the current limitations of neuromorphic data processing and invite a surge of new approaches in event-based action recognition techniques, which paves the way for future explorations in neuromorphic computing and beyond. The dataset and source code are available at https://github.com/QiWang233/DailyDVS-200.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to ECCV 2024"
    },
    {
        "paper id": "2407.05138",
        "abstract url": "https://arxiv.org/abs/2407.05138",
        "title": "Vortex under Ripplet: An Empirical Study of RAG-enabled Applications",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large language models (LLMs) enhanced by retrieval-augmented generation (RAG) provide effective solutions in various application scenarios. However, developers face challenges in integrating RAG-enhanced LLMs into software systems, due to lack of interface specification, requirements from software context, and complicated system management. In this paper, we manually studied 100 open-source applications that incorporate RAG-enhanced LLMs, and their issue reports. We have found that more than 98% of applications contain multiple integration defects that harm software functionality, efficiency, and security. We have also generalized 19 defect patterns and proposed guidelines to tackle them. We hope this work could aid LLM-enabled software development and motivate future research.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05145",
        "abstract url": "https://arxiv.org/abs/2407.05145",
        "title": "On high-dimensional modifications of the nearest neighbor classifier",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Nearest neighbor classifier is arguably the most simple and popular nonparametric classifier available in the literature. However, due to the concentration of pairwise distances and the violation of the neighborhood structure, this classifier often suffers in high-dimension, low-sample size (HDLSS) situations, especially when the scale difference between the competing classes dominates their location difference. Several attempts have been made in the literature to take care of this problem. In this article, we discuss some of these existing methods and propose some new ones. We carry out some theoretical investigations in this regard and analyze several simulated and benchmark datasets to compare the empirical performances of proposed methods with some of the existing ones.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05186",
        "abstract url": "https://arxiv.org/abs/2407.05186",
        "title": "Understanding Political Communication and Political Communicators on Twitch",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI",
                "cs.CY"
            ]
        ],
        "abstract": "As new technologies rapidly reshape patterns of political communication, platforms like Twitch are transforming how people consume political information. This entertainment-oriented live streaming platform allows us to observe the impact of technologies such as ``live-streaming'' and ``streaming-chat'' on political communication. Despite its entertainment focus, Twitch hosts a variety of political actors, including politicians and pundits. This study explores Twitch politics by addressing three main questions: 1) Who are the political Twitch streamers? 2) What content is covered in political streams? 3) How do audiences of political streams interact with each other? To identify political streamers, I leveraged the Twitch API and supervised machine-learning techniques, identifying 574 political streamers. I used topic modeling to analyze the content of political streams, revealing seven broad categories of political topics and a unique pattern of communication involving context-specific ``emotes.'' Additionally, I created user-reference networks to examine interaction patterns, finding that a small number of users dominate the communication network. This research contributes to our understanding of how new social media technologies influence political communication, particularly among younger audiences.",
        "subjects": [
            "cs.CY",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05202",
        "abstract url": "https://arxiv.org/abs/2407.05202",
        "title": "Harnessing the Power of LLMs: Automating Unit Test Generation for High-Performance Computing",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Unit testing is crucial in software engineering for ensuring quality. However, it's not widely used in parallel and high-performance computing software, particularly scientific applications, due to their smaller, diverse user base and complex logic. These factors make unit testing challenging and expensive, as it requires specialized knowledge and existing automated tools are often ineffective. To address this, we propose an automated method for generating unit tests for such software, considering their unique features like complex logic and parallel processing. Recently, large language models (LLMs) have shown promise in coding and testing. We explored the capabilities of Davinci (text-davinci-002) and ChatGPT (gpt-3.5-turbo) in creating unit tests for C++ parallel programs. Our results show that LLMs can generate mostly correct and comprehensive unit tests, although they have some limitations, such as repetitive assertions and blank test cases.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05237",
        "abstract url": "https://arxiv.org/abs/2407.05237",
        "title": "Privacy of the last iterate in cyclically-sampled DP-SGD on nonconvex composite losses",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Differentially private stochastic gradient descent (DP-SGD) refers to a family of optimization algorithms that provide a guaranteed level of differential privacy (DP) through DP accounting techniques. However, current accounting techniques make assumptions that diverge significantly from practical DP-SGD implementations. For example, they may assume the loss function is Lipschitz continuous and convex, sample the batches randomly with replacement, or omit the gradient clipping step. In this work, we analyze the most commonly used variant of DP-SGD, in which we sample batches cyclically with replacement, perform gradient clipping, and only release the last DP-SGD iterate. More specifically - without assuming convexity, smoothness, or Lipschitz continuity of the loss function - we establish new R\u00e9nyi differential privacy (RDP) bounds for the last DP-SGD iterate under the mild assumption that (i) the DP-SGD stepsize is small relative to the topological constants in the loss function, and (ii) the loss function is weakly-convex. Moreover, we show that our bounds converge to previously established convex bounds when the weak-convexity parameter of the objective function approaches zero. In the case of non-Lipschitz smooth loss functions, we provide a weaker bound that scales well in terms of the number of DP-SGD iterations.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "cs.DS",
            "math.OC",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05254",
        "abstract url": "https://arxiv.org/abs/2407.05254",
        "title": "GaussReg: Fast 3D Registration with Gaussian Splatting",
        "rating": "0.5",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "Point cloud",
                "NeRF",
                "Radiance Fields"
            ],
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Point cloud registration is a fundamental problem for large-scale 3D scene scanning and reconstruction. With the help of deep learning, registration methods have evolved significantly, reaching a nearly-mature stage. As the introduction of Neural Radiance Fields (NeRF), it has become the most popular 3D scene representation as its powerful view synthesis capabilities. Regarding NeRF representation, its registration is also required for large-scale scene reconstruction. However, this topic extremly lacks exploration. This is due to the inherent challenge to model the geometric relationship among two scenes with implicit representations. The existing methods usually convert the implicit representation to explicit representation for further registration. Most recently, Gaussian Splatting (GS) is introduced, employing explicit 3D Gaussian. This method significantly enhances rendering speed while maintaining high rendering quality. Given two scenes with explicit GS representations, in this work, we explore the 3D registration task between them. To this end, we propose GaussReg, a novel coarse-to-fine framework, both fast and accurate. The coarse stage follows existing point cloud registration methods and estimates a rough alignment for point clouds from GS. We further newly present an image-guided fine registration approach, which renders images from GS to provide more detailed geometric information for precise alignment. To support comprehensive evaluation, we carefully build a scene-level dataset called ScanNet-GSReg with 1379 scenes obtained from the ScanNet dataset and collect an in-the-wild dataset called GSReg. Experimental results demonstrate our method achieves state-of-the-art performance on multiple datasets. Our GaussReg is 44 times faster than HLoc (SuperPoint as the feature extractor and SuperGlue as the matcher) with comparable accuracy.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "ECCV 2024"
    },
    {
        "paper id": "2407.04963",
        "abstract url": "https://arxiv.org/abs/2407.04963",
        "title": "Towards Context-Aware Emotion Recognition Debiasing from a Causal Demystification Perspective via De-confounded Training",
        "rating": "0",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Understanding emotions from diverse contexts has received widespread attention in computer vision communities. The core philosophy of Context-Aware Emotion Recognition (CAER) is to provide valuable semantic cues for recognizing the emotions of target persons by leveraging rich contextual information. Current approaches invariably focus on designing sophisticated structures to extract perceptually critical representations from contexts. Nevertheless, a long-neglected dilemma is that a severe context bias in existing datasets results in an unbalanced distribution of emotional states among different contexts, causing biased visual representation learning. From a causal demystification perspective, the harmful bias is identified as a confounder that misleads existing models to learn spurious correlations based on likelihood estimation, limiting the models' performance. To address the issue, we embrace causal inference to disentangle the models from the impact of such bias, and formulate the causalities among variables in the CAER task via a customized causal graph. Subsequently, we present a Contextual Causal Intervention Module (CCIM) to de-confound the confounder, which is built upon backdoor adjustment theory to facilitate seeking approximate causal effects during model training. As a plug-and-play component, CCIM can easily integrate with existing approaches and bring significant improvements. Systematic experiments on three datasets demonstrate the effectiveness of our CCIM.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "TPAMI 2024"
    },
    {
        "paper id": "2407.04973",
        "abstract url": "https://arxiv.org/abs/2407.04973",
        "title": "LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts",
        "rating": "0",
        "keywords": [
            [
                "navigation"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "We propose LogicVista, an evaluation benchmark that assesses the integrated logical reasoning capabilities of multimodal large language models (MLLMs) in Visual contexts. Recent advancements in MLLMs have demonstrated various fascinating abilities, from crafting poetry based on an image to performing mathematical reasoning. However, there is still a lack of systematic evaluation of MLLMs' proficiency in logical reasoning tasks, which are essential for activities like navigation and puzzle-solving. Thus we evaluate general logical cognition abilities across 5 logical reasoning tasks encompassing 9 different capabilities, using a sample of 448 multiple-choice questions. Each question is annotated with the correct answer and the human-written reasoning behind the selection, enabling both open-ended and multiple-choice evaluation. A total of 8 MLLMs are comprehensively evaluated using LogicVista. Code and Data Available at https://github.com/Yijia-Xiao/LogicVista.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "LogicVista benchmarks the logical reasoning of multimodal large language models in visual tasks"
    },
    {
        "paper id": "2407.05100",
        "abstract url": "https://arxiv.org/abs/2407.05100",
        "title": "Ask Questions with Double Hints: Visual Question Generation with Answer-awareness and Region-reference",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "The visual question generation (VQG) task aims to generate human-like questions from an image and potentially other side information (e.g. answer type). Previous works on VQG fall in two aspects: i) They suffer from one image to many questions mapping problem, which leads to the failure of generating referential and meaningful questions from an image. ii) They fail to model complex implicit relations among the visual objects in an image and also overlook potential interactions between the side information and image. To address these limitations, we first propose a novel learning paradigm to generate visual questions with answer-awareness and region-reference. Concretely, we aim to ask the right visual questions with Double Hints - textual answers and visual regions of interests, which could effectively mitigate the existing one-to-many mapping issue. Particularly, we develop a simple methodology to self-learn the visual hints without introducing any additional human annotations. Furthermore, to capture these sophisticated relationships, we propose a new double-hints guided Graph-to-Sequence learning framework, which first models them as a dynamic graph and learns the implicit topology end-to-end, and then utilizes a graph-to-sequence model to generate the questions with double hints. Experimental results demonstrate the priority of our proposed method.",
        "subjects": [
            "cs.CV",
            "cs.CL",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05131",
        "abstract url": "https://arxiv.org/abs/2407.05131",
        "title": "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models",
        "rating": "0",
        "keywords": [
            [
                "Vision Language"
            ],
            [
                "Medical",
                "diagnosis"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CY",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has enhanced medical diagnosis. However, current Med-LVLMs frequently encounter factual issues, often generating responses that do not align with established medical facts. Retrieval-Augmented Generation (RAG), which utilizes external knowledge, can improve the factual accuracy of these models but introduces two major challenges. First, limited retrieved contexts might not cover all necessary information, while excessive retrieval can introduce irrelevant and inaccurate references, interfering with the model's generation. Second, in cases where the model originally responds correctly, applying RAG can lead to an over-reliance on retrieved contexts, resulting in incorrect answers. To address these issues, we propose RULE, which consists of two components. First, we introduce a provably effective strategy for controlling factuality risk through the calibrated selection of the number of retrieved contexts. Second, based on samples where over-reliance on retrieved contexts led to errors, we curate a preference dataset to fine-tune the model, balancing its dependence on inherent knowledge and retrieved contexts for generation. We demonstrate the effectiveness of RULE on three medical VQA datasets, achieving an average improvement of 20.8% in factual accuracy. We publicly release our benchmark and code in https://github.com/richard-peng-xia/RULE.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05194",
        "abstract url": "https://arxiv.org/abs/2407.05194",
        "title": "LLMCloudHunter: Harnessing LLMs for Automated Extraction of Detection Rules from Cloud-Based CTI",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "As the number and sophistication of cyber attacks have increased, threat hunting has become a critical aspect of active security, enabling proactive detection and mitigation of threats before they cause significant harm. Open-source cyber threat intelligence (OS-CTI) is a valuable resource for threat hunters, however, it often comes in unstructured formats that require further manual analysis. Previous studies aimed at automating OSCTI analysis are limited since (1) they failed to provide actionable outputs, (2) they did not take advantage of images present in OSCTI sources, and (3) they focused on on-premises environments, overlooking the growing importance of cloud environments. To address these gaps, we propose LLMCloudHunter, a novel framework that leverages large language models (LLMs) to automatically generate generic-signature detection rule candidates from textual and visual OSCTI data. We evaluated the quality of the rules generated by the proposed framework using 12 annotated real-world cloud threat reports. The results show that our framework achieved a precision of 92% and recall of 98% for the task of accurately extracting API calls made by the threat actor and a precision of 99% with a recall of 98% for IoCs. Additionally, 99.18% of the generated detection rule candidates were successfully compiled and converted into Splunk queries.",
        "subjects": [
            "cs.CR",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05206",
        "abstract url": "https://arxiv.org/abs/2407.05206",
        "title": "Helios: An extremely low power event-based gesture recognition for always-on smart eyewear",
        "rating": "0",
        "keywords": [
            [
                "event camera"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "This paper introduces Helios, the first extremely low-power, real-time, event-based hand gesture recognition system designed for all-day on smart eyewear. As augmented reality (AR) evolves, current smart glasses like the Meta Ray-Bans prioritize visual and wearable comfort at the expense of functionality. Existing human-machine interfaces (HMIs) in these devices, such as capacitive touch and voice controls, present limitations in ergonomics, privacy and power consumption. Helios addresses these challenges by leveraging natural hand interactions for a more intuitive and comfortable user experience. Our system utilizes a extremely low-power and compact 3mmx4mm/20mW event camera to perform natural hand-based gesture recognition for always-on smart eyewear. The camera's output is processed by a convolutional neural network (CNN) running on a NXP Nano UltraLite compute platform, consuming less than 350mW. Helios can recognize seven classes of gestures, including subtle microgestures like swipes and pinches, with 91% accuracy. We also demonstrate real-time performance across 20 users at a remarkably low latency of 60ms. Our user testing results align with the positive feedback we received during our recent successful demo at AWE-USA-2024.",
        "subjects": [
            "cs.CV",
            "cs.HC",
            "cs.LG"
        ],
        "comment": "18 pages, 10 figures. First three authors contributed equally to this paper"
    },
    {
        "paper id": "2407.04969",
        "abstract url": "https://arxiv.org/abs/2407.04969",
        "title": "EVA-Score: Evaluation of Long-form Summarization on Informativeness through Extraction and Validation",
        "rating": "-0.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Summarization is a fundamental task in natural language processing (NLP) and since large language models (LLMs), such as GPT-4 and Claude, come out, increasing attention has been paid to long-form summarization whose input sequences are much longer, indicating more information contained. The current evaluation metrics either use similarity-based metrics like ROUGE and BERTScore which rely on similarity and fail to consider informativeness or LLM-based metrics, lacking quantitative analysis of information richness and are rather subjective. In this paper, we propose a new evaluation metric called EVA-Score using Atomic Fact Chain Generation and Document-level Relation Extraction together to automatically calculate the informativeness and give a definite number as an information score. Experiment results show that our metric shows a state-of-the-art correlation with humans. We also re-evaluate the performance of LLMs on long-form summarization comprehensively from the information aspect, forecasting future ways to use LLMs for long-form summarization.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "16 pages, 3 figures, submitted to EMNLP"
    },
    {
        "paper id": "2407.04988",
        "abstract url": "https://arxiv.org/abs/2407.04988",
        "title": "The Reachability Problem for Neural-Network Control Systems",
        "rating": "-0.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "A control system consists of a plant component and a controller which periodically computes a control input for the plant. We consider systems where the controller is implemented by a feedforward neural network with ReLU activations. The reachability problem asks, given a set of initial states, whether a set of target states can be reached. We show that this problem is undecidable even for trivial plants and fixed-depth neural networks with three inputs and outputs. We also show that the problem becomes semi-decidable when the plant as well as the input and target sets are given by automata over infinite words.",
        "subjects": [
            "cs.LG",
            "cs.CC",
            "cs.LO",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2407.04992",
        "abstract url": "https://arxiv.org/abs/2407.04992",
        "title": "Scalable Variational Causal Discovery Unconstrained by Acyclicity",
        "rating": "-0.5",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Bayesian causal discovery offers the power to quantify epistemic uncertainties among a broad range of structurally diverse causal theories potentially explaining the data, represented in forms of directed acyclic graphs (DAGs). However, existing methods struggle with efficient DAG sampling due to the complex acyclicity constraint. In this study, we propose a scalable Bayesian approach to effectively learn the posterior distribution over causal graphs given observational data thanks to the ability to generate DAGs without explicitly enforcing acyclicity. Specifically, we introduce a novel differentiable DAG sampling method that can generate a valid acyclic causal graph by mapping an unconstrained distribution of implicit topological orders to a distribution over DAGs. Given this efficient DAG sampling scheme, we are able to model the posterior distribution over causal graphs using a simple variational distribution over a continuous domain, which can be learned via the variational inference framework. Extensive empirical experiments on both simulated and real datasets demonstrate the superior performance of the proposed model compared to several state-of-the-art baselines.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ME"
        ],
        "comment": "Accepted at ECAI 2024"
    },
    {
        "paper id": "2407.04999",
        "abstract url": "https://arxiv.org/abs/2407.04999",
        "title": "Rethinking the Effectiveness of Graph Classification Datasets in Benchmarks for Assessing GNNs",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph classification benchmarks, vital for assessing and developing graph neural networks (GNNs), have recently been scrutinized, as simple methods like MLPs have demonstrated comparable performance. This leads to an important question: Do these benchmarks effectively distinguish the advancements of GNNs over other methodologies? If so, how do we quantitatively measure this effectiveness? In response, we first propose an empirical protocol based on a fair benchmarking framework to investigate the performance discrepancy between simple methods and GNNs. We further propose a novel metric to quantify the dataset effectiveness by considering both dataset complexity and model performance. To the best of our knowledge, our work is the first to thoroughly study and provide an explicit definition for dataset effectiveness in the graph learning area. Through testing across 16 real-world datasets, we found our metric to align with existing studies and intuitive assumptions. Finally, we explore the causes behind the low effectiveness of certain datasets by investigating the correlation between intrinsic graph properties and class labels, and we developed a novel technique supporting the correlation-controllable synthetic dataset generation. Our findings shed light on the current understanding of benchmark datasets, and our new platform could fuel the future evolution of graph classification benchmarks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05008",
        "abstract url": "https://arxiv.org/abs/2407.05008",
        "title": "T-CorresNet: Template Guided 3D Point Cloud Completion with Correspondence Pooling Query Generation Strategy",
        "rating": "-0.5",
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "autonomous driving"
            ],
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Point clouds are commonly used in various practical applications such as autonomous driving and the manufacturing industry. However, these point clouds often suffer from incompleteness due to limited perspectives, scanner resolution and occlusion. Therefore the prediction of missing parts performs a crucial task. In this paper, we propose a novel method for point cloud completion. We utilize a spherical template to guide the generation of the coarse complete template and generate the dynamic query tokens through a correspondence pooling (Corres-Pooling) query generator. Specifically, we first generate the coarse complete template by embedding a Gaussian spherical template into the partial input and transforming the template to best match the input. Then we use the Corres-Pooling query generator to refine the coarse template and generate dynamic query tokens which could be used to predict the complete point proxies. Finally, we generate the complete point cloud with a FoldingNet following the coarse-to-fine paradigm, according to the fine template and the predicted point proxies. Experimental results demonstrate that our T-CorresNet outperforms the state-of-the-art methods on several benchmarks. Our Codes are available at https://github.com/df-boy/T-CorresNet.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to ECCV 2024"
    },
    {
        "paper id": "2407.05079",
        "abstract url": "https://arxiv.org/abs/2407.05079",
        "title": "Form Forge: Latent Space Exploration of Architectural Forms via Explicit Latent Variable Manipulation",
        "rating": "-0.5",
        "keywords": [
            [
                "navigation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper presents 'Form Forge,' a prototype of a creative system for interactively exploring the latent space of architectural forms, inspired by Franois Blanciak's SITELESS: 1001 Building Forms via direct manipulation of latent variables. Utilizing a fine-tuned StyleGAN2-ADA model, the system allows users to navigate an array of possible building forms derived from Blanciak's sketches. Distinct from common latent space exploration tools that often rely on projected navigation landmarks, Form Forge provides direct access to manipulate each latent variable, aiming to offer a more granular exploration of the model's capabilities. Form Forge's design is intended to simplify the interaction with a complex, high-dimensional space and to serve as a preliminary investigation into how such tools might support creative processes in architectural design.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05108",
        "abstract url": "https://arxiv.org/abs/2407.05108",
        "title": "The Role of Depth, Width, and Tree Size in Expressiveness of Deep Forest",
        "rating": "-0.5",
        "keywords": [
            [
                "Depth"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Random forests are classical ensemble algorithms that construct multiple randomized decision trees and aggregate their predictions using naive averaging. \\citet{zhou2019deep} further propose a deep forest algorithm with multi-layer forests, which outperforms random forests in various tasks. The performance of deep forests is related to three hyperparameters in practice: depth, width, and tree size, but little has been known about its theoretical explanation. This work provides the first upper and lower bounds on the approximation complexity of deep forests concerning the three hyperparameters. Our results confirm the distinctive role of depth, which can exponentially enhance the expressiveness of deep forests compared with width and tree size. Experiments confirm the theoretical findings.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05125",
        "abstract url": "https://arxiv.org/abs/2407.05125",
        "title": "A Joint Approach to Local Updating and Gradient Compression for Efficient Asynchronous Federated Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Asynchronous Federated Learning (AFL) confronts inherent challenges arising from the heterogeneity of devices (e.g., their computation capacities) and low-bandwidth environments, both potentially causing stale model updates (e.g., local gradients) for global aggregation. Traditional approaches mitigating the staleness of updates typically focus on either adjusting the local updating or gradient compression, but not both. Recognizing this gap, we introduce a novel approach that synergizes local updating with gradient compression. Our research begins by examining the interplay between local updating frequency and gradient compression rate, and their collective impact on convergence speed. The theoretical upper bound shows that the local updating frequency and gradient compression rate of each device are jointly determined by its computing power, communication capabilities and other factors. Building on this foundation, we propose an AFL framework called FedLuck that adaptively optimizes both local update frequency and gradient compression rates. Experiments on image classification and speech recognization show that FedLuck reduces communication consumption by 56% and training time by 55% on average, achieving competitive performance in heterogeneous and low-bandwidth scenarios compared to the baselines.",
        "subjects": [
            "cs.DC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05141",
        "abstract url": "https://arxiv.org/abs/2407.05141",
        "title": "Impact of Network Topology on Byzantine Resilience in Decentralized Federated Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning (FL) enables a collaborative environment for training machine learning models without sharing training data between users. This is typically achieved by aggregating model gradients on a central server. Decentralized federated learning is a rising paradigm that enables users to collaboratively train machine learning models in a peer-to-peer manner, without the need for a central aggregation server. However, before applying decentralized FL in real-world use training environments, nodes that deviate from the FL process (Byzantine nodes) must be considered when selecting an aggregation function. Recent research has focused on Byzantine-robust aggregation for client-server or fully connected networks, but has not yet evaluated such aggregation schemes for complex topologies possible with decentralized FL. Thus, the need for empirical evidence of Byzantine robustness in differing network topologies is evident. This work investigates the effects of state-of-the-art Byzantine-robust aggregation methods in complex, large-scale network structures. We find that state-of-the-art Byzantine robust aggregation strategies are not resilient within large non-fully connected networks. As such, our findings point the field towards the development of topology-aware aggregation schemes, especially necessary within the context of large scale real-world deployment.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": "8 pages, 6 figures"
    },
    {
        "paper id": "2407.05161",
        "abstract url": "https://arxiv.org/abs/2407.05161",
        "title": "A Survey of Datasets for Information Diffusion Tasks",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Information diffusion across various new media platforms gradually influences perceptions, decisions, and social behaviors of individual users. In communication studies, the famous Five W's of Communication model (5W Model) has displayed the process of information diffusion clearly. At present, although plenty of studies and corresponding datasets about information diffusion have emerged, a systematic categorization of tasks and an integration of datasets are still lacking. To address this gap, we survey a systematic taxonomy of information diffusion tasks and datasets based on the \"5W Model\" framework. We first categorize the information diffusion tasks into ten subtasks with definitions and datasets analysis, from three main tasks of information diffusion prediction, social bot detection, and misinformation detection. We also collect the publicly available dataset repository of information diffusion tasks with the available links and compare them based on six attributes affiliated to users and content: user information, social network, bot label, propagation content, propagation network, and veracity label. In addition, we discuss the limitations and future directions of current datasets and research topics to advance the future development of information diffusion. The dataset repository can be accessed at our website https://github.com/fuxiaG/Information-Diffusion-Datasets.",
        "subjects": [
            "cs.SI",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05174",
        "abstract url": "https://arxiv.org/abs/2407.05174",
        "title": "Synthetic Data Aided Federated Learning Using Foundation Models",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In heterogeneous scenarios where the data distribution amongst the Federated Learning (FL) participants is Non-Independent and Identically distributed (Non-IID), FL suffers from the well known problem of data heterogeneity. This leads the performance of FL to be significantly degraded, as the global model tends to struggle to converge. To solve this problem, we propose Differentially Private Synthetic Data Aided Federated Learning Using Foundation Models (DPSDA-FL), a novel data augmentation strategy that aids in homogenizing the local data present on the clients' side. DPSDA-FL improves the training of the local models by leveraging differentially private synthetic data generated from foundation models. We demonstrate the effectiveness of our approach by evaluating it on the benchmark image dataset: CIFAR-10. Our experimental results have shown that DPSDA-FL can improve class recall and classification accuracy of the global model by up to 26% and 9%, respectively, in FL with Non-IID issues.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05182",
        "abstract url": "https://arxiv.org/abs/2407.05182",
        "title": "A Novel Bifurcation Method for Observation Perturbation Attacks on Reinforcement Learning Agents: Load Altering Attacks on a Cyber Physical Power System",
        "rating": "-0.5",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Components of cyber physical systems, which affect real-world processes, are often exposed to the internet. Replacing conventional control methods with Deep Reinforcement Learning (DRL) in energy systems is an active area of research, as these systems become increasingly complex with the advent of renewable energy sources and the desire to improve their efficiency. Artificial Neural Networks (ANN) are vulnerable to specific perturbations of their inputs or features, called adversarial examples. These perturbations are difficult to detect when properly regularized, but have significant effects on the ANN's output. Because DRL uses ANN to map optimal actions to observations, they are similarly vulnerable to adversarial examples. This work proposes a novel attack technique for continuous control using Group Difference Logits loss with a bifurcation layer. By combining aspects of targeted and untargeted attacks, the attack significantly increases the impact compared to an untargeted attack, with drastically smaller distortions than an optimally targeted attack. We demonstrate the impacts of powerful gradient-based attacks in a realistic smart energy environment, show how the impacts change with different DRL agents and training procedures, and use statistical and time-series analysis to evaluate attacks' stealth. The results show that adversarial attacks can have significant impacts on DRL controllers, and constraining an attack's perturbations makes it difficult to detect. However, certain DRL architectures are far more robust, and robust training methods can further reduce the impact.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": "12 pages, 5 figures"
    },
    {
        "paper id": "2407.04971",
        "abstract url": "https://arxiv.org/abs/2407.04971",
        "title": "Counting Permutation Patterns with Multidimensional Trees",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "We consider the well-studied pattern counting problem: given a permutation $\u03c0\\in \\mathbb{S}_n$ and an integer $k > 1$, count the number of order-isomorphic occurrences of every pattern $\u03c4\\in \\mathbb{S}_k$ in $\u03c0$. Our first result is an $\\widetilde{\\mathcal{O}}(n^2)$-time algorithm for $k=6$ and $k=7$. The proof relies heavily on a new family of graphs that we introduce, called pattern-trees. Every such tree corresponds to an integer linear combination of permutations in $\\mathbb{S}_k$, and is associated with linear extensions of partially ordered sets. We design an evaluation algorithm for these combinations, and apply it to a family of linearly-independent trees. For $k=8$, we show a barrier: the subspace spanned by trees in the previous family has dimension exactly $|\\mathbb{S}_8| - 1$, one less than required. Our second result is an $\\widetilde{\\mathcal{O}}(n^{7/4})$-time algorithm for $k=5$. This algorithm extends the framework of pattern-trees by speeding-up their evaluation in certain cases. A key component of the proof is the introduction of pair-rectangle-trees, a data structure for dominance counting.",
        "subjects": [
            "cs.DS",
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2407.04976",
        "abstract url": "https://arxiv.org/abs/2407.04976",
        "title": "Congestion-Approximators from the Bottom Up",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "We develop a novel algorithm to construct a congestion-approximator with polylogarithmic quality on a capacitated, undirected graph in nearly-linear time. Our approach is the first *bottom-up* hierarchical construction, in contrast to previous *top-down* approaches including that of Racke, Shah, and Taubig (SODA 2014), the only other construction achieving polylogarithmic quality that is implementable in nearly-linear time (Peng, SODA 2016). Similar to Racke, Shah, and Taubig, our construction at each hierarchical level requires calls to an approximate max-flow/min-cut subroutine. However, the main advantage to our bottom-up approach is that these max-flow calls can be implemented directly *without recursion*. More precisely, the previously computed levels of the hierarchy can be converted into a *pseudo-congestion-approximator*, which then translates to a max-flow algorithm that is sufficient for the particular max-flow calls used in the construction of the next hierarchical level. As a result, we obtain the first non-recursive algorithms for congestion-approximator and approximate max-flow that run in nearly-linear time, a conceptual improvement to the aforementioned algorithms that recursively alternate between the two problems.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "46 pages"
    },
    {
        "paper id": "2407.04989",
        "abstract url": "https://arxiv.org/abs/2407.04989",
        "title": "FPTAS for Holant Problems with Log-Concave Signatures",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "For an integer $b\\ge 0$, a $b$-matching in a graph $G=(V,E)$ is a set $S\\subseteq E$ such that each vertex $v\\in V$ is incident to at most $b$ edges in $S$. We design a fully polynomial-time approximation scheme (FPTAS) for counting the number of $b$-matchings in graphs with bounded degrees. Our FPTAS also applies to a broader family of counting problems, namely Holant problems with log-concave signatures. Our algorithm is based on Moitra's linear programming approach (JACM'19). Using a novel construction called the extended coupling tree, we derandomize the coupling designed by Chen and Gu (SODA'24).",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05007",
        "abstract url": "https://arxiv.org/abs/2407.05007",
        "title": "BlessemFlood21: Advancing Flood Analysis with a High-Resolution Georeferenced Dataset for Humanitarian Aid Support",
        "rating": "-1",
        "keywords": [
            [
                "remotely sensed"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Floods are an increasingly common global threat, causing emergencies and severe damage to infrastructure. During crises, organisations such as the World Food Programme use remotely sensed imagery, typically obtained through drones, for rapid situational analysis to plan life-saving actions. Computer Vision tools are needed to support task force experts on-site in the evaluation of the imagery to improve their efficiency and to allocate resources strategically. We introduce the BlessemFlood21 dataset to stimulate research on efficient flood detection tools. The imagery was acquired during the 2021 Erftstadt-Blessem flooding event and consists of high-resolution and georeferenced RGB-NIR images. In the resulting RGB dataset, the images are supplemented with detailed water masks, obtained via a semi-supervised human-in-the-loop technique, where in particular the NIR information is leveraged to classify pixels as either water or non-water. We evaluate our dataset by training and testing established Deep Learning models for semantic segmentation. With BlessemFlood21 we provide labeled high-resolution RGB data and a baseline for further development of algorithmic solutions tailored to flood detection in RGB imagery.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05015",
        "abstract url": "https://arxiv.org/abs/2407.05015",
        "title": "How do you know that? Teaching Generative Language Models to Reference Answers to Biomedical Questions",
        "rating": "-1",
        "keywords": [
            [
                "Biomedical"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have recently become the leading source of answers for users' questions online. Despite their ability to offer eloquent answers, their accuracy and reliability can pose a significant challenge. This is especially true for sensitive domains such as biomedicine, where there is a higher need for factually correct answers. This paper introduces a biomedical retrieval-augmented generation (RAG) system designed to enhance the reliability of generated responses. The system is based on a fine-tuned LLM for the referenced question-answering, where retrieved relevant abstracts from PubMed are passed to LLM's context as input through a prompt. Its output is an answer based on PubMed abstracts, where each statement is referenced accordingly, allowing the users to verify the answer. Our retrieval system achieves an absolute improvement of 23% compared to the PubMed search engine. Based on the manual evaluation on a small sample, our fine-tuned LLM component achieves comparable results to GPT-4 Turbo in referencing relevant abstracts. We make the dataset used to fine-tune the models and the fine-tuned models based on Mistral-7B-instruct-v0.1 and v0.2 publicly available.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Accepted at BioNLP Workshop 2024, colocated with ACL 2024"
    },
    {
        "paper id": "2407.05021",
        "abstract url": "https://arxiv.org/abs/2407.05021",
        "title": "Incremental Multiview Point Cloud Registration",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we present a novel approach for multiview point cloud registration. Different from previous researches that typically employ a global scheme for multiview registration, we propose to adopt an incremental pipeline to progressively align scans into a canonical coordinate system. Specifically, drawing inspiration from image-based 3D reconstruction, our approach first builds a sparse scan graph with scan retrieval and geometric verification. Then, we perform incremental registration via initialization, next scan selection and registration, Track create and continue, and Bundle Adjustment. Additionally, for detector-free matchers, we incorporate a Track refinement process. This process primarily constructs a coarse multiview registration and refines the model by adjusting the positions of the keypoints on the Track. Experiments demonstrate that the proposed framework outperforms existing multiview registration methods on three benchmark datasets. The code is available at https://github.com/Choyaa/IncreMVR.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05025",
        "abstract url": "https://arxiv.org/abs/2407.05025",
        "title": "ProACT: An Augmented Reality Testbed for Intelligent Prosthetic Arms",
        "rating": "-1",
        "keywords": [
            [
                "robotics"
            ]
        ],
        "abstract": "Upper-limb amputees face tremendous difficulty in operating dexterous powered prostheses. Previous work has shown that aspects of prosthetic hand, wrist, or elbow control can be improved through \"intelligent\" control, by combining movement-based or gaze-based intent estimation with low-level robotic autonomy. However, no such solutions exist for whole-arm control. Moreover, hardware platforms for advanced prosthetic control are expensive, and existing simulation platforms are not well-designed for integration with robotics software frameworks. We present the Prosthetic Arm Control Testbed (ProACT), a platform for evaluating intelligent control methods for prosthetic arms in an immersive (Augmented Reality) simulation setting. Using ProACT with non-amputee participants, we compare performance in a Box-and-Blocks Task using a virtual myoelectric prosthetic arm, with and without intent estimation. Our results show that methods using intent estimation improve both user satisfaction and the degree of success in the task. To the best of our knowledge, this constitutes the first study of semi-autonomous control for complex whole-arm prostheses, the first study including sequential task modeling in the context of wearable prosthetic arms, and the first testbed of its kind. Towards the goal of supporting future research in intelligent prosthetics, the system is built upon on existing open-source frameworks for robotics.",
        "subjects": [
            "cs.RO",
            "cs.HC",
            "eess.SY"
        ],
        "comment": "10 pages, 8 figures. Under review. Code and data will be available at https://armlabstanford.github.io/proact after publication"
    },
    {
        "paper id": "2407.05051",
        "abstract url": "https://arxiv.org/abs/2407.05051",
        "title": "BrainMetDetect: Predicting Primary Tumor from Brain Metastasis MRI Data Using Radiomic Features and Machine Learning Algorithms",
        "rating": "-1",
        "keywords": [
            [
                "MRI",
                "cancer",
                "clinical",
                "Tumor"
            ],
            [
                "cs.LG",
                "eess.IV"
            ]
        ],
        "abstract": "Objective: Brain metastases (BMs) are common in cancer patients and determining the primary tumor site is crucial for effective treatment. This study aims to predict the primary tumor site from BM MRI data using radiomic features and advanced machine learning algorithms. Methods: We utilized a comprehensive dataset from Ocana-Tienda et al. (2023) comprising MRI and clinical data from 75 patients with BMs. Radiomic features were extracted from post-contrast T1-weighted MRI sequences. Feature selection was performed using the GINI index, and data normalization was applied to ensure consistent scaling. We developed and evaluated Random Forest and XGBoost classifiers, both with and without hyperparameter optimization using the FOX (Fox optimizer) algorithm. Model interpretability was enhanced using SHAP (SHapley Additive exPlanations) values. Results: The baseline Random Forest model achieved an accuracy of 0.85, which improved to 0.93 with FOX optimization. The XGBoost model showed an initial accuracy of 0.96, increasing to 0.99 after optimization. SHAP analysis revealed the most influential radiomic features contributing to the models' predictions. The FOX-optimized XGBoost model exhibited the best performance with a precision, recall, and F1-score of 0.99. Conclusion: This study demonstrates the effectiveness of using radiomic features and machine learning to predict primary tumor sites from BM MRI data. The FOX optimization algorithm significantly enhanced model performance, and SHAP provided valuable insights into feature importance. These findings highlight the potential of integrating radiomics and machine learning into clinical practice for improved diagnostic accuracy and personalized treatment planning.",
        "subjects": [
            "cs.LG",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05057",
        "abstract url": "https://arxiv.org/abs/2407.05057",
        "title": "Crossing Numbers of Beyond Planar Graphs Re-revisited: A Framework Approach",
        "rating": "-1",
        "keywords": [
            [
                "Graphs"
            ]
        ],
        "abstract": "Beyond planarity concepts (prominent examples include k-planarity or fan-planarity) apply certain restrictions on the allowed patterns of crossings in drawings. It is natural to ask, how much the number of crossings may increase over the traditional (unrestricted) crossing number. Previous approaches to bound such ratios, e.g. [arXiv:1908.03153, arXiv:2105.12452], require very specialized constructions and arguments for each considered beyond planarity concept, and mostly only yield asymptotically non-tight bounds. We propose a very general proof framework that allows us to obtain asymptotically tight bounds, and where the concept-specific parts of the proof typically boil down to a couple of lines. We show the strength of our approach by giving improved or first bounds for several beyond planarity concepts.",
        "subjects": [
            "cs.CG",
            "cs.DM"
        ],
        "comment": "18 pages, 5 figures"
    },
    {
        "paper id": "2407.05096",
        "abstract url": "https://arxiv.org/abs/2407.05096",
        "title": "Database Technology Evolution III: Knowledge Graphs and Linked Data",
        "rating": "-1",
        "keywords": [
            [
                "Graphs"
            ]
        ],
        "abstract": "This paper reviews the changes for database technology represented by the current development of the draft international standard ISO 39075 (Database Languages - GQL), which seeks a unified specification for property graphs and knowledge graphs. This paper examines these current developments as part of our review of the evolution of database technology, and their relation to the longer-term goal of supporting the Semantic Web using relational technology.",
        "subjects": [
            "cs.DB"
        ],
        "comment": "10 pages, 2 figures"
    },
    {
        "paper id": "2407.05107",
        "abstract url": "https://arxiv.org/abs/2407.05107",
        "title": "Phase-Only Beam Shaping for Transmitting Array Antennas in Radar Applications",
        "rating": "-1",
        "keywords": [
            [
                "Radar"
            ]
        ],
        "abstract": "Two beam broadening methods for active electronically scanned array (AESA) antennas with uniform amplitude excitation are proposed and compared: phase tapering optimization (PTO) and a novel time-varying phase tapering (TPT). The PTO is a simple and efficient approach assuming continuous polynomial phase distribution and requiring optimization of only few parameters. The TPT is valid mainly for radar applications, taking advantage of the fact that radars typically transmit pulse trains for coherent integration. By varying the array elements' phases from pulse to pulse, the TPT achieves effective amplitude tapering, thus providing a method of beam shaping, occasionally with a simple analytic form. The TPT also makes it possible to produce beam shaping with very low side lobe levels in comparison to the PTO. As a preliminary step, the dimensionality of the radiation pattern characterization for all scan directions is reduced from five to only two variables. This is crucial for efficient optimization of the radiation pattern which needs to be evaluated over a judiciously specified two-dimensional domain.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05116",
        "abstract url": "https://arxiv.org/abs/2407.05116",
        "title": "Automatic Prediction of the Performance of Every Parser",
        "rating": "-1",
        "keywords": [
            [
                "grammatical"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "We present a new parser performance prediction (PPP) model using machine translation performance prediction system (MTPPS), statistically independent of any language or parser, relying only on extrinsic and novel features based on textual, link structural, and bracketing tree structural information. This new system, MTPPS-PPP, can predict the performance of any parser in any language and can be useful for estimating the grammatical difficulty when understanding a given text, for setting expectations from parsing output, for parser selection for a specific domain, and for parser combination systems. We obtain SoA results in PPP of bracketing $F_1$ with better results over textual features and similar performance with previous results that use parser and linguistic label specific information. Our results show the contribution of different types of features as well as rankings of individual features in different experimental settings (cased vs. uncased), in different learning tasks (in-domain vs. out-of-domain), with different training sets, with different learning algorithms, and with different dimensionality reduction techniques. We achieve $0.0678$ MAE and $0.85$ RAE in setting +Link, which corresponds to about $7.4\\%$ error when predicting the bracketing $F_1$ score for the Charniak and Johnson parser on the WSJ23 test set. MTPPS-PPP system can predict without parsing using only the text, without a supervised parser using only an unsupervised parser, without any parser or language dependent information, without using a reference parser output, and can be used to predict the performance of any parser in any language.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "8 pages, 2 figures, 7 tables"
    },
    {
        "paper id": "2407.05120",
        "abstract url": "https://arxiv.org/abs/2407.05120",
        "title": "Wireless teleoperation of HSURF artificial fish in complex paths",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "In this paper we show the application of the new robotic multi-platform system HSURF to a specific use case of teleoperation, aimed at monitoring and inspection. The HSURF system, consists of 3 different kinds of platforms: floater, sinker and robotic fishes. The collaborative control of the 3 platforms allows a remotely based operator to control the fish in order to visit and inspect several targets underwater following a complex trajectory. A shared autonomy solution shows to be the most suitable, in order to minimize the effect of limited bandwidth and relevant delay intrinsic to acoustic communications. The control architecture is described and preliminary results of the acoustically teleoperated visits of multiple targets in a testing pool are provided.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05133",
        "abstract url": "https://arxiv.org/abs/2407.05133",
        "title": "Control Density Function for Robust Safety and Convergence",
        "rating": "-1",
        "keywords": [
            [
                "navigation"
            ]
        ],
        "abstract": "We introduce a novel approach for safe control design based on the density function. A control density function (CDF) is introduced to synthesize a safe controller for a nonlinear dynamic system. The CDF can be viewed as a dual to the control barrier function (CBF), a popular approach used for safe control design. While the safety certificate using the barrier function is based on the notion of invariance, the dual certificate involving the density function has a physical interpretation of occupancy. This occupancy-based physical interpretation is instrumental in providing an analytical construction of density function used for safe control synthesis. The safe control design problem is formulated using the density function as a quadratic programming (QP) problem. In contrast to the QP proposed for control synthesis using CBF, the proposed CDF-based QP can combine both the safety and convergence conditions to target state into single constraints. Further, we consider robustness against uncertainty in system dynamics and the initial condition and provide theoretical results for robust navigation using the CDF. Finally, we present simulation results for safe navigation with single integrator and double-gyre fluid flow-field examples, followed by robust navigation using the bicycle model and autonomous lane-keeping examples.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05135",
        "abstract url": "https://arxiv.org/abs/2407.05135",
        "title": "Theory and Explicit Design of a Path Planner for an SE(3) Robot",
        "rating": "-1",
        "keywords": [
            [
                "robotics",
                "Robot"
            ]
        ],
        "abstract": "We consider path planning for a rigid spatial robot with 6 degrees of freedom (6 DOFs), moving amidst polyhedral obstacles. A correct, complete and practical path planner for such a robot has never been achieved, although this is widely recognized as a key challenge in robotics. This paper provides a complete \"explicit\" design, down to explicit geometric primitives that are easily implementable. Our design is within an algorithmic framework for path planners, called Soft Subdivision Search (SSS). The framework is based on the twin foundations of $\u03b5$-exactness and soft predicates, which are critical for rigorous numerical implementations. The practicality of SSS has been previously demonstrated for various robots including 5-DOF spatial robots. In this paper, we solve several significant technical challenges for SE(3) robots: (1) We first ensure the correct theory by proving a general form of the Fundamental Theorem of the SSS theory. We prove this within an axiomatic framework, thus making it easy for future applications of this theory. (2) One component of $SE(3) = R^3 \\times SO(3)$ is the non-Euclidean, non-orientable space SO(3). We design a novel topologically correct data structure for SO(3). Using the concept of subdivision charts and atlases for SO(3), we can now carry out subdivision of SO(3). (3) The geometric problem of collision detection takes place in $R^3$, via the footprint map. Unlike sampling-based approaches, we must reason with the notion of footprints of configuration boxes, which is much harder to characterize. Exploiting the theory of soft predicates, we design suitable approximate footprints which, when combined with the highly effective feature-set technique, lead to soft predicates. (4) Finally, we make the underlying geometric computation \"explicit\", i.e., avoiding a general solver of polynomial systems, in order to allow a direct implementation.",
        "subjects": [
            "cs.RO",
            "cs.CG"
        ],
        "comment": "An earlier version has been submitted for a conference publication and is under review. This is a full version, 39 pages, including appendices"
    },
    {
        "paper id": "2407.05148",
        "abstract url": "https://arxiv.org/abs/2407.05148",
        "title": "Learning Velocity-based Humanoid Locomotion: Massively Parallel Learning with Brax and MJX",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Humanoid locomotion is a key skill to bring humanoids out of the lab and into the real-world. Many motion generation methods for locomotion have been proposed including reinforcement learning (RL). RL locomotion policies offer great versatility and generalizability along with the ability to experience new knowledge to improve over time. This work presents a velocity-based RL locomotion policy for the REEM-C robot. The policy uses a periodic reward formulation and is implemented in Brax/MJX for fast training. Simulation results for the policy are demonstrated with future experimental results in progress.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05219",
        "abstract url": "https://arxiv.org/abs/2407.05219",
        "title": "Flood of Techniques and Drought of Theories: Emotion Mining in Disasters",
        "rating": "-1",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Emotion mining has become a crucial tool for understanding human emotions during disasters, leveraging the extensive data generated on social media platforms. This paper aims to summarize existing research on emotion mining within disaster contexts, highlighting both significant discoveries and persistent issues. On the one hand, emotion mining techniques have achieved acceptable accuracy enabling applications such as rapid damage assessment and mental health surveillance. On the other hand, with many studies adopting data-driven approaches, several methodological issues remain. These include arbitrary emotion classification, ignoring biases inherent in data collection from social media, such as the overrepresentation of individuals from higher socioeconomic status on Twitter, and the lack of application of theoretical frameworks like cross-cultural comparisons. These problems can be summarized as a notable lack of theory-driven research and ignoring insights from social and behavioral sciences. This paper underscores the need for interdisciplinary collaboration between computer scientists and social scientists to develop more robust and theoretically grounded approaches in emotion mining. By addressing these gaps, we aim to enhance the effectiveness and reliability of emotion mining methodologies, ultimately contributing to improved disaster preparedness, response, and recovery. Keywords: emotion mining, sentiment analysis, natural disasters, psychology, technological disasters",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05238",
        "abstract url": "https://arxiv.org/abs/2407.05238",
        "title": "P2P: Part-to-Part Motion Cues Guide a Strong Tracking Framework for LiDAR Point Clouds",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "voxel"
            ],
            [
                "LiDAR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D single object tracking (SOT) methods based on appearance matching has long suffered from insufficient appearance information incurred by incomplete, textureless and semantically deficient LiDAR point clouds. While motion paradigm exploits motion cues instead of appearance matching for tracking, it incurs complex multi-stage processing and segmentation module. In this paper, we first provide in-depth explorations on motion paradigm, which proves that (\\textbf{i}) it is feasible to directly infer target relative motion from point clouds across consecutive frames; (\\textbf{ii}) fine-grained information comparison between consecutive point clouds facilitates target motion modeling. We thereby propose to perform part-to-part motion modeling for consecutive point clouds and introduce a novel tracking framework, termed \\textbf{P2P}. The novel framework fuses each corresponding part information between consecutive point clouds, effectively exploring detailed information changes and thus modeling accurate target-related motion cues. Following this framework, we present P2P-point and P2P-voxel models, incorporating implicit and explicit part-to-part motion modeling by point- and voxel-based representation, respectively. Without bells and whistles, P2P-voxel sets a new state-of-the-art performance ($\\sim$\\textbf{89\\%}, \\textbf{72\\%} and \\textbf{63\\%} precision on KITTI, NuScenes and Waymo Open Dataset, respectively). Moreover, under the same point-based representation, P2P-point outperforms the previous motion tracker M$^2$Track by \\textbf{3.3\\%} and \\textbf{6.7\\%} on the KITTI and NuScenes, while running at a considerably high speed of \\textbf{107 Fps} on a single RTX3090 GPU. The source code and pre-trained models are available at \\url{https://github.com/haooozi/P2P}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The source code and pre-trained models are available at https://github.com/haooozi/P2P"
    },
    {
        "paper id": "2407.05240",
        "abstract url": "https://arxiv.org/abs/2407.05240",
        "title": "Neighborhood Stability in Assignments on Graphs",
        "rating": "-1",
        "keywords": [
            [
                "Graphs"
            ]
        ],
        "abstract": "We study the problem of assigning agents to the vertices of a graph such that no pair of neighbors can benefit from swapping assignments -- a property we term neighborhood stability. We further assume that agents' utilities are based solely on their preferences over the assignees of adjacent vertices and that those preferences are binary. Having shown that even this very restricted setting does not guarantee neighborhood stable assignments, we focus on special cases that provide such guarantees. We show that when the graph is a cycle or a path, a neighborhood stable assignment always exists for any preference profile. Furthermore, we give a general condition under which neighborhood stable assignments always exist. For each of these results, we give a polynomial-time algorithm to compute a neighborhood stable assignment.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05248",
        "abstract url": "https://arxiv.org/abs/2407.05248",
        "title": "Self-Paced Sample Selection for Barely-Supervised Medical Image Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "Medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The existing barely-supervised medical image segmentation (BSS) methods, adopting a registration-segmentation paradigm, aim to learn from data with very few annotations to mitigate the extreme label scarcity problem. However, this paradigm poses a challenge: pseudo-labels generated by image registration come with significant noise. To address this issue, we propose a self-paced sample selection framework (SPSS) for BSS. Specifically, SPSS comprises two main components: 1) self-paced uncertainty sample selection (SU) for explicitly improving the quality of pseudo labels in the image space, and 2) self-paced bidirectional feature contrastive learning (SC) for implicitly improving the quality of pseudo labels through enhancing the separability between class semantics in the feature space. Both SU and SC are trained collaboratively in a self-paced learning manner, ensuring that SPSS can learn from high-quality pseudo labels for BSS. Extensive experiments on two public medical image segmentation datasets demonstrate the effectiveness and superiority of SPSS over the state-of-the-art. Our code is release at https://github.com/SuuuJM/SPSS.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to MICCAI 2024"
    },
    {
        "paper id": "2407.05249",
        "abstract url": "https://arxiv.org/abs/2407.05249",
        "title": "RIS-assisted Coverage Enhancement in mmWave Integrated Sensing and Communication Networks",
        "rating": "-1",
        "keywords": [
            [
                "super-resolution"
            ]
        ],
        "abstract": "Integrated sensing and communication (ISAC) has emerged as a promising technology to facilitate high-rate communications and super-resolution sensing, particularly operating in the millimeter wave (mmWave) band. However, the vulnerability of mmWave signals to blockages severely impairs ISAC capabilities and coverage. To tackle this, an efficient and low-cost solution is to deploy distributed reconfigurable intelligent surfaces (RISs) to construct virtual links between the base stations (BSs) and users in a controllable fashion. In this paper, we investigate the generalized RIS-assisted mmWave ISAC networks considering the blockage effect, and examine the beneficial impact of RISs on the coverage rate utilizing stochastic geometry. Specifically, taking into account the coupling effect of ISAC dual functions within the same network topology, we derive the conditional coverage probability of ISAC performance for two association cases, based on the proposed beam pattern model and user association policies. Then, the marginal coverage rate is calculated by combining these two cases through the distance-dependent thinning method. Simulation results verify the accuracy of derived theoretical formulations and provide valuable guidelines for the practical network deployment. Specifically, our results indicate the superiority of the RIS deployment with the density of 40 km${}^{-2}$ BSs, and that the joint coverage rate of ISAC performance exhibits potential growth from $67.1\\%$ to $92.2\\%$ with the deployment of RISs.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05250",
        "abstract url": "https://arxiv.org/abs/2407.05250",
        "title": "CLIMB: A Benchmark of Clinical Bias in Large Language Models",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "Clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are increasingly applied to clinical decision-making. However, their potential to exhibit bias poses significant risks to clinical equity. Currently, there is a lack of benchmarks that systematically evaluate such clinical bias in LLMs. While in downstream tasks, some biases of LLMs can be avoided such as by instructing the model to answer \"I'm not sure...\", the internal bias hidden within the model still lacks deep studies. We introduce CLIMB (shorthand for A Benchmark of Clinical Bias in Large Language Models), a pioneering comprehensive benchmark to evaluate both intrinsic (within LLMs) and extrinsic (on downstream tasks) bias in LLMs for clinical decision tasks. Notably, for intrinsic bias, we introduce a novel metric, AssocMAD, to assess the disparities of LLMs across multiple demographic groups. Additionally, we leverage counterfactual intervention to evaluate extrinsic bias in a task of clinical diagnosis prediction. Our experiments across popular and medically adapted LLMs, particularly from the Mistral and LLaMA families, unveil prevalent behaviors with both intrinsic and extrinsic bias. This work underscores the critical need to mitigate clinical bias and sets a new standard for future evaluations of LLMs' clinical bias.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05255",
        "abstract url": "https://arxiv.org/abs/2407.05255",
        "title": "Estimation of the Area and Precipitation Associated with a Tropical Cyclone Biparjoy by using Image Processing",
        "rating": "-1",
        "keywords": [
            [
                "Remote Sensing",
                "satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The rainfall associated with Topical Cyclone(TC) contributes a major amount to the annual rainfall in India. Due to the limited research on the quantitative precipitation associated with Tropical Cyclones (TC), the prediction of the amount of precipitation and area that it may cover remains a challenge. This paper proposes an approach to estimate the accumulated precipitation and impact on affected area using Remote Sensing data. For this study, an instance of Extremely Severe Cyclonic Storm, Biparjoy that formed over the Arabian Sea and hit India in 2023 is considered in which we have used the satellite images of IMERG-Late Run of Global Precipitation Measurement (GPM). Image processing techniques were employed to identify and extract precipitation clusters linked to the cyclone. The results indicate that Biparjoy contributed a daily average rainfall of 53.14 mm/day across India and the Arabian Sea, with the Indian boundary receiving 11.59 mm/day, covering an extensive 411.76 thousand square kilometers. The localized intensity and variability observed in states like Gujarat, Rajasthan, Madhya Pradesh, and Uttar Pradesh highlight the need for tailored response measures, emphasizing the importance of further research to enhance predictive models and disaster readiness, crucial for building resilience against the diverse impacts of tropical cyclones.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2407.04970",
        "abstract url": "https://arxiv.org/abs/2407.04970",
        "title": "Idiographic Personality Gaussian Process for Psychological Assessment",
        "rating": "-1.5",
        "keywords": [
            [
                "diagnosis",
                "Psychological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We develop a novel measurement framework based on a Gaussian process coregionalization model to address a long-lasting debate in psychometrics: whether psychological features like personality share a common structure across the population, vary uniquely for individuals, or some combination. We propose the idiographic personality Gaussian process (IPGP) framework, an intermediate model that accommodates both shared trait structure across a population and \"idiographic\" deviations for individuals. IPGP leverages the Gaussian process coregionalization model to handle the grouped nature of battery responses, but adjusted to non-Gaussian ordinal data. We further exploit stochastic variational inference for efficient latent factor estimation required for idiographic modeling at scale. Using synthetic and real data, we show that IPGP improves both prediction of actual responses and estimation of individualized factor structures relative to existing benchmarks. In a third study, we show that IPGP also identifies unique clusters of personality taxonomies in real-world data, displaying great potential in advancing individualized approaches to psychological diagnosis and treatment.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "9 pages, 4 figures"
    },
    {
        "paper id": "2407.05098",
        "abstract url": "https://arxiv.org/abs/2407.05098",
        "title": "FedTSA: A Cluster-based Two-Stage Aggregation Method for Model-heterogeneous Federated Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "Federated Learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Despite extensive research into data heterogeneity in federated learning (FL), system heterogeneity remains a significant yet often overlooked challenge. Traditional FL approaches typically assume homogeneous hardware resources across FL clients, implying that clients can train a global model within a comparable time. However, in practical FL systems, clients often have heterogeneous resources, which impacts their capacity for training tasks. This discrepancy highlights the significance of exploring model-heterogeneous FL, a paradigm that allows clients to train different models based on their resource capabilities. To address this, we introduce FedTSA, a cluster-based two-stage aggregation method tailored for system heterogeneity in FL. FedTSA starts by clustering clients based on their capabilities, then conducts a two-stage aggregation, i.e., conventional weight averaging for homogeneous models as Stage 1, and deep mutual learning with a diffusion model for aggregating heterogeneous models as Stage 2. Extensive experiments not only show that FedTSA outperforms the baselines, but also explore various factors influencing model performance, thereby validating FedTSA as a promising approach for model-heterogeneous FL.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05102",
        "abstract url": "https://arxiv.org/abs/2407.05102",
        "title": "Towards Auto-Building of Embedded FPGA-based Soft Sensors for Wastewater Flow Estimation",
        "rating": "-1.5",
        "keywords": [
            [
                "IoT",
                "FPGA"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Executing flow estimation using Deep Learning (DL)-based soft sensors on resource-limited IoT devices has demonstrated promise in terms of reliability and energy efficiency. However, its application in the field of wastewater flow estimation remains underexplored due to: (1) a lack of available datasets, (2) inconvenient toolchains for on-device AI model development and deployment, and (3) hardware platforms designed for general DL purposes rather than being optimized for energy-efficient soft sensor applications. This study addresses these gaps by proposing an automated, end-to-end solution for wastewater flow estimation using a prototype IoT device.",
        "subjects": [
            "eess.SP",
            "cs.AI"
        ],
        "comment": "This paper is accepted by 2024 IEEE Annual Congress on Artificial Intelligence of Things (IEEE AIoT)"
    },
    {
        "paper id": "2407.05112",
        "abstract url": "https://arxiv.org/abs/2407.05112",
        "title": "Releasing Malevolence from Benevolence: The Menace of Benign Data on Machine Unlearning",
        "rating": "-1.5",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "attacks"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Machine learning models trained on vast amounts of real or synthetic data often achieve outstanding predictive performance across various domains. However, this utility comes with increasing concerns about privacy, as the training data may include sensitive information. To address these concerns, machine unlearning has been proposed to erase specific data samples from models. While some unlearning techniques efficiently remove data at low costs, recent research highlights vulnerabilities where malicious users could request unlearning on manipulated data to compromise the model. Despite these attacks' effectiveness, perturbed data differs from original training data, failing hash verification. Existing attacks on machine unlearning also suffer from practical limitations and require substantial additional knowledge and resources. To fill the gaps in current unlearning attacks, we introduce the Unlearning Usability Attack. This model-agnostic, unlearning-agnostic, and budget-friendly attack distills data distribution information into a small set of benign data. These data are identified as benign by automatic poisoning detection tools due to their positive impact on model training. While benign for machine learning, unlearning these data significantly degrades model information. Our evaluation demonstrates that unlearning this benign data, comprising no more than 1% of the total training data, can reduce model accuracy by up to 50%. Furthermore, our findings show that well-prepared benign data poses challenges for recent unlearning techniques, as erasing these synthetic instances demands higher resources than regular data. These insights underscore the need for future research to reconsider \"data poisoning\" in the context of machine unlearning.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05153",
        "abstract url": "https://arxiv.org/abs/2407.05153",
        "title": "Lucy: Think and Reason to Solve Text-to-SQL",
        "rating": "-1.5",
        "keywords": [
            [
                "SQL"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have made significant progress in assisting users to query databases in natural language. While LLM-based techniques provide state-of-the-art results on many standard benchmarks, their performance significantly drops when applied to large enterprise databases. The reason is that these databases have a large number of tables with complex relationships that are challenging for LLMs to reason about. We analyze challenges that LLMs face in these settings and propose a new solution that combines the power of LLMs in understanding questions with automated reasoning techniques to handle complex database constraints. Based on these ideas, we have developed a new framework that outperforms state-of-the-art techniques in zero-shot text-to-SQL on complex benchmarks",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05198",
        "abstract url": "https://arxiv.org/abs/2407.05198",
        "title": "Medfluencer: A Network Representation of Medical Influencers' Identities and Discourse on Social Media",
        "rating": "-1.5",
        "keywords": [
            [
                "Medical",
                "health"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "In our study, we first constructed a dataset from the tweets of the top 100 medical influencers with the highest Influencer Score during the COVID-19 pandemic. This dataset was then used to construct a socio-semantic network, mapping both their identities and key topics, which are crucial for understanding their impact on public health discourse. To achieve this, we developed a few-shot multi-label classifier to identify influencers and their network actors' identities, employed BERTopic for extracting thematic content, and integrated these components into a network model to analyze their impact on health discourse.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "ACM SIGKDD 2024 Workshop epiDAMIK 2024: The 7th International Workshop on Epidemiology meets Data Mining and Knowledge Discovery"
    },
    {
        "paper id": "2407.05224",
        "abstract url": "https://arxiv.org/abs/2407.05224",
        "title": "On the importance of learning non-local dynamics for stable data-driven climate modeling: A 1D gravity wave-QBO testbed",
        "rating": "-1.5",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine learning (ML) techniques, especially neural networks (NNs), have shown promise in learning subgrid-scale (SGS) parameterizations for climate modeling. However, a major problem with data-driven parameterizations, particularly those learned with supervised algorithms, is instability when integrated with numerical solvers of large-scale processes. Current remedies are often ad-hoc and lack a theoretical foundation. Here, we combine ML theory and climate physics to address a source of instability in NN-based parameterization. We demonstrate the importance of learning spatially non-local dynamics using a 1D model of the quasi-biennial oscillation (QBO) with gravity wave (GW) parameterization as a testbed. While common offline metrics fail to identify shortcomings in learning non-local dynamics, we show that the receptive field (RF)-the region of the input an NN uses to predict an output-can identify instability a-priori. We find that NN-based parameterizations that seem to accurately predict GW forcings from wind profiles ($\\mathbf{R^2 \\approx 0.99}$) cause unstable simulations when RF is too small to capture the non-local dynamics, while NNs of the same size but large-enough RF are stable. Some architectures, e.g., Fourier neural operators, have inherently large RF. We also demonstrate that learning non-local dynamics can be crucial for the stability and accuracy of a data-driven spatiotemporal emulator of the entire zonal wind field. Given the ubiquity of non-local dynamics in the climate system, we expect the use of effective RF, which can be computed for any NN architecture, to be important for many applications. This work highlights the need to integrate ML theory with physics for designing/analyzing data-driven algorithms for weather/climate modeling.",
        "subjects": [
            "physics.ao-ph",
            "cs.LG"
        ],
        "comment": "14 pages, 6 figures"
    },
    {
        "paper id": "2407.04961",
        "abstract url": "https://arxiv.org/abs/2407.04961",
        "title": "A PRISMA-Driven Bibliometric Analysis of the Scientific Literature on Assurance Case Patterns",
        "rating": "-2",
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "Justifying the correct implementation of the non-functional requirements (e.g., safety, security) of mission-critical systems is crucial to prevent system failure. The later could have severe consequences such as the death of people and financial losses. Assurance cases can be used to prevent system failure, They are structured arguments that allow arguing and relaying various safety-critical systems' requirements extensively as well as checking the compliance of such systems with industrial standards to support their certification. Still, the creation of assurance cases is usually manual, error-prone, and time-consuming. Besides, it may involve numerous alterations as the system evolves. To overcome the bottlenecks in creating assurance cases, existing approaches usually promote the reuse of common structured evidence-based arguments (i.e. patterns) to aid the creation of assurance cases. To gain insights into the advancements of the research on assurance case patterns, we relied on SEGRESS to conduct a bibliometric analysis of 92 primary studies published within the past two decades. This allows capturing the evolutionary trends and patterns characterizing the research in that field. Our findings notably indicate the emergence of new assurance case patterns to support the assurance of ML-enabled systems that are characterized by their evolving requirements (e.g., cybersecurity and ethics).",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05020",
        "abstract url": "https://arxiv.org/abs/2407.05020",
        "title": "A game theory analysis of decentralized epidemic management with opinion dynamics",
        "rating": "-2",
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "In this paper, we introduce a static game that allows one to numerically assess the loss of efficiency induced by decentralized control or management of a global epidemic. Each player represents a region which is assumed to choose its control to implement a tradeoff between socio-economic aspects and health aspects; the control comprises both epidemic control physical measures and influence actions on the region opinion. The Generalized Nash equilibrium $(\\mathrm{GNE})$ analysis of the proposed game model is conducted. The direct analysis of this game of practical interest is non-trivial but it turns out that one can construct an auxiliary game which allows one: to prove existence and uniqueness; to compute the GNE and the optimal centralized solution (sum-cost) of the game. These results allow us to assess numerically the loss (measured in terms of Price of Anarchy ($\\mathrm{PoA}$)) induced by decentralization with or without taking into account the opinion dynamics.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05027",
        "abstract url": "https://arxiv.org/abs/2407.05027",
        "title": "Listen-While-Talking: Toward dApp-based Real-Time Spectrum Sharing in O-RAN",
        "rating": "-2",
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "This demo paper presents a dApp-based real-time spectrum sharing scenario where a 5th generation (5G) base station implementing the NR stack adapts its transmission and reception strategies based on the incumbent priority users in the Citizen Broadband Radio Service (CBRS) band. The dApp is responsible for obtaining relevant measurements from the Next Generation Node Base (gNB), running the spectrum sensing inference, and configuring the gNB with a control action upon detecting the primary incumbent user transmissions. This approach is built on dApps, which extend the O-RAN framework to the real-time and user plane domains. Thus, it avoids the need of dedicated Spectrum Access Systems (SASs) in the CBRS band. The demonstration setup is based on the open-source 5G OpenAirInterface (OAI) framework, where we have implemented a dApp interfaced with a gNB and communicating with a Commercial Off-the-Shelf (COTS) User Equipment (UE) in an over-the-air wireless environment. When an incumbent user has active transmission, the dApp will detect and inform the primary user presence to the gNB. The dApps will also enforce a control policy that adapts the scheduling and transmission policy of the Radio Access Network (RAN). This demo provides valuable insights into the potential of using dApp-based spectrum sensing with O-RAN architecture in next generation cellular networks.",
        "subjects": [
            "cs.NI",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05033",
        "abstract url": "https://arxiv.org/abs/2407.05033",
        "title": "Preference Distillation for Personalized Generative Recommendation",
        "rating": "-2",
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "Recently, researchers have investigated the capabilities of Large Language Models (LLMs) for generative recommender systems. Existing LLM-based recommender models are trained by adding user and item IDs to a discrete prompt template. However, the disconnect between IDs and natural language makes it difficult for the LLM to learn the relationship between users. To address this issue, we propose a PErsonAlized PrOmpt Distillation (PeaPOD) approach, to distill user preferences as personalized soft prompts. Considering the complexities of user preferences in the real world, we maintain a shared set of learnable prompts that are dynamically weighted based on the user's interests to construct the user-personalized prompt in a compositional manner. Experimental results on three real-world datasets demonstrate the effectiveness of our PeaPOD model on sequential recommendation, top-n recommendation, and explanation generation tasks.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05045",
        "abstract url": "https://arxiv.org/abs/2407.05045",
        "title": "Robust Skin Color Driven Privacy Preserving Face Recognition via Function Secret Sharing",
        "rating": "-2",
        "keywords": [
            [
                "GAN"
            ],
            [
                "image restoration"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this work, we leverage the pure skin color patch from the face image as the additional information to train an auxiliary skin color feature extractor and face recognition model in parallel to improve performance of state-of-the-art (SOTA) privacy-preserving face recognition (PPFR) systems. Our solution is robust against black-box attacking and well-established generative adversarial network (GAN) based image restoration. We analyze the potential risk in previous work, where the proposed cosine similarity computation might directly leak the protected precomputed embedding stored on the server side. We propose a Function Secret Sharing (FSS) based face embedding comparison protocol without any intermediate result leakage. In addition, we show in experiments that the proposed protocol is more efficient compared to the Secret Sharing (SS) based protocol.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at ICIP2024"
    },
    {
        "paper id": "2407.05087",
        "abstract url": "https://arxiv.org/abs/2407.05087",
        "title": "Linear Attention Based Deep Nonlocal Means Filtering for Multiplicative Noise Removal",
        "rating": "-2",
        "keywords": [
            [
                "radar"
            ],
            [
                "medical"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Multiplicative noise widely exists in radar images, medical images and other important fields' images. Compared to normal noises, multiplicative noise has a generally stronger effect on the visual expression of images. Aiming at the denoising problem of multiplicative noise, we linearize the nonlocal means algorithm with deep learning and propose a linear attention mechanism based deep nonlocal means filtering (LDNLM). Starting from the traditional nonlocal means filtering, we employ deep channel convolution neural networks to extract the information of the neighborhood matrix and obtain representation vectors of every pixel. Then we replace the similarity calculation and weighted averaging processes with the inner operations of the attention mechanism. To reduce the computational overhead, through the formula of similarity calculation and weighted averaging, we derive a nonlocal filter with linear complexity. Experiments on both simulated and real multiplicative noise demonstrate that the LDNLM is more competitive compared with the state-of-the-art methods. Additionally, we prove that the LDNLM possesses interpretability close to traditional NLM.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05088",
        "abstract url": "https://arxiv.org/abs/2407.05088",
        "title": "Leveraging Task-Specific Knowledge from LLM for Semi-Supervised 3D Medical Image Segmentation",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "voxel"
            ],
            [
                "Medical",
                "CT"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Traditional supervised 3D medical image segmentation models need voxel-level annotations, which require huge human effort, time, and cost. Semi-supervised learning (SSL) addresses this limitation of supervised learning by facilitating learning with a limited annotated and larger amount of unannotated training samples. However, state-of-the-art SSL models still struggle to fully exploit the potential of learning from unannotated samples. To facilitate effective learning from unannotated data, we introduce LLM-SegNet, which exploits a large language model (LLM) to integrate task-specific knowledge into our co-training framework. This knowledge aids the model in comprehensively understanding the features of the region of interest (ROI), ultimately leading to more efficient segmentation. Additionally, to further reduce erroneous segmentation, we propose a Unified Segmentation loss function. This loss function reduces erroneous segmentation by not only prioritizing regions where the model is confident in predicting between foreground or background pixels but also effectively addressing areas where the model lacks high confidence in predictions. Experiments on publicly available Left Atrium, Pancreas-CT, and Brats-19 datasets demonstrate the superior performance of LLM-SegNet compared to the state-of-the-art. Furthermore, we conducted several ablation studies to demonstrate the effectiveness of various modules and loss functions leveraged by LLM-SegNet.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "Under Review"
    },
    {
        "paper id": "2407.05155",
        "abstract url": "https://arxiv.org/abs/2407.05155",
        "title": "Wi-Fi Beyond Communications: Experimental Evaluation of Respiration Monitoring and Motion Detection Using COTS Devices",
        "rating": "-2",
        "keywords": [
            [
                "healthcare"
            ]
        ],
        "abstract": "Wi-Fi sensing has become an attractive option for non-invasive monitoring of human activities and vital signs. This paper explores the feasibility of using state-of-the-art commercial off-the-shelf (COTS) devices for Wi-Fi sensing applications, particularly respiration monitoring and motion detection. We utilize the Intel AX210 network interface card (NIC) to transmit Wi-Fi signals in both 2.4 GHz and 6 GHz frequency bands. Our experiments rely on channel frequency response (CFR) and received signal strength indicator (RSSI) data, which are processed using a moving average algorithm to extract human behavior patterns. The experimental results demonstrate the effectiveness of our approach in capturing and representing human respiration and motion patterns. Furthermore, we compare the performance of Wi-Fi sensing across different frequency bands, highlighting the advantages of using higher frequencies for improved sensitivity and clarity. Our findings showcase the practicality of using COTS devices for Wi-Fi sensing and lay the groundwork for the development of non-invasive, contactless sensing systems. These systems have potential applications in various fields, including healthcare, smart homes, and Metaverse.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "This work has been accepted by IEEE ICCC Workshop 2024. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2407.05163",
        "abstract url": "https://arxiv.org/abs/2407.05163",
        "title": "A Domain Adaptation Model for Carotid Ultrasound: Image Harmonization, Noise Reduction, and Impact on Cardiovascular Risk Markers",
        "rating": "-2",
        "keywords": [
            [
                "GAN"
            ],
            [
                "medical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Deep learning has been used extensively for medical image analysis applications, assuming the training and test data adhere to the same probability distributions. However, a common challenge arises when dealing with medical images generated by different systems or even the same system with varying parameter settings. Such images often contain diverse textures and noise patterns, violating the assumption. Consequently, models trained on data from one machine or setting usually struggle to perform effectively on data from another. To address this issue in ultrasound images, we proposed a Generative Adversarial Network (GAN) based model in this paper. We formulated image harmonization and denoising tasks as an image-to-image translation task, wherein we modified the texture pattern and reduced noise in Carotid ultrasound images while keeping the image content (the anatomy) unchanged. The performance was evaluated using feature distribution and pixel-space similarity metrics. In addition, blood-to-tissue contrast and influence on computed risk markers (Gray scale median, GSM) were evaluated. The results showed that domain adaptation was achieved in both tasks (histogram correlation 0.920 and 0.844), as compared to no adaptation (0.890 and 0.707), and that the anatomy of the images was retained (structure similarity index measure of the arterial wall 0.71 and 0.80). In addition, the image noise level (contrast) did not change in the image harmonization task (-34.1 vs 35.2 dB) but was improved in the noise reduction task (-23.5 vs -46.7 dB). The model outperformed the CycleGAN in both tasks. Finally, the risk marker GSM increased by 7.6 (p<0.001) in task 1 but not in task 2. We conclude that domain translation models are powerful tools for ultrasound image improvement while retaining the underlying anatomy but that downstream calculations of risk markers may be affected.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "17 pages, 7 figures, 7 tables"
    },
    {
        "paper id": "2407.05196",
        "abstract url": "https://arxiv.org/abs/2407.05196",
        "title": "Collective Upkeep",
        "rating": "-2",
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "We design mechanisms for maintaining public goods which require periodic non-monetary contributions. Utilitarian welfare is maximized by concentrating contributions among low-cost group members, but such policies generally induce some members to leave the group or misreport their preferences. To forestall exit, contributions must be shifted from members with intermediate costs to some high-cost members. To deter misreporting, members must be screened using up to two membership tiers, which reward larger contributions with increased access to the good. We apply our results to the design of platforms such as Netflix and TikTok hosting crowd-sourced recommendation engines, which function as public goods supported by user feedback about new content.",
        "subjects": [
            "econ.TH",
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05221",
        "abstract url": "https://arxiv.org/abs/2407.05221",
        "title": "Ensemble Boost: Greedy Selection for Superior Recommender Systems",
        "rating": "-2",
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "Ensemble techniques have demonstrated remarkable success in improving predictive performance across various domains by aggregating predictions from multiple models [1]. In the realm of recommender systems, this research explores the application of ensemble technique to enhance recommendation quality. Specifically, we propose a novel approach to combine top-k recommendations from ten diverse recommendation models resulting in superior top-n recommendations using this novel ensemble technique. Our method leverages a Greedy Ensemble Selection(GES) strategy, effectively harnessing the collective intelligence of multiple models. We conduct experiments on five distinct datasets to evaluate the effectiveness of our approach. Evaluation across five folds using the NDCG metric reveals significant improvements in recommendation accuracy across all datasets compared to single best performing model. Furthermore, comprehensive comparisons against existing models underscore the efficacy of our ensemble approach in enhancing recommendation quality. Our ensemble approach yielded an average improvement of 21.67% across different NDCG@N metrics and the five datasets, compared to single best model. The popularity recommendation model serves as the baseline for comparison. This research contributes to the advancement of ensemble-based recommender systems, offering insights into the potential of combining diverse recommendation strategies to enhance user experience and satisfaction. By presenting a novel approach and demonstrating its superiority over existing methods, we aim to inspire further exploration and innovation in this domain.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05232",
        "abstract url": "https://arxiv.org/abs/2407.05232",
        "title": "PAPM: A Physics-aware Proxy Model for Process Systems",
        "rating": "-2",
        "keywords": [
            [
                "depth"
            ],
            [
                "Physics"
            ],
            [
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "In the context of proxy modeling for process systems, traditional data-driven deep learning approaches frequently encounter significant challenges, such as substantial training costs induced by large amounts of data, and limited generalization capabilities. As a promising alternative, physics-aware models incorporate partial physics knowledge to ameliorate these challenges. Although demonstrating efficacy, they fall short in terms of exploration depth and universality. To address these shortcomings, we introduce a physics-aware proxy model (PAPM) that fully incorporates partial prior physics of process systems, which includes multiple input conditions and the general form of conservation relations, resulting in better out-of-sample generalization. Additionally, PAPM contains a holistic temporal-spatial stepping module for flexible adaptation across various process systems. Through systematic comparisons with state-of-the-art pure data-driven and physics-aware models across five two-dimensional benchmarks in nine generalization tasks, PAPM notably achieves an average performance improvement of 6.7%, while requiring fewer FLOPs, and just 1% of the parameters compared to the prior leading method. The code is available at https://github.com/pengwei07/PAPM.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "ICML 2024"
    },
    {
        "paper id": "2407.05235",
        "abstract url": "https://arxiv.org/abs/2407.05235",
        "title": "Tracking Reflected Objects: A Benchmark",
        "rating": "-2",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "industrial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Visual tracking has advanced significantly in recent years, mainly due to the availability of large-scale training datasets. These datasets have enabled the development of numerous algorithms that can track objects with high accuracy and robustness.However, the majority of current research has been directed towards tracking generic objects, with less emphasis on more specialized and challenging scenarios. One such challenging scenario involves tracking reflected objects. Reflections can significantly distort the appearance of objects, creating ambiguous visual cues that complicate the tracking process. This issue is particularly pertinent in applications such as autonomous driving, security, smart homes, and industrial production, where accurately tracking objects reflected in surfaces like mirrors or glass is crucial. To address this gap, we introduce TRO, a benchmark specifically for Tracking Reflected Objects. TRO includes 200 sequences with around 70,000 frames, each carefully annotated with bounding boxes. This dataset aims to encourage the development of new, accurate methods for tracking reflected objects, which present unique challenges not sufficiently covered by existing benchmarks. We evaluated 20 state-of-the-art trackers and found that they struggle with the complexities of reflections. To provide a stronger baseline, we propose a new tracker, HiP-HaTrack, which uses hierarchical features to improve performance, significantly outperforming existing algorithms. We believe our benchmark, evaluation, and HiP-HaTrack will inspire further research and applications in tracking reflected objects. The TRO and code are available at https://github.com/OpenCodeGithub/HIP-HaTrack.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05242",
        "abstract url": "https://arxiv.org/abs/2407.05242",
        "title": "Abdominal aortic aneurysm wall stress: A 7-line code in MATLAB and a one-click software application",
        "rating": "-2",
        "keywords": [
            [
                "biomechanical",
                "medical",
                "health",
                "clinical"
            ]
        ],
        "abstract": "An abdominal aortic aneurysm (AAA) is a life-threatening condition characterized by the irreversible dilation of the lower aorta, usually detected incidentally during imaging for other health issues. Current clinical practice for managing AAA relies on a one-size-fits-all approach, based on the aneurysm's maximum diameter and growth rate, which can lead to underestimation or overestimation of AAA rupture risk. Patient-specific AAA wall stress, computed using biomechanical models derived from medical images without needing patient-specific material properties, has been widely investigated for developing individualized AAA rupture risk predictors. Therefore, AAA wall stress, determined reliably and quickly, has the potential to enhance patient-specific treatment plans. This paper presents a 7-line code, written in MATLAB using the Partial Differential Equation Toolbox, for AAA wall stress computations via finite element analysis. The code takes AAA wall geometry as input and outputs stress components over the AAA wall domain. Additionally, we present a one-click standalone software application for AAA wall stress computation, developed based on our 7-line code using MATLAB Compiler. After verification, we used our code to compute AAA wall stress in ten patients. Our analysis indicated that the 99th percentile of maximum principal stress across all patients ranged from 0.320 MPa to 0.522 MPa, with an average of 0.401 MPa and a standard deviation of 0.056 MPa. Moreover, for every case, the MATLAB simulation time was less than a minute on a laptop workstation.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05247",
        "abstract url": "https://arxiv.org/abs/2407.05247",
        "title": "An efficient pipeline to compute patient-specific cerebral aneurysm wall tension",
        "rating": "-2",
        "keywords": [
            [
                "biomechanics-based",
                "diagnosis",
                "clinical"
            ]
        ],
        "abstract": "Cerebral aneurysm rupture, leading to subarachnoid hemorrhage with a high mortality rate, disproportionately affects younger populations, resulting in a significant loss of productive life years. A significant proportion of these deaths is due to aneurysmal re-bleeding within the first three days following the initial bleed, prior to treatment. While early aneurysm treatment is recommended, there is no consensus on the ideal timing, and emergency treatment offers only an incremental benefit at a significant cost. Although various multivariable prediction models have been proposed to provide personalized risk assessments, no validated patient-specific predictor is available to rationalize emergency treatment. Furthermore, no model has yet incorporated emerging computational biomechanics-based biomarkers such as wall tension. In this paper, we proposed and validated an efficient semi-automatic pipeline to compute patient-specific cerebral aneurysm wall tension as a potential biomarker for the likelihood of re-bleeding. Our pipeline uses the patient's computed tomography angiography (CTA) image obtained at the time of subarachnoid hemorrhage diagnosis to create a patient-specific biomechanical model of the cerebral aneurysm using the finite element method. A distinctive feature of our approach is the straightforward model creation and wall tension computation using shell finite elements, without requiring patient-specific material properties or aneurysm wall thickness. Our non-invasive, patient-specific method for cerebral aneurysm wall tension can potentially provide individualized risk prediction and enhance clinical decision-making.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05126",
        "abstract url": "https://arxiv.org/abs/2407.05126",
        "title": "Consistency and Discrepancy-Based Contrastive Tripartite Graph Learning for Recommendations",
        "rating": "-2.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "Recommendation"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Tripartite graph-based recommender systems markedly diverge from traditional models by recommending unique combinations such as user groups and item bundles. Despite their effectiveness, these systems exacerbate the longstanding cold-start problem in traditional recommender systems, because any number of user groups or item bundles can be formed among users or items. To address this issue, we introduce a Consistency and Discrepancy-based graph contrastive learning method for tripartite graph-based Recommendation. This approach leverages two novel meta-path-based metrics consistency and discrepancy to capture nuanced, implicit associations between the recommended objects and the recommendees. These metrics, indicative of high-order similarities, can be efficiently calculated with infinite graph convolutional networks layers under a multi-objective optimization framework, using the limit theory of GCN.",
        "subjects": [
            "cs.IR",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05197",
        "abstract url": "https://arxiv.org/abs/2407.05197",
        "title": "A Generalized Transformer-based Radio Link Failure Prediction Framework in 5G RANs",
        "rating": "-2.5",
        "keywords": [
            [
                "GNN",
                "graph"
            ],
            [
                "5G"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Radio link failure (RLF) prediction system in Radio Access Networks (RANs) is critical for ensuring seamless communication and meeting the stringent requirements of high data rates, low latency, and improved reliability in 5G networks. However, weather conditions such as precipitation, humidity, temperature, and wind impact these communication links. Usually, historical radio link Key Performance Indicators (KPIs) and their surrounding weather station observations are utilized for building learning-based RLF prediction models. However, such models must be capable of learning the spatial weather context in a dynamic RAN and effectively encoding time series KPIs with the weather observation data. Existing works fail to incorporate both of these essential design aspects of the prediction models. This paper fills the gap by proposing GenTrap, a novel RLF prediction framework that introduces a graph neural network (GNN)-based learnable weather effect aggregation module and employs state-of-the-art time series transformer as the temporal feature extractor for radio link failure prediction. The proposed aggregation method of GenTrap can be integrated into any existing prediction model to achieve better performance and generalizability. We evaluate GenTrap on two real-world datasets (rural and urban) with 2.6 million KPI data points and show that GenTrap offers a significantly higher F1-score (0.93 for rural and 0.79 for urban) compared to its counterparts while possessing generalization capability.",
        "subjects": [
            "cs.AI",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05017",
        "abstract url": "https://arxiv.org/abs/2407.05017",
        "title": "VIPS-Odom: Visual-Inertial Odometry Tightly-coupled with Parking Slots for Autonomous Parking",
        "rating": "-3",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "BEV"
            ]
        ],
        "abstract": "Precise localization is of great importance for autonomous parking task since it provides service for the downstream planning and control modules, which significantly affects the system performance. For parking scenarios, dynamic lighting, sparse textures, and the instability of global positioning system (GPS) signals pose challenges for most traditional localization methods. To address these difficulties, we propose VIPS-Odom, a novel semantic visual-inertial odometry framework for underground autonomous parking, which adopts tightly-coupled optimization to fuse measurements from multi-modal sensors and solves odometry. Our VIPS-Odom integrates parking slots detected from the synthesized bird-eye-view (BEV) image with traditional feature points in the frontend, and conducts tightly-coupled optimization with joint constraints introduced by measurements from the inertial measurement unit, wheel speed sensor and parking slots in the backend. We develop a multi-object tracking framework to robustly track parking slots' states. To prove the superiority of our method, we equip an electronic vehicle with related sensors and build an experimental platform based on ROS2 system. Extensive experiments demonstrate the efficacy and advantages of our method compared with other baselines for parking scenarios.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "A SLAM Method for Autonomous Parking"
    },
    {
        "paper id": "2407.05023",
        "abstract url": "https://arxiv.org/abs/2407.05023",
        "title": "SurgicalGaussian: Deformable 3D Gaussians for High-Fidelity Surgical Scene Reconstruction",
        "rating": "-3",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "depth",
                "radiance fields"
            ],
            [
                "robot"
            ],
            [
                "Surgical",
                "surgery",
                "endoscopic"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Dynamic reconstruction of deformable tissues in endoscopic video is a key technology for robot-assisted surgery. Recent reconstruction methods based on neural radiance fields (NeRFs) have achieved remarkable results in the reconstruction of surgical scenes. However, based on implicit representation, NeRFs struggle to capture the intricate details of objects in the scene and cannot achieve real-time rendering. In addition, restricted single view perception and occluded instruments also propose special challenges in surgical scene reconstruction. To address these issues, we develop SurgicalGaussian, a deformable 3D Gaussian Splatting method to model dynamic surgical scenes. Our approach models the spatio-temporal features of soft tissues at each time stamp via a forward-mapping deformation MLP and regularization to constrain local 3D Gaussians to comply with consistent movement. With the depth initialization strategy and tool mask-guided training, our method can remove surgical instruments and reconstruct high-fidelity surgical scenes. Through experiments on various surgical videos, our network outperforms existing method on many aspects, including rendering quality, rendering speed and GPU usage. The project page can be found at https://surgicalgaussian.github.io.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05053",
        "abstract url": "https://arxiv.org/abs/2407.05053",
        "title": "Adaptive Stiffness: A Biomimetic Robotic System with Tensegrity-Based Compliant Mechanism",
        "rating": "-3",
        "keywords": [
            [
                "robotics",
                "robot"
            ],
            [
                "Biomimetic"
            ]
        ],
        "abstract": "Biomimicry has played a pivotal role in robotics. In contrast to rigid robots, bio-inspired robots exhibit an inherent compliance, facilitating versatile movements and operations in constrained spaces. The robot implementation in fabrication, however, has posed technical challenges and mechanical complexity, thereby underscoring a noticeable gap between research and practice. To address the limitation, the research draws inspiration from the unique musculoskeletal feature of vertebrate physiology, which displays significant capabilities for sophisticated locomotion. The research converts the biological paradigm into a tensegrity-based robotic system, which is formed by the design of rigid-flex coupling and a compliant mechanism. This integrated technique enables the robot to achieve a wide range of motions with variable stiffness and adaptability, holding great potential for advanced performance in ill-defined environments. In summation, the research aims to provide a robust foundation for tensegrity-based biomimetic robots in practice, enhancing the feasibility of undertaking intricate robotic constructions.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": "14 pages, 21 figures"
    },
    {
        "paper id": "2407.05162",
        "abstract url": "https://arxiv.org/abs/2407.05162",
        "title": "Low-depth Quantum Circuit Decomposition of Multi-controlled Gates",
        "rating": "-3",
        "keywords": [
            [
                "depth"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "Multi-controlled gates are fundamental components in the design of quantum algorithms, where efficient decompositions of these operators can enhance algorithm performance. The best asymptotic decomposition of an n-controlled X gate with one borrowed ancilla into single qubit and CNOT gates produces circuits with degree 3 polylogarithmic depth and employs a divide-and-conquer strategy. In this paper, we reduce the number of recursive calls in the divide-and-conquer algorithm and decrease the depth of n-controlled X gate decomposition to a degree of 2.799 polylogarithmic depth. With this optimized decomposition, we also reduce the depth of n-controlled SU(2) gates and approximate n-controlled U(2) gates. Decompositions described in this work achieve the lowest asymptotic depth reported in the literature. We also perform an optimization in the base of the recursive approach. Starting at 52 control qubits, the proposed n-controlled X gate with one borrowed ancilla has the shortest circuit depth in the literature. One can reproduce all the results with the freely available open-source code provided in a public repository.",
        "subjects": [
            "quant-ph",
            "cs.ET"
        ],
        "comment": "6 pages, 8 figures"
    },
    {
        "paper id": "2407.05213",
        "abstract url": "https://arxiv.org/abs/2407.05213",
        "title": "BadCLM: Backdoor Attack in Clinical Language Models for Electronic Health Records",
        "rating": "-3",
        "keywords": [
            [
                "depth"
            ],
            [
                "Attack"
            ],
            [
                "Health",
                "Clinical"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The advent of clinical language models integrated into electronic health records (EHR) for clinical decision support has marked a significant advancement, leveraging the depth of clinical notes for improved decision-making. Despite their success, the potential vulnerabilities of these models remain largely unexplored. This paper delves into the realm of backdoor attacks on clinical language models, introducing an innovative attention-based backdoor attack method, BadCLM (Bad Clinical Language Models). This technique clandestinely embeds a backdoor within the models, causing them to produce incorrect predictions when a pre-defined trigger is present in inputs, while functioning accurately otherwise. We demonstrate the efficacy of BadCLM through an in-hospital mortality prediction task with MIMIC III dataset, showcasing its potential to compromise model integrity. Our findings illuminate a significant security risk in clinical decision support systems and pave the way for future endeavors in fortifying clinical language models against such vulnerabilities.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "AMIA 2024"
    },
    {
        "paper id": "2407.05029",
        "abstract url": "https://arxiv.org/abs/2407.05029",
        "title": "Ubiquitous Integrated Sensing and Communications for Massive MIMO LEO Satellite Systems",
        "rating": "-4",
        "keywords": [
            [
                "6G"
            ],
            [
                "Satellite"
            ]
        ],
        "abstract": "The next sixth generation (6G) networks are envisioned to integrate sensing and communications in a single system, thus greatly improving spectrum utilization and reducing hardware costs. Low earth orbit (LEO) satellite communications combined with massive multiple-input multiple-output (MIMO) technology holds significant promise in offering ubiquitous and seamless connectivity with high data rates. Existing integrated sensing and communications (ISAC) studies mainly focus on terrestrial systems, while operating ISAC in massive MIMO LEO satellite systems is promising to provide high-capacity communication and flexible sensing ubiquitously. In this paper, we first give an overview of LEO satellite systems and ISAC and consider adopting ISAC in the massive MIMO LEO satellite systems. Then, the recent research advances are presented. A discussion on related challenges and key enabling technologies follows. Finally, we point out some open issues and promising research directions.",
        "subjects": [
            "eess.SP",
            "cs.IT"
        ],
        "comment": "6 pages,4 figures"
    },
    {
        "paper id": "2407.05034",
        "abstract url": "https://arxiv.org/abs/2407.05034",
        "title": "GCON: Differentially Private Graph Convolutional Network via Objective Perturbation",
        "rating": "-4",
        "keywords": [
            [
                "Graph"
            ],
            [
                "attacks"
            ],
            [
                "healthcare"
            ]
        ],
        "abstract": "Graph Convolutional Networks (GCNs) are a popular machine learning model with a wide range of applications in graph analytics, including healthcare, transportation, and finance. Similar to other neural networks, a GCN may memorize parts of the training data through its model weights. Thus, when the underlying graph data contains sensitive information such as interpersonal relationships, a GCN trained without privacy-protection measures could be exploited to extract private data, leading to potential violations of privacy regulations such as GDPR. To defend against such attacks, a promising approach is to train the GCN with differential privacy (DP), which is a rigorous framework that provides strong privacy protection by injecting random noise into the trained model weights. However, training a large graph neural network under DP is a highly challenging task. Existing solutions either introduce random perturbations in the graph topology, which leads to severe distortions of the network's message passing, or inject randomness into each neighborhood aggregation operation, which leads to a high noise scale when the GCN performs multiple levels of aggregations. Motivated by this, we propose GCON, a novel and effective solution for training GCNs with edge differential privacy. The main idea is to (i) convert the GCN training process into a convex optimization problem, and then (ii) apply the classic idea of perturbing the objective function to satisfy DP. Extensive experiments using multiple benchmark datasets demonstrate GCON's consistent and superior performance over existing solutions in a wide variety of settings.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05048",
        "abstract url": "https://arxiv.org/abs/2407.05048",
        "title": "Cellular Automata as a Network Topology",
        "rating": "-4",
        "keywords": [
            [
                "biological"
            ],
            [
                "chemical"
            ]
        ],
        "abstract": "Cellular automata represent physical systems where both space and time are discrete, and the associated physical quantities assume a limited set of values. While previous research has applied cellular automata in modeling chemical, biological, and physical systems, its potential for modeling topological systems, specifically network topologies, remains underexplored. This paper investigates the use of cellular automata to model decentralized network topologies, which could enhance load balancing, fault tolerance, scalability, and the propagation and dissemination of information in distributed systems.",
        "subjects": [
            "nlin.CG",
            "cs.NI"
        ],
        "comment": "10 pages"
    },
    {
        "paper id": "2407.05059",
        "abstract url": "https://arxiv.org/abs/2407.05059",
        "title": "Slice-Consistent 3D Volumetric Brain CT-to-MRI Translation with 2D Brownian Bridge Diffusion Model",
        "rating": "-4",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "trajectory"
            ],
            [
                "medical",
                "MRI",
                "CT"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "In neuroimaging, generally, brain CT is more cost-effective and accessible imaging option compared to MRI. Nevertheless, CT exhibits inferior soft-tissue contrast and higher noise levels, yielding less precise structural clarity. In response, leveraging more readily available CT to construct its counterpart MRI, namely, medical image-to-image translation (I2I), serves as a promising solution. Particularly, while diffusion models (DMs) have recently risen as a powerhouse, they also come with a few practical caveats for medical I2I. First, DMs' inherent stochasticity from random noise sampling cannot guarantee consistent MRI generation that faithfully reflects its CT. Second, for 3D volumetric images which are prevalent in medical imaging, naively using 2D DMs leads to slice inconsistency, e.g., abnormal structural and brightness changes. While 3D DMs do exist, significant training costs and data dependency bring hesitation. As a solution, we propose novel style key conditioning (SKC) and inter-slice trajectory alignment (ISTA) sampling for the 2D Brownian bridge diffusion model. Specifically, SKC ensures a consistent imaging style (e.g., contrast) across slices, and ISTA interconnects the independent sampling of each slice, deterministically achieving style and shape consistent 3D CT-to-MRI translation. To the best of our knowledge, this study is the first to achieve high-quality 3D medical I2I based only on a 2D DM with no extra architectural models. Our experimental results show superior 3D medical I2I than existing 2D and 3D baselines, using in-house CT-MRI dataset and BraTS2023 FLAIR-T1 MRI dataset.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "13 pages, 7 figures, Early accepted at Medical Image Computing and Computer Assisted Intervention (MICCAI) 2024"
    },
    {
        "paper id": "2407.04959",
        "abstract url": "https://arxiv.org/abs/2407.04959",
        "title": "Embedding Digital Signature into CSV Files Using Data Hiding",
        "rating": "-10",
        "keywords": [],
        "abstract": "Open data is an important basis for open science and evidence-based policymaking. Governments of many countries disclose government-related statistics as open data. Some of these data are provided as CSV files. However, since CSV files are plain texts, we cannot ensure the integrity of a downloaded CSV file. A popular way to prove the data's integrity is a digital signature; however, it is difficult to embed a signature into a CSV file. This paper proposes a method for embedding a digital signature into a CSV file using a data hiding technique. The proposed method exploits a redundancy of the CSV format related to the use of double quotes. The experiment revealed we could embed a 512-bit signature into actual open data CSV files.",
        "subjects": [
            "cs.CR",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2407.04960",
        "abstract url": "https://arxiv.org/abs/2407.04960",
        "title": "MemoCRS: Memory-enhanced Sequential Conversational Recommender Systems with Large Language Models",
        "rating": "-10",
        "keywords": [],
        "abstract": "Conversational recommender systems (CRSs) aim to capture user preferences and provide personalized recommendations through multi-round natural language dialogues. However, most existing CRS models mainly focus on dialogue comprehension and preferences mining from the current dialogue session, overlooking user preferences in historical dialogue sessions. The preferences embedded in the user's historical dialogue sessions and the current session exhibit continuity and sequentiality, and we refer to CRSs with this characteristic as sequential CRSs. In this work, we leverage memory-enhanced LLMs to model the preference continuity, primarily focusing on addressing two key issues: (1) redundancy and noise in historical dialogue sessions, and (2) the cold-start users problem. To this end, we propose a Memory-enhanced Conversational Recommender System Framework with Large Language Models (dubbed MemoCRS) consisting of user-specific memory and general memory. User-specific memory is tailored to each user for their personalized interests and implemented by an entity-based memory bank to refine preferences and retrieve relevant memory, thereby reducing the redundancy and noise of historical sessions. The general memory, encapsulating collaborative knowledge and reasoning guidelines, can provide shared knowledge for users, especially cold-start users. With the two kinds of memory, LLMs are empowered to deliver more precise and tailored recommendations for each user. Extensive experiments on both Chinese and English datasets demonstrate the effectiveness of MemoCRS.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2407.04977",
        "abstract url": "https://arxiv.org/abs/2407.04977",
        "title": "Cost and Power-Consumption Analysis for Power Profile Monitoring with Multiple Monitors per Link in Optical Networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "Network monitoring is essential to collect compre-hensive data on signal quality in optical networks. As deploying large amounts of monitoring equipment results in elevated cost and power consumption, novel low-cost monitoring methods are continuously being investigated. A new technique called Power Profile Monitoring (PPM) has recently gained traction thanks to its ability to monitor an entire lightpath using a single post-processing unit at the lightpath receiver. PPM does not require to deploy an individual monitor for each span, as in the traditional monitoring technique using Optical Time-Domain Reflectometer (OTDR). PPM and OTDR have different monitoring applications, which will be elaborated in our discussion, hence they can be considered either alternative or complementary techniques according to the targeted monitoring capabilities to be implemented in the network. In this work, we aim to quantify the cost and power consumption of PPM (using OTDR as a baseline reference), as this analysis can provide guidelines for the implementation and deployment of PPM. First, we discuss how PPM and OTDR monitors are deployed, and we formally state a new Optimized Monitoring Placement (OMP) problem for PPM. Solving the OMP problem allows to identify the minimum number of PPM monitors that guarantees that all links in the networks are monitored by at least n PPM monitors (note that using n > 1 allows for increased monitoring accuracy). We prove the NP-hardness of the OMP problem and formulate it using an Integer Linear Programming (ILP) model. Finally, we also devise a heuristic algorithm for the OMP problem to scale to larger topologies. Our numerical results, obtained on realistic topologies, suggest that the cost (power) of one PPM module should be lower than 2.6 times and 10.2 times that of one OTDR for nation-wide and continental-wide topology, respectively.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05003",
        "abstract url": "https://arxiv.org/abs/2407.05003",
        "title": "DCitizens Roles Unveiled: SIG Navigating Identities in Digital Civics and the Spectrum of Societal Impact",
        "rating": "-10",
        "keywords": [],
        "abstract": "The DCitizens SIG aims to navigate ethical dimensions in forthcoming Digital Civics projects, ensuring enduring benefits and community resilience. Additionally, it seeks to shape the future landscape of digital civics for ethical and sustainable interventions. As we dive into these interactive processes, a challenge arises of discerning authentic intentions and validating perspectives. This exploration extends to evaluating the sustainability of future interactions and scrutinising biases impacting engaged communities. The commitment is to ensure future outcomes align with genuine community needs and address the ethical imperative of a considerate departure strategy. This dialogue encourages future researchers and practitioners to integrate ethical considerations and community-centric principles, fostering a more sustainable and responsible approach to technology-driven interventions in future urban regeneration and beyond.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05016",
        "abstract url": "https://arxiv.org/abs/2407.05016",
        "title": "Envisioning Collaborative Futures: Advancing the Frontiers of Embedded Research",
        "rating": "-10",
        "keywords": [],
        "abstract": "Participatory design initiatives, especially within the realm of digital civics, are often integrated and co-developed with the very citizens and communities they intend to assist. Digital civics research aims to create positive social change using a variety of digital technologies. These research projects commonly adopt various embedded processes, such as commissioning models \\cite{dcitizensproj22}. Despite the adoption of this process within a range of domains, there isn't currently a framework for best practices and accountability procedures to ensure we engage with citizens ethically and ensure the sustainability of our projects. This workshop aims to provide a space to start collaboratively constructing a dynamic framework of best practices, laying the groundwork for the future of sustainable embedded research processes. The overarching goal is to foster discussions and share insights that contribute to developing effective practices, ensuring the longevity and impact of participatory digital civics projects.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05028",
        "abstract url": "https://arxiv.org/abs/2407.05028",
        "title": "Testing Compositionality",
        "rating": "-10",
        "keywords": [],
        "abstract": "Compositionality supports the manipulation of large systems by working on their components. For model-based testing, this means that large systems can be tested by modelling and testing their components: passing tests for all components implies passing tests for the whole system. In previous work [13], we defined mutual acceptance for specification models and proved that this is a sufficient condition for compositionality in model-based testing. In this paper, we present an algorithm for verifying mutual acceptance on specifications and a sound and exhaustive model-based test procedure for checking mutual acceptance on black-box implementations, both inspired by the idea of environmental conformance [7,8]. The result is that correctness of large systems can be determined by testing the component implementations for conformance to their component specification and for environmental conformance to the specification of their environment.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Preprint version of article to be presented and published at FACS 2024"
    },
    {
        "paper id": "2407.05032",
        "abstract url": "https://arxiv.org/abs/2407.05032",
        "title": "And this is where we fu***d up! Lessons learned from Participatory Design in Digital Civic Initiatives",
        "rating": "-10",
        "keywords": [],
        "abstract": "Participatory design in digital civics aims to foster mutual learning and co-creation between public services and citizens. However, rarely do we collectively explore the challenges and failures we experience within PD and digital civics, to enable us to grow as a community. This workshop will explore real-world experiences that had to adapt to unforeseen circumstances. Through case presentations and thematic group discussions, participants will reflect on the challenges faced, the causes that led to these challenges, and collaboratively problem-solve effective solutions. Furthermore, we aim to discuss well-being impact on researchers and communities when faced with these obstacles, the strategies participants use to overcome them and how this can be fed back into the digital civics community. By that, the workshop seeks to foster dialogue, reflection, and collective learning, empowering participants with insights to navigate complexities effectively and promote resilient design practices in digital civics.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05035",
        "abstract url": "https://arxiv.org/abs/2407.05035",
        "title": "Equitable Congestion Pricing under the Markovian Traffic Model: An Application to Bogota",
        "rating": "-10",
        "keywords": [],
        "abstract": "Congestion pricing is used to raise revenues and reduce traffic and pollution. However, people have heterogeneous spatial demand patterns and willingness (or ability) to pay tolls, and so pricing may have substantial equity implications. We develop a data-driven approach to design congestion pricing given policymakers' equity and efficiency objectives. First, algorithmically, we extend the Markovian traffic equilibrium setting introduced by Baillon & Cominetti (2008) to model heterogeneous populations and incorporate prices and outside options such as public transit. Second, we empirically evaluate various pricing schemes using data collected by an industry partner in the city of Bogota, one of the most congested cities in the world. We find that pricing personalized to each economic stratum can be substantially more efficient and equitable than uniform pricing; however, non-personalized but area-based pricing can recover much of the gap.",
        "subjects": [
            "cs.GT",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05052",
        "abstract url": "https://arxiv.org/abs/2407.05052",
        "title": "A Marginal Distributionally Robust Kalman Filter for Centralized Fusion",
        "rating": "-10",
        "keywords": [],
        "abstract": "State estimation is a fundamental problem for multi-sensor information fusion, essential in applications such as target tracking, power systems, and control automation. Previous research mostly ignores the correlation between sensors and assumes independent or known distributions. However, in practice, these distributions are often correlated and difAcult to estimate. This paper proposes a novel moment constrained marginal distributionally robust Kalman Alter (MC-MDRKF) for centralized state estimation in multi-sensor systems. First, we introduce a marginal distributional uncertainty set using a moment-constrained approach, which can better capture the uncertainties of Gaussian noises compared to Kullback-Leibler (KL) divergence-based methods. Based on that, a minimax optimization problem is formulated to identify the least favorable joint distribution and the optimal MMSE estimator thereunder. It is proved that this problem can be reformulated as a convex optimization problem, allowing for efficient solution Anding. Subsequently, by accounting for marginal distributional uncertainty within the state space model, the proposed MC-MDRKF is devised in a minimax approach. Simulation result demonstrates the robustness and superiority of the proposed method in a multi-sensor target tracking scenario.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05063",
        "abstract url": "https://arxiv.org/abs/2407.05063",
        "title": "Co-Scale Cross-Attentional Transformer for Rearrangement Target Detection",
        "rating": "-10",
        "keywords": [],
        "abstract": "Rearranging objects (e.g. vase, door) back in their original positions is one of the most fundamental skills for domestic service robots (DSRs). In rearrangement tasks, it is crucial to detect the objects that need to be rearranged according to the goal and current states. In this study, we focus on Rearrangement Target Detection (RTD), where the model generates a change mask for objects that should be rearranged. Although many studies have been conducted in the field of Scene Change Detection (SCD), most SCD methods often fail to segment objects with complex shapes and fail to detect the change in the angle of objects that can be opened or closed. In this study, we propose a Co-Scale Cross-Attentional Transformer for RTD. We introduce the Serial Encoder which consists of a sequence of serial blocks and the Cross-Attentional Encoder which models the relationship between the goal and current states. We built a new dataset consisting of RGB images and change masks regarding the goal and current states. We validated our method on the dataset and the results demonstrated that our method outperformed baseline methods on $F_1$-score and mean IoU.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted for publication in Advanced Robotics"
    },
    {
        "paper id": "2407.05064",
        "abstract url": "https://arxiv.org/abs/2407.05064",
        "title": "Reverse Engineered MiniFS File System",
        "rating": "-10",
        "keywords": [],
        "abstract": "In an era where digital connectivity is increasingly foundational to daily life, the security of Wi-Fi Access Points (APs) is a critical concern. This paper addresses the vulnerabilities inherent in Wi-Fi APs, with a particular focus on those using proprietary file systems like MiniFS found in TP-Link's AC1900 WiFi router. Through reverse engineering, we unravel the structure and operation of MiniFS, marking a significant advancement in our understanding of this previously opaque file system. Our investigation reveals not only the architecture of MiniFS but also identifies several private keys and underscores a concerning lack of cryptographic protection. These findings point to broader security vulnerabilities, emphasizing the risks of security-by-obscurity practices in an interconnected environment. Our contributions are twofold: firstly, based, on the file system structure, we develop a methodology for the extraction and analysis of MiniFS, facilitating the identification and mitigation of potential vulnerabilities. Secondly, our work lays the groundwork for further research into WiFi APs' security, particularly those running on similar proprietary systems. By highlighting the critical need for transparency and community engagement in firmware analysis, this study contributes to the development of more secure network devices, thus enhancing the overall security posture of digital infrastructures.",
        "subjects": [
            "cs.CR",
            "cs.NI"
        ],
        "comment": "The 19th International Conference on Availability, Reliability and Security (ARES 2024), July 30-August 2, 2024, Vienna, Austria"
    },
    {
        "paper id": "2407.05067",
        "abstract url": "https://arxiv.org/abs/2407.05067",
        "title": "Smooth Sensitivity Revisited: Towards Optimality",
        "rating": "-10",
        "keywords": [],
        "abstract": "Smooth sensitivity is one of the most commonly used techniques for designing practical differentially private mechanisms. In this approach, one computes the smooth sensitivity of a given query $q$ on the given input $D$ and releases $q(D)$ with noise added proportional to this smooth sensitivity. One question remains: what distribution should we pick the noise from? In this paper, we give a new class of distributions suitable for the use with smooth sensitivity, which we name the PolyPlace distribution. This distribution improves upon the state-of-the-art Student's T distribution in terms of standard deviation by arbitrarily large factors, depending on a \"smoothness parameter\" $\u03b3$, which one has to set in the smooth sensitivity framework. Moreover, our distribution is defined for a wider range of parameter $\u03b3$, which can lead to significantly better performance. Moreover, we prove that the PolyPlace distribution converges for $\u03b3\\rightarrow 0$ to the Laplace distribution and so does its variance. This means that the Laplace mechanism is a limit special case of the PolyPlace mechanism. This implies that out mechanism is in a certain sense optimal for $\u03b3\\to 0$.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05069",
        "abstract url": "https://arxiv.org/abs/2407.05069",
        "title": "What's the Best Seat in the Game Left, Center, Right?",
        "rating": "-10",
        "keywords": [],
        "abstract": "Left, Center, Right is a popular dice game. We analyze the game using Markov chain and Monte Carlo methods. We compute the expected game length for two to eight players and determine the probability of winning for each player in the game. We discuss the surprising conclusions of which players have the highest and lowest chance of winning, and we propose a small rule change that makes the game a little more fair.",
        "subjects": [
            "math.HO",
            "cs.GT",
            "math.PR"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05090",
        "abstract url": "https://arxiv.org/abs/2407.05090",
        "title": "Automatically Analyzing Performance Issues in Android Apps: How Far Are We?",
        "rating": "-10",
        "keywords": [],
        "abstract": "Performance plays a critical role in ensuring the smooth operation of any mobile application, directly influencing user engagement and retention. Android applications are no exception. However, unlike functionality issues, performance issues are more challenging to discover as their root causes are sophisticated and typically emerge under specific payloads. To tackle this problem, researchers have dedicated substantial efforts to proposing automatic approaches for understanding, detecting, and resolving performance issues. Despite these endeavors, it still remains unknown what the status quo of Android performance analysis is, and whether existing approaches can indeed accurately reflect real performance issues. To fill this research gap, we conducted a systematic literature review followed by an explanatory study to explore relevant studies and real-world challenges. Our findings reveal that current tools have limited capabilities, covering only 17.50% of the performance issues. Additionally, existing datasets encompass only 27.50% of the issues and are very limited in size. We also show real-world issue patterns, underscoring the huge gap between the identified techniques and practical concerns. Furthermore, possible solutions are provided to guide future research towards achieving effective performance issue detection and resolution.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05114",
        "abstract url": "https://arxiv.org/abs/2407.05114",
        "title": "Approximating the Fr\u00e9chet distance when only one curve is $c$-packed",
        "rating": "-10",
        "keywords": [],
        "abstract": "One approach to studying the Fr\u00e9chet distance is to consider curves that satisfy realistic assumptions. By now, the most popular realistic assumption for curves is $c$-packedness. Existing algorithms for computing the Fr\u00e9chet distance between $c$-packed curves require both curves to be $c$-packed. In this paper, we only require one of the two curves to be $c$-packed. Our result is a nearly-linear time algorithm that $(1+\\varepsilon)$-approximates the Fr\u00e9chet distance between a $c$-packed curve and a general curve in $\\mathbb R^d$, for constant values of $\\varepsilon$, $d$ and $c$.",
        "subjects": [
            "cs.CG"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05132",
        "abstract url": "https://arxiv.org/abs/2407.05132",
        "title": "Fair Money -- Public Good Value Pricing With Karma Economies",
        "rating": "-10",
        "keywords": [],
        "abstract": "City road infrastructure is a public good, and over-consumption by self-interested, rational individuals leads to traffic jams. Congestion pricing is effective in reducing demand to sustainable levels, but also controversial, as it introduces equity issues and systematically discriminates lower-income groups. Karma is a non-monetary, fair, and efficient resource allocation mechanism, that employs an artificial currency different from money, that incentivizes cooperation amongst selfish individuals, and achieves a balance between giving and taking. Where money does not do its job, Karma achieves socially more desirable resource allocations by being aligned with consumers' needs rather than their financial power. This work highlights the value proposition of Karma, gives guidance on important Karma mechanism design elements, and equips the reader with a useful software framework to model Karma economies and predict consumers' behaviour. A case study demonstrates the potential of this feasible alternative to money, without the burden of additional fees.",
        "subjects": [
            "cs.MA",
            "econ.TH"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05136",
        "abstract url": "https://arxiv.org/abs/2407.05136",
        "title": "Collaborative Estimation of Real Valued Function by Two Agents and a Fusion Center with Knowledge Exchange",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider a collaborative iterative algorithm with two agents and a fusion center for estimation of a real valued function (or ``model\") on the set of real numbers. While the data collected by the agents is private, in every iteration of the algorithm, the models estimated by the agents are uploaded to the fusion center, fused, and, subsequently downloaded by the agents. We consider the estimation spaces at the agents and the fusion center to be Reproducing Kernel Hilbert Spaces (RKHS). Under suitable assumptions on these spaces, we prove that the algorithm is consistent, i.e., there exists a subsequence of the estimated models which converges to a model in the strong topology. To this end, we define estimation operators for the agents, fusion center, and, for every iteration of the algorithm constructively. We define valid input data sequences, study the asymptotic properties of the norm of the estimation operators, and, find sufficient conditions under which the estimation operator until any iteration is uniformly bounded. Using these results, we prove the existence of an estimation operator for the algorithm which implies the consistency of the considered estimation algorithm.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2401.03012"
    },
    {
        "paper id": "2407.05139",
        "abstract url": "https://arxiv.org/abs/2407.05139",
        "title": "Almost Envy-free Allocation of Indivisible Goods: A Tale of Two Valuations",
        "rating": "-10",
        "keywords": [],
        "abstract": "The existence of $\\textsf{EFX}$ allocations stands as one of the main challenges in discrete fair division. In this paper, we present a collection of symmetrical results on the existence of $\\textsf{EFX}$ notion and its approximate variations. These results pertain to two seemingly distinct valuation settings: the restricted additive valuations and $(p,q)$-bounded valuations recently introduced by Christodoulou \\textit{et al.} \\cite{christodoulou2023fair}. In a $(p,q)$-bonuded instance, each good holds relevance (i.e., has a non-zero marginal value) for at most $p$ agents, and any pair of agents share at most $q$ common relevant goods. The only known guarantees on $(p,q)$-bounded valuations is that $(2,1)$-bounded instances always admit $\\textsf{EFX}$ allocations (EC'22) \\cite{christodoulou2023fair}. Here we show that instances with $(\\infty,1)$-bounded valuations always admit $\\textsf{EF2X}$ allocations, and $\\textsf{EFX}$ allocations with at most $\\lfloor {n}/{2} \\rfloor - 1$ discarded goods. These results mirror the existing results for the restricted additive setting \\cite{akrami2023efx}. Moreover, we present $({\\sqrt{2}}/{2})-\\textsf{EFX}$ allocation algorithms for both the restricted additive and $(\\infty,1)$-bounded settings. The symmetry of these results suggests that these valuations exhibit symmetric structures. Building on this observation, we conjectured that the $(2,\\infty)$-bounded and restricted additive setting might admit $\\textsf{EFX}$ guarantee. Intriguingly, our investigation confirms this conjecture. We propose a rather complex $\\textsf{EFX}$ allocation algorithm for restricted additive valuations when $p=2$ and $q=\\infty$.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05157",
        "abstract url": "https://arxiv.org/abs/2407.05157",
        "title": "Data-driven model predictive control of battery storage units",
        "rating": "-10",
        "keywords": [],
        "abstract": "In many state-of-the-art control approaches for power systems with storage units, an explicit model of the storage dynamics is required. With growing numbers of storage units, identifying these dynamics can be cumbersome. This paper employs recent data-driven control approaches that do not require an explicit identification step. Instead, they use measured input/output data in control formulations. In detail, we propose an economic data-driven model predictive control (MPC) scheme to operate a small power system with input-nonlinear battery dynamics. First, a linear data-driven MPC approach that uses a slack variable to account for plant-model-mismatch is proposed. In a second step, an input-nonlinear data-driven MPC scheme is deduced. Comparisons with a reference indicate that the linear data-driven MPC approximates the nonlinear plant in an acceptable manner. Even better results, however, can be obtained with the input-nonlinear data-driven MPC scheme which provides increased prediction accuracy.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05165",
        "abstract url": "https://arxiv.org/abs/2407.05165",
        "title": "Feedback-Driven Automated Whole Bug Report Reproduction for Android Apps",
        "rating": "-10",
        "keywords": [],
        "abstract": "In software development, bug report reproduction is a challenging task. This paper introduces ReBL, a novel feedback-driven approach that leverages GPT-4, a large-scale language model, to automatically reproduce Android bug reports. Unlike traditional methods, ReBL bypasses the use of Step to Reproduce (S2R) entities. Instead, it leverages the entire textual bug report and employs innovative prompts to enhance GPT's contextual reasoning. This approach is more flexible and context-aware than the traditional step-by-step entity matching approach, resulting in improved accuracy and effectiveness. In addition to handling crash reports, ReBL has the capability of handling non-crash bug reports. Our evaluation of 96 Android bug reports (73 crash and 23 non-crash) demonstrates that ReBL successfully reproduced 90.63% of these reports, averaging only 74.98 seconds per bug report. Additionally, ReBL outperformed three existing tools in both success rate and speed.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Accepted by ISSTA 2024"
    },
    {
        "paper id": "2407.05168",
        "abstract url": "https://arxiv.org/abs/2407.05168",
        "title": "Deception in Nash Equilibrium Seeking",
        "rating": "-10",
        "keywords": [],
        "abstract": "In socio-technical multi-agent systems, deception exploits privileged information to induce false beliefs in \"victims,\" keeping them oblivious and leading to outcomes detrimental to them or advantageous to the deceiver. We consider model-free Nash-equilibrium-seeking for non-cooperative games with asymmetric information and introduce model-free deceptive algorithms with stability guarantees. In the simplest algorithm, the deceiver includes in his action policy the victim's exploration signal, with an amplitude tuned by an integrator of the regulation error between the deceiver's actual and desired payoff. The integral feedback drives the deceiver's payoff to the payoff's reference value, while the victim is led to adopt a suboptimal action, at which the pseudogradient of the deceiver's payoff is zero. The deceiver's and victim's actions turn out to constitute a \"deceptive\" Nash equilibrium of a different game, whose structure is managed - in real time - by the deceiver. We examine quadratic, aggregative, and more general games and provide conditions for a successful deception, mutual and benevolent deception, and immunity to deception. Stability results are established using techniques based on averaging and singular perturbations. Among the examples in the paper is a microeconomic duopoly in which the deceiver induces in the victim a belief that the buyers disfavor the deceiver more than they actually do, leading the victim to increase the price above the Nash price, and resulting in an increased profit for the deceiver and a decreased profit for the victim. A study of the deceiver's integral feedback for the desired profit reveals that, in duopolies with equal marginal costs, a deceiver that is greedy for very high profit can attain any such profit, and pursue this with arbitrarily high integral gain (impatiently), irrespective of the market preference for the victim.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05184",
        "abstract url": "https://arxiv.org/abs/2407.05184",
        "title": "Teaching Research Design in Software Engineering",
        "rating": "-10",
        "keywords": [],
        "abstract": "In the dynamic field of Software Engineering (SE), where practice is constantly evolving and adapting to new technologies, conducting research is a daunting quest. This poses a challenge for researchers: how to stay relevant and effective in their studies? Empirical Software Engineering (ESE) has emerged as a contending force aiming to critically evaluate and provide knowledge that informs practice in adopting new technologies. Empirical research requires a rigorous process of collecting and analyzing data to obtain evidence-based findings. Challenges to this process are numerous, and many researchers, novice and experienced, found difficulties due to many complexities involved in designing their research. The core of this chapter is to teach foundational skills in research design, essential for educating software engineers and researchers in ESE. It focuses on developing a well-structured research design, which includes defining a clear area of investigation, formulating relevant research questions, and choosing appropriate methodologies. While the primary focus is on research design, this chapter also covers aspects of research scoping and selecting research methods. This approach prepares students to handle the complexities of the ever-changing technological landscape in SE, making it a critical component of their educational curriculum.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05191",
        "abstract url": "https://arxiv.org/abs/2407.05191",
        "title": "On the Decidability of Presburger Arithmetic Expanded with Powers",
        "rating": "-10",
        "keywords": [],
        "abstract": "We prove that for any integers $\u03b1, \u03b2> 1$, the existential fragment of the first-order theory of the structure $\\langle \\mathbb{Z}; 0,1,<, +, \u03b1^{\\mathbb{N}}, \u03b2^{\\mathbb{N}}\\rangle$ is decidable (where $\u03b1^{\\mathbb{N}}$ is the set of positive integer powers of $\u03b1$, and likewise for $\u03b2^{\\mathbb{N}}$). On the other hand, we show by way of hardness that decidability of the existential fragment of the theory of $\\langle \\mathbb{N}; 0,1, <, +, x\\mapsto \u03b1^x, x \\mapsto \u03b2^x\\rangle$ for any multiplicatively independent $\u03b1,\u03b2> 1$ would lead to mathematical breakthroughs regarding base-$\u03b1$ and base-$\u03b2$ expansions of certain transcendental numbers.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05192",
        "abstract url": "https://arxiv.org/abs/2407.05192",
        "title": "Optimizing Bipolar Constellations for High-Rate Transmission in Short-Reach Fiber Links with Direct Detection",
        "rating": "-10",
        "keywords": [],
        "abstract": "Bipolar modulation increases the achievable information rate of communication links with direct-detection receivers. This paper optimizes bipolar transmission with a modulator bias offset for short-reach fiber links. A neural network equalizer with successive interference cancellation is shown to gain over 100 Gbit/s compared to standard receivers.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "will be presented at the 50th European Conference on Optical Communications (ECOC) 2024"
    },
    {
        "paper id": "2407.05203",
        "abstract url": "https://arxiv.org/abs/2407.05203",
        "title": "Toward a Unified Metadata Schema for Ecological Momentary Assessment with Voice-First Virtual Assistants",
        "rating": "-10",
        "keywords": [],
        "abstract": "Ecological momentary assessment (EMA) is used to evaluate subjects' behaviors and moods in their natural environments, yet collecting real-time and self-report data with EMA is challenging due to user burden. Integrating voice into EMA data collection platforms through today's intelligent virtual assistants (IVAs) is promising due to hands-free and eye-free nature. However, efficiently managing conversations and EMAs is non-trivial and time consuming due to the ambiguity of the voice input. We approach this problem by rethinking the data modeling of EMA questions and what is needed to deploy them on voice-first user interfaces. We propose a unified metadata schema that models EMA questions and the necessary attributes to effectively and efficiently integrate voice as a new EMA modality. Our schema allows user experience researchers to write simple rules that can be rendered at run-time, instead of having to edit the source code. We showcase an example EMA survey implemented with our schema, which can run on multiple voice-only and voice-first devices. We believe that our work will accelerate the iterative prototyping and design process of real-world voice-based EMA data collection platforms.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "6 pages, In Proceedings of the 3rd Conference on Conversational User Interfaces (CUI '21). Association for Computing Machinery, New York, NY, USA, Article 31, 1-6"
    },
    {
        "paper id": "2407.05231",
        "abstract url": "https://arxiv.org/abs/2407.05231",
        "title": "Fr\u00e9chet Distance in Subquadratic Time",
        "rating": "-10",
        "keywords": [],
        "abstract": "Let $m$ and $n$ be the numbers of vertices of two polygonal curves in $\\mathbb{R}^d$ for any fixed $d$ such that $m \\leq n$. Since it was known in 1995 how to compute the Fr\u00e9chet distance of these two curves in $O(mn\\log (mn))$ time, it has been an open problem whether the running time can be reduced to $o(n^2)$ when $m = \u03a9(n)$. In the mean time, several well-known quadratic time barriers in computational geometry have been overcome: 3SUM, some 3SUM-hard problems, and the computation of some distances between two polygonal curves, including the discrete Fr\u00e9chet distance, the dynamic time warping distance, and the geometric edit distance. It is curious that the quadratic time barrier for Fr\u00e9chet distance still stands. We present an algorithm to compute the Fr\u00e9chet distance in $O(mn(\\log\\log n)^{2+\u03bc}\\log n/\\log^{1+\u03bc} m)$ expected time for some constant $\u03bc\\in (0,1)$. It is the first algorithm that returns the Fr\u00e9chet distance in $o(mn)$ time when $m = \u03a9(n^{\\varepsilon})$ for any fixed $\\varepsilon \\in (0,1]$.",
        "subjects": [
            "cs.CG",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2407.05239",
        "abstract url": "https://arxiv.org/abs/2407.05239",
        "title": "Competitive Analysis of Online Path Selection: Impacts of Path Length, Topology, and System-Level Costs",
        "rating": "-10",
        "keywords": [],
        "abstract": "Consider a communication network to which a sequence of self-interested users come and send requests for data transmission between nodes. This work studies the question of how to guide the path selection choices made by those online-arriving users and maximize the social welfare. Competitive analysis is the main technical tool. Specifically, the impacts of path length bounds and topology on the competitive ratio of the designed algorithm are analyzed theoretically and explored experimentally. We observe intricate and interesting relationships between the empirical performance and the studied network parameters, which shed some light on how to design the network. We also investigate the influence of system-level costs on the optimal algorithm design.",
        "subjects": [
            "cs.DS",
            "cs.NI"
        ],
        "comment": null
    }
]