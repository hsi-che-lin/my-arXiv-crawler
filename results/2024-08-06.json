[
    {
        "paper id": "2408.03282",
        "abstract url": "https://arxiv.org/abs/2408.03282",
        "title": "AMES: Asymmetric and Memory-Efficient Similarity Estimation for Instance-level Retrieval",
        "rating": "2.5",
        "keywords": [
            [
                "Memory-Efficient"
            ],
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "This work investigates the problem of instance-level image retrieval re-ranking with the constraint of memory efficiency, ultimately aiming to limit memory usage to 1KB per image. Departing from the prevalent focus on performance enhancements, this work prioritizes the crucial trade-off between performance and memory requirements. The proposed model uses a transformer-based architecture designed to estimate image-to-image similarity by capturing interactions within and across images based on their local descriptors. A distinctive property of the model is the capability for asymmetric similarity estimation. Database images are represented with a smaller number of descriptors compared to query images, enabling performance improvements without increasing memory consumption. To ensure adaptability across different applications, a universal model is introduced that adjusts to a varying number of local descriptors during the testing phase. Results on standard benchmarks demonstrate the superiority of our approach over both hand-crafted and learned models. In particular, compared with current state-of-the-art methods that overlook their memory footprint, our approach not only attains superior performance but does so with a significantly reduced memory footprint. The code and pretrained models are publicly available at: https://github.com/pavelsuma/ames",
        "subjects": [
            "cs.CV"
        ],
        "comment": "ECCV 2024"
    },
    {
        "paper id": "2408.03033",
        "abstract url": "https://arxiv.org/abs/2408.03033",
        "title": "L3iTC at the FinLLM Challenge Task: Quantization for Financial Text Classification & Summarization",
        "rating": "2",
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This article details our participation (L3iTC) in the FinLLM Challenge Task 2024, focusing on two key areas: Task 1, financial text classification, and Task 2, financial text summarization. To address these challenges, we fine-tuned several large language models (LLMs) to optimize performance for each task. Specifically, we used 4-bit quantization and LoRA to determine which layers of the LLMs should be trained at a lower precision. This approach not only accelerated the fine-tuning process on the training data provided by the organizers but also enabled us to run the models on low GPU memory. Our fine-tuned models achieved third place for the financial classification task with an F1-score of 0.7543 and secured sixth place in the financial summarization task on the official test datasets.",
        "subjects": [
            "cs.CL",
            "cs.CE"
        ],
        "comment": "Joint Workshop of the 8th Financial Technology and Natural Language Processing (FinNLP) and the 1st Agent AI for Scenario Planning (AgentScen), 2024"
    },
    {
        "paper id": "2408.03149",
        "abstract url": "https://arxiv.org/abs/2408.03149",
        "title": "Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization",
        "rating": "2",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "The rapid increase in multimedia data has spurred advancements in Multimodal Summarization with Multimodal Output (MSMO), which aims to produce a multimodal summary that integrates both text and relevant images. The inherent heterogeneity of content within multimodal inputs and outputs presents a significant challenge to the execution of MSMO. Traditional approaches typically adopt a holistic perspective on coarse image-text data or individual visual objects, overlooking the essential connections between objects and the entities they represent. To integrate the fine-grained entity knowledge, we propose an Entity-Guided Multimodal Summarization model (EGMS). Our model, building on BART, utilizes dual multimodal encoders with shared weights to process text-image and entity-image information concurrently. A gating mechanism then combines visual data for enhanced textual summary generation, while image selection is refined through knowledge distillation from a pre-trained vision-language model. Extensive experiments on public MSMO dataset validate the superiority of the EGMS method, which also prove the necessity to incorporate entity information into MSMO problem.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": "In ACL-Findings 2024"
    },
    {
        "paper id": "2408.03172",
        "abstract url": "https://arxiv.org/abs/2408.03172",
        "title": "Leveraging Parameter Efficient Training Methods for Low Resource Text Classification: A Case Study in Marathi",
        "rating": "2",
        "keywords": [
            [
                "Parameter Efficient",
                "PEFT",
                "Efficient Fine-Tuning"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "With the surge in digital content in low-resource languages, there is an escalating demand for advanced Natural Language Processing (NLP) techniques tailored to these languages. BERT (Bidirectional Encoder Representations from Transformers), serving as the foundational framework for numerous NLP architectures and language models, is increasingly employed for the development of low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a method for fine-tuning Large Language Models (LLMs) and reducing the training parameters to some extent to decrease the computational costs needed for training the model and achieve results comparable to a fully fine-tuned model. In this work, we present a study of PEFT methods for the Indic low-resource language Marathi. We conduct a comprehensive analysis of PEFT methods applied to various monolingual and multilingual Marathi BERT models. These approaches are evaluated on prominent text classification datasets like MahaSent, MahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated to significantly expedite the training speed of the models, addressing a critical aspect of model development and deployment. In this study, we explore Low-Rank Adaptation of Large Language Models (LoRA) and adapter methods for low-resource text classification. We show that these methods are competitive with full fine-tuning and can be used without loss in accuracy. This study contributes valuable insights into the effectiveness of Marathi BERT models, offering a foundation for the continued advancement of NLP capabilities in Marathi and similar Indic languages.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Accepted at I2CT 2024"
    },
    {
        "paper id": "2408.03290",
        "abstract url": "https://arxiv.org/abs/2408.03290",
        "title": "SARA: Singular-Value Based Adaptive Low-Rank Adaption",
        "rating": "2",
        "keywords": [
            [
                "parameter-efficient",
                "PEFT",
                "efficient fine-tuning"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "With the increasing number of parameters in large pre-trained models, LoRA as a parameter-efficient fine-tuning(PEFT) method is widely used for not adding inference overhead. The LoRA method assumes that weight changes during fine-tuning can be approximated by low-rank matrices. However, the rank values need to be manually verified to match different downstream tasks, and they cannot accommodate the varying importance of different layers in the model. In this work, we first analyze the relationship between the performance of different layers and their ranks using SVD. Based on this, we design the Singular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds the rank during initialization by performing SVD on the pre-trained weights. Additionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly reduces the number of parameters by fine-tuning only multiple parallel sets of singular values controlled by a router. Extensive experiments on various complex tasks demonstrate the simplicity and parameter efficiency of our methods. They can effectively and adaptively find the most suitable rank for each layer of each model.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03511",
        "abstract url": "https://arxiv.org/abs/2408.03511",
        "title": "MoExtend: Tuning New Experts for Modality and Task Extension",
        "rating": "2",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) excel in various tasks but are primarily trained on text data, limiting their application scope. Expanding LLM capabilities to include vision-language understanding is vital, yet training them on multimodal data from scratch is challenging and costly. Existing instruction tuning methods, e.g., LLAVA, often connects a pretrained CLIP vision encoder and LLMs via fully fine-tuning LLMs to bridge the modality gap. However, full fine-tuning is plagued by catastrophic forgetting, i.e., forgetting previous knowledge, and high training costs particularly in the era of increasing tasks and modalities. To solve this issue, we introduce MoExtend, an effective framework designed to streamline the modality adaptation and extension of Mixture-of-Experts (MoE) models. MoExtend seamlessly integrates new experts into pre-trained MoE models, endowing them with novel knowledge without the need to tune pretrained models such as MoE and vision encoders. This approach enables rapid adaptation and extension to new modal data or tasks, effectively addressing the challenge of accommodating new modalities within LLMs. Furthermore, MoExtend avoids tuning pretrained models, thus mitigating the risk of catastrophic forgetting. Experimental results demonstrate the efficacy and efficiency of MoExtend in enhancing the multimodal capabilities of LLMs, contributing to advancements in multimodal AI research. Code: https://github.com/zhongshsh/MoExtend.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": "ACL 2024 - SRW"
    },
    {
        "paper id": "2408.02971",
        "abstract url": "https://arxiv.org/abs/2408.02971",
        "title": "Wave Interpolation Neural Operator: Interpolated Prediction of Electric Fields Across Untrained Wavelengths",
        "rating": "1.5",
        "keywords": [
            [
                "parameter efficiency"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Designing photonic structures requires electromagnetic simulations, which often require high computational costs. Researchers have developed surrogate solvers for predicting electric fields to alleviate the computational issues. However, existing surrogate solvers are limited to performing inference at fixed simulation conditions and require retraining for different conditions. To address this, we propose Wave Interpolation Neural Operator (WINO), a novel surrogate solver enabling simulation condition interpolation across a continuous spectrum of broadband wavelengths. WINO introduces the Fourier Group Convolution Shuffling operator and a new conditioning method to efficiently predict electric fields from both trained and untrained wavelength data, achieving significant improvements in parameter efficiency and spectral interpolation performance. Our model demonstrates approximately 100 times faster performance than traditional finite-difference frequency-domain simulations. Moreover, compared to the state-of-the-art model, we achieve a 74% reduction in parameters and 80.5% improvements in prediction accuracy for untrained wavelengths, and 13.2% improvements for trained wavelengths.",
        "subjects": [
            "cs.LG",
            "cs.ET",
            "physics.optics"
        ],
        "comment": "9 pages, 5 figures, 4 tables / Appendix: 4 pages, 4 figures, 3 tables"
    },
    {
        "paper id": "2408.03438",
        "abstract url": "https://arxiv.org/abs/2408.03438",
        "title": "Enhanced Reverberation as Supervision for Unsupervised Speech Separation",
        "rating": "1.5",
        "keywords": [
            [
                "cs.SD",
                "eess.AS"
            ],
            [
                "Interspeech"
            ]
        ],
        "abstract": "Reverberation as supervision (RAS) is a framework that allows for training monaural speech separation models from multi-channel mixtures in an unsupervised manner. In RAS, models are trained so that sources predicted from a mixture at an input channel can be mapped to reconstruct a mixture at a target channel. However, stable unsupervised training has so far only been achieved in over-determined source-channel conditions, leaving the key determined case unsolved. This work proposes enhanced RAS (ERAS) for solving this problem. Through qualitative analysis, we found that stable training can be achieved by leveraging the loss term to alleviate the frequency-permutation problem. Separation performance is also boosted by adding a novel loss term where separated signals mapped back to their own input mixture are used as pseudo-targets for the signals separated from other channels and mapped to the same channel. Experimental results demonstrate high stability and performance of ERAS.",
        "subjects": [
            "eess.AS",
            "cs.SD"
        ],
        "comment": "Accepted to Interspeech 2024"
    },
    {
        "paper id": "2408.02970",
        "abstract url": "https://arxiv.org/abs/2408.02970",
        "title": "EC-Guide: A Comprehensive E-Commerce Guide for Instruction Tuning and Quantization",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have attracted considerable attention in various fields for their cost-effective solutions to diverse challenges, especially with advancements in instruction tuning and quantization. E-commerce, with its complex tasks and extensive product-user interactions, presents a promising application area for LLMs. However, the domain-specific concepts and knowledge inherent in e-commerce pose significant challenges for adapting general LLMs. To address this issue, we developed EC-Guide \\href{https://github.com/fzp0424/EC-Guide-KDDUP-2024}, a comprehensive e-commerce guide for instruction tuning and quantization of LLMs. We also heuristically integrated Chain-of-Thought (CoT) during inference to enhance arithmetic performance. Our approach achieved the 2nd place in Track 2 and 5th place in Track 5 at the Amazon KDD Cup'24 \\href{https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms}. Additionally, our solution is model-agnostic, enabling effective scalability across larger systems.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.02976",
        "abstract url": "https://arxiv.org/abs/2408.02976",
        "title": "Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Empathetic response generation, aiming at understanding the user's situation and feelings and respond empathically, is crucial in building human-like dialogue systems. Previous methods mainly focus on using maximum likelihood estimation as the optimization objective for training response generation models, without taking into account the empathy level alignment between generated responses and target responses. To this end, we propose an empathetic response generation using reinforcement learning (EmpRL) framework. The framework designs an effective empathy reward function and generates empathetic responses by maximizing the expected reward through reinforcement learning. Given the powerful text generation capability of pre-trained language models, EmpRL utilizes the pre-trained T5 model as the generator and conducts further training to initialize the policy. To align the empathy level between generated responses and target responses in the context, an empathy reward function containing three empathy communication mechanisms, i.e., emotional reaction, interpretation, and exploration, is constructed using pre-designed and pre-trained empathy identifiers. Finally, the proximal policy optimization algorithm is used to further train the policy to produce empathetic responses. Both automatic and manual evaluations demonstrate that the proposed EmpRL framework can improve the quality of generated responses, enhance the empathy level similarity between generated and target responses, and produce empathetic responses covering both affective and cognitive aspects.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.02978",
        "abstract url": "https://arxiv.org/abs/2408.02978",
        "title": "ASR-enhanced Multimodal Representation Learning for Cross-Domain Product Retrieval",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "E-commerce is increasingly multimedia-enriched, with products exhibited in a broad-domain manner as images, short videos, or live stream promotions. A unified and vectorized cross-domain production representation is essential. Due to large intra-product variance and high inter-product similarity in the broad-domain scenario, a visual-only representation is inadequate. While Automatic Speech Recognition (ASR) text derived from the short or live-stream videos is readily accessible, how to de-noise the excessively noisy text for multimodal representation learning is mostly untouched. We propose ASR-enhanced Multimodal Product Representation Learning (AMPere). In order to extract product-specific information from the raw ASR text, AMPere uses an easy-to-implement LLM-based ASR text summarizer. The LLM-summarized text, together with visual data, is then fed into a multi-branch network to generate compact multimodal embeddings. Extensive experiments on a large-scale tri-domain dataset verify the effectiveness of AMPere in obtaining a unified multimodal product representation that clearly improves cross-domain product retrieval.",
        "subjects": [
            "cs.MM",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "10 pages, 5 figures"
    },
    {
        "paper id": "2408.02980",
        "abstract url": "https://arxiv.org/abs/2408.02980",
        "title": "Sample-agnostic Adversarial Perturbation for Vision-Language Pre-training Models",
        "rating": "1",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "attacks"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent studies on AI security have highlighted the vulnerability of Vision-Language Pre-training (VLP) models to subtle yet intentionally designed perturbations in images and texts. Investigating multimodal systems' robustness via adversarial attacks is crucial in this field. Most multimodal attacks are sample-specific, generating a unique perturbation for each sample to construct adversarial samples. To the best of our knowledge, it is the first work through multimodal decision boundaries to explore the creation of a universal, sample-agnostic perturbation that applies to any image. Initially, we explore strategies to move sample points beyond the decision boundaries of linear classifiers, refining the algorithm to ensure successful attacks under the top $k$ accuracy metric. Based on this foundation, in visual-language tasks, we treat visual and textual modalities as reciprocal sample points and decision hyperplanes, guiding image embeddings to traverse text-constructed decision boundaries, and vice versa. This iterative process consistently refines a universal perturbation, ultimately identifying a singular direction within the input space which is exploitable to impair the retrieval performance of VLP models. The proposed algorithms support the creation of global perturbations or adversarial patches. Comprehensive experiments validate the effectiveness of our method, showcasing its data, task, and model transferability across various VLP models and datasets. Code: https://github.com/LibertazZ/MUAP",
        "subjects": [
            "cs.CV"
        ],
        "comment": "13 pages, 8 figures, published in ACMMM2024"
    },
    {
        "paper id": "2408.02993",
        "abstract url": "https://arxiv.org/abs/2408.02993",
        "title": "DreamLCM: Towards High-Quality Text-to-3D Generation via Latent Consistency Model",
        "rating": "1",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently, the text-to-3D task has developed rapidly due to the appearance of the SDS method. However, the SDS method always generates 3D objects with poor quality due to the over-smooth issue. This issue is attributed to two factors: 1) the DDPM single-step inference produces poor guidance gradients; 2) the randomness from the input noises and timesteps averages the details of the 3D contents.In this paper, to address the issue, we propose DreamLCM which incorporates the Latent Consistency Model (LCM). DreamLCM leverages the powerful image generation capabilities inherent in LCM, enabling generating consistent and high-quality guidance, i.e., predicted noises or images. Powered by the improved guidance, the proposed method can provide accurate and detailed gradients to optimize the target 3D models.In addition, we propose two strategies to enhance the generation quality further. Firstly, we propose a guidance calibration strategy, utilizing Euler Solver to calibrate the guidance distribution to accelerate 3D models to converge. Secondly, we propose a dual timestep strategy, increasing the consistency of guidance and optimizing 3D models from geometry to appearance in DreamLCM. Experiments show that DreamLCM achieves state-of-the-art results in both generation quality and training efficiency. The code is available at https://github.com/1YimingZhong/DreamLCM.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "15 pages, 9 figures, ACM MM 2024"
    },
    {
        "paper id": "2408.03006",
        "abstract url": "https://arxiv.org/abs/2408.03006",
        "title": "Dual-path Collaborative Generation Network for Emotional Video Captioning",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Emotional Video Captioning is an emerging task that aims to describe factual content with the intrinsic emotions expressed in videos. The essential of the EVC task is to effectively perceive subtle and ambiguous visual emotional cues during the caption generation, which is neglected by the traditional video captioning. Existing emotional video captioning methods perceive global visual emotional cues at first, and then combine them with the video features to guide the emotional caption generation, which neglects two characteristics of the EVC task. Firstly, their methods neglect the dynamic subtle changes in the intrinsic emotions of the video, which makes it difficult to meet the needs of common scenes with diverse and changeable emotions. Secondly, as their methods incorporate emotional cues into each step, the guidance role of emotion is overemphasized, which makes factual content more or less ignored during generation. To this end, we propose a dual-path collaborative generation network, which dynamically perceives visual emotional cues evolutions while generating emotional captions by collaborative learning. Specifically, in the dynamic emotion perception path, we propose a dynamic emotion evolution module, which first aggregates visual features and historical caption features to summarize the global visual emotional cues, and then dynamically selects emotional cues required to be re-composed at each stage. Besides, in the adaptive caption generation path, to balance the description of factual content and emotional cues, we propose an emotion adaptive decoder. Thus, our methods can generate emotion-related words at the necessary time step, and our caption generation balances the guidance of factual content and emotional cues well. Extensive experiments on three challenging datasets demonstrate the superiority of our approach and each proposed module.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Acccepted by ACM Multimedia 2024, oral"
    },
    {
        "paper id": "2408.03030",
        "abstract url": "https://arxiv.org/abs/2408.03030",
        "title": "Nighttime Pedestrian Detection Based on Fore-Background Contrast Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The significance of background information is frequently overlooked in contemporary research concerning channel attention mechanisms. This study addresses the issue of suboptimal single-spectral nighttime pedestrian detection performance under low-light conditions by incorporating background information into the channel attention mechanism. Despite numerous studies focusing on the development of efficient channel attention mechanisms, the relevance of background information has been largely disregarded. By adopting a contrast learning approach, we reexamine channel attention with regard to pedestrian objects and background information for nighttime pedestrian detection, resulting in the proposed Fore-Background Contrast Attention (FBCA). FBCA possesses two primary attributes: (1) channel descriptors form remote dependencies with global spatial feature information; (2) the integration of background information enhances the distinction between channels concentrating on low-light pedestrian features and those focusing on background information. Consequently, the acquired channel descriptors exhibit a higher semantic level and spatial accuracy. Experimental outcomes demonstrate that FBCA significantly outperforms existing methods in single-spectral nighttime pedestrian detection, achieving state-of-the-art results on the NightOwls and TJU-DHD-pedestrian datasets. Furthermore, this methodology also yields performance improvements for the multispectral LLVIP dataset. These findings indicate that integrating background information into the channel attention mechanism effectively mitigates detector performance degradation caused by illumination factors in nighttime scenarios.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03046",
        "abstract url": "https://arxiv.org/abs/2408.03046",
        "title": "Comb, Prune, Distill: Towards Unified Pruning for Vision Model Compression",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Lightweight and effective models are essential for devices with limited resources, such as intelligent vehicles. Structured pruning offers a promising approach to model compression and efficiency enhancement. However, existing methods often tie pruning techniques to specific model architectures or vision tasks. To address this limitation, we propose a novel unified pruning framework Comb, Prune, Distill (CPD), which addresses both model-agnostic and task-agnostic concerns simultaneously. Our framework employs a combing step to resolve hierarchical layer-wise dependency issues, enabling architecture independence. Additionally, the pruning pipeline adaptively remove parameters based on the importance scoring metrics regardless of vision tasks. To support the model in retaining its learned information, we introduce knowledge distillation during the pruning step. Extensive experiments demonstrate the generalizability of our framework, encompassing both convolutional neural network (CNN) and transformer models, as well as image classification and segmentation tasks. In image classification we achieve a speedup of up to x4.3 with a accuracy loss of 1.8% and in semantic segmentation up to x1.89 with a 5.1% loss in mIoU.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by ITSC 2024. Code is publicly available at: https://github.com/Cranken/CPD"
    },
    {
        "paper id": "2408.03047",
        "abstract url": "https://arxiv.org/abs/2408.03047",
        "title": "OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Multimodal conversational agents are highly desirable because they offer natural and human-like interaction. However, there is a lack of comprehensive end-to-end solutions to support collaborative development and benchmarking. While proprietary systems like GPT-4o and Gemini demonstrating impressive integration of audio, video, and text with response times of 200-250ms, challenges remain in balancing latency, accuracy, cost, and data privacy. To better understand and quantify these issues, we developed OpenOmni, an open-source, end-to-end pipeline benchmarking tool that integrates advanced technologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented Generation, Large Language Models, along with the ability to integrate customized models. OpenOmni supports local and cloud deployment, ensuring data privacy and supporting latency and accuracy benchmarking. This flexible framework allows researchers to customize the pipeline, focusing on real bottlenecks and facilitating rapid proof-of-concept development. OpenOmni can significantly enhance applications like indoor assistance for visually impaired individuals, advancing human-computer interaction. Our demonstration video is available https://www.youtube.com/watch?v=zaSiT3clWqY, demo is available via https://openomni.ai4wa.com, code is available via https://github.com/AI4WA/OpenOmniFramework.",
        "subjects": [
            "cs.HC",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03062",
        "abstract url": "https://arxiv.org/abs/2408.03062",
        "title": "Analysis of Argument Structure Constructions in a Deep Recurrent Language Model",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Understanding how language and linguistic constructions are processed in the brain is a fundamental question in cognitive computational neuroscience. In this study, we explore the representation and processing of Argument Structure Constructions (ASCs) in a recurrent neural language model. We trained a Long Short-Term Memory (LSTM) network on a custom-made dataset consisting of 2000 sentences, generated using GPT-4, representing four distinct ASCs: transitive, ditransitive, caused-motion, and resultative constructions. We analyzed the internal activations of the LSTM model's hidden layers using Multidimensional Scaling (MDS) and t-Distributed Stochastic Neighbor Embedding (t-SNE) to visualize the sentence representations. The Generalized Discrimination Value (GDV) was calculated to quantify the degree of clustering within these representations. Our results show that sentence representations form distinct clusters corresponding to the four ASCs across all hidden layers, with the most pronounced clustering observed in the last hidden layer before the output layer. This indicates that even a relatively simple, brain-constrained recurrent neural network can effectively differentiate between various construction types. These findings are consistent with previous studies demonstrating the emergence of word class and syntax rule representations in recurrent language models trained on next word prediction tasks. In future work, we aim to validate these results using larger language models and compare them with neuroimaging data obtained during continuous speech perception. This study highlights the potential of recurrent neural language models to mirror linguistic processing in the human brain, providing valuable insights into the computational and neural mechanisms underlying language understanding.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03070",
        "abstract url": "https://arxiv.org/abs/2408.03070",
        "title": "Probing structural constraints of negation in Pretrained Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Contradictory results about the encoding of the semantic impact of negation in pretrained language models (PLMs). have been drawn recently (e.g. Kassner and Sch{\u00fc}tze (2020); Gubelmann and Handschuh (2022)). In this paper we focus rather on the way PLMs encode negation and its formal impact, through the phenomenon of the Negative Polarity Item (NPI) licensing in English. More precisely, we use probes to identify which contextual representations best encode 1) the presence of negation in a sentence, and 2) the polarity of a neighboring masked polarity item. We find that contextual representations of tokens inside the negation scope do allow for (i) a better prediction of the presence of not compared to those outside the scope and (ii) a better prediction of the right polarity of a masked polarity item licensed by not, although the magnitude of the difference varies from PLM to PLM. Importantly, in both cases the trend holds even when controlling for distance to not. This tends to indicate that the embeddings of these models do reflect the notion of negation scope, and do encode the impact of negation on NPI licensing. Yet, further control experiments reveal that the presence of other lexical items is also better captured when using the contextual representation of a token within the same syntactic clause than outside from it, suggesting that PLMs simply capture the more general notion of syntactic clause.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03074",
        "abstract url": "https://arxiv.org/abs/2408.03074",
        "title": "Towards an Analysis of Discourse and Interactional Pragmatic Reasoning Capabilities of Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this work, we want to give an overview on which pragmatic abilities have been tested in LLMs so far and how these tests have been carried out. To do this, we first discuss the scope of the field of pragmatics and suggest a subdivision into discourse pragmatics and interactional pragmatics. We give a non-exhaustive overview of the phenomena of those two subdomains and the methods traditionally used to analyze them. We subsequently consider the resulting heterogeneous set of phenomena and methods as a starting point for our survey of work on discourse pragmatics and interactional pragmatics in the context of LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03092",
        "abstract url": "https://arxiv.org/abs/2408.03092",
        "title": "Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Merging Large Language Models (LLMs) aims to amalgamate multiple homologous LLMs into one with all the capabilities. Ideally, any LLMs sharing the same backbone should be mergeable, irrespective of whether they are Fine-Tuned (FT) with minor parameter changes or Pre-Trained (PT) with substantial parameter shifts. However, existing methods often manually assign the model importance, rendering them feasible only for LLMs with similar parameter alterations, such as multiple FT LLMs. The diverse parameter changed ranges between FT and PT LLMs pose challenges for current solutions in empirically determining the optimal combination. In this paper, we make a pioneering effort to broaden the applicability of merging techniques from FT to PT LLMs. We initially examine the efficacy of current methods in merging FT and PT LLMs, discovering that they struggle to deal with PT LLMs. Subsequently, we introduce an approach based on WeIght DisENtanglement (WIDEN) to effectively extend the merging scope, which first disentangles model weights into magnitude and direction components, and then performs adaptive fusion by considering their respective contributions. In the experiments, we merge Qwen1.5-Chat (an FT LLM with instruction-following skills) with Sailor (a PT LLM with multilingual abilities) across 7B and 14B model scales. Results reveal that: (1) existing solutions usually fail when merging Sailor, either losing both abilities or only retaining instruction-following skills; (2) WIDEN successfully injects the multilingual abilities of Sailor into Qwen1.5-Chat and make it proficient in Southeast Asian languages, achieving enhancements in the fundamental capabilities. In light of previous research, we also merge multiple 13B FT LLMs and observe that WIDEN achieves a balanced amalgamation of instruction following, mathematical reasoning, and code generation skills.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "17 pages"
    },
    {
        "paper id": "2408.03094",
        "abstract url": "https://arxiv.org/abs/2408.03094",
        "title": "500xCompressor: Generalized Prompt Compression for Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Prompt compression is crucial for enhancing inference speed, reducing costs, and improving user experience. However, current methods face challenges such as low compression ratios and potential data leakage during evaluation. To address these issues, we propose 500xCompressor, a method that compresses extensive natural language contexts into a minimum of one single special token. The 500xCompressor introduces approximately 0.3% additional parameters and achieves compression ratios ranging from 6x to 480x. It is designed to compress any text, answer various types of questions, and could be utilized by the original large language model (LLM) without requiring fine-tuning. Initially, 500xCompressor was pretrained on the Arxiv Corpus, followed by fine-tuning on the ArxivQA dataset, and subsequently evaluated on strictly unseen and classical question answering (QA) datasets. The results demonstrate that the LLM retained 62.26-72.89% of its capabilities compared to using non-compressed prompts. This study also shows that not all the compressed tokens are equally utilized and that K V values have significant advantages over embeddings in preserving information at high compression ratios. The highly compressive nature of natural language prompts, even for fine-grained complex information, suggests promising potential for future applications and further research into developing a new LLM language.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03097",
        "abstract url": "https://arxiv.org/abs/2408.03097",
        "title": "Prototype Learning for Micro-gesture Classification",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we briefly introduce the solution developed by our team, HFUT-VUT, for the track of Micro-gesture Classification in the MiGA challenge at IJCAI 2024. The task of micro-gesture classification task involves recognizing the category of a given video clip, which focuses on more fine-grained and subtle body movements compared to typical action recognition tasks. Given the inherent complexity of micro-gesture recognition, which includes large intra-class variability and minimal inter-class differences, we utilize two innovative modules, i.e., the cross-modal fusion module and prototypical refinement module, to improve the discriminative ability of MG features, thereby improving the classification accuracy. Our solution achieved significant success, ranking 1st in the track of Micro-gesture Classification. We surpassed the performance of last year's leading team by a substantial margin, improving Top-1 accuracy by 6.13%.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "1st Place in Micro-gesture Classification in MiGA at IJCAI-2024"
    },
    {
        "paper id": "2408.03099",
        "abstract url": "https://arxiv.org/abs/2408.03099",
        "title": "Topic Modeling with Fine-tuning LLMs and Bag of Sentences",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLM)'s are increasingly used for topic modeling outperforming classical topic models such as LDA. Commonly, pre-trained LLM encoders such as BERT are used out-of-the-box despite the fact that fine-tuning is known to improve LLMs considerably. The challenge lies in obtaining a suitable (labeled) dataset for fine-tuning. In this paper, we use the recent idea to use bag of sentences as the elementary unit in computing topics. In turn, we derive an approach FT-Topic to perform unsupervised fine-tuning relying primarily on two steps for constructing a training dataset in an automatic fashion. First, a heuristic method to identifies pairs of sentence groups that are either assumed to be of the same or different topics. Second, we remove sentence pairs that are likely labeled incorrectly. The dataset is then used to fine-tune an encoder LLM, which can be leveraged by any topic modeling approach using embeddings. However, in this work, we demonstrate its effectiveness by deriving a novel state-of-the-art topic modeling method called SenClu, which achieves fast inference through an expectation-maximization algorithm and hard assignments of sentence groups to a single topic, while giving users the possibility to encode prior knowledge on the topic-document distribution. Code is at \\url{https://github.com/JohnTailor/FT-Topic}",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "This is the submitted journal version of enhanced with the novel fine-tuning part of \"Efficient and Flexible Topic Modeling using Pretrained Embeddings and Bag of Sentences'' which appeared at the International Conference on Agents and Artificial Intelligence(ICAART) in 2024"
    },
    {
        "paper id": "2408.03119",
        "abstract url": "https://arxiv.org/abs/2408.03119",
        "title": "Evaluating the Translation Performance of Large Language Models Based on Euas-20",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In recent years, with the rapid development of deep learning technology, large language models (LLMs) such as BERT and GPT have achieved breakthrough results in natural language processing tasks. Machine translation (MT), as one of the core tasks of natural language processing, has also benefited from the development of large language models and achieved a qualitative leap. Despite the significant progress in translation performance achieved by large language models, machine translation still faces many challenges. Therefore, in this paper, we construct the dataset Euas-20 to evaluate the performance of large language models on translation tasks, the translation ability on different languages, and the effect of pre-training data on the translation ability of LLMs for researchers and developers.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "15 pages, 8 figures"
    },
    {
        "paper id": "2408.03125",
        "abstract url": "https://arxiv.org/abs/2408.03125",
        "title": "COMMENTATOR: A Code-mixed Multilingual Text Annotation Framework",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "As the NLP community increasingly addresses challenges associated with multilingualism, robust annotation tools are essential to handle multilingual datasets efficiently. In this paper, we introduce a code-mixed multilingual text annotation framework, COMMENTATOR, specifically designed for annotating code-mixed text. The tool demonstrates its effectiveness in token-level and sentence-level language annotation tasks for Hinglish text. We perform robust qualitative human-based evaluations to showcase COMMENTATOR led to 5x faster annotations than the best baseline. Our code is publicly available at \\url{https://github.com/lingo-iitgn/commentator}. The demonstration video is available at \\url{https://bit.ly/commentator_video}.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03143",
        "abstract url": "https://arxiv.org/abs/2408.03143",
        "title": "SuperSimpleNet: Unifying Unsupervised and Supervised Learning for Fast and Reliable Surface Defect Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The aim of surface defect detection is to identify and localise abnormal regions on the surfaces of captured objects, a task that's increasingly demanded across various industries. Current approaches frequently fail to fulfil the extensive demands of these industries, which encompass high performance, consistency, and fast operation, along with the capacity to leverage the entirety of the available training data. Addressing these gaps, we introduce SuperSimpleNet, an innovative discriminative model that evolved from SimpleNet. This advanced model significantly enhances its predecessor's training consistency, inference time, as well as detection performance. SuperSimpleNet operates in an unsupervised manner using only normal training images but also benefits from labelled abnormal training images when they are available. SuperSimpleNet achieves state-of-the-art results in both the supervised and the unsupervised settings, as demonstrated by experiments across four challenging benchmark datasets. Code: https://github.com/blaz-r/SuperSimpleNet .",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to ICPR 2024"
    },
    {
        "paper id": "2408.03150",
        "abstract url": "https://arxiv.org/abs/2408.03150",
        "title": "Conditioning LLMs with Emotion in Neural Machine Translation",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have shown remarkable performance in Natural Language Processing tasks, including Machine Translation (MT). In this work, we propose a novel MT pipeline that integrates emotion information extracted from a Speech Emotion Recognition (SER) model into LLMs to enhance translation quality. We first fine-tune five existing LLMs on the Libri-trans dataset and select the most performant model. Subsequently, we augment LLM prompts with different dimensional emotions and train the selected LLM under these different configurations. Our experiments reveal that integrating emotion information, especially arousal, into LLM prompts leads to notable improvements in translation quality.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "6 pages, In Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT), Bangkok, Thailand, 2024"
    },
    {
        "paper id": "2408.03164",
        "abstract url": "https://arxiv.org/abs/2408.03164",
        "title": "Dilated Convolution with Learnable Spacings makes visual models more aligned with humans: a Grad-CAM study",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced convolution method that allows enlarging the receptive fields (RF) without increasing the number of parameters, like the dilated convolution, yet without imposing a regular grid. DCLS has been shown to outperform the standard and dilated convolutions on several computer vision benchmarks. Here, we show that, in addition, DCLS increases the models' interpretability, defined as the alignment with human visual strategies. To quantify it, we use the Spearman correlation between the models' GradCAM heatmaps and the ClickMe dataset heatmaps, which reflect human visual attention. We took eight reference models - ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and 36) - and drop-in replaced the standard convolution layers with DCLS ones. This improved the interpretability score in seven of them. Moreover, we observed that Grad-CAM generated random heatmaps for two models in our study: CAFormer and ConvFormer models, leading to low interpretability scores. We addressed this issue by introducing Threshold-Grad-CAM, a modification built on top of Grad-CAM that enhanced interpretability across nearly all models. The code and checkpoints to reproduce this study are available at: https://github.com/rabihchamas/DCLS-GradCAM-Eval.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Accepted at The Trustworthy AI Workshop, IJCAI 2024"
    },
    {
        "paper id": "2408.03185",
        "abstract url": "https://arxiv.org/abs/2408.03185",
        "title": "MaskAnyone Toolkit: Offering Strategies for Minimizing Privacy Risks and Maximizing Utility in Audio-Visual Data Archiving",
        "rating": "1",
        "keywords": [
            [
                "Audio-Visual"
            ]
        ],
        "abstract": "This paper introduces MaskAnyone, a novel toolkit designed to navigate some privacy and ethical concerns of sharing audio-visual data in research. MaskAnyone offers a scalable, user-friendly solution for de-identifying individuals in video and audio content through face-swapping and voice alteration, supporting multi-person masking and real-time bulk processing. By integrating this tool within research practices, we aim to enhance data reproducibility and utility in social science research. Our approach draws on Design Science Research, proposing that MaskAnyone can facilitate safer data sharing and potentially reduce the storage of fully identifiable data. We discuss the development and capabilities of MaskAnyone, explore its integration into ethical research practices, and consider the broader implications of audio-visual data masking, including issues of consent and the risk of misuse. The paper concludes with a preliminary evaluation framework for assessing the effectiveness and ethical integration of masking tools in such research settings.",
        "subjects": [
            "cs.CR",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03202",
        "abstract url": "https://arxiv.org/abs/2408.03202",
        "title": "A Debiased Nearest Neighbors Framework for Multi-Label Text Classification",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Multi-Label Text Classification (MLTC) is a practical yet challenging task that involves assigning multiple non-exclusive labels to each document. Previous studies primarily focus on capturing label correlations to assist label prediction by introducing special labeling schemes, designing specific model structures, or adding auxiliary tasks. Recently, the $k$ Nearest Neighbor ($k$NN) framework has shown promise by retrieving labeled samples as references to mine label co-occurrence information in the embedding space. However, two critical biases, namely embedding alignment bias and confidence estimation bias, are often overlooked, adversely affecting prediction performance. In this paper, we introduce a DEbiased Nearest Neighbors (DENN) framework for MLTC, specifically designed to mitigate these biases. To address embedding alignment bias, we propose a debiased contrastive learning strategy, enhancing neighbor consistency on label co-occurrence. For confidence estimation bias, we present a debiased confidence estimation strategy, improving the adaptive combination of predictions from $k$NN and inductive binary classifications. Extensive experiments conducted on four public benchmark datasets (i.e., AAPD, RCV1-V2, Amazon-531, and EUR-LEX57K) showcase the effectiveness of our proposed method. Besides, our method does not introduce any extra parameters.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03215",
        "abstract url": "https://arxiv.org/abs/2408.03215",
        "title": "FedBAT: Communication-Efficient Federated Learning via Learnable Binarization",
        "rating": "1",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Federated learning is a promising distributed machine learning paradigm that can effectively exploit large-scale data without exposing users' privacy. However, it may incur significant communication overhead, thereby potentially impairing the training efficiency. To address this challenge, numerous studies suggest binarizing the model updates. Nonetheless, traditional methods usually binarize model updates in a post-training manner, resulting in significant approximation errors and consequent degradation in model accuracy. To this end, we propose Federated Binarization-Aware Training (FedBAT), a novel framework that directly learns binary model updates during the local training process, thus inherently reducing the approximation errors. FedBAT incorporates an innovative binarization operator, along with meticulously designed derivatives to facilitate efficient learning. In addition, we establish theoretical guarantees regarding the convergence of FedBAT. Extensive experiments are conducted on four popular datasets. The results show that FedBAT significantly accelerates the convergence and exceeds the accuracy of baselines by up to 9\\%, even surpassing that of FedAvg in some cases.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": "Accepted by ICML 2024"
    },
    {
        "paper id": "2408.03219",
        "abstract url": "https://arxiv.org/abs/2408.03219",
        "title": "Learning to Learn without Forgetting using Attention",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Continual learning (CL) refers to the ability to continually learn over time by accommodating new knowledge while retaining previously learned experience. While this concept is inherent in human learning, current machine learning methods are highly prone to overwrite previously learned patterns and thus forget past experience. Instead, model parameters should be updated selectively and carefully, avoiding unnecessary forgetting while optimally leveraging previously learned patterns to accelerate future learning. Since hand-crafting effective update mechanisms is difficult, we propose meta-learning a transformer-based optimizer to enhance CL. This meta-learned optimizer uses attention to learn the complex relationships between model parameters across a stream of tasks, and is designed to generate effective weight updates for the current task while preventing catastrophic forgetting on previously encountered tasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and SplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both forward and backward transfer, even on small sets of labeled data, highlighting the advantages of integrating a meta-learned optimizer within the continual learning framework.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": "Published at 3rd Conference on Lifelong Learning Agents (CoLLAs), 2024"
    },
    {
        "paper id": "2408.03230",
        "abstract url": "https://arxiv.org/abs/2408.03230",
        "title": "Contrastive Learning for Image Complexity Representation",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Quantifying and evaluating image complexity can be instrumental in enhancing the performance of various computer vision tasks. Supervised learning can effectively learn image complexity features from well-annotated datasets. However, creating such datasets requires expensive manual annotation costs. The models may learn human subjective biases from it. In this work, we introduce the MoCo v2 framework. We utilize contrastive learning to represent image complexity, named CLIC (Contrastive Learning for Image Complexity). We find that there are complexity differences between different local regions of an image, and propose Random Crop and Mix (RCM), which can produce positive samples consisting of multi-scale local crops. RCM can also expand the train set and increase data diversity without introducing additional data. We conduct extensive experiments with CLIC, comparing it with both unsupervised and supervised methods. The results demonstrate that the performance of CLIC is comparable to that of state-of-the-art supervised methods. In addition, we establish the pipelines that can apply CLIC to computer vision tasks to effectively improve their performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03246",
        "abstract url": "https://arxiv.org/abs/2408.03246",
        "title": "Making Long-Context Language Models Better Multi-Hop Reasoners",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications. Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased performance in the presence of noisy contexts. In this paper, we introduce Reasoning with Attributions, a novel approach that prompts LMs to supply attributions for each assertion during their reasoning. We validate our approach through experiments on three multi-hop datasets, employing both proprietary and open-source models, and demonstrate its efficacy and resilience. Furthermore, we explore methods to augment reasoning capabilities via fine-tuning and offer an attribution-annotated dataset and a specialized training strategy. Our fine-tuned model achieves competitive performance on multi-hop reasoning benchmarks, closely paralleling proprietary LMs such as ChatGPT and Claude-instant.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "ACL 2024 Main Conference Camera Ready; Dataset, model, and code are available at https://github.com/LaVi-Lab/LongContextReasoner"
    },
    {
        "paper id": "2408.03247",
        "abstract url": "https://arxiv.org/abs/2408.03247",
        "title": "Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we investigate whether Large Language Models (LLMs) actively recall or retrieve their internal repositories of factual knowledge when faced with reasoning tasks. Through an analysis of LLMs' internal factual recall at each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness the critical factual associations under certain circumstances. Instead, they tend to opt for alternative, shortcut-like pathways to answer reasoning questions. By manually manipulating the recall process of parametric knowledge in LLMs, we demonstrate that enhancing this recall process directly improves reasoning performance whereas suppressing it leads to notable degradation. Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a powerful technique for addressing complex reasoning tasks. Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and reliable reasoning. Furthermore, we explored how contextual conflicts affect the retrieval of facts during the reasoning process to gain a comprehensive understanding of the factual recall behaviors of LLMs. Code and data will be available soon.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03265",
        "abstract url": "https://arxiv.org/abs/2408.03265",
        "title": "BVI-AOM: A New Training Dataset for Deep Video Compression Optimization",
        "rating": "1",
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "Deep learning is now playing an important role in enhancing the performance of conventional hybrid video codecs. These learning-based methods typically require diverse and representative training material for optimization in order to achieve model generalization and optimal coding performance. However, existing datasets either offer limited content variability or come with restricted licensing terms constraining their use to research purposes only. To address these issues, we propose a new training dataset, named BVI-AOM, which contains 956 uncompressed sequences at various resolutions from 270p to 2160p, covering a wide range of content and texture types. The dataset comes with more flexible licensing terms and offers competitive performance when used as a training set for optimizing deep video coding tools. The experimental results demonstrate that when used as a training set to optimize two popular network architectures for two different coding tools, the proposed dataset leads to additional bitrate savings of up to 0.29 and 2.98 percentage points in terms of PSNR-Y and VMAF, respectively, compared to an existing training dataset, BVI-DVC, which has been widely used for deep video coding. The BVI-AOM dataset is available for download under this link: (TBD).",
        "subjects": [
            "eess.IV"
        ],
        "comment": "6 pages, 5 figures. Swapped the PSNR-HVS plot in Fig. 3 for a PSNR-YUV plot"
    },
    {
        "paper id": "2408.03281",
        "abstract url": "https://arxiv.org/abs/2408.03281",
        "title": "StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Evaluation is the baton for the development of large language models. Current evaluations typically employ a single-item assessment paradigm for each atomic test objective, which struggles to discern whether a model genuinely possesses the required capabilities or merely memorizes/guesses the answers to specific questions. To this end, we propose a novel evaluation framework referred to as StructEval. Starting from an atomic test objective, StructEval deepens and broadens the evaluation by conducting a structured assessment across multiple cognitive levels and critical concepts, and therefore offers a comprehensive, robust and consistent evaluation for LLMs. Experiments on three widely-used benchmarks demonstrate that StructEval serves as a reliable tool for resisting the risk of data contamination and reducing the interference of potential biases, thereby providing more reliable and consistent conclusions regarding model capabilities. Our framework also sheds light on the design of future principled and trustworthy LLM evaluation protocols.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "ACL 2024;Benchmark at https://github.com/c-box/StructEval ;Leaderboard at https://huggingface.co/spaces/Bowieee/StructEval_leaderboard"
    },
    {
        "paper id": "2408.03291",
        "abstract url": "https://arxiv.org/abs/2408.03291",
        "title": "DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Vision transformers (ViTs) have garnered significant attention for their performance in vision tasks; however, the high computational cost and significant latency issues have hinder widespread adoption. Post-training quantization (PTQ), a promising method for model compression, still faces accuracy degradation challenges with ViTs. There are two reasons for this: the existing quantization paradigm does not fit the power-law distribution of post-Softmax activations well, and accuracy inevitably decreases after reparameterizing post-LayerNorm activations. We propose a Distribution-Friendly and Outlier-Aware Post-training Quantization method for Vision Transformers, named DopQ-ViT. DopQ-ViT analyzes the inefficiencies of current quantizers and introduces a distribution-friendly Tan Quantizer called TanQ. TanQ focuses more on values near 1, more accurately preserving the power-law distribution of post-Softmax activations, and achieves favorable results. Moreover, when reparameterizing post-LayerNorm activations from channel-wise to layer-wise quantization, the accuracy degradation is mainly due to the significant impact of outliers in the scaling factors. Therefore, DopQ-ViT proposes a method to Search for the Optimal Scaling Factor, denoted as SOSF, which compensates for the influence of outliers and preserves the performance of the quantization model. DopQ-ViT has undergone extensive validation and demonstrates significant performance improvements in quantization models, particularly in low-bit settings.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03297",
        "abstract url": "https://arxiv.org/abs/2408.03297",
        "title": "KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks. However, in the process of integrating external non-parametric supporting evidence with internal parametric knowledge, inevitable knowledge conflicts may arise, leading to confusion in the model's responses. To enhance the knowledge selection of LLMs in various contexts, some research has focused on refining their behavior patterns through instruction-tuning. Nonetheless, due to the absence of explicit negative signals and comparative objectives, models fine-tuned in this manner may still exhibit undesirable behaviors in the intricate and realistic retrieval scenarios. To this end, we propose a Knowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving controllable knowledge selection in real retrieval scenarios. Concretely, we explore and simulate error types across diverse context combinations and learn how to avoid these negative signals through preference optimization methods. Simultaneously, by adjusting the balance between response length and the proportion of preference data representing different behavior patterns, we enhance the adherence capabilities and noise robustness of LLMs in a balanced manner. Experimental results show that KaPO outperforms previous methods for handling knowledge conflicts by over 37%, while also exhibiting robust generalization across various out-of-distribution datasets.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03304",
        "abstract url": "https://arxiv.org/abs/2408.03304",
        "title": "Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Etruscan mirrors constitute a significant category in Etruscan art, characterized by elaborate figurative illustrations featured on their backside. A laborious and costly aspect of their analysis and documentation is the task of manually tracing these illustrations. In previous work, a methodology has been proposed to automate this process, involving photometric-stereo scanning in combination with deep neural networks. While achieving quantitative performance akin to an expert annotator, some results still lack qualitative precision and, thus, require annotators for inspection and potential correction, maintaining resource intensity. In response, we propose a deep neural network trained to interactively refine existing annotations based on human guidance. Our human-in-the-loop approach streamlines annotation, achieving equal quality with up to 75% less manual input required. Moreover, during the refinement process, the relative improvement of our methodology over pure manual labeling reaches peak values of up to 26%, attaining drastically better quality quicker. By being tailored to the complex task of segmenting intricate lines, specifically distinguishing it from previous methods, our approach offers drastic improvements in efficacy, transferable to a broad spectrum of applications beyond Etruscan mirrors.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.HC",
            "cs.LG"
        ],
        "comment": "16 pages, accepted at ICPR2024"
    },
    {
        "paper id": "2408.03314",
        "abstract url": "https://arxiv.org/abs/2408.03314",
        "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a \"compute-optimal\" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03319",
        "abstract url": "https://arxiv.org/abs/2408.03319",
        "title": "Training LLMs to Recognize Hedges in Spontaneous Narratives",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Hedges allow speakers to mark utterances as provisional, whether to signal non-prototypicality or \"fuzziness\", to indicate a lack of commitment to an utterance, to attribute responsibility for a statement to someone else, to invite input from a partner, or to soften critical feedback in the service of face-management needs. Here we focus on hedges in an experimentally parameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced from memory by 21 speakers for co-present addressees, transcribed to text (Galati and Brennan, 2010). We created a gold standard of hedges annotated by human coders (the Roadrunner-Hedge corpus) and compared three LLM-based approaches for hedge detection: fine-tuning BERT, and zero and few-shot prompting with GPT-4o and LLaMA-3. The best-performing approach was a fine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on the top performing approaches, we used an LLM-in-the-Loop approach to improve the gold standard coding, as well as to highlight cases in which hedges are ambiguous in linguistically interesting ways that will guide future research. This is the first step in our research program to train LLMs to interpret and generate collateral signals appropriately and meaningfully in conversation.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Amie Paige, Adil Soubki, and John Murzaku contributed equally to this study"
    },
    {
        "paper id": "2408.03325",
        "abstract url": "https://arxiv.org/abs/2408.03325",
        "title": "CoverBench: A Challenging Benchmark for Complex Claim Verification",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "There is a growing line of research on verifying the correctness of language models' outputs. At the same time, LMs are being used to tackle complex queries that require reasoning. We introduce CoverBench, a challenging benchmark focused on verifying LM outputs in complex reasoning settings. Datasets that can be used for this purpose are often designed for other complex reasoning tasks (e.g., QA) targeting specific use-cases (e.g., financial tables), requiring transformations, negative sampling and selection of hard examples to collect such a benchmark. CoverBench provides a diversified evaluation for complex claim verification in a variety of domains, types of reasoning, relatively long inputs, and a variety of standardizations, such as multiple representations for tables where available, and a consistent schema. We manually vet the data for quality to ensure low levels of label noise. Finally, we report a variety of competitive baseline results to show CoverBench is challenging and has very significant headroom. The data is available at https://huggingface.co/datasets/google/coverbench .",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03326",
        "abstract url": "https://arxiv.org/abs/2408.03326",
        "title": "LLaVA-OneVision: Easy Visual Task Transfer",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "Project Homepage: https://llava-vl.github.io/blog/2024-08-05-llava-onevision/"
    },
    {
        "paper id": "2408.03354",
        "abstract url": "https://arxiv.org/abs/2408.03354",
        "title": "The Use of Large Language Models (LLM) for Cyber Threat Intelligence (CTI) in Cybercrime Forums",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) can be used to analyze cyber threat intelligence (CTI) data from cybercrime forums, which contain extensive information and key discussions about emerging cyber threats. However, to date, the level of accuracy and efficiency of LLMs for such critical tasks has yet to be thoroughly evaluated. Hence, this study assesses the accuracy of an LLM system built on the OpenAI GPT-3.5-turbo model [7] to extract CTI information. To do so, a random sample of 500 daily conversations from three cybercrime forums, XSS, Exploit_in, and RAMP, was extracted, and the LLM system was instructed to summarize the conversations and code 10 key CTI variables, such as whether a large organization and/or a critical infrastructure is being targeted. Then, two coders reviewed each conversation and evaluated whether the information extracted by the LLM was accurate. The LLM system performed strikingly well, with an average accuracy score of 98%. Various ways to enhance the model were uncovered, such as the need to help the LLM distinguish between stories and past events, as well as being careful with verb tenses in prompts. Nevertheless, the results of this study highlight the efficiency and relevance of using LLMs for cyber threat intelligence.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03355",
        "abstract url": "https://arxiv.org/abs/2408.03355",
        "title": "FastEdit: Fast Text-Guided Single-Image Editing via Semantic-Aware Diffusion Fine-Tuning",
        "rating": "1",
        "keywords": [
            [
                "parameter-efficient",
                "efficient fine-tuning"
            ],
            [
                "Diffusion",
                "Image Editing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Conventional Text-guided single-image editing approaches require a two-step process, including fine-tuning the target text embedding for over 1K iterations and the generative model for another 1.5K iterations. Although it ensures that the resulting image closely aligns with both the input image and the target text, this process often requires 7 minutes per image, posing a challenge for practical application due to its time-intensive nature. To address this bottleneck, we introduce FastEdit, a fast text-guided single-image editing method with semantic-aware diffusion fine-tuning, dramatically accelerating the editing process to only 17 seconds. FastEdit streamlines the generative model's fine-tuning phase, reducing it from 1.5K to a mere 50 iterations. For diffusion fine-tuning, we adopt certain time step values based on the semantic discrepancy between the input image and target text. Furthermore, FastEdit circumvents the initial fine-tuning step by utilizing an image-to-image model that conditions on the feature space, rather than the text embedding space. It can effectively align the target text prompt and input image within the same feature space and save substantial processing time. Additionally, we apply the parameter-efficient fine-tuning technique LoRA to U-net. With LoRA, FastEdit minimizes the model's trainable parameters to only 0.37\\% of the original size. At the same time, we can achieve comparable editing outcomes with significantly reduced computational overhead. We conduct extensive experiments to validate the editing performance of our approach and show promising editing capabilities, including content addition, style transfer, background replacement, and posture manipulation, etc.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Technical Report"
    },
    {
        "paper id": "2408.03359",
        "abstract url": "https://arxiv.org/abs/2408.03359",
        "title": "LAMPO: Large Language Models as Preference Machines for Few-shot Ordinal Classification",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We introduce LAMPO, a novel paradigm that leverages Large Language Models (LLMs) for solving few-shot multi-class ordinal classification tasks. Unlike conventional methods, which concatenate all demonstration examples with the test instance and prompt LLMs to produce the pointwise prediction, our framework uses the LLM as a preference machine that makes a relative comparative decision between the test instance and each demonstration. A self-supervised method is then introduced to aggregate these binary comparisons into the final ordinal decision. LAMPO addresses several limitations inherent in previous methods, including context length constraints, ordering biases, and challenges associated with absolute point-wise estimation. Extensive experiments on seven public datasets demonstrate LAMPO's remarkably competitive performance across a diverse spectrum of applications (e.g., movie review analysis and hate speech detection). Notably, in certain applications, the improvement can be substantial, exceeding 20% in an absolute term. Moreover, we believe LAMPO represents an interesting addition to the non-parametric application layered on top of LLMs, as it supports black-box LLMs without necessitating the outputting of LLM's internal states (e.g., embeddings), as seen in previous approaches.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "COLM 2024"
    },
    {
        "paper id": "2408.03388",
        "abstract url": "https://arxiv.org/abs/2408.03388",
        "title": "A Non-negative VAE:the Generalized Gamma Belief Network",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "The gamma belief network (GBN), often regarded as a deep topic model, has demonstrated its potential for uncovering multi-layer interpretable latent representations in text data. Its notable capability to acquire interpretable latent factors is partially attributed to sparse and non-negative gamma-distributed latent variables. However, the existing GBN and its variations are constrained by the linear generative model, thereby limiting their expressiveness and applicability. To address this limitation, we introduce the generalized gamma belief network (Generalized GBN) in this paper, which extends the original linear generative model to a more expressive non-linear generative model. Since the parameters of the Generalized GBN no longer possess an analytic conditional posterior, we further propose an upward-downward Weibull inference network to approximate the posterior distribution of the latent variables. The parameters of both the generative model and the inference network are jointly trained within the variational inference framework. Finally, we conduct comprehensive experiments on both expressivity and disentangled representation learning tasks to evaluate the performance of the Generalized GBN against state-of-the-art Gaussian variational autoencoders serving as baselines.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03402",
        "abstract url": "https://arxiv.org/abs/2408.03402",
        "title": "ULLME: A Unified Framework for Large Language Model Embeddings with Generation-Augmented Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) excel in various natural language processing tasks, but leveraging them for dense passage embedding remains challenging. This is due to their causal attention mechanism and the misalignment between their pre-training objectives and the text ranking tasks. Despite some recent efforts to address these issues, existing frameworks for LLM-based text embeddings have been limited by their support for only a limited range of LLM architectures and fine-tuning strategies, limiting their practical application and versatility. In this work, we introduce the Unified framework for Large Language Model Embedding (ULLME), a flexible, plug-and-play implementation that enables bidirectional attention across various LLMs and supports a range of fine-tuning strategies. We also propose Generation-augmented Representation Learning (GRL), a novel fine-tuning method to boost LLMs for text embedding tasks. GRL enforces consistency between representation-based and generation-based relevance scores, leveraging LLMs' powerful generative abilities for learning passage embeddings. To showcase our framework's flexibility and effectiveness, we release three pre-trained models from ULLME with different backbone architectures, ranging from 1.5B to 8B parameters, all of which demonstrate strong performance on the Massive Text Embedding Benchmark. Our framework is publicly available at: https://github.com/nlp-uoregon/ullme. A demo video for ULLME can also be found at https://rb.gy/ws1ile.",
        "subjects": [
            "cs.CL",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03404",
        "abstract url": "https://arxiv.org/abs/2408.03404",
        "title": "Set2Seq Transformer: Learning Permutation Aware Set Representations of Artistic Sequences",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We propose Set2Seq Transformer, a novel sequential multiple instance architecture, that learns to rank permutation aware set representations of sequences. First, we illustrate that learning temporal position-aware representations of discrete timesteps can greatly improve static visual multiple instance learning methods that do not regard temporality and concentrate almost exclusively on visual content analysis. We further demonstrate the significant advantages of end-to-end sequential multiple instance learning, integrating visual content and temporal information in a multimodal manner. As application we focus on fine art analysis related tasks. To that end, we show that our Set2Seq Transformer can leverage visual set and temporal position-aware representations for modelling visual artists' oeuvres for predicting artistic success. Finally, through extensive quantitative and qualitative evaluation using a novel dataset, WikiArt-Seq2Rank, and a visual learning-to-rank downstream task, we show that our Set2Seq Transformer captures essential temporal information improving the performance of strong static and sequential multiple instance learning methods for predicting artistic success.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03414",
        "abstract url": "https://arxiv.org/abs/2408.03414",
        "title": "Logistic Regression makes small LLMs strong and explainable \"tens-of-shot\" classifiers",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "For simple classification tasks, we show that users can benefit from the advantages of using small, local, generative language models instead of large commercial models without a trade-off in performance or introducing extra labelling costs. These advantages, including those around privacy, availability, cost, and explainability, are important both in commercial applications and in the broader democratisation of AI. Through experiments on 17 sentence classification tasks (2-4 classes), we show that penalised logistic regression on the embeddings from a small LLM equals (and usually betters) the performance of a large LLM in the \"tens-of-shot\" regime. This requires no more labelled instances than are needed to validate the performance of the large LLM. Finally, we extract stable and sensible explanations for classification decisions.",
        "subjects": [
            "cs.CL",
            "cs.LG",
            "stat.ML"
        ],
        "comment": "41 pages, 24 figures"
    },
    {
        "paper id": "2408.03433",
        "abstract url": "https://arxiv.org/abs/2408.03433",
        "title": "Hybrid diffusion models: combining supervised and generative pretraining for label-efficient fine-tuning of segmentation models",
        "rating": "1",
        "keywords": [
            [
                "efficient fine-tuning"
            ],
            [
                "diffusion"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We are considering in this paper the task of label-efficient fine-tuning of segmentation models: We assume that a large labeled dataset is available and allows to train an accurate segmentation model in one domain, and that we have to adapt this model on a related domain where only a few samples are available. We observe that this adaptation can be done using two distinct methods: The first method, supervised pretraining, is simply to take the model trained on the first domain using classical supervised learning, and fine-tune it on the second domain with the available labeled samples. The second method is to perform self-supervised pretraining on the first domain using a generic pretext task in order to get high-quality representations which can then be used to train a model on the second domain in a label-efficient way. We propose in this paper to fuse these two approaches by introducing a new pretext task, which is to perform simultaneously image denoising and mask prediction on the first domain. We motivate this choice by showing that in the same way that an image denoiser conditioned on the noise level can be considered as a generative model for the unlabeled image distribution using the theory of diffusion models, a model trained using this new pretext task can be considered as a generative model for the joint distribution of images and segmentation masks under the assumption that the mapping from images to segmentation masks is deterministic. We then empirically show on several datasets that fine-tuning a model pretrained using this approach leads to better results than fine-tuning a similar model trained using either supervised or unsupervised pretraining only.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "19 pages"
    },
    {
        "paper id": "2408.03440",
        "abstract url": "https://arxiv.org/abs/2408.03440",
        "title": "TF-Locoformer: Transformer with Local Modeling by Convolution for Speech Separation and Enhancement",
        "rating": "1",
        "keywords": [
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Time-frequency (TF) domain dual-path models achieve high-fidelity speech separation. While some previous state-of-the-art (SoTA) models rely on RNNs, this reliance means they lack the parallelizability, scalability, and versatility of Transformer blocks. Given the wide-ranging success of pure Transformer-based architectures in other fields, in this work we focus on removing the RNN from TF-domain dual-path models, while maintaining SoTA performance. This work presents TF-Locoformer, a Transformer-based model with LOcal-modeling by COnvolution. The model uses feed-forward networks (FFNs) with convolution layers, instead of linear layers, to capture local information, letting the self-attention focus on capturing global patterns. We place two such FFNs before and after self-attention to enhance the local-modeling capability. We also introduce a novel normalization for TF-domain dual-path models. Experiments on separation and enhancement datasets show that the proposed model meets or exceeds SoTA in multiple benchmarks with an RNN-free architecture.",
        "subjects": [
            "eess.AS",
            "cs.SD"
        ],
        "comment": "Accepted to IWAENC 2024"
    },
    {
        "paper id": "2408.03492",
        "abstract url": "https://arxiv.org/abs/2408.03492",
        "title": "Automated Theorem Provers Help Improve Large Language Model Reasoning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In this paper we demonstrate how logic programming systems and Automated first-order logic Theorem Provers (ATPs) can improve the accuracy of Large Language Models (LLMs) for logical reasoning tasks where the baseline performance is given by direct LLM solutions. We first evaluate LLM reasoning on steamroller problems using the PRONTOQA benchmark. We show how accuracy can be improved with a neuro-symbolic architecture where the LLM acts solely as a front-end for translating a given problem into a formal logic language and an automated reasoning engine is called for solving it. However, this approach critically hinges on the correctness of the LLM translation. To assess this translation correctness, we secondly define a framework of syntactic and semantic error categories. We implemented the framework and used it to identify errors that LLMs make in the benchmark domain. Based on these findings, we thirdly extended our method with capabilities for automatically correcting syntactic and semantic errors. For semantic error correction we integrate first-order logic ATPs, which is our main and novel contribution. We demonstrate that this approach reduces semantic errors significantly and further increases the accurracy of LLM logical reasoning.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03506",
        "abstract url": "https://arxiv.org/abs/2408.03506",
        "title": "1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your Language Model Thrives on Quality Data",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper presents a compute-efficient approach to pre-training a Language Model-the \"1.5-Pints\"-in only 9 days, while outperforming state-of-the-art models as an instruction-following assistant.Based on MT-Bench (a benchmark that emulates human judgments), 1.5-Pints outperforms Apple's OpenELM and Microsoft's Phi.This is achieved by a carefully curated pre-training dataset of 57 billion tokens, using a mix of automated workflows and manual human review. The selection of the dataset prioritizes content that is considered expository and \"textbook-like\" to aid the model in reasoning and logical deduction, culminating in its overall ability as a strong and versatile AI model. In terms of the model architecture, we employed a modified Mistral tokenizer, alongside a Llama-2 architecture for wider compatibility. For training, we adopted the methodologies used by StableLM, TinyLlama, and Huggingface Zephyr. 1.5-Pints demonstrates that by focusing on data quality over quantity in LLM training, we can significantly reduce training time and resources required. We believe this approach will not only make pre-training more accessible but also reduce our carbon footprint. Our findings and resources from this research are open-sourced, aiming to facilitate further advancements in the field. The 1.5-Pints model is available in two versions: 2K and 16K context windows.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Technical Report for 1.5-Pints"
    },
    {
        "paper id": "2408.03507",
        "abstract url": "https://arxiv.org/abs/2408.03507",
        "title": "GUI Element Detection Using SOTA YOLO Deep Learning Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Detection of Graphical User Interface (GUI) elements is a crucial task for automatic code generation from images and sketches, GUI testing, and GUI search. Recent studies have leveraged both old-fashioned and modern computer vision (CV) techniques. Oldfashioned methods utilize classic image processing algorithms (e.g. edge detection and contour detection) and modern methods use mature deep learning solutions for general object detection tasks. GUI element detection, however, is a domain-specific case of object detection, in which objects overlap more often, and are located very close to each other, plus the number of object classes is considerably lower, yet there are more objects in the images compared to natural images. Hence, the studies that have been carried out on comparing various object detection models, might not apply to GUI element detection. In this study, we evaluate the performance of the four most recent successful YOLO models for general object detection tasks on GUI element detection and investigate their accuracy performance in detecting various GUI elements.",
        "subjects": [
            "cs.CV",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03521",
        "abstract url": "https://arxiv.org/abs/2408.03521",
        "title": "SwinShadow: Shifted Window for Ambiguous Adjacent Shadow Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Shadow detection is a fundamental and challenging task in many computer vision applications. Intuitively, most shadows come from the occlusion of light by the object itself, resulting in the object and its shadow being contiguous (referred to as the adjacent shadow in this paper). In this case, when the color of the object is similar to that of the shadow, existing methods struggle to achieve accurate detection. To address this problem, we present SwinShadow, a transformer-based architecture that fully utilizes the powerful shifted window mechanism for detecting adjacent shadows. The mechanism operates in two steps. Initially, it applies local self-attention within a single window, enabling the network to focus on local details. Subsequently, it shifts the attention windows to facilitate inter-window attention, enabling the capture of a broader range of adjacent information. These combined steps significantly improve the network's capacity to distinguish shadows from nearby objects. And the whole process can be divided into three parts: encoder, decoder, and feature integration. During encoding, we adopt Swin Transformer to acquire hierarchical features. Then during decoding, for shallow layers, we propose a deep supervision (DS) module to suppress the false positives and boost the representation capability of shadow features for subsequent processing, while for deep layers, we leverage a double attention (DA) module to integrate local and shifted window in one stage to achieve a larger receptive field and enhance the continuity of information. Ultimately, a new multi-level aggregation (MLA) mechanism is applied to fuse the decoded features for mask prediction. Extensive experiments on three shadow detection benchmark datasets, SBU, UCF, and ISTD, demonstrate that our network achieves good performance in terms of balance error rate (BER).",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03524",
        "abstract url": "https://arxiv.org/abs/2408.03524",
        "title": "EgyBERT: A Large Language Model Pretrained on Egyptian Dialect Corpora",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This study presents EgyBERT, an Arabic language model pretrained on 10.4 GB of Egyptian dialectal texts. We evaluated EgyBERT's performance by comparing it with five other multidialect Arabic language models across 10 evaluation datasets. EgyBERT achieved the highest average F1-score of 84.25% and an accuracy of 87.33%, significantly outperforming all other comparative models, with MARBERTv2 as the second best model achieving an F1-score 83.68% and an accuracy 87.19%. Additionally, we introduce two novel Egyptian dialectal corpora: the Egyptian Tweets Corpus (ETC), containing over 34.33 million tweets (24.89 million sentences) amounting to 2.5 GB of text, and the Egyptian Forums Corpus (EFC), comprising over 44.42 million sentences (7.9 GB of text) collected from various Egyptian online forums. Both corpora are used in pretraining the new model, and they are the largest Egyptian dialectal corpora to date reported in the literature. Furthermore, this is the first study to evaluate the performance of various language models on Egyptian dialect datasets, revealing significant differences in performance that highlight the need for more dialect-specific models. The results confirm the effectiveness of EgyBERT model in processing and analyzing Arabic text expressed in Egyptian dialect, surpassing other language models included in the study. EgyBERT model is publicly available on \\url{https://huggingface.co/faisalq/EgyBERT}.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03541",
        "abstract url": "https://arxiv.org/abs/2408.03541",
        "title": "EXAONE 3.0 7.8B Instruction Tuned Language Model",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "We introduce EXAONE 3.0 instruction-tuned language model, the first open model in the family of Large Language Models (LLMs) developed by LG AI Research. Among different model sizes, we publicly release the 7.8B instruction-tuned model to promote open research and innovations. Through extensive evaluations across a wide range of public and in-house benchmarks, EXAONE 3.0 demonstrates highly competitive real-world performance with instruction-following capability against other state-of-the-art open models of similar size. Our comparative analysis shows that EXAONE 3.0 excels particularly in Korean, while achieving compelling performance across general tasks and complex reasoning. With its strong real-world effectiveness and bilingual proficiency, we hope that EXAONE keeps contributing to advancements in Expert AI. Our EXAONE 3.0 instruction-tuned model is available at https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03542",
        "abstract url": "https://arxiv.org/abs/2408.03542",
        "title": "Automatic identification of the area covered by acorn trees in the dehesa (pastureland) Extremadura of Spain",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The acorn is the fruit of the oak and is an important crop in the Spanish dehesa extreme\u00f1a, especially for the value it provides in the Iberian pig food to obtain the \"acorn\" certification. For this reason, we want to maximise the production of Iberian pigs with the appropriate weight. Hence the need to know the area covered by the crowns of the acorn trees, to determine the covered wooded area (CWA, from the Spanish Superficie Arbolada Cubierta SAC) and thereby estimate the number of Iberian pigs that can be released per hectare, as indicated by the royal decree 4/2014. In this work, we propose the automatic estimation of the CWA, through aerial digital images (orthophotos) of the pastureland of Extremadura, and with this, to offer the possibility of determining the number of Iberian pigs to be released in a specific plot of land. Among the main issues for automatic detection are, first, the correct identification of acorn trees, secondly, correctly discriminating the shades of the acorn trees and, finally, detect the arbuscles (young acorn trees not yet productive, or shrubs that are not oaks). These difficulties represent a real challenge, both for the automatic segmentation process and for manual segmentation. In this work, the proposed method for automatic segmentation is based on the clustering algorithm proposed by Gustafson-Kessel (GK) but the modified version of Babuska (GK-B) and on the use of real orthophotos. The obtained results are promising both in their comparison with the real images and when compared with the images segmented by hand. The whole set of orthophotos used in this work correspond to an approximate area of 142 hectares, and the results are of great interest to producers of certified \"acorn\" pork.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "22 pages, 15 Figures, 2 Tables"
    },
    {
        "paper id": "2408.03544",
        "abstract url": "https://arxiv.org/abs/2408.03544",
        "title": "Unlocking the Non-Native Language Context Limitation: Native Language Prompting Facilitates Knowledge Elicitation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Multilingual large language models (MLLMs) struggle to answer questions posed in non-dominant languages, even though they have already acquired the relevant knowledge from their dominant language corpus. In contrast, human multilinguals can overcome this issue by invoking the relatively rich knowledge acquired from native language texts through Positive Native Language Transfer (PNLT). Inspired by this, we analogize the dominant language of MLLMs to the native language of human multilinguals, and propose Native Language Prompting (NatLan) to simulate the PNLT observed in human multilinguals. It explicitly creates native language contexts for MLLMs to facilitate the elicitation of the rich native language knowledge during question-answering, unlocking the limitations imposed by non-native language contexts on the effective application of knowledge. By employing multi-MLLM collaboration, NatLan reduces the workload on each MLLM in simulating PNLT and refines semantic transfer. On the C-Eval benchmark, NatLan provides up to a 10.1% average accuracy improvement and up to a 5.0% increase in the hard-level subset across five MLLMs, surpassing all top-notch related methods. Our code is available at https://github.com/AnonyNLP/NatLan.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03545",
        "abstract url": "https://arxiv.org/abs/2408.03545",
        "title": "CLIP-based Point Cloud Classification via Point Cloud to Image Translation",
        "rating": "1",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "3D",
                "Point Cloud",
                "depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Point cloud understanding is an inherently challenging problem because of the sparse and unordered structure of the point cloud in the 3D space. Recently, Contrastive Vision-Language Pre-training (CLIP) based point cloud classification model i.e. PointCLIP has added a new direction in the point cloud classification research domain. In this method, at first multi-view depth maps are extracted from the point cloud and passed through the CLIP visual encoder. To transfer the 3D knowledge to the network, a small network called an adapter is fine-tuned on top of the CLIP visual encoder. PointCLIP has two limitations. Firstly, the point cloud depth maps lack image information which is essential for tasks like classification and recognition. Secondly, the adapter only relies on the global representation of the multi-view features. Motivated by this observation, we propose a Pretrained Point Cloud to Image Translation Network (PPCITNet) that produces generalized colored images along with additional salient visual cues to the point cloud depth maps so that it can achieve promising performance on point cloud classification and understanding. In addition, we propose a novel viewpoint adapter that combines the view feature processed by each viewpoint as well as the global intertwined knowledge that exists across the multi-view features. The experimental results demonstrate the superior performance of the proposed model over existing state-of-the-art CLIP-based models on ModelNet10, ModelNet40, and ScanobjectNN datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by ICPR2024"
    },
    {
        "paper id": "2408.02959",
        "abstract url": "https://arxiv.org/abs/2408.02959",
        "title": "Enhancing Stability and Assessing Uncertainty in Community Detection through a Consensus-based Approach",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Complex data in social and natural sciences find effective representation through networks, wherein quantitative and categorical information can be associated with nodes and connecting edges. The internal structure of networks can be explored using unsupervised machine learning methods known as community detection algorithms. The process of community detection is inherently subject to uncertainty as algorithms utilize heuristic approaches and randomised procedures to explore vast solution spaces, resulting in non-deterministic outcomes and variability in detected communities across multiple runs. Moreover, many algorithms are not designed to identify outliers and may fail to take into account that a network is an unordered mathematical entity. The main aim of our work is to address these issues through a consensus-based approach by introducing a new framework called Consensus Community Detection (CCD). Our method can be applied to different community detection algorithms, allowing the quantification of uncertainty for the whole network as well as for each node, and providing three strategies for dealing with outliers: incorporate, highlight, or group. The effectiveness of our approach is evaluated on artificial benchmark networks.",
        "subjects": [
            "cs.SI",
            "stat.AP"
        ],
        "comment": "22 pages, 11 figures. Submitted to Machine Learning"
    },
    {
        "paper id": "2408.02960",
        "abstract url": "https://arxiv.org/abs/2408.02960",
        "title": "Anytime Multi-Agent Path Finding with an Adaptive Delay-Based Heuristic",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Anytime multi-agent path finding (MAPF) is a promising approach to scalable path optimization in multi-agent systems. MAPF-LNS, based on Large Neighborhood Search (LNS), is the current state-of-the-art approach where a fast initial solution is iteratively optimized by destroying and repairing selected paths of the solution. Current MAPF-LNS variants commonly use an adaptive selection mechanism to choose among multiple destroy heuristics. However, to determine promising destroy heuristics, MAPF-LNS requires a considerable amount of exploration time. As common destroy heuristics are non-adaptive, any performance bottleneck caused by these heuristics cannot be overcome via adaptive heuristic selection alone, thus limiting the overall effectiveness of MAPF-LNS in terms of solution cost. In this paper, we propose Adaptive Delay-based Destroy-and-Repair Enhanced with Success-based Self-Learning (ADDRESS) as a single-destroy-heuristic variant of MAPF-LNS. ADDRESS applies restricted Thompson Sampling to the top-K set of the most delayed agents to select a seed agent for adaptive LNS neighborhood generation. We evaluate ADDRESS in multiple maps from the MAPF benchmark set and demonstrate cost improvements by at least 50% in large-scale scenarios with up to a thousand agents, compared with the original MAPF-LNS and other state-of-the-art methods.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2312.16767"
    },
    {
        "paper id": "2408.02961",
        "abstract url": "https://arxiv.org/abs/2408.02961",
        "title": "Synaptic Modulation using Interspike Intervals Increases Energy Efficiency of Spiking Neural Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Despite basic differences between Spiking Neural Networks (SNN) and Artificial Neural Networks (ANN), most research on SNNs involve adapting ANN-based methods for SNNs. Pruning (dropping connections) and quantization (reducing precision) are often used to improve energy efficiency of SNNs. These methods are very effective for ANNs whose energy needs are determined by signals transmitted on synapses. However, the event-driven paradigm in SNNs implies that energy is consumed by spikes. In this paper, we propose a new synapse model whose weights are modulated by Interspike Intervals (ISI) i.e. time difference between two spikes. SNNs composed of this synapse model, termed ISI Modulated SNNs (IMSNN), can use gradient descent to estimate how the ISI of a neuron changes after updating its synaptic parameters. A higher ISI implies fewer spikes and vice-versa. The learning algorithm for IMSNNs exploits this information to selectively propagate gradients such that learning is achieved by increasing the ISIs resulting in a network that generates fewer spikes. The performance of IMSNNs with dense and convolutional layers have been evaluated in terms of classification accuracy and the number of spikes using the MNIST and FashionMNIST datasets. The performance comparison with conventional SNNs shows that IMSNNs exhibit upto 90% reduction in the number of spikes while maintaining similar classification accuracy.",
        "subjects": [
            "cs.NE",
            "cs.LG"
        ],
        "comment": "9 pages, 3 figures"
    },
    {
        "paper id": "2408.02966",
        "abstract url": "https://arxiv.org/abs/2408.02966",
        "title": "Fast Point Cloud Geometry Compression with Context-based Residual Coding and INR-based Refinement",
        "rating": "0.5",
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "cs.CV",
                "eess.IV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Compressing a set of unordered points is far more challenging than compressing images/videos of regular sample grids, because of the difficulties in characterizing neighboring relations in an irregular layout of points. Many researchers resort to voxelization to introduce regularity, but this approach suffers from quantization loss. In this research, we use the KNN method to determine the neighborhoods of raw surface points. This gives us a means to determine the spatial context in which the latent features of 3D points are compressed by arithmetic coding. As such, the conditional probability model is adaptive to local geometry, leading to significant rate reduction. Additionally, we propose a dual-layer architecture where a non-learning base layer reconstructs the main structures of the point cloud at low complexity, while a learned refinement layer focuses on preserving fine details. This design leads to reductions in model complexity and coding latency by two orders of magnitude compared to SOTA methods. Moreover, we incorporate an implicit neural representation (INR) into the refinement layer, allowing the decoder to sample points on the underlying surface at arbitrary densities. This work is the first to effectively exploit content-aware local contexts for compressing irregular raw point clouds, achieving high rate-distortion performance, low complexity, and the ability to function as an arbitrary-scale upsampling network simultaneously.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": "Accepted by ECCV 2024"
    },
    {
        "paper id": "2408.02999",
        "abstract url": "https://arxiv.org/abs/2408.02999",
        "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The emergence of intelligence in large language models (LLMs) has inspired investigations into their integration into automata learning. This paper introduces the probabilistic Minimally Adequate Teacher (pMAT) formulation, which leverages a probabilistic oracle that could give persistent errors randomly during answering the membership queries for deterministic finite automata (DFA) learning. Given the tendency of LLMs to produce hallucinatory content, we have developed techniques to improve answer accuracy and ensure the correctness of the learned automata. We propose the $\\mathtt{Discrimination}$ prompt as well as the $\\mathtt{Verification}$ prompt and explore their advantages over common prompts. Additionally, we compare DFA learning performance between the TTT algorithm and common active learning algorithms. To address the exponential number of persistent errors, we implement a dynamic query cache refinement algorithm that identifies and corrects conflicting queries by combining the active and passive learning algorithms. The empirical results demonstrate the robustness and efficiency of our approach, providing a theoretical foundation for automata learning with LLMs in the loop.",
        "subjects": [
            "cs.FL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03013",
        "abstract url": "https://arxiv.org/abs/2408.03013",
        "title": "NeurDB: On the Design and Implementation of an AI-powered Autonomous Database",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Databases are increasingly embracing AI to provide autonomous system optimization and intelligent in-database analytics, aiming to relieve end-user burdens across various industry sectors. Nonetheless, most existing approaches fail to account for the dynamic nature of databases, which renders them ineffective for real-world applications characterized by evolving data and workloads. This paper introduces NeurDB, an AI-powered autonomous database that deepens the fusion of AI and databases with adaptability to data and workload drift. NeurDB establishes a new in-database AI ecosystem that seamlessly integrates AI workflows within the database. This integration enables efficient and effective in-database AI analytics and fast-adaptive learned system components. Empirical evaluations demonstrate that NeurDB substantially outperforms existing solutions in managing AI analytics tasks, with the proposed learned components more effectively handling environmental dynamism than state-of-the-art approaches.",
        "subjects": [
            "cs.DB",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03018",
        "abstract url": "https://arxiv.org/abs/2408.03018",
        "title": "Integrating Controllable Motion Skills from Demonstrations",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The expanding applications of legged robots require their mastery of versatile motion skills. Correspondingly, researchers must address the challenge of integrating multiple diverse motion skills into controllers. While existing reinforcement learning (RL)-based approaches have achieved notable success in multi-skill integration for legged robots, these methods often require intricate reward engineering or are restricted to integrating a predefined set of motion skills constrained by specific task objectives, resulting in limited flexibility. In this work, we introduce a flexible multi-skill integration framework named Controllable Skills Integration (CSI). CSI enables the integration of a diverse set of motion skills with varying styles into a single policy without the need for complex reward tuning. Furthermore, in a hierarchical control manner, the trained low-level policy can be coupled with a high-level Natural Language Inference (NLI) module to enable preliminary language-directed skill control. Our experiments demonstrate that CSI can flexibly integrate a diverse array of motion skills more comprehensively and facilitate the transitions between different skills. Additionally, CSI exhibits good scalability as the number of motion skills to be integrated increases significantly.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03029",
        "abstract url": "https://arxiv.org/abs/2408.03029",
        "title": "Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Reward shaping addresses the challenge of sparse rewards in reinforcement learning by constructing denser and more informative reward signals. To achieve self-adaptive and highly efficient reward shaping, we propose a novel method that incorporates success rates derived from historical experiences into shaped rewards. Our approach utilizes success rates sampled from Beta distributions, which dynamically evolve from uncertain to reliable values as more data is collected. Initially, the self-adaptive success rates exhibit more randomness to encourage exploration. Over time, they become more certain to enhance exploitation, thus achieving a better balance between exploration and exploitation. We employ Kernel Density Estimation (KDE) combined with Random Fourier Features (RFF) to derive the Beta distributions, resulting in a computationally efficient implementation in high-dimensional continuous state spaces. This method provides a non-parametric and learning-free approach. The proposed method is evaluated on a wide range of continuous control tasks with sparse and delayed rewards, demonstrating significant improvements in sample efficiency and convergence stability compared to relevant baselines.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03076",
        "abstract url": "https://arxiv.org/abs/2408.03076",
        "title": "Solving QUBO on the Loihi 2 Neuromorphic Processor",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In this article, we describe an algorithm for solving Quadratic Unconstrained Binary Optimization problems on the Intel Loihi 2 neuromorphic processor. The solver is based on a hardware-aware fine-grained parallel simulated annealing algorithm developed for Intel's neuromorphic research chip Loihi 2. Preliminary results show that our approach can generate feasible solutions in as little as 1 ms and up to 37x more energy efficient compared to two baseline solvers running on a CPU. These advantages could be especially relevant for size-, weight-, and power-constrained edge computing applications.",
        "subjects": [
            "cs.NE",
            "cs.AI",
            "cs.DM"
        ],
        "comment": "12 pages, 3 figures. Shared first authorship: Alessandro Pierro, Philipp Stratmann, and Gabriel Andres Fonseca Guerra"
    },
    {
        "paper id": "2408.03096",
        "abstract url": "https://arxiv.org/abs/2408.03096",
        "title": "Enhancing Twitter Bot Detection via Multimodal Invariant Representations",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Detecting Twitter Bots is crucial for maintaining the integrity of online discourse, safeguarding democratic processes, and preventing the spread of malicious propaganda. However, advanced Twitter Bots today often employ sophisticated feature manipulation and account farming techniques to blend seamlessly with genuine user interactions, posing significant challenges to existing detection models. In response to these challenges, this paper proposes a novel Twitter Bot Detection framework called BotSAI. This framework enhances the consistency of multimodal user features, accurately characterizing various modalities to distinguish between real users and bots. Specifically, the architecture integrates information from users, textual content, and heterogeneous network topologies, leveraging customized encoders to obtain comprehensive user feature representations. The heterogeneous network encoder efficiently aggregates information from neighboring nodes through oversampling techniques and local relationship transformers. Subsequently, a multi-channel representation mechanism maps user representations into invariant and specific subspaces, enhancing the feature vectors. Finally, a self-attention mechanism is introduced to integrate and refine the enhanced user representations, enabling efficient information interaction. Extensive experiments demonstrate that BotSAI outperforms existing state-of-the-art methods on two major Twitter Bot Detection benchmarks, exhibiting superior performance. Additionally, systematic experiments reveal the impact of different social relationships on detection accuracy, providing novel insights for the identification of social bots.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03144",
        "abstract url": "https://arxiv.org/abs/2408.03144",
        "title": "Active Learning for Level Set Estimation Using Randomized Straddle Algorithms",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Level set estimation (LSE), the problem of identifying the set of input points where a function takes value above (or below) a given threshold, is important in practical applications. When the function is expensive-to-evaluate and black-box, the \\textit{straddle} algorithm, which is a representative heuristic for LSE based on Gaussian process models, and its extensions having theoretical guarantees have been developed. However, many of existing methods include a confidence parameter $\u03b2^{1/2}_t$ that must be specified by the user, and methods that choose $\u03b2^{1/2}_t$ heuristically do not provide theoretical guarantees. In contrast, theoretically guaranteed values of $\u03b2^{1/2}_t$ need to be increased depending on the number of iterations and candidate points, and are conservative and not good for practical performance. In this study, we propose a novel method, the \\textit{randomized straddle} algorithm, in which $\u03b2_t$ in the straddle algorithm is replaced by a random sample from the chi-squared distribution with two degrees of freedom. The confidence parameter in the proposed method has the advantages of not needing adjustment, not depending on the number of iterations and candidate points, and not being conservative. Furthermore, we show that the proposed method has theoretical guarantees that depend on the sample complexity and the number of iterations. Finally, we confirm the usefulness of the proposed method through numerical experiments using synthetic and real data.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "21 pages, 4 figures"
    },
    {
        "paper id": "2408.03146",
        "abstract url": "https://arxiv.org/abs/2408.03146",
        "title": "The Dawn of Decentralized Social Media: An Exploration of Bluesky's Public Opening",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Bluesky is a Twitter-like decentralized social media platform that has recently grown in popularity. After an invite-only period, it opened to the public worldwide on February 6th, 2024. In this paper, we provide a longitudinal analysis of user activity in the two months around the opening, studying changes in the general characteristics of the platform due to the rapid growth of the user base. We observe a broad distribution of activity similar to more established platforms, but a higher volume of original than reshared content, and very low toxicity. After opening to the public, Bluesky experienced a large surge in new users and activity, especially posting English and Japanese content. In particular, several accounts entered the discussion with suspicious behavior, like following many accounts and sharing content from low-credibility news outlets. Some of these have already been classified as spam or suspended, suggesting effective moderation.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "Long paper that has been accepted at the ASONAM 2024 conference"
    },
    {
        "paper id": "2408.03154",
        "abstract url": "https://arxiv.org/abs/2408.03154",
        "title": "Fake News Detection via Wisdom of Synthetic & Representative Crowds",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Social media companies have struggled to provide a democratically legitimate definition of \"Fake News\". Reliance on expert judgment has attracted criticism due to a general trust deficit and political polarisation. Approaches reliant on the ``wisdom of the crowds'' are a cost-effective, transparent and inclusive alternative. This paper provides a novel end-to-end methodology to detect fake news on X via \"wisdom of the synthetic & representative crowds\". We deploy an online survey on the Lucid platform to gather veracity assessments for a number of pandemic-related tweets from crowd-workers. Borrowing from the MrP literature, we train a Hierarchical Bayesian model to predict the veracity of each tweet from the perspective of different personae from the population of interest. We then weight the predicted veracity assessments according to a representative stratification frame, such that decisions about ``fake'' tweets are representative of the overall polity of interest. Based on these aggregated scores, we analyse a corpus of tweets and perform a second MrP to generate state-level estimates of the number of people who share fake news. We find small but statistically meaningful heterogeneity in fake news sharing across US states. At the individual-level: i. sharing fake news is generally rare, with an average sharing probability interval [0.07,0.14]; ii. strong evidence that Democrats share less fake news, accounting for a reduction in the sharing odds of [57.3%,3.9%] relative to the average user; iii. when Republican definitions of fake news are used, it is the latter who show a decrease in the propensity to share fake news worth [50.8%, 2.0%]; iv. some evidence that women share less fake news than men, an effect worth a [29.5%,4.9%] decrease.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "50 pages, 6 figures, 2 tables"
    },
    {
        "paper id": "2408.03199",
        "abstract url": "https://arxiv.org/abs/2408.03199",
        "title": "Convergence Conditions for Stochastic Line Search Based Optimization of Over-parametrized Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we deal with algorithms to solve the finite-sum problems related to fitting over-parametrized models, that typically satisfy the interpolation condition. In particular, we focus on approaches based on stochastic line searches and employing general search directions. We define conditions on the sequence of search directions that guarantee finite termination and bounds for the backtracking procedure. Moreover, we shed light on the additional property of directions needed to prove fast (linear) convergence of the general class of algorithms when applied to PL functions in the interpolation regime. From the point of view of algorithms design, the proposed analysis identifies safeguarding conditions that could be employed in relevant algorithmic framework. In particular, it could be of interest to integrate stochastic line searches within momentum, conjugate gradient or adaptive preconditioning methods.",
        "subjects": [
            "math.OC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03220",
        "abstract url": "https://arxiv.org/abs/2408.03220",
        "title": "Masked Random Noise for Communication Efficient Federaetd Learning",
        "rating": "0.5",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "Federated learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning is a promising distributed training paradigm that effectively safeguards data privacy. However, it may involve significant communication costs, which hinders training efficiency. In this paper, we aim to enhance communication efficiency from a new perspective. Specifically, we request the distributed clients to find optimal model updates relative to global model parameters within predefined random noise. For this purpose, we propose Federated Masked Random Noise (FedMRN), a novel framework that enables clients to learn a 1-bit mask for each model parameter and apply masked random noise (i.e., the Hadamard product of random noise and masks) to represent model updates. To make FedMRN feasible, we propose an advanced mask training strategy, called progressive stochastic masking (PSM). After local training, each client only need to transmit local masks and a random seed to the server. Additionally, we provide theoretical guarantees for the convergence of FedMRN under both strongly convex and non-convex assumptions. Extensive experiments are conducted on four popular datasets. The results show that FedMRN exhibits superior convergence speed and test accuracy compared to relevant baselines, while attaining a similar level of accuracy as FedAvg.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": "Accepted by MM 2024"
    },
    {
        "paper id": "2408.03274",
        "abstract url": "https://arxiv.org/abs/2408.03274",
        "title": "Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "To deploy machine learning models on-device, practitioners use compression algorithms to shrink and speed up models while maintaining their high-quality output. A critical aspect of compression in practice is model comparison, including tracking many compression experiments, identifying subtle changes in model behavior, and negotiating complex accuracy-efficiency trade-offs. However, existing compression tools poorly support comparison, leading to tedious and, sometimes, incomplete analyses spread across disjoint tools. To support real-world comparative workflows, we develop an interactive visual system called Compress and Compare. Within a single interface, Compress and Compare surfaces promising compression strategies by visualizing provenance relationships between compressed models and reveals compression-induced behavior changes by comparing models' predictions, weights, and activations. We demonstrate how Compress and Compare supports common compression analysis tasks through two case studies, debugging failed compression on generative language models and identifying compression artifacts in image classification models. We further evaluate Compress and Compare in a user study with eight compression experts, illustrating its potential to provide structure to compression workflows, help practitioners build intuition about compression, and encourage thorough analysis of compression's effect on model behavior. Through these evaluations, we identify compression-specific challenges that future visual analytics tools should consider and Compress and Compare visualizations that may generalize to broader model comparison tasks.",
        "subjects": [
            "cs.HC",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted to VIS 2024"
    },
    {
        "paper id": "2408.03292",
        "abstract url": "https://arxiv.org/abs/2408.03292",
        "title": "Static IR Drop Prediction with Attention U-Net and Saliency-Based Explainability",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "There has been significant recent progress to reduce the computational effort of static IR drop analysis using neural networks, and modeling as an image-to-image translation task. A crucial issue is the lack of sufficient data from real industry designs to train these networks. Additionally, there is no methodology to explain a high-drop pixel in a predicted IR drop image to its specific root-causes. In this work, we first propose a U-Net neural network model with attention gates which is specifically tailored to achieve fast and accurate image-based static IR drop prediction. Attention gates allow selective emphasis on relevant parts of the input data without supervision which is desired because of the often sparse nature of the IR drop map. We propose a two-phase training process which utilizes a mix of artificially-generated data and a limited number of points from real designs. The results are, on-average, 18% (53%) better in MAE and 14% (113%) in F1 score compared to the winner of the ICCAD 2023 contest (and U-Net only) when tested on real designs. Second, we propose a fast method using saliency maps which can explain a predicted IR drop in terms of specific input pixels contributing the most to a drop. In our experiments, we show the number of high IR drop pixels can be reduced on-average by 18% by mimicking upsize of a tiny portion of PDN's resistive edges.",
        "subjects": [
            "cs.AR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03303",
        "abstract url": "https://arxiv.org/abs/2408.03303",
        "title": "Understanding How Blind Users Handle Object Recognition Errors: Strategies and Challenges",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Object recognition technologies hold the potential to support blind and low-vision people in navigating the world around them. However, the gap between benchmark performances and practical usability remains a significant challenge. This paper presents a study aimed at understanding blind users' interaction with object recognition systems for identifying and avoiding errors. Leveraging a pre-existing object recognition system, URCam, fine-tuned for our experiment, we conducted a user study involving 12 blind and low-vision participants. Through in-depth interviews and hands-on error identification tasks, we gained insights into users' experiences, challenges, and strategies for identifying errors in camera-based assistive technologies and object recognition systems. During interviews, many participants preferred independent error review, while expressing apprehension toward misrecognitions. In the error identification task, participants varied viewpoints, backgrounds, and object sizes in their images to avoid and overcome errors. Even after repeating the task, participants identified only half of the errors, and the proportion of errors identified did not significantly differ from their first attempts. Based on these insights, we offer implications for designing accessible interfaces tailored to the needs of blind and low-vision users in identifying object recognition errors.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03307",
        "abstract url": "https://arxiv.org/abs/2408.03307",
        "title": "Pre-training and in-context learning IS Bayesian inference a la De Finetti",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Accurately gauging uncertainty on the underlying environment is a longstanding goal of intelligent systems. We characterize which latent concepts pre-trained sequence models are naturally able to reason with. We go back to De Finetti's predictive view of Bayesian reasoning: instead of modeling latent parameters through priors and likelihoods like topic models do, De Finetti has long advocated for modeling exchangeable (permutation invariant) sequences of observables. According to this view, pre-training autoregressive models formulates informed beliefs based on prior observations (\"empirical Bayes\"), and forward generation is a simulated instantiation of an environment (\"posterior inference\"). This connection allows extending in-context learning (ICL) beyond predictive settings, highlighting sequence models' ability to perform explicit statistical inference. In particular, we show the sequence prediction loss over exchangeable documents controls performance on downstream tasks where uncertainty quantification is key. Empirically, we propose and demonstrate several approaches for encoding exchangeability in sequence model architectures: data augmentation, regularization, and causal masking.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03407",
        "abstract url": "https://arxiv.org/abs/2408.03407",
        "title": "Deep Clustering via Distribution Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Distribution learning finds probability density functions from a set of data samples, whereas clustering aims to group similar data points to form clusters. Although there are deep clustering methods that employ distribution learning methods, past work still lacks theoretical analysis regarding the relationship between clustering and distribution learning. Thus, in this work, we provide a theoretical analysis to guide the optimization of clustering via distribution learning. To achieve better results, we embed deep clustering guided by a theoretical analysis. Furthermore, the distribution learning method cannot always be directly applied to data. To overcome this issue, we introduce a clustering-oriented distribution learning method called Monte-Carlo Marginalization for Clustering. We integrate Monte-Carlo Marginalization for Clustering into Deep Clustering, resulting in Deep Clustering via Distribution Learning (DCDL). Eventually, the proposed DCDL achieves promising results compared to state-of-the-art methods on popular datasets. Considering a clustering task, the new distribution learning method outperforms previous methods as well.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03408",
        "abstract url": "https://arxiv.org/abs/2408.03408",
        "title": "LLM-Aided Compilation for Tensor Accelerators",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Hardware accelerators, in particular accelerators for tensor processing, have many potential application domains. However, they currently lack the software infrastructure to support the majority of domains outside of deep learning. Furthermore, a compiler that can easily be updated to reflect changes at both application and hardware levels would enable more agile development and design space exploration of accelerators, allowing hardware designers to realize closer-to-optimal performance. In this work, we discuss how large language models (LLMs) could be leveraged to build such a compiler. Specifically, we demonstrate the ability of GPT-4 to achieve high pass rates in translating code to the Gemmini accelerator, and prototype a technique for decomposing translation into smaller, more LLM-friendly steps. Additionally, we propose a 2-phase workflow for utilizing LLMs to generate hardware-optimized code.",
        "subjects": [
            "cs.AR",
            "cs.LG",
            "cs.PL"
        ],
        "comment": "4 page workshop paper"
    },
    {
        "paper id": "2408.03424",
        "abstract url": "https://arxiv.org/abs/2408.03424",
        "title": "Working with Color: How Color Quantization Can Aid Researchers of Problematic Information",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Analyzing large sets of visual media remains a challenging task, particularly in mixed-method studies dealing with problematic information and human subjects. Using AI tools in such analyses risks reifying and exacerbating biases, as well as untenable computational and cost limitations. As such, we turn to adopting geometric computer graphics and vision methods towards analyzing a large set of images from a problematic information campaign, in conjunction with human-in-the-loop qualitative analysis. We illustrate an effective case of this approach with the implementation of color quantization towards analyzing online hate image at the US-Mexico border, along with a historicist trace of the history of color quantization and skin tone scales, to inform our usage and reclamation of these methodologies from their racist origins. To that end, we scaffold motivations and the need for more researchers to consider the advantages and risks of reclaiming such methodologies in their own work, situated in our case study.",
        "subjects": [
            "cs.CY",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03457",
        "abstract url": "https://arxiv.org/abs/2408.03457",
        "title": "Transit Rider Heat Stress in Atlanta, GA under Current and Future Climate Scenarios",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI",
                "cs.CY"
            ]
        ],
        "abstract": "Transit is a crucial mode of transportation, especially in urban areas and for urban and rural disadvantaged communities. Because extreme temperatures often pose threats to the elderly, members of the disability community, and other vulnerable populations, this study seeks to understand the level of influence that extreme temperatures may have on transit users across different demographic groups. In this case study for Atlanta, GA, heat stress is predicted for 2019 transit riders (using transit rider activity survey data) and for three future climate scenarios, SSP245, SSP370, and SSP585, into the year 2100. The HeatPath Analyzer and TransitSim 4.0 models were applied to predict cumulative heat exposure and trip-level risk for 35,999 trip equivalents for an average Atlanta area weekday in the summer of 2019. The analyses show that under 2019 weather conditions, 8.33% of summer trips were estimated to be conducted under extreme heat. With the projected future climate conditions, the percentage of trips under extreme heat risk grows steadily. By 2100, 37.1%, 56.1%, and 76.4% are projected to be under extreme heat risk for scenarios SSP245, SSP370, and SSP585, respectively. Under current weather conditions, Atlanta transit riders that own no vehicles and transit riders that are African American are disproportionately influenced by extreme heat. The disparity between these two groups and other groups of transit riders becomes wider as climate change continues to exacerbate. The findings of the study highlight an urgent need to implement heat mitigation and adaptation strategies in urban transit networks.",
        "subjects": [
            "cs.SI",
            "cs.CY",
            "physics.soc-ph"
        ],
        "comment": "24 pages, 1 table, 4 figures"
    },
    {
        "paper id": "2408.03461",
        "abstract url": "https://arxiv.org/abs/2408.03461",
        "title": "When does the mean network capture the topology of a sample of networks?",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "The notion of Fr\u00e9chet mean (also known as \"barycenter\") network is the workhorse of most machine learning algorithms that require the estimation of a \"location\" parameter to analyse network-valued data. In this context, it is critical that the network barycenter inherits the topological structure of the networks in the training dataset. The metric - which measures the proximity between networks - controls the structural properties of the barycenter. This work is significant because it provides for the first time analytical estimates of the sample Fr\u00e9chet mean for the stochastic blockmodel, which is at the cutting edge of rigorous probabilistic analysis of random networks. We show that the mean network computed with the Hamming distance is unable to capture the topology of the networks in the training sample, whereas the mean network computed using the effective resistance distance recovers the correct partitions and associated edge density. From a practical standpoint, our work informs the choice of metrics in the context where the sample Fr\u00e9chet mean network is used to characterise the topology of networks for network-valued machine learning",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "cs.SI",
            "physics.data-an"
        ],
        "comment": "23 pages"
    },
    {
        "paper id": "2408.03472",
        "abstract url": "https://arxiv.org/abs/2408.03472",
        "title": "Integrating HCI Datasets in Project-Based Machine Learning Courses: A College-Level Review and Case Study",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "This study explores the integration of real-world machine learning (ML) projects using human-computer interfaces (HCI) datasets in college-level courses to enhance both teaching and learning experiences. Employing a comprehensive literature review, course websites analysis, and a detailed case study, the research identifies best practices for incorporating HCI datasets into project-based ML education. Key f indings demonstrate increased student engagement, motivation, and skill development through hands-on projects, while instructors benefit from effective tools for teaching complex concepts. The study also addresses challenges such as data complexity and resource allocation, offering recommendations for future improvements. These insights provide a valuable framework for educators aiming to bridge the gap between",
        "subjects": [
            "cs.LG",
            "cs.CY",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03489",
        "abstract url": "https://arxiv.org/abs/2408.03489",
        "title": "Harnessing the Power of LLMs in Source Code Vulnerability Detection",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Software vulnerabilities, caused by unintentional flaws in source code, are a primary root cause of cyberattacks. Static analysis of source code has been widely used to detect these unintentional defects introduced by software developers. Large Language Models (LLMs) have demonstrated human-like conversational abilities due to their capacity to capture complex patterns in sequential data, such as natural languages. In this paper, we harness LLMs' capabilities to analyze source code and detect known vulnerabilities. To ensure the proposed vulnerability detection method is universal across multiple programming languages, we convert source code to LLVM IR and train LLMs on these intermediate representations. We conduct extensive experiments on various LLM architectures and compare their accuracy. Our comprehensive experiments on real-world and synthetic codes from NVD and SARD demonstrate high accuracy in identifying source code vulnerabilities.",
        "subjects": [
            "cs.SE",
            "cs.AI",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03497",
        "abstract url": "https://arxiv.org/abs/2408.03497",
        "title": "Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and Tabnet with SMOTEENN",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Bank credit risk is a significant challenge in modern financial transactions, and the ability to identify qualified credit card holders among a large number of applicants is crucial for the profitability of a bank'sbank's credit card business. In the past, screening applicants'applicants' conditions often required a significant amount of manual labor, which was time-consuming and labor-intensive. Although the accuracy and reliability of previously used ML models have been continuously improving, the pursuit of more reliable and powerful AI intelligent models is undoubtedly the unremitting pursuit by major banks in the financial industry. In this study, we used a dataset of over 40,000 records provided by a commercial bank as the research object. We compared various dimensionality reduction techniques such as PCA and T-SNE for preprocessing high-dimensional datasets and performed in-depth adaptation and tuning of distributed models such as LightGBM and XGBoost, as well as deep models like Tabnet. After a series of research and processing, we obtained excellent research results by combining SMOTEENN with these techniques. The experiments demonstrated that LightGBM combined with PCA and SMOTEENN techniques can assist banks in accurately predicting potential high-quality customers, showing relatively outstanding performance compared to other models.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "8 pagess on IEEE ICPICS"
    },
    {
        "paper id": "2408.03502",
        "abstract url": "https://arxiv.org/abs/2408.03502",
        "title": "Role Identification based Method for Cyberbullying Analysis in Social Edge Computing",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Over the past few years, many efforts have been dedicated to studying cyberbullying in social edge computing devices, and most of them focus on three roles: victims, perpetrators, and bystanders. If we want to obtain a deep insight into the formation, evolution, and intervention of cyberbullying in devices at the edge of the Internet, it is necessary to explore more fine-grained roles. This paper presents a multi-level method for role feature modeling and proposes a differential evolution-assisted K-means (DEK) method to identify diverse roles. Our work aims to provide a role identification scheme for cyberbullying scenarios for social edge computing environments to alleviate the general safety issues that cyberbullying brings. The experiments on ten real-world datasets obtained from Weibo and five public datasets show that the proposed DEK outperforms the existing approaches on the method level. After clustering, we obtained nine roles and analyzed the characteristics of each role and their evolution trends under different cyberbullying scenarios. Our work in this paper can be placed in devices at the edge of the Internet, leading to better real-time identification performance and adapting to the broad geographic location and high mobility of mobile devices.",
        "subjects": [
            "cs.HC",
            "cs.SI"
        ],
        "comment": "This paper has been accepted for publication in the Tsinghua Science and Technology"
    },
    {
        "paper id": "2408.03519",
        "abstract url": "https://arxiv.org/abs/2408.03519",
        "title": "RepoMasterEval: Evaluating Code Completion via Real-World Repositories",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "With the growing reliance on automated code completion tools in software development, the need for robust evaluation benchmarks has become critical. However, existing benchmarks focus more on code generation tasks in function and class level and provide rich text description to prompt the model. By contrast, such descriptive prompt is commonly unavailable in real development and code completion can occur in wider range of situations such as in the middle of a function or a code block. These limitations makes the evaluation poorly align with the practical scenarios of code completion tools. In this paper, we propose RepoMasterEval, a novel benchmark for evaluating code completion models constructed from real-world Python and TypeScript repositories. Each benchmark datum is generated by masking a code snippet (ground truth) from one source code file with existing test suites. To improve test accuracy of model generated code, we employ mutation testing to measure the effectiveness of the test cases and we manually crafted new test cases for those test suites with low mutation score. Our empirical evaluation on 6 state-of-the-art models shows that test argumentation is critical in improving the accuracy of the benchmark and RepoMasterEval is able to report difference in model performance in real-world scenarios. The deployment of RepoMasterEval in a collaborated company for one month also revealed that the benchmark is useful to give accurate feedback during model training and the score is in high correlation with the model's performance in practice. Based on our findings, we call for the software engineering community to build more LLM benchmarks tailored for code generation tools taking the practical and complex development environment into consideration.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.02983",
        "abstract url": "https://arxiv.org/abs/2408.02983",
        "title": "Diffusion Model Meets Non-Exemplar Class-Incremental Learning and Beyond",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Non-exemplar class-incremental learning (NECIL) is to resist catastrophic forgetting without saving old class samples. Prior methodologies generally employ simple rules to generate features for replaying, suffering from large distribution gap between replayed features and real ones. To address the aforementioned issue, we propose a simple, yet effective \\textbf{Diff}usion-based \\textbf{F}eature \\textbf{R}eplay (\\textbf{DiffFR}) method for NECIL. First, to alleviate the limited representational capacity caused by fixing the feature extractor, we employ Siamese-based self-supervised learning for initial generalizable features. Second, we devise diffusion models to generate class-representative features highly similar to real features, which provides an effective way for exemplar-free knowledge memorization. Third, we introduce prototype calibration to direct the diffusion model's focus towards learning the distribution shapes of features, rather than the entire distribution. Extensive experiments on public datasets demonstrate significant performance gains of our DiffFR, outperforming the state-of-the-art NECIL methods by 3.0\\% in average. The code will be made publicly available soon.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03001",
        "abstract url": "https://arxiv.org/abs/2408.03001",
        "title": "Multitask and Multimodal Neural Tuning for Large Models",
        "rating": "0",
        "keywords": [
            [
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, large-scale multimodal models have demonstrated impressive capabilities across various domains. However, enabling these models to effectively perform multiple multimodal tasks simultaneously remains a significant challenge. To address this, we introduce a novel tuning method called neural tuning, designed to handle diverse multimodal tasks concurrently, including reasoning segmentation, referring segmentation, image captioning, and text-to-image generation. Neural tuning emulates sparse distributed representation in human brain, where only specific subsets of neurons are activated for each task. Additionally, we present a new benchmark, MMUD, where each sample is annotated with multiple task labels. By applying neural tuning to pretrained large models on the MMUD benchmark, we achieve simultaneous task handling in a streamlined and efficient manner. All models, code, and datasets will be publicly available after publication, facilitating further research and development in this field.",
        "subjects": [
            "cs.CV",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03014",
        "abstract url": "https://arxiv.org/abs/2408.03014",
        "title": "CKNN: Cleansed k-Nearest Neighbor for Unsupervised Video Anomaly Detection",
        "rating": "0",
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we address the problem of unsupervised video anomaly detection (UVAD). The task aims to detect abnormal events in test video using unlabeled videos as training data. The presence of anomalies in the training data poses a significant challenge in this task, particularly because they form clusters in the feature space. We refer to this property as the \"Anomaly Cluster\" issue. The condensed nature of these anomalies makes it difficult to distinguish between normal and abnormal data in the training set. Consequently, training conventional anomaly detection techniques using an unlabeled dataset often leads to sub-optimal results. To tackle this difficulty, we propose a new method called Cleansed k-Nearest Neighbor (CKNN), which explicitly filters out the Anomaly Clusters by cleansing the training dataset. Following the k-nearest neighbor algorithm in the feature space provides powerful anomaly detection capability. Although the identified Anomaly Cluster issue presents a significant challenge to applying k-nearest neighbor in UVAD, our proposed cleansing scheme effectively addresses this problem. We evaluate the proposed method on various benchmark datasets and demonstrate that CKNN outperforms the previous state-of-the-art UVAD method by up to 8.5% (from 82.0 to 89.0) in terms of AUROC. Moreover, we emphasize that the performance of the proposed method is comparable to that of the state-of-the-art method trained using anomaly-free data.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03065",
        "abstract url": "https://arxiv.org/abs/2408.03065",
        "title": "SCOPE: A Synthetic Multi-Modal Dataset for Collective Perception Including Physical-Correct Weather Conditions",
        "rating": "0",
        "keywords": [
            [
                "autonomous driving",
                "LiDAR",
                "vehicle"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Collective perception has received considerable attention as a promising approach to overcome occlusions and limited sensing ranges of vehicle-local perception in autonomous driving. In order to develop and test novel collective perception technologies, appropriate datasets are required. These datasets must include not only different environmental conditions, as they strongly influence the perception capabilities, but also a wide range of scenarios with different road users as well as realistic sensor models. Therefore, we propose the Synthetic COllective PErception (SCOPE) dataset. SCOPE is the first synthetic multi-modal dataset that incorporates realistic camera and LiDAR models as well as parameterized and physically accurate weather simulations for both sensor types. The dataset contains 17,600 frames from over 40 diverse scenarios with up to 24 collaborative agents, infrastructure sensors, and passive traffic, including cyclists and pedestrians. In addition, recordings from two novel digital-twin maps from Karlsruhe and T\u00fcbingen are included. The dataset is available at https://ekut-es.github.io/scope",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03079",
        "abstract url": "https://arxiv.org/abs/2408.03079",
        "title": "Enhancing Complex Causality Extraction via Improved Subtask Interaction and Knowledge Fusion",
        "rating": "0",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Event Causality Extraction (ECE) aims at extracting causal event pairs from texts. Despite ChatGPT's recent success, fine-tuning small models remains the best approach for the ECE task. However, existing fine-tuning based ECE methods cannot address all three key challenges in ECE simultaneously: 1) Complex Causality Extraction, where multiple causal-effect pairs occur within a single sentence; 2) Subtask~ Interaction, which involves modeling the mutual dependence between the two subtasks of ECE, i.e., extracting events and identifying the causal relationship between extracted events; and 3) Knowledge Fusion, which requires effectively fusing the knowledge in two modalities, i.e., the expressive pretrained language models and the structured knowledge graphs. In this paper, we propose a unified ECE framework (UniCE to address all three issues in ECE simultaneously. Specifically, we design a subtask interaction mechanism to enable mutual interaction between the two ECE subtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in the two modalities. Furthermore, we employ separate decoders for each subtask to facilitate complex causality extraction. Experiments on three benchmark datasets demonstrate that our method achieves state-of-the-art performance and outperforms ChatGPT with a margin of at least 30% F1-score. More importantly, our model can also be used to effectively improve the ECE performance of ChatGPT via in-context learning.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "NLPCC 2024 Oral"
    },
    {
        "paper id": "2408.03130",
        "abstract url": "https://arxiv.org/abs/2408.03130",
        "title": "Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations",
        "rating": "0",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models are ubiquitous in natural language processing because they can adapt to new tasks without retraining. However, their sheer scale and complexity present unique challenges and opportunities, prompting researchers and practitioners to explore novel model training, optimization, and deployment methods. This literature review focuses on various techniques for reducing resource requirements and compressing large language models, including quantization, pruning, knowledge distillation, and architectural optimizations. The primary objective is to explore each method in-depth and highlight its unique challenges and practical applications. The discussed methods are categorized into a taxonomy that presents an overview of the optimization landscape and helps navigate it to understand the research trajectory better.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03209",
        "abstract url": "https://arxiv.org/abs/2408.03209",
        "title": "IPAdapter-Instruct: Resolving Ambiguity in Image-based Conditioning using Instruct Prompts",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion models continuously push the boundary of state-of-the-art image generation, but the process is hard to control with any nuance: practice proves that textual prompts are inadequate for accurately describing image style or fine structural details (such as faces). ControlNet and IPAdapter address this shortcoming by conditioning the generative process on imagery instead, but each individual instance is limited to modeling a single conditional posterior: for practical use-cases, where multiple different posteriors are desired within the same workflow, training and using multiple adapters is cumbersome. We propose IPAdapter-Instruct, which combines natural-image conditioning with ``Instruct'' prompts to swap between interpretations for the same conditioning image: style transfer, object extraction, both, or something else still? IPAdapterInstruct efficiently learns multiple tasks with minimal loss in quality compared to dedicated per-task models.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "17 pages, 10 figures, Project page: https://unity-research.github.io/IP-Adapter-Instruct.github.io/"
    },
    {
        "paper id": "2408.03225",
        "abstract url": "https://arxiv.org/abs/2408.03225",
        "title": "Line-based 6-DoF Object Pose Estimation and Tracking With an Event Camera",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "6-DoF",
                "Event Camera"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Pose estimation and tracking of objects is a fundamental application in 3D vision. Event cameras possess remarkable attributes such as high dynamic range, low latency, and resilience against motion blur, which enables them to address challenging high dynamic range scenes or high-speed motion. These features make event cameras an ideal complement over standard cameras for object pose estimation. In this work, we propose a line-based robust pose estimation and tracking method for planar or non-planar objects using an event camera. Firstly, we extract object lines directly from events, then provide an initial pose using a globally-optimal Branch-and-Bound approach, where 2D-3D line correspondences are not known in advance. Subsequently, we utilize event-line matching to establish correspondences between 2D events and 3D models. Furthermore, object poses are refined and continuously tracked by minimizing event-line distances. Events are assigned different weights based on these distances, employing robust estimation algorithms. To evaluate the precision of the proposed methods in object pose estimation and tracking, we have devised and established an event-based moving object dataset. Compared against state-of-the-art methods, the robustness and accuracy of our methods have been validated both on synthetic experiments and the proposed dataset. The source code is available at https://github.com/Zibin6/LOPET.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by IEEE Transactions on Image Processing,2024"
    },
    {
        "paper id": "2408.03356",
        "abstract url": "https://arxiv.org/abs/2408.03356",
        "title": "RayGauss: Volumetric Gaussian-Based Ray Casting for Photorealistic Novel View Synthesis",
        "rating": "0",
        "keywords": [
            [
                "NeRF",
                "Radiance Fields"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Differentiable volumetric rendering-based methods made significant progress in novel view synthesis. On one hand, innovative methods have replaced the Neural Radiance Fields (NeRF) network with locally parameterized structures, enabling high-quality renderings in a reasonable time. On the other hand, approaches have used differentiable splatting instead of NeRF's ray casting to optimize radiance fields rapidly using Gaussian kernels, allowing for fine adaptation to the scene. However, differentiable ray casting of irregularly spaced kernels has been scarcely explored, while splatting, despite enabling fast rendering times, is susceptible to clearly visible artifacts. Our work closes this gap by providing a physically consistent formulation of the emitted radiance c and density \u03c3, decomposed with Gaussian functions associated with Spherical Gaussians/Harmonics for all-frequency colorimetric representation. We also introduce a method enabling differentiable ray casting of irregularly distributed Gaussians using an algorithm that integrates radiance fields slab by slab and leverages a BVH structure. This allows our approach to finely adapt to the scene while avoiding splatting artifacts. As a result, we achieve superior rendering quality compared to the state-of-the-art while maintaining reasonable training times and achieving inference speeds of 25 FPS on the Blender dataset. Project page with videos and code: https://raygauss.github.io/",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "Project page with videos and code: https://raygauss.github.io/"
    },
    {
        "paper id": "2408.03361",
        "abstract url": "https://arxiv.org/abs/2408.03361",
        "title": "GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI",
        "rating": "0",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "Medical",
                "diagnosis",
                "clinical",
                "physiological"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Large Vision-Language Models (LVLMs) are capable of handling diverse data types such as imaging, text, and physiological signals, and can be applied in various fields. In the medical field, LVLMs have a high potential to offer substantial assistance for diagnosis and treatment. Before that, it is crucial to develop benchmarks to evaluate LVLMs' effectiveness in various medical applications. Current benchmarks are often built upon specific academic literature, mainly focusing on a single domain, and lacking varying perceptual granularities. Thus, they face specific challenges, including limited clinical relevance, incomplete evaluations, and insufficient guidance for interactive LVLMs. To address these limitations, we developed the GMAI-MMBench, the most comprehensive general medical AI benchmark with well-categorized data structure and multi-perceptual granularity to date. It is constructed from 285 datasets across 39 medical image modalities, 18 clinical-related tasks, 18 departments, and 4 perceptual granularities in a Visual Question Answering (VQA) format. Additionally, we implemented a lexical tree structure that allows users to customize evaluation tasks, accommodating various assessment needs and substantially supporting medical AI research and applications. We evaluated 50 LVLMs, and the results show that even the advanced GPT-4o only achieves an accuracy of 52%, indicating significant room for improvement. Moreover, we identified five key insufficiencies in current cutting-edge LVLMs that need to be addressed to advance the development of better medical applications. We believe that GMAI-MMBench will stimulate the community to build the next generation of LVLMs toward GMAI. Project Page: https://uni-medical.github.io/GMAI-MMBench.github.io/",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03503",
        "abstract url": "https://arxiv.org/abs/2408.03503",
        "title": "Opening the Black Box of 3D Reconstruction Error Analysis with VECTOR",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Reconstruction of 3D scenes from 2D images is a technical challenge that impacts domains from Earth and planetary sciences and space exploration to augmented and virtual reality. Typically, reconstruction algorithms first identify common features across images and then minimize reconstruction errors after estimating the shape of the terrain. This bundle adjustment (BA) step optimizes around a single, simplifying scalar value that obfuscates many possible causes of reconstruction errors (e.g., initial estimate of the position and orientation of the camera, lighting conditions, ease of feature detection in the terrain). Reconstruction errors can lead to inaccurate scientific inferences or endanger a spacecraft exploring a remote environment. To address this challenge, we present VECTOR, a visual analysis tool that improves error inspection for stereo reconstruction BA. VECTOR provides analysts with previously unavailable visibility into feature locations, camera pose, and computed 3D points. VECTOR was developed in partnership with the Perseverance Mars Rover and Ingenuity Mars Helicopter terrain reconstruction team at the NASA Jet Propulsion Laboratory. We report on how this tool was used to debug and improve terrain reconstruction for the Mars 2020 mission.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03505",
        "abstract url": "https://arxiv.org/abs/2408.03505",
        "title": "Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Multimodal large language models (MLLMs) have extended the success of large language models (LLMs) to multiple data types, such as image, text and audio, achieving significant performance in various domains, including multimodal translation, visual question answering and content generation. Nonetheless, existing systems are inefficient to train MLLMs due to substantial GPU bubbles caused by the heterogeneous modality models and complex data dependencies in 3D parallelism. This paper proposes Optimus, a distributed MLLM training system that reduces end-to-end MLLM training time. Optimus is based on our principled analysis that scheduling the encoder computation within the LLM bubbles can reduce bubbles in MLLM training. To make scheduling encoder computation possible for all GPUs, Optimus searches the separate parallel plans for encoder and LLM, and adopts a bubble scheduling algorithm to enable exploiting LLM bubbles without breaking the original data dependencies in the MLLM model architecture. We further decompose encoder layer computation into a series of kernels, and analyze the common bubble pattern of 3D parallelism to carefully optimize the sub-millisecond bubble scheduling, minimizing the overall training time. Our experiments in a production cluster show that Optimus accelerates MLLM training by 20.5%-21.3% with ViT-22B and GPT-175B model over 3072 GPUs compared to baselines.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03538",
        "abstract url": "https://arxiv.org/abs/2408.03538",
        "title": "PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time High-Quality Relighting",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian splatting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We proposed Precomputed RadianceTransfer of GaussianSplats (PRTGS), a real-time high-quality relighting method for Gaussian splats in low-frequency lighting environments that captures soft shadows and interreflections by precomputing 3D Gaussian splats' radiance transfer. Existing studies have demonstrated that 3D Gaussian splatting (3DGS) outperforms neural fields' efficiency for dynamic lighting scenarios. However, the current relighting method based on 3DGS still struggles to compute high-quality shadow and indirect illumination in real time for dynamic light, leading to unrealistic rendering results. We solve this problem by precomputing the expensive transport simulations required for complex transfer functions like shadowing, the resulting transfer functions are represented as dense sets of vectors or matrices for every Gaussian splat. We introduce distinct precomputing methods tailored for training and rendering stages, along with unique ray tracing and indirect lighting precomputation techniques for 3D Gaussian splats to accelerate training speed and compute accurate indirect lighting related to environment light. Experimental analyses demonstrate that our approach achieves state-of-the-art visual quality while maintaining competitive training times and allows high-quality real-time (30+ fps) relighting for dynamic light and relatively complex scenes at 1080p resolution.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03540",
        "abstract url": "https://arxiv.org/abs/2408.03540",
        "title": "PoseMamba: Monocular 3D Human Pose Estimation with Bidirectional Global-Local Spatio-Temporal State Space Model",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Transformers have significantly advanced the field of 3D human pose estimation (HPE). However, existing transformer-based methods primarily use self-attention mechanisms for spatio-temporal modeling, leading to a quadratic complexity, unidirectional modeling of spatio-temporal relationships, and insufficient learning of spatial-temporal correlations. Recently, the Mamba architecture, utilizing the state space model (SSM), has exhibited superior long-range modeling capabilities in a variety of vision tasks with linear complexity. In this paper, we propose PoseMamba, a novel purely SSM-based approach with linear complexity for 3D human pose estimation in monocular video. Specifically, we propose a bidirectional global-local spatio-temporal SSM block that comprehensively models human joint relations within individual frames as well as temporal correlations across frames. Within this bidirectional global-local spatio-temporal SSM block, we introduce a reordering strategy to enhance the local modeling capability of the SSM. This strategy provides a more logical geometric scanning order and integrates it with the global SSM, resulting in a combined global-local spatial scan. We have quantitatively and qualitatively evaluated our approach using two benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments demonstrate that PoseMamba achieves state-of-the-art performance on both datasets while maintaining a smaller model size and reducing computational costs. The code and models will be released.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.04474",
        "abstract url": "https://arxiv.org/abs/2408.04474",
        "title": "LumiGauss: High-Fidelity Outdoor Relighting with 2D Gaussian Splatting",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "NeRF"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Decoupling lighting from geometry using unconstrained photo collections is notoriously challenging. Solving it would benefit many users, as creating complex 3D assets takes days of manual labor. Many previous works have attempted to address this issue, often at the expense of output fidelity, which questions the practicality of such methods. We introduce LumiGauss, a technique that tackles 3D reconstruction of scenes and environmental lighting through 2D Gaussian Splatting. Our approach yields high-quality scene reconstructions and enables realistic lighting synthesis under novel environment maps. We also propose a method for enhancing the quality of shadows, common in outdoor scenes, by exploiting spherical harmonics properties. Our approach facilitates seamless integration with game engines and enables the use of fast precomputed radiance transfer. We validate our method on the NeRF-OSR dataset, demonstrating superior performance over baseline methods. Moreover, LumiGauss can synthesize realistic images when applying novel environment maps.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Includes video files in src"
    },
    {
        "paper id": "2408.02965",
        "abstract url": "https://arxiv.org/abs/2408.02965",
        "title": "Data-Driven Stochastic Closure Modeling via Conditional Diffusion Model and Neural Operator",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Closure models are widely used in simulating complex multiscale dynamical systems such as turbulence and the earth system, for which direct numerical simulation that resolves all scales is often too expensive. For those systems without a clear scale separation, deterministic and local closure models often lack enough generalization capability, which limits their performance in many real-world applications. In this work, we propose a data-driven modeling framework for constructing stochastic and non-local closure models via conditional diffusion model and neural operator. Specifically, the Fourier neural operator is incorporated into a score-based diffusion model, which serves as a data-driven stochastic closure model for complex dynamical systems governed by partial differential equations (PDEs). We also demonstrate how accelerated sampling methods can improve the efficiency of the data-driven stochastic closure model. The results show that the proposed methodology provides a systematic approach via generative machine learning techniques to construct data-driven stochastic closure models for multiscale dynamical systems with continuous spatiotemporal fields.",
        "subjects": [
            "cs.LG",
            "math.DS",
            "physics.comp-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2408.02987",
        "abstract url": "https://arxiv.org/abs/2408.02987",
        "title": "A Differential Smoothness-based Compact-Dynamic Graph Convolutional Network for Spatiotemporal Signal Recovery",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "High quality spatiotemporal signal is vitally important for real application scenarios like energy management, traffic planning and cyber security. Due to the uncontrollable factors like abrupt sensors breakdown or communication fault, the spatiotemporal signal collected by sensors is always incomplete. A dynamic graph convolutional network (DGCN) is effective for processing spatiotemporal signal recovery. However, it adopts a static GCN and a sequence neural network to explore the spatial and temporal patterns, separately. Such a separated two-step processing is loose spatiotemporal, thereby failing to capture the complex inner spatiotemporal correlation. To address this issue, this paper proposes a Compact-Dynamic Graph Convolutional Network (CDGCN) for spatiotemporal signal recovery with the following two-fold ideas: a) leveraging the tensor M-product to build a unified tensor graph convolution framework, which considers both spatial and temporal patterns simultaneously; and b) constructing a differential smoothness-based objective function to reduce the noise interference in spatiotemporal signal, thereby further improve the recovery accuracy. Experiments on real-world spatiotemporal datasets demonstrate that the proposed CDGCN significantly outperforms the state-of-the-art models in terms of recovery accuracy.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03003",
        "abstract url": "https://arxiv.org/abs/2408.03003",
        "title": "Cross-cultural analysis of pedestrian group behaviour influence on crossing decisions in interactions with autonomous vehicles",
        "rating": "-0.5",
        "keywords": [
            [
                "autonomous driving",
                "vehicle"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Understanding cultural backgrounds is crucial for the seamless integration of autonomous driving into daily life as it ensures that systems are attuned to diverse societal norms and behaviours, enhancing acceptance and safety in varied cultural contexts. In this work, we investigate the impact of co-located pedestrians on crossing behaviour, considering cultural and situational factors. To accomplish this, a full-scale virtual reality (VR) environment was created in the CARLA simulator, enabling the identical experiment to be replicated in both Spain and Australia. Participants (N=30) attempted to cross the road at an urban crosswalk alongside other pedestrians exhibiting conservative to more daring behaviours, while an autonomous vehicle (AV) approached with different driving styles. For the analysis of interactions, we utilized questionnaires and direct measures of the moment when participants entered the lane. Our findings indicate that pedestrians tend to cross the same traffic gap together, even though reckless behaviour by the group reduces confidence and makes the situation perceived as more complex. Australian participants were willing to take fewer risks than Spanish participants, adopting more cautious behaviour when it was uncertain whether the AV would yield.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": "Paper accepted at the 27th IEEE International Conference on Intelligent Transportation Systems (ITSC 2024)"
    },
    {
        "paper id": "2408.03084",
        "abstract url": "https://arxiv.org/abs/2408.03084",
        "title": "Research on Autonomous Driving Decision-making Strategies based Deep Reinforcement Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Autonomous Driving",
                "vehicle"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The behavior decision-making subsystem is a key component of the autonomous driving system, which reflects the decision-making ability of the vehicle and the driver, and is an important symbol of the high-level intelligence of the vehicle. However, the existing rule-based decision-making schemes are limited by the prior knowledge of designers, and it is difficult to cope with complex and changeable traffic scenarios. In this work, an advanced deep reinforcement learning model is adopted, which can autonomously learn and optimize driving strategies in a complex and changeable traffic environment by modeling the driving decision-making process as a reinforcement learning problem. Specifically, we used Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) for comparative experiments. DQN guides the agent to choose the best action by approximating the state-action value function, while PPO improves the decision-making quality by optimizing the policy function. We also introduce improvements in the design of the reward function to promote the robustness and adaptability of the model in real-world driving situations. Experimental results show that the decision-making strategy based on deep reinforcement learning has better performance than the traditional rule-based method in a variety of driving tasks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03093",
        "abstract url": "https://arxiv.org/abs/2408.03093",
        "title": "Learning Provably Robust Policies in Uncertain Parametric Environments",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We present a data-driven approach for learning MDP policies that are robust across stochastic environments whose transition probabilities are defined by parameters with an unknown distribution. We produce probably approximately correct (PAC) guarantees for the performance of these learned policies in a new, unseen environment over the unknown distribution. Our approach is based on finite samples of the MDP environments, for each of which we build an approximation of the model as an interval MDP, by exploring a set of generated trajectories. We use the built approximations to synthesise a single policy that performs well (meets given requirements) across the sampled environments, and furthermore bound its risk (of not meeting the given requirements) when deployed in an unseen environment. Our procedure offers a trade-off between the guaranteed performance of the learned policy and the risk of not meeting the guarantee in an unseen environment. Our approach exploits knowledge of the environment's state space and graph structure, and we show how additional knowledge of its parametric structure can be leveraged to optimize learning and to obtain tighter guarantees from less samples. We evaluate our approach on a diverse range of established benchmarks, demonstrating that we can generate highly performing and robust policies, along with guarantees that tightly quantify their performance and the associated risk.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03168",
        "abstract url": "https://arxiv.org/abs/2408.03168",
        "title": "Training on the Fly: On-device Self-supervised Learning aboard Nano-drones within 20 mW",
        "rating": "-0.5",
        "keywords": [
            [
                "robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning (TinyML), such as nano-drones, are becoming an increasingly attractive technology. Their small form factor (i.e., ~10cm diameter) ensures vast applicability, ranging from the exploration of narrow disaster scenarios to safe human-robot interaction. Simple electronics make these CPSes inexpensive, but strongly limit the computational, memory, and sensing resources available on board. In real-world applications, these limitations are further exacerbated by domain shift. This fundamental machine learning problem implies that model perception performance drops when moving from the training domain to a different deployment one. To cope with and mitigate this general problem, we present a novel on-device fine-tuning approach that relies only on the limited ultra-low power resources available aboard nano-drones. Then, to overcome the lack of ground-truth training labels aboard our CPS, we also employ a self-supervised method based on ego-motion consistency. Albeit our work builds on top of a specific real-world vision-based human pose estimation task, it is widely applicable for many embedded TinyML use cases. Our 512-image on-device training procedure is fully deployed aboard an ultra-low power GWT GAP9 System-on-Chip and requires only 1MB of memory while consuming as low as 19mW or running in just 510ms (at 38mW). Finally, we demonstrate the benefits of our on-device learning approach by field-testing our closed-loop CPS, showing a reduction in horizontal position error of up to 26% vs. a non-fine-tuned state-of-the-art baseline. In the most challenging never-seen-before environment, our on-device learning procedure makes the difference between succeeding or failing the mission.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "eess.SY"
        ],
        "comment": "This paper has been accepted for publication in the IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. Copyright 2024 IEEE"
    },
    {
        "paper id": "2408.03195",
        "abstract url": "https://arxiv.org/abs/2408.03195",
        "title": "RELIEF: Reinforcement Learning Empowered Graph Feature Prompt Tuning",
        "rating": "-0.5",
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The advent of the \"pre-train, prompt\" paradigm has recently extended its generalization ability and data efficiency to graph representation learning, following its achievements in Natural Language Processing (NLP). Initial graph prompt tuning approaches tailored specialized prompting functions for Graph Neural Network (GNN) models pre-trained with specific strategies, such as edge prediction, thus limiting their applicability. In contrast, another pioneering line of research has explored universal prompting via adding prompts to the input graph's feature space, thereby removing the reliance on specific pre-training strategies. However, the necessity to add feature prompts to all nodes remains an open question. Motivated by findings from prompt tuning research in the NLP domain, which suggest that highly capable pre-trained models need less conditioning signal to achieve desired behaviors, we advocate for strategically incorporating necessary and lightweight feature prompts to certain graph nodes to enhance downstream task performance. This introduces a combinatorial optimization problem, requiring a policy to decide 1) which nodes to prompt and 2) what specific feature prompts to attach. We then address the problem by framing the prompt incorporation process as a sequential decision-making problem and propose our method, RELIEF, which employs Reinforcement Learning (RL) to optimize it. At each step, the RL agent selects a node (discrete action) and determines the prompt content (continuous action), aiming to maximize cumulative performance gain. Extensive experiments on graph and node-level tasks with various pre-training strategies in few-shot scenarios demonstrate that our RELIEF outperforms fine-tuning and other prompt-based approaches in classification performance and data efficiency.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03200",
        "abstract url": "https://arxiv.org/abs/2408.03200",
        "title": "Adversarial Safety-Critical Scenario Generation using Naturalistic Human Driving Priors",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Evaluating the decision-making system is indispensable in developing autonomous vehicles, while realistic and challenging safety-critical test scenarios play a crucial role. Obtaining these scenarios is non-trivial, thanks to the long-tailed distribution, sparsity, and rarity in real-world data sets. To tackle this problem, in this paper, we introduce a natural adversarial scenario generation solution using naturalistic human driving priors and reinforcement learning techniques. By doing this, we can obtain large-scale test scenarios that are both diverse and realistic. Specifically, we build a simulation environment that mimics natural traffic interaction scenarios. Informed by this environment, we implement a two-stage procedure. The first stage incorporates conventional rule-based models, e.g., IDM~(Intelligent Driver Model) and MOBIL~(Minimizing Overall Braking Induced by Lane changes) model, to coarsely and discretely capture and calibrate key control parameters from the real-world dataset. Next, we leverage GAIL~(Generative Adversarial Imitation Learning) to represent driver behaviors continuously. The derived GAIL can be further used to design a PPO~(Proximal Policy Optimization)-based actor-critic network framework to fine-tune the reward function, and then optimizes our natural adversarial scenario generation solution. Extensive experiments have been conducted in the NGSIM dataset including the trajectory of 3,000 vehicles. Essential traffic parameters were measured in comparison with the baseline model, e.g., the collision rate, accelerations, steering, and the number of lane changes. Our findings demonstrate that the proposed model can generate realistic safety-critical test scenarios covering both naturalness and adversariality, which can be a cornerstone for the development of autonomous vehicles.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "Published in IEEE Transactions on Intelligent Vehicles, 2023"
    },
    {
        "paper id": "2408.03284",
        "abstract url": "https://arxiv.org/abs/2408.03284",
        "title": "ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually Synced Facial Performer",
        "rating": "-0.5",
        "keywords": [
            [
                "audio-visual"
            ],
            [
                "3D"
            ],
            [
                "Facial"
            ],
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Lip-syncing videos with given audio is the foundation for various applications including the creation of virtual presenters or performers. While recent studies explore high-fidelity lip-sync with different techniques, their task-orientated models either require long-term videos for clip-specific training or retain visible artifacts. In this paper, we propose a unified and effective framework ReSyncer, that synchronizes generalized audio-visual facial information. The key design is revisiting and rewiring the Style-based generator to efficiently adopt 3D facial dynamics predicted by a principled style-injected Transformer. By simply re-configuring the information insertion mechanisms within the noise and style space, our framework fuses motion and appearance with unified training. Extensive experiments demonstrate that ReSyncer not only produces high-fidelity lip-synced videos according to audio, but also supports multiple appealing properties that are suitable for creating virtual presenters and performers, including fast personalized fine-tuning, video-driven lip-syncing, the transfer of speaking styles, and even face swapping. Resources can be found at https://guanjz20.github.io/projects/ReSyncer.",
        "subjects": [
            "cs.CV",
            "cs.GR",
            "cs.MM"
        ],
        "comment": "Accepted to European Conference on Computer Vision (ECCV), 2024. Project page: https://guanjz20.github.io/projects/ReSyncer"
    },
    {
        "paper id": "2408.03287",
        "abstract url": "https://arxiv.org/abs/2408.03287",
        "title": "Malicious Internet Entity Detection Using Local Graph Inference",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Detection of malicious behavior in a large network is a challenging problem for machine learning in computer security, since it requires a model with high expressive power and scalable inference. Existing solutions struggle to achieve this feat -- current cybersec-tailored approaches are still limited in expressivity, and methods successful in other domains do not scale well for large volumes of data, rendering frequent retraining impossible. This work proposes a new perspective for learning from graph data that is modeling network entity interactions as a large heterogeneous graph. High expressivity of the method is achieved with neural network architecture HMILnet that naturally models this type of data and provides theoretical guarantees. The scalability is achieved by pursuing local graph inference, i.e., classifying individual vertices and their neighborhood as independent samples. Our experiments exhibit improvement over the state-of-the-art Probabilistic Threat Propagation (PTP) algorithm, show a further threefold accuracy improvement when additional data is used, which is not possible with the PTP algorithm, and demonstrate the generalization capabilities of the method to new, previously unseen entities.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": "A preprint. Full publication: https://ieeexplore.ieee.org/document/10418120"
    },
    {
        "paper id": "2408.03360",
        "abstract url": "https://arxiv.org/abs/2408.03360",
        "title": "Prioritize Alignment in Dataset Distillation",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Dataset Distillation aims to compress a large dataset into a significantly more compact, synthetic one without compromising the performance of the trained models. To achieve this, existing methods use the agent model to extract information from the target dataset and embed it into the distilled dataset. Consequently, the quality of extracted and embedded information determines the quality of the distilled dataset. In this work, we find that existing methods introduce misaligned information in both information extraction and embedding stages. To alleviate this, we propose Prioritize Alignment in Dataset Distillation (PAD), which aligns information from the following two perspectives. 1) We prune the target dataset according to the compressing ratio to filter the information that can be extracted by the agent model. 2) We use only deep layers of the agent model to perform the distillation to avoid excessively introducing low-level information. This simple strategy effectively filters out misaligned information and brings non-trivial improvement for mainstream matching-based distillation algorithms. Furthermore, built on trajectory matching, \\textbf{PAD} achieves remarkable improvements on various benchmarks, achieving state-of-the-art performance.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "18 pages, 9 figures"
    },
    {
        "paper id": "2408.03413",
        "abstract url": "https://arxiv.org/abs/2408.03413",
        "title": "A TVD neural network closure and application to turbulent combustion",
        "rating": "-0.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Trained neural networks (NN) have attractive features for closing governing equations, but in the absence of additional constraints, they can stray from physical reality. A NN formulation is introduced to preclude spurious oscillations that violate solution boundedness or positivity. It is embedded in the discretized equations as a machine learning closure and strictly constrained, inspired by total variation diminishing (TVD) methods for hyperbolic conservation laws. The constraint is exactly enforced during gradient-descent training by rescaling the NN parameters, which maps them onto an explicit feasible set. Demonstrations show that the constrained NN closure model usefully recovers linear and nonlinear hyperbolic phenomena and anti-diffusion while enforcing the non-oscillatory property. Finally, the model is applied to subgrid-scale (SGS) modeling of a turbulent reacting flow, for which it suppresses spurious oscillations in scalar fields that otherwise violate the solution boundedness. It outperforms a simple penalization of oscillations in the loss function.",
        "subjects": [
            "cs.LG",
            "cs.CE",
            "physics.flu-dyn"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03425",
        "abstract url": "https://arxiv.org/abs/2408.03425",
        "title": "Sequential Conditional Transport on Probabilistic Graphs for Interpretable Counterfactual Fairness",
        "rating": "-0.5",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we link two existing approaches to derive counterfactuals: adaptations based on a causal graph, as suggested in Ple\u010dko and Meinshausen (2020) and optimal transport, as in De Lara et al. (2024). We extend \"Knothe's rearrangement\" Bonnotte (2013) and \"triangular transport\" Zech and Marzouk (2022a) to probabilistic graphical models, and use this counterfactual approach, referred to as sequential transport, to discuss individual fairness. After establishing the theoretical foundations of the proposed method, we demonstrate its application through numerical experiments on both synthetic and real datasets.",
        "subjects": [
            "cs.LG",
            "stat.ME"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03435",
        "abstract url": "https://arxiv.org/abs/2408.03435",
        "title": "Communication-Aware Consistent Edge Selection for Mobile Users and Autonomous Vehicles",
        "rating": "-0.5",
        "keywords": [
            [
                "autonomous driving",
                "vehicle"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Offloading time-sensitive, computationally intensive tasks-such as advanced learning algorithms for autonomous driving-from vehicles to nearby edge servers, vehicle-to-infrastructure (V2I) systems, or other collaborating vehicles via vehicle-to-vehicle (V2V) communication enhances service efficiency. However, whence traversing the path to the destination, the vehicle's mobility necessitates frequent handovers among the access points (APs) to maintain continuous and uninterrupted wireless connections to maintain the network's Quality of Service (QoS). These frequent handovers subsequently lead to task migrations among the edge servers associated with the respective APs. This paper addresses the joint problem of task migration and access-point handover by proposing a deep reinforcement learning framework based on the Deep Deterministic Policy Gradient (DDPG) algorithm. A joint allocation method of communication and computation of APs is proposed to minimize computational load, service latency, and interruptions with the overarching goal of maximizing QoS. We implement and evaluate our proposed framework on simulated experiments to achieve smooth and seamless task switching among edge servers, ultimately reducing latency.",
        "subjects": [
            "cs.NI",
            "cs.AI",
            "cs.RO"
        ],
        "comment": "Accepted by Vehicular Technology Conference (VTC) Fall 2024"
    },
    {
        "paper id": "2408.03441",
        "abstract url": "https://arxiv.org/abs/2408.03441",
        "title": "Simple Perturbations Subvert Ethereum Phishing Transactions Detection: An Empirical Analysis",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper explores the vulnerability of machine learning models, specifically Random Forest, Decision Tree, and K-Nearest Neighbors, to very simple single-feature adversarial attacks in the context of Ethereum fraudulent transaction detection. Through comprehensive experimentation, we investigate the impact of various adversarial attack strategies on model performance metrics, such as accuracy, precision, recall, and F1-score. Our findings, highlighting how prone those techniques are to simple attacks, are alarming, and the inconsistency in the attacks' effect on different algorithms promises ways for attack mitigation. We examine the effectiveness of different mitigation strategies, including adversarial training and enhanced feature selection, in enhancing model robustness.",
        "subjects": [
            "cs.CR",
            "cs.LG"
        ],
        "comment": "12 pages, 1 figure, 5 tables, accepted for presentation at WISA 2024"
    },
    {
        "paper id": "2408.03450",
        "abstract url": "https://arxiv.org/abs/2408.03450",
        "title": "Probabilistic Surrogate Model for Accelerating the Design of Electric Vehicle Battery Enclosures for Crash Performance",
        "rating": "-0.5",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper presents a probabilistic surrogate model for the accelerated design of electric vehicle battery enclosures with a focus on crash performance. The study integrates high-throughput finite element simulations and Gaussian Process Regression to develop a surrogate model that predicts crash parameters with high accuracy while providing uncertainty estimates. The model was trained using data generated from thermoforming and crash simulations over a range of material and process parameters. Validation against new simulation data demonstrated the model's predictive accuracy with mean absolute percentage errors within 8.08% for all output variables. Additionally, a Monte Carlo uncertainty propagation study revealed the impact of input variability on outputs. The results highlight the efficacy of the Gaussian Process Regression model in capturing complex relationships within the dataset, offering a robust and efficient tool for the design optimization of composite battery enclosures.",
        "subjects": [
            "cs.LG",
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03459",
        "abstract url": "https://arxiv.org/abs/2408.03459",
        "title": "On the Generalization of Preference Learning with DPO",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities but often struggle to align with human preferences, leading to harmful or undesirable outputs. Preference learning, which trains models to distinguish between preferred and non-preferred responses based on human feedback, has become a crucial component for ensuring that LLMs align with human values. Despite the widespread adoption in real-world systems, a thorough theoretical understanding of the generalization guarantees for these models remain lacking. This paper bridges that gap by introducing a new theoretical framework to analyze the generalization guarantees of models trained with direct preference optimization (DPO). While existing generalization theory often focuses on overparameterized models achieving near-optimal loss or models independent of the training process, our framework rigorously assesses how well models generalize after a finite number of gradient steps, reflecting real-world LLM training practices. By analyzing the reward margin associated with each sample and its trajectory throughout training, we can effectively bound the generalization error. We derive learning guarantees showing that, under specific conditions, models trained with DPO can correctly discern preferred responses on unseen data with high probability. These insights are empirically validated on contemporary LLMs, underscoring the practical relevance of our theoretical findings.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03525",
        "abstract url": "https://arxiv.org/abs/2408.03525",
        "title": "Hierarchical learning control for autonomous robots inspired by central nervous system",
        "rating": "-0.5",
        "keywords": [
            [
                "robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Mammals can generate autonomous behaviors in various complex environments through the coordination and interaction of activities at different levels of their central nervous system. In this paper, we propose a novel hierarchical learning control framework by mimicking the hierarchical structure of the central nervous system along with their coordination and interaction behaviors. The framework combines the active and passive control systems to improve both the flexibility and reliability of the control system as well as to achieve more diverse autonomous behaviors of robots. Specifically, the framework has a backbone of independent neural network controllers at different levels and takes a three-level dual descending pathway structure, inspired from the functionality of the cerebral cortex, cerebellum, and spinal cord. We comprehensively validated the proposed approach through the simulation as well as the experiment of a hexapod robot in various complex environments, including obstacle crossing and rapid recovery after partial damage. This study reveals the principle that governs the autonomous behavior in the central nervous system and demonstrates the effectiveness of the hierarchical control approach with the salient features of the hierarchical learning control architecture and combination of active and passive control systems.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03528",
        "abstract url": "https://arxiv.org/abs/2408.03528",
        "title": "Exploring the extent of similarities in software failures across industries using LLMs",
        "rating": "-0.5",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The rapid evolution of software development necessitates enhanced safety measures. Extracting information about software failures from companies is becoming increasingly more available through news articles. This research utilizes the Failure Analysis Investigation with LLMs (FAIL) model to extract industry-specific information. Although the FAIL model's database is rich in information, it could benefit from further categorization and industry-specific insights to further assist software engineers. In previous work news articles were collected from reputable sources and categorized by incidents inside a database. Prompt engineering and Large Language Models (LLMs) were then applied to extract relevant information regarding the software failure. This research extends these methods by categorizing articles into specific domains and types of software failures. The results are visually represented through graphs. The analysis shows that throughout the database some software failures occur significantly more often in specific industries. This categorization provides a valuable resource for software engineers and companies to identify and address common failures. This research highlights the synergy between software engineering and Large Language Models (LLMs) to automate and enhance the analysis of software failures. By transforming data from the database into an industry specific model, we provide a valuable resource that can be used to identify common vulnerabilities, predict potential risks, and implement proactive measures for preventing software failures. Leveraging the power of the current FAIL database and data visualization, we aim to provide an avenue for safer and more secure software in the future.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03533",
        "abstract url": "https://arxiv.org/abs/2408.03533",
        "title": "Lifelong Personalized Low-Rank Adaptation of Large Language Models for Recommendation",
        "rating": "-0.5",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "Recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "We primarily focus on the field of large language models (LLMs) for recommendation, which has been actively explored recently and poses a significant challenge in effectively enhancing recommender systems with logical reasoning abilities and open-world knowledge. Current mainstream efforts mainly center around injecting personalized information from recommendation models into LLMs by customizing input templates or aligning representations between semantic and recommendation spaces at the prediction layer. However, they face three significant limitations: (1) LoRA is mostly used as a core component in existing works, but personalization is not well established in LoRA parameters as the LoRA matrix shared by every user may not cater to different users' characteristics, leading to suboptimal performance. (2) Although lifelong personalized behavior sequences are ideal for personalization, their use raises effectiveness and efficiency issues since LLMs require escalating training and inference time to extend text lengths. (3) Existing approaches aren't scalable for large datasets due to training efficiency constraints. Thus, LLMs only see a small fraction of the datasets (e.g., less than 10%) instead of the whole datasets, limiting their exposure to the full training space. To address these problems, we propose RecLoRA. This model incorporates a Personalized LoRA module that maintains independent LoRAs for different users and a Long-Short Modality Retriever that retrieves different history lengths for different modalities, significantly improving performance while adding minimal time cost. Furthermore, we design a Few2Many Learning Strategy, using a conventional recommendation model as a lens to magnify small training spaces to full spaces. Extensive experiments on public datasets demonstrate the efficacy of our RecLoRA compared to existing baseline models.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03539",
        "abstract url": "https://arxiv.org/abs/2408.03539",
        "title": "Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes",
        "rating": "-0.5",
        "keywords": [
            [
                "Robotics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Reinforcement learning (RL), particularly its combination with deep neural networks referred to as deep RL (DRL), has shown tremendous promise across a wide range of applications, suggesting its potential for enabling the development of sophisticated robotic behaviors. Robotics problems, however, pose fundamental difficulties for the application of RL, stemming from the complexity and cost of interacting with the physical world. This article provides a modern survey of DRL for robotics, with a particular focus on evaluating the real-world successes achieved with DRL in realizing several key robotic competencies. Our analysis aims to identify the key factors underlying those exciting successes, reveal underexplored areas, and provide an overall characterization of the status of DRL in robotics. We highlight several important avenues for future work, emphasizing the need for stable and sample-efficient real-world RL paradigms, holistic approaches for discovering and integrating various competencies to tackle complex long-horizon, open-world tasks, and principled development and evaluation procedures. This survey is designed to offer insights for both RL practitioners and roboticists toward harnessing RL's power to create generally capable real-world robotic systems.",
        "subjects": [
            "cs.RO",
            "cs.LG"
        ],
        "comment": "The first three authors contributed equally. Accepted to Annual Review of Control, Robotics, and Autonomous Systems"
    },
    {
        "paper id": "2408.03966",
        "abstract url": "https://arxiv.org/abs/2408.03966",
        "title": "Large-Scale Graphs Community Detection using Spark GraphFrames",
        "rating": "-0.5",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "With the emergence of social networks, online platforms dedicated to different use cases, and sensor networks, the emergence of large-scale graph community detection has become a steady field of research with real-world applications. Community detection algorithms have numerous practical applications, particularly due to their scalability with data size. Nonetheless, a notable drawback of community detection algorithms is their computational intensity~\\cite{Apostol2014}, resulting in decreasing performance as data size increases. For this purpose, new frameworks that employ distributed systems such as Apache Hadoop and Apache Spark which can seamlessly handle large-scale graphs must be developed. In this paper, we propose a novel framework for community detection algorithms, i.e., K-Cliques, Louvain, and Fast Greedy, developed using Apache Spark GraphFrames. We test their performance and scalability on two real-world datasets. The experimental results prove the feasibility of developing graph mining algorithms using Apache Spark GraphFrames.",
        "subjects": [
            "cs.SI",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2408.02963",
        "abstract url": "https://arxiv.org/abs/2408.02963",
        "title": "Adversarial Robustness of Open-source Text Classification Models and Fine-Tuning Chains",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Context:With the advancement of artificial intelligence (AI) technology and applications, numerous AI models have been developed, leading to the emergence of open-source model hosting platforms like Hugging Face (HF). Thanks to these platforms, individuals can directly download and use models, as well as fine-tune them to construct more domain-specific models. However, just like traditional software supply chains face security risks, AI models and fine-tuning chains also encounter new security risks, such as adversarial attacks. Therefore, the adversarial robustness of these models has garnered attention, potentially influencing people's choices regarding open-source models. Objective:This paper aims to explore the adversarial robustness of open-source AI models and their chains formed by the upstream-downstream relationships via fine-tuning to provide insights into the potential adversarial risks. Method:We collect text classification models on HF and construct the fine-tuning chains.Then, we conduct an empirical analysis of model reuse and associated robustness risks under existing adversarial attacks from two aspects, i.e., models and their fine-tuning chains. Results:Despite the models' widespread downloading and reuse, they are generally susceptible to adversarial attack risks, with an average of 52.70% attack success rate. Moreover, fine-tuning typically exacerbates this risk, resulting in an average 12.60% increase in attack success rates. We also delve into the influence of factors such as attack techniques, datasets, and model architectures on the success rate, as well as the transitivity along the model chains.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2408.02964",
        "abstract url": "https://arxiv.org/abs/2408.02964",
        "title": "Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge Retrieval",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "health",
                "clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are fundamentally transforming human-facing applications in the health and well-being domains: boosting patient engagement, accelerating clinical decision-making, and facilitating medical education. Although state-of-the-art LLMs have shown superior performance in several conversational applications, evaluations within nutrition and diet applications are still insufficient. In this paper, we propose to employ the Registered Dietitian (RD) exam to conduct a standard and comprehensive evaluation of state-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing both accuracy and consistency in nutrition queries. Our evaluation includes 1050 RD exam questions encompassing several nutrition topics and proficiency levels. In addition, for the first time, we examine the impact of Zero-Shot (ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC), and Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the responses. Our findings revealed that while these LLMs obtained acceptable overall performance, their results varied considerably with different prompts and question domains. GPT-4o with CoT-SC prompting outperformed the other approaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency. For GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both accuracy and consistency. RAP was particularly effective for GPT-4o to answer Expert level questions. Consequently, choosing the appropriate LLM and prompting technique, tailored to the proficiency level and specific domain, can mitigate errors and potential risks in diet and nutrition chatbots.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03043",
        "abstract url": "https://arxiv.org/abs/2408.03043",
        "title": "Targeted Visual Prompting for Medical Visual Question Answering",
        "rating": "-1",
        "keywords": [
            [
                "Medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "With growing interest in recent years, medical visual question answering (Med-VQA) has rapidly evolved, with multimodal large language models (MLLMs) emerging as an alternative to classical model architectures. Specifically, their ability to add visual information to the input of pre-trained LLMs brings new capabilities for image interpretation. However, simple visual errors cast doubt on the actual visual understanding abilities of these models. To address this, region-based questions have been proposed as a means to assess and enhance actual visual understanding through compositional evaluation. To combine these two perspectives, this paper introduces targeted visual prompting to equip MLLMs with region-based questioning capabilities. By presenting the model with both the isolated region and the region in its context in a customized visual prompt, we show the effectiveness of our method across multiple datasets while comparing it to several baseline models. Our code and data are available at https://github.com/sergiotasconmorales/locvqallm.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at the MICCAI AMAI Workshop 2024"
    },
    {
        "paper id": "2408.03045",
        "abstract url": "https://arxiv.org/abs/2408.03045",
        "title": "Coherent FDA Radar: Transmitter and Receiver Design and Analysis",
        "rating": "-1",
        "keywords": [
            [
                "Radar"
            ]
        ],
        "abstract": "The combination of frequency diverse array (FDA) radar technology with the multiple input multiple output (MIMO) radar architecture and waveform diversity techniques potentially promises a high integration gain with respect to conventional phased array (PA) radars. In this paper, we propose an approach to the design of the transmitter and the receiver of a coherent FDA (C-FDA) radar, that enables it to perform the demodulation with spectral overlapping, due to the small frequency offset. To this purpose, we derive the generalized space-time-range signal model and we prove that the proposed C-FDA radar has a higher coherent array gain than a PA radar, and at the same time, it effectively resolves the secondary range-ambiguous (SRA) problem of FDA-MIMO radar, allowing for mainlobe interference suppression and range-ambiguous clutter suppression. Numerical analysis results prove the effectiveness of the proposed C-FDA radar in terms on anti-interference and anti-clutter capabilities over conventional radars.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03050",
        "abstract url": "https://arxiv.org/abs/2408.03050",
        "title": "FDA Jamming Against Airborne Phased-MIMO Radar-Part I: Matched Filtering and Spatial Filtering",
        "rating": "-1",
        "keywords": [
            [
                "Radar"
            ]
        ],
        "abstract": "Phased multiple-input multiple-output (Phased-MIMO) radar has received increasing attention for enjoying the advantages of waveform diversity and range-dependency from frequency diverse array MIMO (FDA-MIMO) radar without sacrificing coherent processing gain through partitioning transmit subarray. This two-part series proposes a framework of electronic countermeasures (ECM) inspired by frequency diverse array (FDA) radar, called FDA jamming, evaluating its effectiveness for countering airborne phased-MIMO radar. This part introduces the principles and categories of FDA jammer and proposes the FDA jamming signal model based on the two cases of phased-MIMO radar, phased-array (PA) radar and FDA-MIMO radar. Moreover, the effects of FDA jamming on matched filtering and spatial filtering of PA and FDA-MIMO radar are analyzed. Numerical results verify the theoretical analysis and validate the effectiveness of the proposed FDA jamming in countering phased-MIMO radar.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03055",
        "abstract url": "https://arxiv.org/abs/2408.03055",
        "title": "FDA Jamming Against Airborne Phased-MIMO Radar-Part II: Jamming STAP Performance Analysis",
        "rating": "-1",
        "keywords": [
            [
                "trajectory",
                "Radar"
            ]
        ],
        "abstract": "The first part of this series introduced the effectiveness of frequency diverse array (FDA) jamming through direct wave propagation in countering airborne phased multiple-input multiple-output (Phased-MIMO) radar. This part focuses on the effectiveness of FDA scattered wave (FDA-SW) jamming on the space-time adaptive processing (STAP) for airborne phased-MIMO radar. Distinguished from the clutter signals, the ground equidistant scatterers of FDA-SW jamming constitute an elliptical ring, whose trajectory equations are mathematically derived to further determine the spatial frequency and Doppler frequency. For the phased-MIMO radar with different transmitting partitions, the effects of jamming frequency offset of FDA-SW on the clutter rank and STAP performance are discussed. Theoretical analysis provides the variation interval of clutter rank and the relationship between the jamming frequency offset and the improvement factor (IF) notch of phased-MIMO-STAP. Importantly, the requirements of jamming frequency offset for both two-part applications are discussed in this part. Numerical results verify these mathematical findings and validate the effectiveness of the proposed FDA jamming in countering the phased-MIMO radar.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03060",
        "abstract url": "https://arxiv.org/abs/2408.03060",
        "title": "MGFs: Masked Gaussian Fields for Meshing Building based on Multi-View Images",
        "rating": "-1",
        "keywords": [
            [
                "time-efficient"
            ],
            [
                "3D",
                "NeRF"
            ],
            [
                "UAV"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Over the last few decades, image-based building surface reconstruction has garnered substantial research interest and has been applied across various fields, such as heritage preservation, architectural planning, etc. Compared to the traditional photogrammetric and NeRF-based solutions, recently, Gaussian fields-based methods have exhibited significant potential in generating surface meshes due to their time-efficient training and detailed 3D information preservation. However, most gaussian fields-based methods are trained with all image pixels, encompassing building and nonbuilding areas, which results in a significant noise for building meshes and degeneration in time efficiency. This paper proposes a novel framework, Masked Gaussian Fields (MGFs), designed to generate accurate surface reconstruction for building in a time-efficient way. The framework first applies EfficientSAM and COLMAP to generate multi-level masks of building and the corresponding masked point clouds. Subsequently, the masked gaussian fields are trained by integrating two innovative losses: a multi-level perceptual masked loss focused on constructing building regions and a boundary loss aimed at enhancing the details of the boundaries between different masks. Finally, we improve the tetrahedral surface mesh extraction method based on the masked gaussian spheres. Comprehensive experiments on UAV images demonstrate that, compared to the traditional method and several NeRF-based and Gaussian-based SOTA solutions, our approach significantly improves both the accuracy and efficiency of building surface reconstruction. Notably, as a byproduct, there is an additional gain in the novel view synthesis of building.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03098",
        "abstract url": "https://arxiv.org/abs/2408.03098",
        "title": "Dedicated Nonlinear Control of Robot Manipulators in the Presence of External Vibration and Uncertain Payload",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "Robot manipulators are often tasked with working in environments with vibrations and are subject to load uncertainty. Providing an accurate tracking control design with implementable torque input for these robots is a complex topic. This paper presents two approaches to solve this problem. The approaches consider joint space tracking control design in the presence of nonlinear uncertain torques caused by external vibration and payload variation. The properties of the uncertain torques are used in both approaches. The first approach is based on the boundedness property, while the second approach considers the differentiability and boundedness together. The controllers derived from each approach differ from the perspectives of accuracy, control effort, and disturbance properties. A Lyapunov-based analysis is utilized to guarantee the stability of the control design in each case. Simulation results validate the approaches and demonstrate the performance of the controllers. The derived controllers show stable results at the cost of the mentioned properties.",
        "subjects": [
            "eess.SY",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03120",
        "abstract url": "https://arxiv.org/abs/2408.03120",
        "title": "Benchmarking In-the-wild Multimodal Disease Recognition and A Versatile Baseline",
        "rating": "-1",
        "keywords": [
            [
                "Disease"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Existing plant disease classification models have achieved remarkable performance in recognizing in-laboratory diseased images. However, their performance often significantly degrades in classifying in-the-wild images. Furthermore, we observed that in-the-wild plant images may exhibit similar appearances across various diseases (i.e., small inter-class discrepancy) while the same diseases may look quite different (i.e., large intra-class variance). Motivated by this observation, we propose an in-the-wild multimodal plant disease recognition dataset that contains the largest number of disease classes but also text-based descriptions for each disease. Particularly, the newly provided text descriptions are introduced to provide rich information in textual modality and facilitate in-the-wild disease classification with small inter-class discrepancy and large intra-class variance issues. Therefore, our proposed dataset can be regarded as an ideal testbed for evaluating disease recognition methods in the real world. In addition, we further present a strong yet versatile baseline that models text descriptions and visual data through multiple prototypes for a given class. By fusing the contributions of multimodal prototypes in classification, our baseline can effectively address the small inter-class discrepancy and large intra-class variance issues. Remarkably, our baseline model can not only classify diseases but also recognize diseases in few-shot or training-free scenarios. Extensive benchmarking results demonstrate that our proposed in-the-wild multimodal dataset sets many new challenges to the plant disease recognition task and there is a large space to improve for future works.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03127",
        "abstract url": "https://arxiv.org/abs/2408.03127",
        "title": "Lisbon Computational Linguists at SemEval-2024 Task 2: Using A Mistral 7B Model and Data Augmentation",
        "rating": "-1",
        "keywords": [
            [
                "biomedical",
                "Clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper describes our approach to the SemEval-2024 safe biomedical Natural Language Inference for Clinical Trials (NLI4CT) task, which concerns classifying statements about Clinical Trial Reports (CTRs). We explored the capabilities of Mistral-7B, a generalist open-source Large Language Model (LLM). We developed a prompt for the NLI4CT task, and fine-tuned a quantized version of the model using an augmented version of the training dataset. The experimental results show that this approach can produce notable results in terms of the macro F1-score, while having limitations in terms of faithfulness and consistency. All the developed code is publicly available on a GitHub repository",
        "subjects": [
            "cs.CL"
        ],
        "comment": "8 pages, 1 figure, submitted and accepted into the \"18th International Workshop on Semantic Evaluation (SemEval-2024)\""
    },
    {
        "paper id": "2408.03142",
        "abstract url": "https://arxiv.org/abs/2408.03142",
        "title": "A Graph Signal Processing Perspective of Network Multiple Hypothesis Testing with False Discovery Rate Control",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "We consider a multiple hypothesis testing problem in a sensor network over the joint spatial-time domain. The sensor network is modeled as a graph, with each vertex representing a sensor and a signal over time associated with each vertex. We assume a hypothesis test and an associated p-value for every sample point in the joint spatial-time domain. Our goal is to determine which points have true alternative hypotheses. By parameterizing the unknown alternative distribution of $p$-values and the prior probabilities of hypotheses being null with a bandlimited generalized graph signal, we can obtain consistent estimates for them. Consequently, we also obtain an estimate of the local false discovery rates (lfdr). We prove that by using a step-up procedure on the estimated lfdr, we can achieve asymptotic false discovery rate control at a pre-determined level. Numerical experiments validate the effectiveness of our approach compared to existing methods.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03178",
        "abstract url": "https://arxiv.org/abs/2408.03178",
        "title": "An Object is Worth 64x64 Pixels: Generating 3D Object via Image Diffusion",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We introduce a new approach for generating realistic 3D models with UV maps through a representation termed \"Object Images.\" This approach encapsulates surface geometry, appearance, and patch structures within a 64x64 pixel image, effectively converting complex 3D shapes into a more manageable 2D format. By doing so, we address the challenges of both geometric and semantic irregularity inherent in polygonal meshes. This method allows us to use image generation models, such as Diffusion Transformers, directly for 3D shape generation. Evaluated on the ABO dataset, our generated shapes with patch structures achieve point cloud FID comparable to recent 3D generative models, while naturally supporting PBR material generation.",
        "subjects": [
            "cs.CV",
            "cs.GR",
            "cs.LG"
        ],
        "comment": "Project Page: https://omages.github.io/"
    },
    {
        "paper id": "2408.03191",
        "abstract url": "https://arxiv.org/abs/2408.03191",
        "title": "Integrated Intention Prediction and Decision-Making with Spectrum Attention Net and Proximal Policy Optimization",
        "rating": "-1",
        "keywords": [
            [
                "autonomous driving"
            ]
        ],
        "abstract": "For autonomous driving in highly dynamic environments, it is anticipated to predict the future behaviors of surrounding vehicles (SVs) and make safe and effective decisions. However, modeling the inherent coupling effect between the prediction and decision-making modules has been a long-standing challenge, especially when there is a need to maintain appropriate computational efficiency. To tackle these problems, we propose a novel integrated intention prediction and decision-making approach, which explicitly models the coupling relationship and achieves efficient computation. Specifically, a spectrum attention net is designed to predict the intentions of SVs by capturing the trends of each frequency component over time and their interrelations. Fast computation of the intention prediction module is attained as the predicted intentions are not decoded to trajectories in the executing process. Furthermore, the proximal policy optimization (PPO) algorithm is employed to address the non-stationary problem in the framework through a modest policy update enabled by a clipping mechanism within its objective function. On the basis of these developments, the intention prediction and decision-making modules are integrated through joint learning. Experiments are conducted in representative traffic scenarios, and the results reveal that the proposed integrated framework demonstrates superior performance over several deep reinforcement learning (DRL) baselines in terms of success rate, efficiency, and safety in driving tasks.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03193",
        "abstract url": "https://arxiv.org/abs/2408.03193",
        "title": "Efficient NeRF Optimization -- Not All Samples Remain Equally Hard",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "NeRF",
                "Radiance Fields"
            ],
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We propose an application of online hard sample mining for efficient training of Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art quality for many 3D reconstruction and rendering tasks but require substantial computational resources. The encoding of the scene information within the NeRF network parameters necessitates stochastic sampling. We observe that during the training, a major part of the compute time and memory usage is spent on processing already learnt samples, which no longer affect the model update significantly. We identify the backward pass on the stochastic samples as the computational bottleneck during the optimization. We thus perform the first forward pass in inference mode as a relatively low-cost search for hard samples. This is followed by building the computational graph and updating the NeRF network parameters using only the hard samples. To demonstrate the effectiveness of the proposed approach, we apply our method to Instant-NGP, resulting in significant improvements of the view-synthesis quality over the baseline (1 dB improvement on average per training time, or 2x speedup to reach the same PSNR level) along with approx. 40% memory savings coming from using only the hard samples to build the computational graph. As our method only interfaces with the network module, we expect it to be widely applicable.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03238",
        "abstract url": "https://arxiv.org/abs/2408.03238",
        "title": "LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for Accurate Robotic Grasping Under the Occlusion",
        "rating": "-1",
        "keywords": [
            [
                "depth"
            ],
            [
                "robot"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper addresses the challenge of perceiving complete object shapes through visual perception. While prior studies have demonstrated encouraging outcomes in segmenting the visible parts of objects within a scene, amodal segmentation, in particular, has the potential to allow robots to infer the occluded parts of objects. To this end, this paper introduces a new framework that explores amodal segmentation for robotic grasping in cluttered scenes, thus greatly enhancing robotic grasping abilities. Initially, we use a conventional segmentation algorithm to detect the visible segments of the target object, which provides shape priors for completing the full object mask. Particularly, to explore how to utilize semantic features from RGB images and geometric information from depth images, we propose a Linear-fusion Attention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the linear-fusion strategy to effectively fuse this cross-modal data, and then uses the prior visible mask as attention map to guide the network to focus on target feature locations for further complete mask recovery. Using the amodal mask of the target object provides advantages in selecting more accurate and robust grasp points compared to relying solely on the visible segments. The results on different datasets show that our method achieves state-of-the-art performance. Furthermore, the robot experiments validate the feasibility and robustness of this method in the real world. Our code and demonstrations are available on the project page: https://jrryzh.github.io/LAC-Net.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": "accepted by IROS2024"
    },
    {
        "paper id": "2408.03256",
        "abstract url": "https://arxiv.org/abs/2408.03256",
        "title": "Synthesizing Text-to-SQL Data from Weak and Strong LLMs",
        "rating": "-1",
        "keywords": [
            [
                "SQL"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we introduce a synthetic data approach that combines data produced by larger, more powerful models (strong models) with error information data generated by smaller, not well-aligned models (weak models). The method not only enhances the domain generalization of text-to-SQL models but also explores the potential of error data supervision through preference learning. Furthermore, we employ the synthetic data approach for instruction tuning on open-source LLMs, resulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is demonstrated through state-of-the-art results on the SPIDER and BIRD benchmarks, bridging the performance gap between open-source models and methods prompted by closed-source models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "12 pages, 7 figures, ACL 2024"
    },
    {
        "paper id": "2408.03286",
        "abstract url": "https://arxiv.org/abs/2408.03286",
        "title": "Biomedical SAM 2: Segment Anything in Biomedical Images and Videos",
        "rating": "-1",
        "keywords": [
            [
                "Biomedical",
                "Medical",
                "diagnosing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Medical image segmentation and video object segmentation are essential for diagnosing and analyzing diseases by identifying and measuring biological structures. Recent advances in natural domain have been driven by foundation models like the Segment Anything Model 2 (SAM 2). To explore the performance of SAM 2 in biomedical applications, we designed two evaluation pipelines for single-frame image segmentation and multi-frame video segmentation with varied prompt designs, revealing SAM 2's limitations in medical contexts. Consequently, we developed BioSAM 2, an enhanced foundation model optimized for biomedical data based on SAM 2. Our experiments show that BioSAM 2 not only surpasses the performance of existing state-of-the-art foundation models but also matches or even exceeds specialist models, demonstrating its efficacy and potential in the medical domain.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03302",
        "abstract url": "https://arxiv.org/abs/2408.03302",
        "title": "TextIM: Part-aware Interactive Motion Synthesis from Text",
        "rating": "-1",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this work, we propose TextIM, a novel framework for synthesizing TEXT-driven human Interactive Motions, with a focus on the precise alignment of part-level semantics. Existing methods often overlook the critical roles of interactive body parts and fail to adequately capture and align part-level semantics, resulting in inaccuracies and even erroneous movement outcomes. To address these issues, TextIM utilizes a decoupled conditional diffusion framework to enhance the detailed alignment between interactive movements and corresponding semantic intents from textual descriptions. Our approach leverages large language models, functioning as a human brain, to identify interacting human body parts and to comprehend interaction semantics to generate complicated and subtle interactive motion. Guided by the refined movements of the interacting parts, TextIM further extends these movements into a coherent whole-body motion. We design a spatial coherence module to complement the entire body movements while maintaining consistency and harmony across body parts using a part graph convolutional network. For training and evaluation, we carefully selected and re-labeled interactive motions from HUMANML3D to develop a specialized dataset. Experimental results demonstrate that TextIM produces semantically accurate human interactive motions, significantly enhancing the realism and applicability of synthesized interactive motions in diverse scenarios, even including interactions with deformable and dynamically changing objects.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03312",
        "abstract url": "https://arxiv.org/abs/2408.03312",
        "title": "MDT-A2G: Exploring Masked Diffusion Transformers for Co-Speech Gesture Generation",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in the field of Diffusion Transformers have substantially improved the generation of high-quality 2D images, 3D videos, and 3D shapes. However, the effectiveness of the Transformer architecture in the domain of co-speech gesture generation remains relatively unexplored, as prior methodologies have predominantly employed the Convolutional Neural Network (CNNs) or simple a few transformer layers. In an attempt to bridge this research gap, we introduce a novel Masked Diffusion Transformer for co-speech gesture generation, referred to as MDT-A2G, which directly implements the denoising process on gesture sequences. To enhance the contextual reasoning capability of temporally aligned speech-driven gestures, we incorporate a novel Masked Diffusion Transformer. This model employs a mask modeling scheme specifically designed to strengthen temporal relation learning among sequence gestures, thereby expediting the learning process and leading to coherent and realistic motions. Apart from audio, Our MDT-A2G model also integrates multi-modal information, encompassing text, emotion, and identity. Furthermore, we propose an efficient inference strategy that diminishes the denoising computation by leveraging previously calculated results, thereby achieving a speedup with negligible performance degradation. Experimental results demonstrate that MDT-A2G excels in gesture generation, boasting a learning speed that is over 6$\\times$ faster than traditional diffusion transformers and an inference speed that is 5.7$\\times$ than the standard diffusion model.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03394",
        "abstract url": "https://arxiv.org/abs/2408.03394",
        "title": "Faster Model Predictive Control via Self-Supervised Initialization Learning",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Optimization for robot control tasks, spanning various methodologies, includes Model Predictive Control (MPC). However, the complexity of the system, such as non-convex and non-differentiable cost functions and prolonged planning horizons often drastically increases the computation time, limiting MPC's real-world applicability. Prior works in speeding up the optimization have limitations on solving convex problem and generalizing to hold out domains. To overcome this challenge, we develop a novel framework aiming at expediting optimization processes. In our framework, we combine offline self-supervised learning and online fine-tuning through reinforcement learning to improve the control performance and reduce optimization time. We demonstrate the effectiveness of our method on a novel, challenging Formula-1-track driving task, achieving 3.9\\% higher performance in optimization time and 3.6\\% higher performance in tracking accuracy on challenging holdout tracks.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03426",
        "abstract url": "https://arxiv.org/abs/2408.03426",
        "title": "Dissecting the Infrastructure Used in Web-based Cryptojacking: A Measurement Perspective",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "This paper conducts a comprehensive examination of the infrastructure supporting cryptojacking operations. The analysis elucidates the methodologies, frameworks, and technologies malicious entities employ to misuse computational resources for unauthorized cryptocurrency mining. The investigation focuses on identifying websites serving as platforms for cryptojacking activities. A dataset of 887 websites, previously identified as cryptojacking sites, was compiled and analyzed to categorize the attacks and malicious activities observed. The study further delves into the DNS IP addresses, registrars, and name servers associated with hosting these websites to understand their structure and components. Various malware and illicit activities linked to these sites were identified, indicating the presence of unauthorized cryptocurrency mining via compromised sites. The findings highlight the vulnerability of website infrastructures to cryptojacking.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "12 pages, 3 figures, 6 tables, accepted for presentation in WISA 2024"
    },
    {
        "paper id": "2408.03448",
        "abstract url": "https://arxiv.org/abs/2408.03448",
        "title": "Post-Mortem Human Iris Segmentation Analysis with Deep Learning",
        "rating": "-1",
        "keywords": [
            [
                "BioBase-PostMortem-Iris-v1"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Iris recognition is widely used in several fields such as mobile phones, financial transactions, identification cards, airport security, international border control, voter registration for living persons. However, the possibility of identifying deceased individuals based on their iris patterns has emerged recently as a supplementary or alternative method valuable in forensic analysis. Simultaneously, it poses numerous new technological challenges and one of the most challenging among them is the image segmentation stage as conventional iris recognition approaches have struggled to reliably execute it. This paper presents and compares Deep Learning (DL) models designed for segmenting iris images collected from the deceased subjects, by training SegNet and DeepLabV3+ semantic segmentation methods where using VGG19, ResNet18, ResNet50, MobileNetv2, Xception, or InceptionResNetv2 as backbones. In this study, our experiments demonstrate that our proposed method effectively learns and identifies specific deformations inherent in post-mortem samples and providing a significant improvement in accuracy. By employing our novel method MobileNetv2 as the backbone of DeepLabV3+ and replacing the final layer with a hybrid loss function combining Boundary and Dice loss, we achieve Mean Intersection over Union of 95.54% on the Warsaw-BioBase-PostMortem-Iris-v1 dataset. To the best of our knowledge, this study provides the most extensive evaluation of DL models for post-mortem iris segmentation.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "submitted to ijcb 2024 special session"
    },
    {
        "paper id": "2408.03453",
        "abstract url": "https://arxiv.org/abs/2408.03453",
        "title": "An Interactive Augmented Reality Interface for Personalized Proxemics Modeling",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Understanding and respecting personal space preferences is essential for socially assistive robots designed for older adult users. This work introduces and evaluates a novel personalized context-aware method for modeling users' proxemics preferences during human-robot interactions. Using an interactive augmented reality interface, we collected a set of user-preferred distances from the robot and employed an active transfer learning approach to fine-tune a specialized deep learning model. We evaluated this approach through two user studies: 1) a convenience population study (N = 24) to validate the efficacy of the active transfer learning approach; and 2) a user study involving older adults (N = 15) to assess the system's usability. We compared the data collected with the augmented reality interface and with the physical robot to examine the relationship between proxemics preferences for a virtual robot versus a physically embodied robot. We found that fine-tuning significantly improved model performance: on average, the error in testing decreased by 26.97% after fine-tuning. The system was well-received by older adult participants, who provided valuable feedback and suggestions for future work.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "M. Nigro, A. O'Connell, T. Groechel, A.M. Velentza and M. Matari\u0107, \"An Interactive Augmented Reality Interface for Personalized Proxemics Modeling: Comfort and Human-Robot Interactions,\" in IEEE Robotics & Automation Magazine, doi: 10.1109/MRA.2024.3415108"
    },
    {
        "paper id": "2408.03464",
        "abstract url": "https://arxiv.org/abs/2408.03464",
        "title": "AI Foundation Models in Remote Sensing: A Survey",
        "rating": "-1",
        "keywords": [
            [
                "Remote Sensing"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Artificial Intelligence (AI) technologies have profoundly transformed the field of remote sensing, revolutionizing data collection, processing, and analysis. Traditionally reliant on manual interpretation and task-specific models, remote sensing has been significantly enhanced by the advent of foundation models--large-scale, pre-trained AI models capable of performing a wide array of tasks with unprecedented accuracy and efficiency. This paper provides a comprehensive survey of foundation models in the remote sensing domain, covering models released between June 2021 and June 2024. We categorize these models based on their applications in computer vision and domain-specific tasks, offering insights into their architectures, pre-training datasets, and methodologies. Through detailed performance comparisons, we highlight emerging trends and the significant advancements achieved by these foundation models. Additionally, we discuss the technical challenges, practical implications, and future research directions, addressing the need for high-quality data, computational resources, and improved model generalization. Our research also finds that pre-training methods, particularly self-supervised learning techniques like contrastive learning and masked autoencoders, significantly enhance the performance and robustness of foundation models in remote sensing tasks such as scene classification, object detection, and other applications. This survey aims to serve as a resource for researchers and practitioners by providing a panorama of advances and promising pathways for continued development and application of foundation models in remote sensing.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03466",
        "abstract url": "https://arxiv.org/abs/2408.03466",
        "title": "Rapid mixing of the down-up walk on matchings of a fixed size",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Let $G = (V,E)$ be a graph on $n$ vertices and let $m^*(G)$ denote the size of a maximum matching in $G$. We show that for any $\u03b4> 0$ and for any $1 \\leq k \\leq (1-\u03b4)m^*(G)$, the down-up walk on matchings of size $k$ in $G$ mixes in time polynomial in $n$. Previously, polynomial mixing was not known even for graphs with maximum degree $\u0394$, and our result makes progress on a conjecture of Jain, Perkins, Sah, and Sawhney [STOC, 2022] that the down-up walk mixes in optimal time $O_{\u0394,\u03b4}(n\\log{n})$. In contrast with recent works analyzing mixing of down-up walks in various settings using the spectral independence framework, we bound the spectral gap by constructing and analyzing a suitable multi-commodity flow. In fact, we present constructions demonstrating the limitations of the spectral independence approach in our setting.",
        "subjects": [
            "cs.DS",
            "math.CO"
        ],
        "comment": "12 pages; comments welcome"
    },
    {
        "paper id": "2408.03469",
        "abstract url": "https://arxiv.org/abs/2408.03469",
        "title": "Performance Classification and Remaining Useful Life Prediction of Lithium Batteries Using Machine Learning and Early Cycle Electrochemical Impedance Spectroscopy Measurements",
        "rating": "-1",
        "keywords": [
            [
                "SVM",
                "support vector machine"
            ]
        ],
        "abstract": "We presents an approach for early cycle classification of lithium-ion batteries into high and low-performing categories, coupled with the prediction of their remaining useful life (RUL) using a linear lasso technique. Traditional methods often rely on extensive cycling and the measurement of a large number of electrochemical impedance spectroscopy (EIS) frequencies to assess battery performance, which can be time and resource consuming. In this study, we propose a methodology that leverages specific EIS frequencies to achieve accurate classification and RUL prediction within the first few cycles of battery operation. Notably, given only the 20 kHz impedance response, our support vector machine (SVM) model classifies batteries with 100\\% accuracy. Additionally, our findings reveal that battery performance classification is frequency agnostic within the high frequency ($<20$ kHz) to low-frequency (32 mHz) range. Our model also demonstrates accurate RUL predictions with $R^2>0.96$ based on the out of phase impedance response at a single high (20 kHz) and a single mid-frequency (8.8 Hz), in conjunction with temperature data. This research underscores the significance of the mid-frequency impedance response as merely one among several crucial features in determining battery performance, thereby broadening the understanding of factors influencing battery behavior.",
        "subjects": [
            "cond-mat.mtrl-sci",
            "eess.SP"
        ],
        "comment": "10 pages, 4 figures"
    },
    {
        "paper id": "2408.03499",
        "abstract url": "https://arxiv.org/abs/2408.03499",
        "title": "FacialPulse: An Efficient RNN-based Depression Detection via Temporal Facial Landmarks",
        "rating": "-1",
        "keywords": [
            [
                "health",
                "Facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Depression is a prevalent mental health disorder that significantly impacts individuals' lives and well-being. Early detection and intervention are crucial for effective treatment and management of depression. Recently, there are many end-to-end deep learning methods leveraging the facial expression features for automatic depression detection. However, most current methods overlook the temporal dynamics of facial expressions. Although very recent 3DCNN methods remedy this gap, they introduce more computational cost due to the selection of CNN-based backbones and redundant facial features. To address the above limitations, by considering the timing correlation of facial expressions, we propose a novel framework called FacialPulse, which recognizes depression with high accuracy and speed. By harnessing the bidirectional nature and proficiently addressing long-term dependencies, the Facial Motion Modeling Module (FMMM) is designed in FacialPulse to fully capture temporal features. Since the proposed FMMM has parallel processing capabilities and has the gate mechanism to mitigate gradient vanishing, this module can also significantly boost the training speed. Besides, to effectively use facial landmarks to replace original images to decrease information redundancy, a Facial Landmark Calibration Module (FLCM) is designed to eliminate facial landmark errors to further improve recognition accuracy. Extensive experiments on the AVEC2014 dataset and MMDA dataset (a depression dataset) demonstrate the superiority of FacialPulse on recognition accuracy and speed, with the average MAE (Mean Absolute Error) decreased by 21% compared to baselines, and the recognition speed increased by 100% compared to state-of-the-art methods. Codes are released at https://github.com/volatileee/FacialPulse.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03500",
        "abstract url": "https://arxiv.org/abs/2408.03500",
        "title": "e-Health CSIRO at RRG24: Entropy-Augmented Self-Critical Sequence Training for Radiology Report Generation",
        "rating": "-1",
        "keywords": [
            [
                "Health",
                "X-ray",
                "Radiology"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The Shared Task on Large-Scale Radiology Report Generation (RRG24) aims to expedite the development of assistive systems for interpreting and reporting on chest X-ray (CXR) images. This task challenges participants to develop models that generate the findings and impression sections of radiology reports from CXRs from a patient's study, using five different datasets. This paper outlines the e-Health CSIRO team's approach, which achieved multiple first-place finishes in RRG24. The core novelty of our approach lies in the addition of entropy regularisation to self-critical sequence training, to maintain a higher entropy in the token distribution. This prevents overfitting to common phrases and ensures a broader exploration of the vocabulary during training, essential for handling the diversity of the radiology reports in the RRG24 datasets. Our model is available on Hugging Face https://huggingface.co/aehrc/cxrmate-rrg24.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03516",
        "abstract url": "https://arxiv.org/abs/2408.03516",
        "title": "Leveraging LLMs for Enhanced Open-Vocabulary 3D Scene Understanding in Autonomous Driving",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "Autonomous Driving"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "This paper introduces a novel method for open-vocabulary 3D scene understanding in autonomous driving by combining Language Embedded 3D Gaussians with Large Language Models (LLMs) for enhanced inference. We propose utilizing LLMs to generate contextually relevant canonical phrases for segmentation and scene interpretation. Our method leverages the contextual and semantic capabilities of LLMs to produce a set of canonical phrases, which are then compared with the language features embedded in the 3D Gaussians. This LLM-guided approach significantly improves zero-shot scene understanding and detection of objects of interest, even in the most challenging or unfamiliar environments. Experimental results on the WayveScenes101 dataset demonstrate that our approach surpasses state-of-the-art methods in terms of accuracy and flexibility for open-vocabulary object detection and segmentation. This work represents a significant advancement towards more intelligent, context-aware autonomous driving systems, effectively bridging 3D scene representation with high-level semantic understanding.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03887",
        "abstract url": "https://arxiv.org/abs/2408.03887",
        "title": "Central Kurdish Text-to-Speech Synthesis with Novel End-to-End Transformer Training",
        "rating": "-1",
        "keywords": [
            [
                "Text-to-Speech"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Recent advancements in text-to-speech (TTS) models have aimed to streamline the two-stage process into a single-stage training approach. However, many single-stage models still lag behind in audio quality, particularly when handling Kurdish text and speech. There is a critical need to enhance text-to-speech conversion for the Kurdish language, particularly for the Sorani dialect, which has been relatively neglected and is underrepresented in recent text-to-speech advancements. This study introduces an end-to-end TTS model for efficiently generating high-quality Kurdish audio. The proposed method leverages a variational autoencoder (VAE) that is pre-trained for audio waveform reconstruction and is augmented by adversarial training. This involves aligning the prior distribution established by the pre-trained encoder with the posterior distribution of the text encoder within latent variables. Additionally, a stochastic duration predictor is incorporated to imbue synthesized Kurdish speech with diverse rhythms. By aligning latent distributions and integrating the stochastic duration predictor, the proposed method facilitates the real-time generation of natural Kurdish speech audio, offering flexibility in pitches and rhythms. Empirical evaluation via the mean opinion score (MOS) on a custom dataset confirms the superior performance of our approach (MOS of 3.94) compared with that of a one-stage system and other two-staged systems as assessed through a subjective human evaluation.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": "19 pages"
    },
    {
        "paper id": "2408.03085",
        "abstract url": "https://arxiv.org/abs/2408.03085",
        "title": "Matrix Multiplication on Quantum Computer",
        "rating": "-1.5",
        "keywords": [
            [
                "Quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper introduces an innovative and practical approach to universal quantum matrix multiplication. We designed optimized quantum adders and multipliers based on Quantum Fourier Transform (QFT), which significantly reduced the number of gates used compared to classical adders and multipliers. Subsequently, we construct a basic universal quantum matrix multiplication and extend it to the Strassen algorithm. We conduct comparative experiments to analyze the performance of the quantum matrix multiplication and evaluate the acceleration provided by the optimized quantum adder and multiplier. Furthermore, we investigate the advantages and disadvantages of the quantum Strassen algorithm compared to basic quantum matrix multiplication.",
        "subjects": [
            "quant-ph",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03088",
        "abstract url": "https://arxiv.org/abs/2408.03088",
        "title": "QADQN: Quantum Attention Deep Q-Network for Financial Market Prediction",
        "rating": "-1.5",
        "keywords": [
            [
                "Quantum"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Financial market prediction and optimal trading strategy development remain challenging due to market complexity and volatility. Our research in quantum finance and reinforcement learning for decision-making demonstrates the approach of quantum-classical hybrid algorithms to tackling real-world financial challenges. In this respect, we corroborate the concept with rigorous backtesting and validate the framework's performance under realistic market conditions, by including fixed transaction cost per trade. This paper introduces a Quantum Attention Deep Q-Network (QADQN) approach to address these challenges through quantum-enhanced reinforcement learning. Our QADQN architecture uses a variational quantum circuit inside a traditional deep Q-learning framework to take advantage of possible quantum advantages in decision-making. We gauge the QADQN agent's performance on historical data from major market indices, including the S&P 500. We evaluate the agent's learning process by examining its reward accumulation and the effectiveness of its experience replay mechanism. Our empirical results demonstrate the QADQN's superior performance, achieving better risk-adjusted returns with Sortino ratios of 1.28 and 1.19 for non-overlapping and overlapping test periods respectively, indicating effective downside risk management.",
        "subjects": [
            "quant-ph",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted at the 2024 IEEE International Conference on Quantum Computing and Engineering (QCE24), QCRL, September 2024"
    },
    {
        "paper id": "2408.03140",
        "abstract url": "https://arxiv.org/abs/2408.03140",
        "title": "Measuring interconnectedness of infectious diseases in funded and unfunded research: a temporal network analysis on bibliometric data 1995-2022",
        "rating": "-1.5",
        "keywords": [
            [
                "health",
                "disease"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Despite substantial investments in infectious disease research over the past decades, the field continues to struggle with inadequate long-term investment strategies and resource disparities, which highlights the critical need for a better understanding of funding and research landscapes to support evidence-based policymaking. Our study presents a novel perspective on the interconnectedness of evolving infectious disease knowledge. Through identifying publications based on funded and unfunded research, the analysis of temporal network of infectious disease associations reveals (i) growing compartmentalisation of funded research, i.e., it focuses on the groups of infectious diseases with readily established connections, and (ii) the growth in global integration in unfunded research, i.e., it tends to be more widely exploratory and links distant diseases. Moreover, we find that in both funded and unfunded research prominent diseases like HIV, malaria and tuberculosis have strong bridging effects facilitating global integration, while diphtheria, tetanus, and pertussis are characterised with strong local connectivity between themselves. We also find that although coronavirus has seen a surge in publications since COVID-19, its systemic impact on the interconnectedness of infectious disease knowledge remains relatively low. Our work highlights the importance of considering the interconnectedness of infectious diseases in health policy making and has potential to contribute to more efficient health resource allocation.",
        "subjects": [
            "cs.SI",
            "physics.soc-ph"
        ],
        "comment": "Submitted to Scientometrics"
    },
    {
        "paper id": "2408.03152",
        "abstract url": "https://arxiv.org/abs/2408.03152",
        "title": "TSC: A Simple Two-Sided Constraint against Over-Smoothing",
        "rating": "-1.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph Convolutional Neural Network (GCN), a widely adopted method for analyzing relational data, enhances node discriminability through the aggregation of neighboring information. Usually, stacking multiple layers can improve the performance of GCN by leveraging information from high-order neighbors. However, the increase of the network depth will induce the over-smoothing problem, which can be attributed to the quality and quantity of neighbors changing: (a) neighbor quality, node's neighbors become overlapping in high order, leading to aggregated information becoming indistinguishable, (b) neighbor quantity, the exponentially growing aggregated neighbors submerges the node's initial feature by recursively aggregating operations. Current solutions mainly focus on one of the above causes and seldom consider both at once. Aiming at tackling both causes of over-smoothing in one shot, we introduce a simple Two-Sided Constraint (TSC) for GCNs, comprising two straightforward yet potent techniques: random masking and contrastive constraint. The random masking acts on the representation matrix's columns to regulate the degree of information aggregation from neighbors, thus preventing the convergence of node representations. Meanwhile, the contrastive constraint, applied to the representation matrix's rows, enhances the discriminability of the nodes. Designed as a plug-in module, TSC can be easily coupled with GCN or SGC architectures. Experimental analyses on diverse real-world graph datasets verify that our approach markedly reduces the convergence of node's representation and the performance degradation in deeper GCN.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "accept by KDD2024"
    },
    {
        "paper id": "2408.03157",
        "abstract url": "https://arxiv.org/abs/2408.03157",
        "title": "Narrowband-IoT (NB-IoT) and IoT Use Cases in Universities, Campuses, and Educational Institutions: A Research Analysis",
        "rating": "-1.5",
        "keywords": [
            [
                "IoT"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "The main objective of this research paper is to analyze the available use cases of Narrowband-IoT and IoT in universities, campuses, and educational institutions. A literature review was conducted using multiple databases such as IEEE Xplore, ACM Digital Library, and Scopus. The study explores the benefits of IoT adoption in higher education. Various use cases of NB-IoT in educational institutions were analyzed, including smart campus management, asset tracking, monitoring, and safety and security systems. Of the six use cases assessed, three focused on the deployment of IoT Things, while three focused on NB-IoT Connectivity. The research paper concludes that NB-IoT technology has significant potential to enhance various aspects of educational institutions, from smart campus management to improving safety and security systems. The study recommends further exploration and implementation of NB-IoT technology in educational settings to improve efficiency, security, and overall campus management. The research highlights the potential applications of NB-IoT in universities and educational institutions, paving the way for future studies in this area. The social implications of this research could involve enhancing the overall learning experience for students, improving campus safety, and promoting technological advancements in educational settings. Keywords: narrowband-IoT, Internet-of-Things, smart campus, smart institutions",
        "subjects": [
            "cs.CY",
            "cs.ET"
        ],
        "comment": "16 pages, 7 figures"
    },
    {
        "paper id": "2408.03223",
        "abstract url": "https://arxiv.org/abs/2408.03223",
        "title": "Don't Think It Twice: Exploit Shift Invariance for Efficient Online Streaming Inference of CNNs",
        "rating": "-1.5",
        "keywords": [
            [
                "biomedical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep learning time-series processing often relies on convolutional neural networks with overlapping windows. This overlap allows the network to produce an output faster than the window length. However, it introduces additional computations. This work explores the potential to optimize computational efficiency during inference by exploiting convolution's shift-invariance properties to skip the calculation of layer activations between successive overlapping windows. Although convolutions are shift-invariant, zero-padding and pooling operations, widely used in such networks, are not efficient and complicate efficient streaming inference. We introduce StreamiNNC, a strategy to deploy Convolutional Neural Networks for online streaming inference. We explore the adverse effects of zero padding and pooling on the accuracy of streaming inference, deriving theoretical error upper bounds for pooling during streaming. We address these limitations by proposing signal padding and pooling alignment and provide guidelines for designing and deploying models for StreamiNNC. We validate our method in simulated data and on three real-world biomedical signal processing applications. StreamiNNC achieves a low deviation between streaming output and normal inference for all three networks (2.03 - 3.55% NRMSE). This work demonstrates that it is possible to linearly speed up the inference of streaming CNNs processing overlapping windows, negating the additional computation typically incurred by overlapping windows.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03236",
        "abstract url": "https://arxiv.org/abs/2408.03236",
        "title": "Analysis of Partially-Calibrated Sparse Subarrays for Direction Finding with Extended Degrees of Freedom",
        "rating": "-1.5",
        "keywords": [
            [
                "MUSIC"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper investigates the problem of direction-of-arrival (DOA) estimation using multiple partially-calibrated sparse subarrays. In particular, we present the Generalized Coarray Multiple Signal Classification (GCA-MUSIC) DOA estimation algorithm to scenarios with partially-calibrated sparse subarrays. The proposed GCA-MUSIC algorithm exploits the difference coarray for each subarray, followed by a specific pseudo-spectrum merging rule that is based on the intersection of the signal subspaces associated to each subarray. This rule assumes that there is no a priori knowledge about the cross-covariance between subarrays. In that way, only the second-order statistics of each subarray are used to estimate the directions with increased degrees of freedom, i.e., the estimation procedure preserves the coarray Multiple Signal Classification and sparse arrays properties to estimate more sources than the number of physical sensors in each subarray. Numerical simulations show that the proposed GCA-MUSIC has better performance than other similar strategies.",
        "subjects": [
            "cs.LG",
            "eess.SP"
        ],
        "comment": "6 pages, 5 figures"
    },
    {
        "paper id": "2408.03320",
        "abstract url": "https://arxiv.org/abs/2408.03320",
        "title": "Hedge Fund Portfolio Construction Using PolyModel Theory and iTransformer",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "When constructing portfolios, a key problem is that a lot of financial time series data are sparse, making it challenging to apply machine learning methods. Polymodel theory can solve this issue and demonstrate superiority in portfolio construction from various aspects. To implement the PolyModel theory for constructing a hedge fund portfolio, we begin by identifying an asset pool, utilizing over 10,000 hedge funds for the past 29 years' data. PolyModel theory also involves choosing a wide-ranging set of risk factors, which includes various financial indices, currencies, and commodity prices. This comprehensive selection mirrors the complexities of the real-world environment. Leveraging on the PolyModel theory, we create quantitative measures such as Long-term Alpha, Long-term Ratio, and SVaR. We also use more classical measures like the Sharpe ratio or Morningstar's MRAR. To enhance the performance of the constructed portfolio, we also employ the latest deep learning techniques (iTransformer) to capture the upward trend, while efficiently controlling the downside, using all the features. The iTransformer model is specifically designed to address the challenges in high-dimensional time series forecasting and could largely improve our strategies. More precisely, our strategies achieve better Sharpe ratio and annualized return. The above process enables us to create multiple portfolio strategies aiming for high returns and low risks when compared to various benchmarks.",
        "subjects": [
            "q-fin.PM",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03323",
        "abstract url": "https://arxiv.org/abs/2408.03323",
        "title": "ClassiFIM: An Unsupervised Method To Detect Phase Transitions",
        "rating": "-1.5",
        "keywords": [
            [
                "quantum",
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Estimation of the Fisher Information Metric (FIM-estimation) is an important task that arises in unsupervised learning of phase transitions, a problem proposed by physicists. This work completes the definition of the task by defining rigorous evaluation metrics distMSE, distMSEPS, and distRE and introduces ClassiFIM, a novel machine learning method designed to solve the FIM-estimation task. Unlike existing methods for unsupervised learning of phase transitions, ClassiFIM directly estimates a well-defined quantity (the FIM), allowing it to be rigorously compared to any present and future other methods that estimate the same. ClassiFIM transforms a dataset for the FIM-estimation task into a dataset for an auxiliary binary classification task and involves selecting and training a model for the latter. We prove that the output of ClassiFIM approaches the exact FIM in the limit of infinite dataset size and under certain regularity conditions. We implement ClassiFIM on multiple datasets, including datasets describing classical and quantum phase transitions, and find that it achieves a good ground truth approximation with modest computational resources. Furthermore, we independently implement two alternative state-of-the-art methods for unsupervised estimation of phase transition locations on the same datasets and find that ClassiFIM predicts such locations at least as well as these other methods. To emphasize the generality of our method, we also propose and generate the MNIST-CNN dataset, which consists of the output of CNNs trained on MNIST for different hyperparameter choices. Using ClassiFIM on this dataset suggests there is a phase transition in the distribution of image-prediction pairs for CNNs trained on MNIST, demonstrating the broad scope of FIM-estimation beyond physics.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "23 pages, 5 figures"
    },
    {
        "paper id": "2408.03399",
        "abstract url": "https://arxiv.org/abs/2408.03399",
        "title": "RHiOTS: A Framework for Evaluating Hierarchical Time Series Forecasting Algorithms",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We introduce the Robustness of Hierarchically Organized Time Series (RHiOTS) framework, designed to assess the robustness of hierarchical time series forecasting models and algorithms on real-world datasets. Hierarchical time series, where lower-level forecasts must sum to upper-level ones, are prevalent in various contexts, such as retail sales across countries. Current empirical evaluations of forecasting methods are often limited to a small set of benchmark datasets, offering a narrow view of algorithm behavior. RHiOTS addresses this gap by systematically altering existing datasets and modifying the characteristics of individual series and their interrelations. It uses a set of parameterizable transformations to simulate those changes in the data distribution. Additionally, RHiOTS incorporates an innovative visualization component, turning complex, multidimensional robustness evaluation results into intuitive, easily interpretable visuals. This approach allows an in-depth analysis of algorithm and model behavior under diverse conditions. We illustrate the use of RHiOTS by analyzing the predictive performance of several algorithms. Our findings show that traditional statistical methods are more robust than state-of-the-art deep learning algorithms, except when the transformation effect is highly disruptive. Furthermore, we found no significant differences in the robustness of the algorithms when applying specific reconciliation methods, such as MinT. RHiOTS provides researchers with a comprehensive tool for understanding the nuanced behavior of forecasting algorithms, offering a more reliable basis for selecting the most appropriate method for a given problem.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '24), August 25--29, 2024, Barcelona, Spain"
    },
    {
        "paper id": "2408.03400",
        "abstract url": "https://arxiv.org/abs/2408.03400",
        "title": "Attacks and Defenses for Generative Diffusion Models: A Comprehensive Survey",
        "rating": "-1.5",
        "keywords": [
            [
                "Diffusion",
                "text-to-image"
            ],
            [
                "Attacks"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Diffusion models (DMs) have achieved state-of-the-art performance on various generative tasks such as image synthesis, text-to-image, and text-guided image-to-image generation. However, the more powerful the DMs, the more harmful they potentially are. Recent studies have shown that DMs are prone to a wide range of attacks, including adversarial attacks, membership inference, backdoor injection, and various multi-modal threats. Since numerous pre-trained DMs are published widely on the Internet, potential threats from these attacks are especially detrimental to the society, making DM-related security a worth investigating topic. Therefore, in this paper, we conduct a comprehensive survey on the security aspect of DMs, focusing on various attack and defense methods for DMs. First, we present crucial knowledge of DMs with five main types of DMs, including denoising diffusion probabilistic models, denoising diffusion implicit models, noise conditioned score networks, stochastic differential equations, and multi-modal conditional DMs. We further survey a variety of recent studies investigating different types of attacks that exploit the vulnerabilities of DMs. Then, we thoroughly review potential countermeasures to mitigate each of the presented threats. Finally, we discuss open challenges of DM-related security and envision certain research directions for this topic.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03405",
        "abstract url": "https://arxiv.org/abs/2408.03405",
        "title": "Combining Diverse Information for Coordinated Action: Stochastic Bandit Algorithms for Heterogeneous Agents",
        "rating": "-1.5",
        "keywords": [
            [
                "medical",
                "disease"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Stochastic multi-agent multi-armed bandits typically assume that the rewards from each arm follow a fixed distribution, regardless of which agent pulls the arm. However, in many real-world settings, rewards can depend on the sensitivity of each agent to their environment. In medical screening, disease detection rates can vary by test type; in preference matching, rewards can depend on user preferences; and in environmental sensing, observation quality can vary across sensors. Since past work does not specify how to allocate agents of heterogeneous but known sensitivity of these types in a stochastic bandit setting, we introduce a UCB-style algorithm, Min-Width, which aggregates information from diverse agents. In doing so, we address the joint challenges of (i) aggregating the rewards, which follow different distributions for each agent-arm pair, and (ii) coordinating the assignments of agents to arms. Min-Width facilitates efficient collaboration among heterogeneous agents, exploiting the known structure in the agents' reward functions to weight their rewards accordingly. We analyze the regret of Min-Width and conduct pseudo-synthetic and fully synthetic experiments to study the performance of different levels of information sharing. Our results confirm that the gains to modeling agent heterogeneity tend to be greater when the sensitivities are more varied across agents, while combining more information does not always improve performance.",
        "subjects": [
            "cs.MA",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "19 pages, 6 figures, to be published in ECAI 2024"
    },
    {
        "paper id": "2408.03421",
        "abstract url": "https://arxiv.org/abs/2408.03421",
        "title": "Probabilistic Scores of Classifiers, Calibration is not Enough",
        "rating": "-1.5",
        "keywords": [
            [
                "medical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In binary classification tasks, accurate representation of probabilistic predictions is essential for various real-world applications such as predicting payment defaults or assessing medical risks. The model must then be well-calibrated to ensure alignment between predicted probabilities and actual outcomes. However, when score heterogeneity deviates from the underlying data probability distribution, traditional calibration metrics lose reliability, failing to align score distribution with actual probabilities. In this study, we highlight approaches that prioritize optimizing the alignment between predicted scores and true probability distributions over minimizing traditional performance or calibration metrics. When employing tree-based models such as Random Forest and XGBoost, our analysis emphasizes the flexibility these models offer in tuning hyperparameters to minimize the Kullback-Leibler (KL) divergence between predicted and true distributions. Through extensive empirical analysis across 10 UCI datasets and simulations, we demonstrate that optimizing tree-based models based on KL divergence yields superior alignment between predicted scores and actual probabilities without significant performance loss. In real-world scenarios, the reference probability is determined a priori as a Beta distribution estimated through maximum likelihood. Conversely, minimizing traditional calibration metrics may lead to suboptimal results, characterized by notable performance declines and inferior KL values. Our findings reveal limitations in traditional calibration metrics, which could undermine the reliability of predictive models for critical decision-making.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03445",
        "abstract url": "https://arxiv.org/abs/2408.03445",
        "title": "Spacecraft inertial parameters estimation using time series clustering and reinforcement learning",
        "rating": "-1.5",
        "keywords": [
            [
                "satellite"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper presents a machine learning approach to estimate the inertial parameters of a spacecraft in cases when those change during operations, e.g. multiple deployments of payloads, unfolding of appendages and booms, propellant consumption as well as during in-orbit servicing and active debris removal operations. The machine learning approach uses time series clustering together with an optimised actuation sequence generated by reinforcement learning to facilitate distinguishing among different inertial parameter sets. The performance of the proposed strategy is assessed against the case of a multi-satellite deployment system showing that the algorithm is resilient towards common disturbances in such kinds of operations.",
        "subjects": [
            "astro-ph.IM",
            "cs.LG",
            "cs.RO"
        ],
        "comment": "6 pages, 3 figures, 1 table. To be presented in ESA - AI for Space (SPAICE)"
    },
    {
        "paper id": "2408.03449",
        "abstract url": "https://arxiv.org/abs/2408.03449",
        "title": "EEGMobile: Enhancing Speed and Accuracy in EEG-Based Gaze Prediction with Advanced Mobile Architectures",
        "rating": "-1.5",
        "keywords": [
            [
                "EEG"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Electroencephalography (EEG) analysis is an important domain in the realm of Brain-Computer Interface (BCI) research. To ensure BCI devices are capable of providing practical applications in the real world, brain signal processing techniques must be fast, accurate, and resource-conscious to deliver low-latency neural analytics. This study presents a model that leverages a pre-trained MobileViT alongside Knowledge Distillation (KD) for EEG regression tasks. Our results showcase that this model is capable of performing at a level comparable (only 3% lower) to the previous State-Of-The-Art (SOTA) on the EEGEyeNet Absolute Position Task while being 33% faster and 60% smaller. Our research presents a cost-effective model applicable to resource-constrained devices and contributes to expanding future research on lightweight, mobile-friendly models for EEG regression.",
        "subjects": [
            "eess.SP",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted HCI International 2024 - Late Breaking Work"
    },
    {
        "paper id": "2408.03463",
        "abstract url": "https://arxiv.org/abs/2408.03463",
        "title": "Identifying treatment response subgroups in observational time-to-event data",
        "rating": "-1.5",
        "keywords": [
            [
                "medical",
                "clinical"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Identifying patient subgroups with different treatment responses is an important task to inform medical recommendations, guidelines, and the design of future clinical trials. Existing approaches for subgroup analysis primarily focus on Randomised Controlled Trials (RCTs), in which treatment assignment is randomised. Furthermore, the patient cohort of an RCT is often constrained by cost, and is not representative of the heterogeneity of patients likely to receive treatment in real-world clinical practice. Therefore, when applied to observational studies, such approaches suffer from significant statistical biases because of the non-randomisation of treatment. Our work introduces a novel, outcome-guided method for identifying treatment response subgroups in observational studies. Our approach assigns each patient to a subgroup associated with two time-to-event distributions: one under treatment and one under control regime. It hence positions itself in between individualised and average treatment effect estimation. The assumptions of our model result in a simple correction of the statistical bias from treatment non-randomisation through inverse propensity weighting. In experiments, our approach significantly outperforms the current state-of-the-art method for outcome-guided subgroup analysis in both randomised and observational treatment regimes.",
        "subjects": [
            "stat.ME",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03478",
        "abstract url": "https://arxiv.org/abs/2408.03478",
        "title": "Effect of Kernel Size on CNN-Vision-Transformer-Based Gaze Prediction Using Electroencephalography Data",
        "rating": "-1.5",
        "keywords": [
            [
                "EEG"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we present an algorithm of gaze prediction from Electroencephalography (EEG) data. EEG-based gaze prediction is a new research topic that can serve as an alternative to traditional video-based eye-tracking. Compared to the existing state-of-the-art (SOTA) method, we improved the root mean-squared-error of EEG-based gaze prediction to 53.06 millimeters, while reducing the training time to less than 33% of its original duration. Our source code can be found at https://github.com/AmCh-Q/CSCI6907Project",
        "subjects": [
            "cs.LG"
        ],
        "comment": "International Conference on Human-Computer Interaction (HCII 2024)"
    },
    {
        "paper id": "2408.03480",
        "abstract url": "https://arxiv.org/abs/2408.03480",
        "title": "Advancing EEG-Based Gaze Prediction Using Depthwise Separable Convolution and Enhanced Pre-Processing",
        "rating": "-1.5",
        "keywords": [
            [
                "EEG"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the field of EEG-based gaze prediction, the application of deep learning to interpret complex neural data poses significant challenges. This study evaluates the effectiveness of pre-processing techniques and the effect of additional depthwise separable convolution on EEG vision transformers (ViTs) in a pretrained model architecture. We introduce a novel method, the EEG Deeper Clustered Vision Transformer (EEG-DCViT), which combines depthwise separable convolutional neural networks (CNNs) with vision transformers, enriched by a pre-processing strategy involving data clustering. The new approach demonstrates superior performance, establishing a new benchmark with a Root Mean Square Error (RMSE) of 51.6 mm. This achievement underscores the impact of pre-processing and model refinement in enhancing EEG-based applications.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03490",
        "abstract url": "https://arxiv.org/abs/2408.03490",
        "title": "Simultaneous and Meshfree Topology Optimization with Physics-informed Gaussian Processes",
        "rating": "-1.5",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Topology optimization (TO) provides a principled mathematical approach for optimizing the performance of a structure by designing its material spatial distribution in a pre-defined domain and subject to a set of constraints. The majority of existing TO approaches leverage numerical solvers for design evaluations during the optimization and hence have a nested nature and rely on discretizing the design variables. Contrary to these approaches, herein we develop a new class of TO methods based on the framework of Gaussian processes (GPs) whose mean functions are parameterized via deep neural networks. Specifically, we place GP priors on all design and state variables to represent them via parameterized continuous functions. These GPs share a deep neural network as their mean function but have as many independent kernels as there are state and design variables. We estimate all the parameters of our model in a single for loop that optimizes a penalized version of the performance metric where the penalty terms correspond to the state equations and design constraints. Attractive features of our approach include $(1)$ having a built-in continuation nature since the performance metric is optimized at the same time that the state equations are solved, and $(2)$ being discretization-invariant and accommodating complex domains and topologies. To test our method against conventional TO approaches implemented in commercial software, we evaluate it on four problems involving the minimization of dissipated power in Stokes flow. The results indicate that our approach does not need filtering techniques, has consistent computational costs, and is highly robust against random initializations and problem setup.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03508",
        "abstract url": "https://arxiv.org/abs/2408.03508",
        "title": "Autonomous, Self-driving Multi-Step Growth of Semiconductor Heterostructures Guided by Machine Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The semiconductor industry has prioritized automating repetitive tasks by closed-loop, autonomous experimentation which enables accelerated optimization of complex multi-step processes. The emergence of machine learning (ML) has ushered in automated process with minimal human intervention. In this work, we develop SemiEpi, a self-driving automation platform capable of executing molecular beam epitaxy (MBE) growth with multi-steps, continuous in-situ monitoring, and on-the-fly feedback control. By integrating standard hardware, homemade software, curve fitting, and multiple ML models, SemiEpi operates autonomously, eliminating the need for extensive expertise in MBE processes to achieve optimal outcomes. The platform actively learns from previous experimental results, identifying favorable conditions and proposing new experiments to achieve the desired results. We standardize and optimize growth for InAs/GaAs quantum dots (QDs) heterostructures to showcase the power of ML-guided multi-step growth. A temperature calibration was implemented to get the initial growth condition, and fine control of the process was executed using ML. Leveraging RHEED movies acquired during the growth, SemiEpi successfully identified and optimized a novel route for multi-step heterostructure growth. This work demonstrates the capabilities of closed-loop, ML-guided systems in addressing challenges in multi-step growth for any device. Our method is critical to achieve repeatable materials growth using commercially scalable tools. Our strategy facilitates the development of a hardware-independent process and enhancing process repeatability and stability, even without exhaustive knowledge of growth parameters.",
        "subjects": [
            "cond-mat.mtrl-sci",
            "cs.LG",
            "eess.SY"
        ],
        "comment": "5 figures"
    },
    {
        "paper id": "2408.03515",
        "abstract url": "https://arxiv.org/abs/2408.03515",
        "title": "A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems",
        "rating": "-1.5",
        "keywords": [
            [
                "robot",
                "navigation"
            ],
            [
                "Attack"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The integration of Large Language Models (LLMs) like GPT-4o into robotic systems represents a significant advancement in embodied artificial intelligence. These models can process multi-modal prompts, enabling them to generate more context-aware responses. However, this integration is not without challenges. One of the primary concerns is the potential security risks associated with using LLMs in robotic navigation tasks. These tasks require precise and reliable responses to ensure safe and effective operation. Multi-modal prompts, while enhancing the robot's understanding, also introduce complexities that can be exploited maliciously. For instance, adversarial inputs designed to mislead the model can lead to incorrect or dangerous navigational decisions. This study investigates the impact of prompt injections on mobile robot performance in LLM-integrated systems and explores secure prompt strategies to mitigate these risks. Our findings demonstrate a substantial overall improvement of approximately 30.8% in both attack detection and system performance with the implementation of robust defence mechanisms, highlighting their critical role in enhancing security and reliability in mission-oriented tasks.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03526",
        "abstract url": "https://arxiv.org/abs/2408.03526",
        "title": "Minimum Enclosing Ball Synthetic Minority Oversampling Technique from a Geometric Perspective",
        "rating": "-1.5",
        "keywords": [
            [
                "medical",
                "diagnosis"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Class imbalance refers to the significant difference in the number of samples from different classes within a dataset, making it challenging to identify minority class samples correctly. This issue is prevalent in real-world classification tasks, such as software defect prediction, medical diagnosis, and fraud detection. The synthetic minority oversampling technique (SMOTE) is widely used to address class imbalance issue, which is based on interpolation between randomly selected minority class samples and their neighbors. However, traditional SMOTE and most of its variants only interpolate between existing samples, which may be affected by noise samples in some cases and synthesize samples that lack diversity. To overcome these shortcomings, this paper proposes the Minimum Enclosing Ball SMOTE (MEB-SMOTE) method from a geometry perspective. Specifically, MEB is innovatively introduced into the oversampling method to construct a representative point. Then, high-quality samples are synthesized by interpolation between this representative point and the existing samples. The rationale behind constructing a representative point is discussed, demonstrating that the center of MEB is more suitable as the representative point. To exhibit the superiority of MEB-SMOTE, experiments are conducted on 15 real-world imbalanced datasets. The results indicate that MEB-SMOTE can effectively improve the classification performance on imbalanced datasets.",
        "subjects": [
            "cs.LG",
            "cs.CG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.02985",
        "abstract url": "https://arxiv.org/abs/2408.02985",
        "title": "Advancing Nonlinear System Stability Analysis with Hessian Matrix Analysis",
        "rating": "-2",
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "This paper introduces an innovative method for ensuring global stability in a broad array of nonlinear systems. The novel approach enhances the traditional analysis based on Jacobian matrices by incorporating the Taylor series boundary error of estimation and the eigenvalues of the Hessian matrix, resulting in a fresh criterion for global stability. The main strength of this methodology lies in its unrestricted nature regarding the number of equilibrium points or the system's dimension, giving it a competitive edge over alternative methods for global stability analysis. The efficacy of this method has been validated through its application to two established benchmark systems within the industrial domain. The results suggest that the expanded Jacobian stability analysis can ensure global stability under specific circumstances, which are thoroughly elaborated upon in the manuscript. The proposed approach serves as a robust tool for assessing the global stability of nonlinear systems and holds promise for advancing the realms of nonlinear control and optimization.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2408.02992",
        "abstract url": "https://arxiv.org/abs/2408.02992",
        "title": "Towards Smart Microfarming in an Urban Computing Continuum",
        "rating": "-2",
        "keywords": [
            [
                "IoT",
                "recommendation"
            ]
        ],
        "abstract": "Microfarming and urban computing have evolved as two distinct sustainability pillars of urban living today. In this paper, we combine these two concepts, while majorly extending them jointly towards novel concepts of smart microfarming and urban computing continuum. Smart microfarming is proposed with applications of artificial intelligence in microfarming, while an urban computing continuum is proposed as a major extension of the concept towards an efficient IoT-edge-cloud continuum. We propose and build a system architecture for a plant recommendation system that uses machine learning at the edge to find, from a pool of given plants, the most suitable ones for a given microfarm using monitored soil values obtained from IoT sensor devices. Moreover, we propose to integrate long-distance LoRa communication solution for sending the data from IoT to the edge system, due to its unlicensed nature and potential for open source implementations. Finally, we propose to integrate open source and less constrained application protocol solutions, such as AMQP and HTTP protocols, for storing the data in the cloud. An experimental setup is used to evaluate and analyze the performance and reliability of the data collection procedure and the quality of the recommendation solution. Furthermore, collaborative filtering is used for the completion of an incomplete information about soils and plants. Finally, various ML algorithms are applied to identify and recommend the optimal plan for a specific microfarm in an urban area.",
        "subjects": [
            "cs.NI",
            "eess.SP"
        ],
        "comment": "This paper is uploaded here for research community, thus it is for non-commercial purposes"
    },
    {
        "paper id": "2408.03002",
        "abstract url": "https://arxiv.org/abs/2408.03002",
        "title": "Damage identification for bridges using machine learning: Development and application to KW51 bridge",
        "rating": "-2",
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "Over the past few decades, structural health monitoring (SHM) has drawn significant attention to identifying damage in structures. However, there are open challenges related to the efficiency and applicability of the existing damage identification approaches. This paper proposes an effective approach that integrates both modal analysis and dynamic analysis strategies for damage identification and applies it to the KW51 railway bridge in Leuven, Belgium. The ML-based damage identification utilizes four types of features: modal analysis input, frequency characteristics, time-frequency characteristics, and stacked time series feature extraction from forced acceleration response. Signal processing methods such as stacking, Fourier transform, and wavelet transform are employed to extract damage-sensitive features from the acceleration series. The ML methods, including the k-nearest neighbors (kNN) algorithm, stacked gated recurrent unit (stacked GRU) network, and convolutional neural network (CNN), are then combined to assess the existence, extent, and location of damage. The proposed method is applied to the KW51 railway bridge. A finite element model (FEM) of the KW51 bridge is developed, which is validated by modal analysis. Various extents and locations of damage are simulated to generate the \"Damaged\" data, while the \"Intact\" data from FEM or measured data serve as a baseline for comparison. The identification results for the KW51 bridge demonstrate the high accuracy and robustness of the proposed approach, confirming its effectiveness in damage identification problems.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03010",
        "abstract url": "https://arxiv.org/abs/2408.03010",
        "title": "Fact Finder -- Enhancing Domain Expertise of Large Language Models by Incorporating Knowledge Graphs",
        "rating": "-2",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "biological",
                "medical",
                "disease"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in Large Language Models (LLMs) have showcased their proficiency in answering natural language queries. However, their effectiveness is hindered by limited domain-specific knowledge, raising concerns about the reliability of their responses. We introduce a hybrid system that augments LLMs with domain-specific knowledge graphs (KGs), thereby aiming to enhance factual correctness using a KG-based retrieval approach. We focus on a medical KG to demonstrate our methodology, which includes (1) pre-processing, (2) Cypher query generation, (3) Cypher query processing, (4) KG retrieval, and (5) LLM-enhanced response generation. We evaluate our system on a curated dataset of 69 samples, achieving a precision of 78\\% in retrieving correct KG nodes. Our findings indicate that the hybrid system surpasses a standalone LLM in accuracy and completeness, as verified by an LLM-as-a-Judge evaluation method. This positions the system as a promising tool for applications that demand factual correctness and completeness, such as target identification -- a critical process in pinpointing biological entities for disease treatment or crop enhancement. Moreover, its intuitive search interface and ability to provide accurate responses within seconds make it well-suited for time-sensitive, precision-focused research contexts. We publish the source code together with the dataset and the prompt templates used.",
        "subjects": [
            "cs.CL",
            "cs.IR"
        ],
        "comment": "10 pages, 7 figures"
    },
    {
        "paper id": "2408.03025",
        "abstract url": "https://arxiv.org/abs/2408.03025",
        "title": "The Crowd in MOOCs: A Study of Learning Patterns at Scale",
        "rating": "-2",
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "The increasing availability of learning activity data in Massive Open Online Courses (MOOCs) enables us to conduct a large-scale analysis of learners' learning behavior. In this paper, we analyze a dataset of 351 million learning activities from 0.8 million unique learners enrolled in over 1.6 thousand courses within two years. Specifically, we mine and identify the learning patterns of the crowd from both temporal and course enrollment perspectives leveraging mutual information theory and sequential pattern mining methods. From the temporal perspective, we find that the time intervals between consecutive learning activities of learners exhibit a mix of power-law and periodic cosine function distribution. By qualifying the relationship between course pairs, we observe that the most frequently co-enrolled courses usually fall in the same category or the same university. We demonstrate these findings can facilitate manifold applications including recommendation tasks on courses. A simple recommendation model utilizing the course enrollment patterns is competitive to the baselines with 200$\\times$ faster training time.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "16 pages"
    },
    {
        "paper id": "2408.03034",
        "abstract url": "https://arxiv.org/abs/2408.03034",
        "title": "A Course in Dynamic Optimization",
        "rating": "-2",
        "keywords": [
            [
                "tabular"
            ]
        ],
        "abstract": "These lecture notes are derived from a graduate-level course in dynamic optimization, offering an introduction to techniques and models extensively used in management science, economics, operations research, engineering, and computer science. The course emphasizes the theoretical underpinnings of discrete-time dynamic programming models and advanced algorithmic strategies for solving these models. Unlike typical treatments, it provides a proof for the principle of optimality for upper semi-continuous dynamic programming, a middle ground between the simpler countable state space case \\cite{bertsekas2012dynamic}, and the involved universally measurable case \\cite{bertsekas1996stochastic}. This approach is sufficiently rigorous to include important examples such as dynamic pricing, consumption-savings, and inventory management models. The course also delves into the properties of value and policy functions, leveraging classical results \\cite{topkis1998supermodularity} and recent developments. Additionally, it offers an introduction to reinforcement learning, including a formal proof of the convergence of Q-learning algorithms. Furthermore, the notes delve into policy gradient methods for the average reward case, presenting a convergence result for the tabular case in this context. This result is simple and similar to the discounted case but appears to be new.",
        "subjects": [
            "math.OC",
            "econ.TH",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03091",
        "abstract url": "https://arxiv.org/abs/2408.03091",
        "title": "Modeling User Intent Beyond Trigger: Incorporating Uncertainty for Trigger-Induced Recommendation",
        "rating": "-2",
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "To cater to users' desire for an immersive browsing experience, numerous e-commerce platforms provide various recommendation scenarios, with a focus on Trigger-Induced Recommendation (TIR) tasks. However, the majority of current TIR methods heavily rely on the trigger item to understand user intent, lacking a higher-level exploration and exploitation of user intent (e.g., popular items and complementary items), which may result in an overly convergent understanding of users' short-term intent and can be detrimental to users' long-term purchasing experiences. Moreover, users' short-term intent shows uncertainty and is affected by various factors such as browsing context and historical behaviors, which poses challenges to user intent modeling. To address these challenges, we propose a novel model called Deep Uncertainty Intent Network (DUIN), comprising three essential modules: i) Explicit Intent Exploit Module extracting explicit user intent using the contrastive learning paradigm; ii) Latent Intent Explore Module exploring latent user intent by leveraging the multi-view relationships between items; iii) Intent Uncertainty Measurement Module offering a distributional estimation and capturing the uncertainty associated with user intent. Experiments on three real-world datasets demonstrate the superior performance of DUIN compared to existing baselines. Notably, DUIN has been deployed across all TIR scenarios in our e-commerce platform, with online A/B testing results conclusively validating its superiority.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "Accepted at CIKM 2024"
    },
    {
        "paper id": "2408.03101",
        "abstract url": "https://arxiv.org/abs/2408.03101",
        "title": "Automated Defects Detection and Fix in Logging Statement",
        "rating": "-2",
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "Developers use logging statements to monitor software, but misleading logs can complicate maintenance by obscuring actual activities. Existing research on logging quality issues is limited, mainly focusing on single defects and manual fixes. To address this, we conducted a study identifying four defect types in logging statements through real-world log changes analysis. We propose LogFixer, a two-stage framework for automatic detection and updating of logging statements. In the offline stage, LogFixer uses a similarity-based classifier on synthetic defective logs to identify defects. During the online phase, this classifier evaluates logs in code snippets to determine necessary improvements, and an LLM-based recommendation framework suggests updates based on historical log changes. We evaluated LogFixer on real-world and synthetic datasets, and new real-world projects, achieving an F1 score of 0.625. LogFixer significantly improved static text and dynamic variables suggestions by 48.12\\% and 24.90\\%, respectively, and achieved a 61.49\\% success rate in recommending correct updates for new projects. We reported 40 problematic logs to GitHub, resulting in 25 confirmed and merged changes across 11 projects.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03102",
        "abstract url": "https://arxiv.org/abs/2408.03102",
        "title": "Adaptive-Sliding Mode Trajectory Control of Robot Manipulators with Uncertainties",
        "rating": "-2",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "Robot"
            ]
        ],
        "abstract": "In this paper, we propose and demonstrate an adaptive-sliding mode control for trajectory tracking control of robot manipulators subjected to uncertain dynamics, vibration disturbance, and payload variation disturbance. Throughout this work we seek a controller that is, robust to the uncertainty and disturbance, accurate, and implementable. To perform these requirements, we use a nonlinear Lyapunov-based approach for designing the controller and guaranteeing its stability. MATLAB-SIMULINK software is used to validate the approach and demonstrate the performance of the controller. Simulation results show that the derived controller is stable, robust to the disturbance and uncertainties, accurate, and implementable.",
        "subjects": [
            "eess.SY",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03131",
        "abstract url": "https://arxiv.org/abs/2408.03131",
        "title": "Stochastic Trajectory Optimization for Demonstration Imitation",
        "rating": "-2",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "Humans often learn new skills by imitating the experts and gradually developing their proficiency. In this work, we introduce Stochastic Trajectory Optimization for Demonstration Imitation (STODI), a trajectory optimization framework for robots to imitate the shape of demonstration trajectories with improved dynamic performance. Consistent with the human learning process, demonstration imitation serves as an initial step, while trajectory optimization aims to enhance robot motion performance. By generating random noise and constructing proper cost functions, the STODI effectively explores and exploits generated noisy trajectories while preserving the demonstration shape characteristics. We employ three metrics to measure the similarity of trajectories in both the time and frequency domains to help with demonstration imitation. Theoretical analysis reveals relationships among these metrics, emphasizing the benefits of frequency-domain analysis for specific tasks. Experiments on a 7-DOF robotic arm in the PyBullet simulator validate the efficacy of the STODI framework, showcasing the improved optimization performance and stability compared to previous methods.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03156",
        "abstract url": "https://arxiv.org/abs/2408.03156",
        "title": "Iterative CT Reconstruction via Latent Variable Optimization of Shallow Diffusion Models",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "MRI",
                "CT"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Image generative AI has garnered significant attention in recent years. In particular, the diffusion model, a core component of recent generative AI, produces high-quality images with rich diversity. In this study, we propose a novel CT reconstruction method by combining the denoising diffusion probabilistic model with iterative CT reconstruction. In sharp contrast to previous studies, we optimize the fidelity loss of CT reconstruction with respect to the latent variable of the diffusion model, instead of the image and model parameters. To suppress anatomical structure changes produced by the diffusion model, we shallow the diffusion and reverse processes, and fix a set of added noises in the reverse process to make it deterministic during inference. We demonstrate the effectiveness of the proposed method through sparse view CT reconstruction of 1/10 view projection data. Despite the simplicity of the implementation, the proposed method shows the capability of reconstructing high-quality images while preserving the patient's anatomical structure, and outperforms existing methods including iterative reconstruction, iterative reconstruction with total variation, and the diffusion model alone in terms of quantitative indices such as SSIM and PSNR. We also explore further sparse view CT using 1/20 view projection data with the same trained diffusion model. As the number of iterations increases, image quality improvement comparable to that of 1/10 sparse view CT reconstruction is achieved. In principle, the proposed method can be widely applied not only to CT but also to other imaging modalities such as MRI, PET, and SPECT.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "physics.med-ph"
        ],
        "comment": "19 pages, 9 figures"
    },
    {
        "paper id": "2408.03174",
        "abstract url": "https://arxiv.org/abs/2408.03174",
        "title": "Joint Transmission and Compression Design for 6G Networked Sensing with Limited-Capacity Backhaul",
        "rating": "-2",
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "This paper considers networked sensing in cellular network, where multiple base stations (BSs) first compress their received echo signals from multiple targets and then forward the quantized signals to the cloud via limited-capacity backhaul links, such that the cloud can leverage all useful echo signals to perform high-resolution localization. Under this setup, we manage to characterize the posterior Cramer-Rao Bound (PCRB) for localizing all the targets as a function of the transmit covariance matrix and the compression noise covariance matrix of each BS. Then, a PCRB minimization problem subject to the transmit power constraints and the backhaul capacity constraints is formulated to jointly design the BSs' transmission and compression strategies. We propose an efficient algorithm to solve this problem based on the alternating optimization technique. Specifically, it is shown that when either the transmit covariance matrices or the compression noise covariance matrices are fixed, the successive convex approximation technique can be leveraged to optimize the other type of covariance matrices locally. Numerical results are provided to verify the effectiveness of our proposed algorithm.",
        "subjects": [
            "eess.SP",
            "cs.IT"
        ],
        "comment": "accepted by IEEE Globecom 2025"
    },
    {
        "paper id": "2408.03177",
        "abstract url": "https://arxiv.org/abs/2408.03177",
        "title": "On Poles and Zeros of Linear Quantum Systems",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "The non-commutative nature of quantum mechanics imposes fundamental constraints on system dynamics, which in the linear realm are manifested by the physical realizability conditions on system matrices. These restrictions endow system matrices with special structure. The purpose of this paper is to study such structure by investigating zeros and poses of linear quantum systems. In particular, we show that $-s_0^\\ast$ is a transmission zero if and only if $s_0$ is a pole, and which is further generalized to the relationship between system eigenvalues and invariant zeros. Additionally, we study left-invertibility and fundamental tradeoff for linear quantum systems in terms of their zeros and poles.",
        "subjects": [
            "quant-ph",
            "eess.SY"
        ],
        "comment": "6 pages, 1 figure. Accepted by the 2024 IEEE Conference on Decision and Control"
    },
    {
        "paper id": "2408.03194",
        "abstract url": "https://arxiv.org/abs/2408.03194",
        "title": "SGSR: Structure-Guided Multi-Contrast MRI Super-Resolution via Spatio-Frequency Co-Query Attention",
        "rating": "-2",
        "keywords": [
            [
                "Super-Resolution"
            ],
            [
                "MRI"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Magnetic Resonance Imaging (MRI) is a leading diagnostic modality for a wide range of exams, where multiple contrast images are often acquired for characterizing different tissues. However, acquiring high-resolution MRI typically extends scan time, which can introduce motion artifacts. Super-resolution of MRI therefore emerges as a promising approach to mitigate these challenges. Earlier studies have investigated the use of multiple contrasts for MRI super-resolution (MCSR), whereas majority of them did not fully exploit the rich contrast-invariant structural information. To fully utilize such crucial prior knowledge of multi-contrast MRI, in this work, we propose a novel structure-guided MCSR (SGSR) framework based on a new spatio-frequency co-query attention (CQA) mechanism. Specifically, CQA performs attention on features of multiple contrasts with a shared structural query, which is particularly designed to extract, fuse, and refine the common structures from different contrasts. We further propose a novel frequency-domain CQA module in addition to the spatial domain, to enable more fine-grained structural refinement. Extensive experiments on fastMRI knee data and low-field brain MRI show that SGSR outperforms state-of-the-art MCSR methods with statistical significance.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "The 15th International Workshop on Machine Learning in Medical Imaging (MLMI 2024)"
    },
    {
        "paper id": "2408.03204",
        "abstract url": "https://arxiv.org/abs/2408.03204",
        "title": "GRAFX: An Open-Source Library for Audio Processing Graphs in PyTorch",
        "rating": "-2",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "music"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "We present GRAFX, an open-source library designed for handling audio processing graphs in PyTorch. Along with various library functionalities, we describe technical details on the efficient parallel computation of input graphs, signals, and processor parameters in GPU. Then, we show its example use under a music mixing scenario, where parameters of every differentiable processor in a large graph are optimized via gradient descent. The code is available at https://github.com/sh-lee97/grafx.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": "Accepted to DAFx 2024 demo"
    },
    {
        "paper id": "2408.03208",
        "abstract url": "https://arxiv.org/abs/2408.03208",
        "title": "Personalizing Federated Instrument Segmentation with Visual Trait Priors in Robotic Surgery",
        "rating": "-2",
        "keywords": [
            [
                "federated learning"
            ],
            [
                "surgical",
                "Surgery",
                "clinical"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Personalized federated learning (PFL) for surgical instrument segmentation (SIS) is a promising approach. It enables multiple clinical sites to collaboratively train a series of models in privacy, with each model tailored to the individual distribution of each site. Existing PFL methods rarely consider the personalization of multi-headed self-attention, and do not account for appearance diversity and instrument shape similarity, both inherent in surgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait priors for SIS, incorporating global-personalized disentanglement (GPD), appearance-regulation personalized enhancement (APE), and shape-similarity global enhancement (SGE), to boost SIS performance in each site. GPD represents the first attempt at head-wise assignment for multi-headed self-attention personalization. To preserve the unique appearance representation of each site and gradually leverage the inter-site difference, APE introduces appearance regulation and provides customized layer-wise aggregation solutions via hypernetworks for each site's personalized parameters. The mutual shape information of instruments is maintained and shared via SGE, which enhances the cross-style shape consistency on the image level and computes the shape-similarity contribution of each site on the prediction level for updating the global parameters. PFedSIS outperforms state-of-the-art methods with +1.51% Dice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding code and models will be released at https://github.com/wzjialang/PFedSIS.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.RO",
            "physics.med-ph"
        ],
        "comment": "9 pages, 3 figures, under review"
    },
    {
        "paper id": "2408.03216",
        "abstract url": "https://arxiv.org/abs/2408.03216",
        "title": "Image Quality Transfer of Diffusion MRI Guided By High-Resolution Structural MRI",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "MRI"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Prior work on the Image Quality Transfer on Diffusion MRI (dMRI) has shown significant improvement over traditional interpolation methods. However, the difficulty in obtaining ultra-high resolution Diffusion MRI scans poses a problem in training neural networks to obtain high-resolution dMRI scans. Here we hypothesise that the inclusion of structural MRI images, which can be acquired at much higher resolutions, can be used as a guide to obtaining a more accurate high-resolution dMRI output. To test our hypothesis, we have constructed a novel framework that incorporates structural MRI scans together with dMRI to obtain high-resolution dMRI scans. We set up tests which evaluate the validity of our claim through various configurations and compare the performance of our approach against a unimodal approach. Our results show that the inclusion of structural MRI scans do lead to an improvement in high-resolution image prediction when T1w data is incorporated into the model input.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03322",
        "abstract url": "https://arxiv.org/abs/2408.03322",
        "title": "Segment Anything in Medical Images and Videos: Benchmark and Deployment",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Medical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Recent advances in segmentation foundation models have enabled accurate and efficient segmentation across a wide range of natural images and videos, but their utility to medical data remains unclear. In this work, we first present a comprehensive benchmarking of the Segment Anything Model 2 (SAM2) across 11 medical image modalities and videos and point out its strengths and weaknesses by comparing it to SAM1 and MedSAM. Then, we develop a transfer learning pipeline and demonstrate SAM2 can be quickly adapted to medical domain by fine-tuning. Furthermore, we implement SAM2 as a 3D slicer plugin and Gradio API for efficient 3D image and video segmentation. The code has been made publicly available at \\url{https://github.com/bowang-lab/MedSAM}.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03409",
        "abstract url": "https://arxiv.org/abs/2408.03409",
        "title": "Electricity Market-Clearing With Extreme Events",
        "rating": "-2",
        "keywords": [
            [
                "forecast"
            ]
        ],
        "abstract": "Extreme events jeopardize power network operations, causing beyond-design failures and massive supply interruptions. Existing market designs fail to internalize and systematically assess the risk of extreme and rare events. With the increasing deployment of renewable generation, electric power systems become increasingly dependent on weather changes. Efficiently maintaining the reliability of renewable-dominant power systems during extreme weather events requires co-optimizing system resources, while differentiating between large/rare and small/frequent deviations from forecast conditions. To address this research and practice gap, we propose efficiently managing the uncertainties associated with extreme weather events through the integration of large deviation theory into chance constraints (LDT-CC). We integrate extreme event statistics into market-clearing, via including LDT-CC to model and price reserve to cope with extreme events, and use weighted chance constraints (WCC) to reduce solution conservatism. We prove that the proposed market design is capable of producing a competitive equilibrium. Numerical experiments on an illustrative system and a modified 8-zone ISO New England system demonstrate the usefulness of the proposed pricing mechanism.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03485",
        "abstract url": "https://arxiv.org/abs/2408.03485",
        "title": "Sub-Resolution mmWave FMCW Radar-based Touch Localization using Deep Learning",
        "rating": "-2",
        "keywords": [
            [
                "Radar"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "Touchscreen-based interaction on display devices are ubiquitous nowadays. However, capacitive touch screens, the core technology that enables its widespread use, are prohibitively expensive to be used in large displays because the cost increases proportionally with the screen area. In this paper, we propose a millimeter wave (mmWave) radar-based solution to achieve subresolution error performance using a network of four mmWave radar sensors. Unfortunately, achieving this is non-trivial due to inherent range resolution limitations of mmWave radars, since the target (human hand, finger etc.) is 'distributed' in space. We overcome this using a deep learning-based approach, wherein we train a deep convolutional neural network (CNN) on range-FFT (range vs power profile)-based features against ground truth (GT) positions obtained using a capacitive touch screen. To emulate the clutter characteristics encountered in radar-based positioning of human fingers, we use a metallic finger mounted on a metallic robot arm as the target. Using this setup, we demonstrate subresolution position error performance. Compared to conventional signal processing (CSP)-based approaches, we achieve a 2-3x reduction in positioning error using the CNN. Furthermore, we observe that the inference time performance and CNN model size support real-time integration of our approach on general purpose processor-based computing platforms.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "7 pages, 9 figures and 2 tables. To appear in the 100th Vehicular Technology Conference (VTC-Fall 2024)"
    },
    {
        "paper id": "2408.03520",
        "abstract url": "https://arxiv.org/abs/2408.03520",
        "title": "AirSLAM: An Efficient and Illumination-Robust Point-Line Visual SLAM System",
        "rating": "-2",
        "keywords": [
            [
                "SLAM"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "In this paper, we present an efficient visual SLAM system designed to tackle both short-term and long-term illumination challenges. Our system adopts a hybrid approach that combines deep learning techniques for feature detection and matching with traditional backend optimization methods. Specifically, we propose a unified convolutional neural network (CNN) that simultaneously extracts keypoints and structural lines. These features are then associated, matched, triangulated, and optimized in a coupled manner. Additionally, we introduce a lightweight relocalization pipeline that reuses the built map, where keypoints, lines, and a structure graph are used to match the query frame with the map. To enhance the applicability of the proposed system to real-world robots, we deploy and accelerate the feature detection and matching networks using C++ and NVIDIA TensorRT. Extensive experiments conducted on various datasets demonstrate that our system outperforms other state-of-the-art visual SLAM systems in illumination-challenging environments. Efficiency evaluations show that our system can run at a rate of 73Hz on a PC and 40Hz on an embedded platform.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "19 pages, 14 figures"
    },
    {
        "paper id": "2408.02998",
        "abstract url": "https://arxiv.org/abs/2408.02998",
        "title": "Federated Learning Architectures: A Performance Evaluation with Crop Yield Prediction Application",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "IoT"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning has become an emerging technology for data analysis for IoT applications. This paper implements centralized and decentralized federated learning frameworks for crop yield prediction based on Long Short-Term Memory Network. For centralized federated learning, multiple clients and one server is considered, where the clients exchange their model updates with the server that works as the aggregator to build the global model. For the decentralized framework, a collaborative network is formed among the devices either using ring topology or using mesh topology. In this network, each device receives model updates from the neighbour devices, and performs aggregation to build the upgraded model. The performance of the centralized and decentralized federated learning frameworks are evaluated in terms of prediction accuracy, precision, recall, F1-Score, and training time. The experimental results present that $\\geq$97% and $>$97.5% prediction accuracy are achieved using the centralized and decentralized federated learning-based frameworks respectively. The results also show that the using centralized federated learning the response time can be reduced by $\\sim$75% than the cloud-only framework. Finally, the future research directions of the use of federated learning in crop yield prediction are explored in this paper.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03353",
        "abstract url": "https://arxiv.org/abs/2408.03353",
        "title": "Adversarial Domain Adaptation for Cross-user Activity Recognition Using Diffusion-based Noise-centred Learning",
        "rating": "-2.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Human Activity Recognition (HAR) plays a crucial role in various applications such as human-computer interaction and healthcare monitoring. However, challenges persist in HAR models due to the data distribution differences between training and real-world data distributions, particularly evident in cross-user scenarios. This paper introduces a novel framework, termed Diffusion-based Noise-centered Adversarial Learning Domain Adaptation (Diff-Noise-Adv-DA), designed to address these challenges by leveraging generative diffusion modeling and adversarial learning techniques. Traditional HAR models often struggle with the diversity of user behaviors and sensor data distributions. Diff-Noise-Adv-DA innovatively integrates the inherent noise within diffusion models, harnessing its latent information to enhance domain adaptation. Specifically, the framework transforms noise into a critical carrier of activity and domain class information, facilitating robust classification across different user domains. Experimental evaluations demonstrate the effectiveness of Diff-Noise-Adv-DA in improving HAR model performance across different users, surpassing traditional domain adaptation methods. The framework not only mitigates distribution mismatches but also enhances data quality through noise-based denoising techniques.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03358",
        "abstract url": "https://arxiv.org/abs/2408.03358",
        "title": "MLC-GCN: Multi-Level Generated Connectome Based GCN for AD Analysis",
        "rating": "-2.5",
        "keywords": [
            [
                "GNN",
                "graph"
            ],
            [
                "fMRI",
                "Disease",
                "clinical"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Alzheimer's Disease (AD) is a currently incurable neurodegeneartive disease. Accurately detecting AD, especially in the early stage, represents a high research priority. AD is characterized by progressive cognitive impairments that are related to alterations in brain functional connectivity (FC). Based on this association, many studies have been published over the decades using FC and machine learning to differentiate AD from healthy aging. The most recent development in this detection method highlights the use of graph neural network (GNN) as the brain functionality analysis. In this paper, we proposed a stack of spatio-temporal feature extraction and graph generation based AD classification model using resting state fMRI. The proposed multi-level generated connectome (MLC) based graph convolutional network (GCN) (MLC-GCN) contains a multi-graph generation block and a GCN prediction block. The multi-graph generation block consists of a hierarchy of spatio-temporal feature extraction layers for extracting spatio-temporal rsfMRI features at different depths and building the corresponding connectomes. The GCN prediction block takes the learned multi-level connectomes to build and optimize GCNs at each level and concatenates the learned graphical features as the final predicting features for AD classification. Through independent cohort validations, MLC-GCN shows better performance for differentiating MCI, AD, and normal aging than state-of-art GCN and rsfMRI based AD classifiers. The proposed MLC-GCN also showed high explainability in terms of learning clinically reasonable connectome node and connectivity features from two independent datasets. While we only tested MLC-GCN on AD, the basic rsfMRI-based multi-level learned GCN based outcome prediction strategy is valid for other diseases or clinical outcomes.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03397",
        "abstract url": "https://arxiv.org/abs/2408.03397",
        "title": "HeTraX: Energy Efficient 3D Heterogeneous Manycore Architecture for Transformer Acceleration",
        "rating": "-2.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "thermal"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Transformers have revolutionized deep learning and generative modeling to enable unprecedented advancements in natural language processing tasks and beyond. However, designing hardware accelerators for executing transformer models is challenging due to the wide variety of computing kernels involved in the transformer architecture. Existing accelerators are either inadequate to accelerate end-to-end transformer models or suffer notable thermal limitations. In this paper, we propose the design of a three-dimensional heterogeneous architecture referred to as HeTraX specifically optimized to accelerate end-to-end transformer models. HeTraX employs hardware resources aligned with the computational kernels of transformers and optimizes both performance and energy. Experimental results show that HeTraX outperforms existing state-of-the-art by up to 5.6x in speedup and improves EDP by 14.5x while ensuring thermally feasibility.",
        "subjects": [
            "cs.AR",
            "cs.LG"
        ],
        "comment": "Presented at ACM/IEEE International Symposium on Low Power Electronics and Design (ISLPED-24)"
    },
    {
        "paper id": "2408.03475",
        "abstract url": "https://arxiv.org/abs/2408.03475",
        "title": "Can LLMs Serve As Time Series Anomaly Detectors?",
        "rating": "-2.5",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "An emerging topic in large language models (LLMs) is their application to time series forecasting, characterizing mainstream and patternable characteristics of time series. A relevant but rarely explored and more challenging question is whether LLMs can detect and explain time series anomalies, a critical task across various real-world applications. In this paper, we investigate the capabilities of LLMs, specifically GPT-4 and LLaMA3, in detecting and explaining anomalies in time series. Our studies reveal that: 1) LLMs cannot be directly used for time series anomaly detection. 2) By designing prompt strategies such as in-context learning and chain-of-thought prompting, GPT-4 can detect time series anomalies with results competitive to baseline methods. 3) We propose a synthesized dataset to automatically generate time series anomalies with corresponding explanations. By applying instruction fine-tuning on this dataset, LLaMA3 demonstrates improved performance in time series anomaly detection tasks. In summary, our exploration shows the promising potential of LLMs as time series anomaly detectors.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03005",
        "abstract url": "https://arxiv.org/abs/2408.03005",
        "title": "Automatic String Data Validation with Pattern Discovery",
        "rating": "-3",
        "keywords": [
            [
                "skeleton"
            ],
            [
                "industrial"
            ]
        ],
        "abstract": "In enterprise data pipelines, data insertions occur periodically and may impact downstream services if data quality issues are not addressed. Typically, such problems can be investigated and fixed by on-call engineers, but locating the cause of such problems and fixing errors are often time-consuming. Therefore, automatic data validation is a better solution to defend the system and downstream services by enabling early detection of errors and providing detailed error messages for quick resolution. This paper proposes a self-validate data management system with automatic pattern discovery techniques to verify the correctness of semi-structural string data in enterprise data pipelines. Our solution extracts patterns from historical data and detects erroneous incoming data in a top-down fashion. High-level information of historical data is analyzed to discover the format skeleton of correct values. Fine-grained semantic patterns are then extracted to strike a balance between generalization and specification of the discovered pattern, thus covering as many correct values as possible while avoiding over-fitting. To tackle cold start and rapid data growth, we propose an incremental update strategy and example generalization strategy. Experiments on large-scale industrial and public datasets demonstrate the effectiveness and efficiency of our method compared to alternative solutions. Furthermore, a case study on an industrial platform (Ant Group Inc.) with thousands of applications shows that our system captures meaningful data patterns in daily operations and helps engineers quickly identify errors.",
        "subjects": [
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03017",
        "abstract url": "https://arxiv.org/abs/2408.03017",
        "title": "Closed-Loop Magnetic Control of Medical Soft Continuum Robots for Deflection",
        "rating": "-3",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "Medical",
                "surgery"
            ]
        ],
        "abstract": "Magnetic soft continuum robots (MSCRs) have emerged as powerful devices in endovascular interventions owing to their hyperelastic fibre matrix and enhanced magnetic manipulability. Effective closed-loop control of tethered magnetic devices contributes to the achievement of autonomous vascular robotic surgery. In this article, we employ a magnetic actuation system equipped with a single rotatable permanent magnet to achieve closed-loop deflection control of the MSCR. To this end, we establish a differential kinematic model of MSCRs exposed to non-uniform magnetic fields. The relationship between the existence and uniqueness of Jacobian and the geometric position between robots is deduced. The accurate control direction induced by Jacobian is demonstrated to be crucial in simulations. Then, the corresponding quasi-static control (QSC) framework integrates a linear extended state observer to estimate model uncertainties. Finally, the effectiveness of the proposed QSC framework is validated through comparative trajectory tracking experiments with the PD controller under external disturbances. The proposed control framework effectively prevents the actuator from reaching the joint limit and achieves fast and low error-tracking performance without overshooting.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03028",
        "abstract url": "https://arxiv.org/abs/2408.03028",
        "title": "Investigating the Jamming Attack on 5G NR Physical Channels",
        "rating": "-3",
        "keywords": [
            [
                "Attack"
            ],
            [
                "5G"
            ]
        ],
        "abstract": "This study investigates the jamming attack on the orthogonal frequency-division multiplexing (OFDM) based physical channels in 5G new radio (NR) technology from the aspect of signal processing. Disrupting the orthogonality property between subcarriers (SCs) is considered as one of the jammers targets in OFDM based generations. Focusing on the orthogonality property, we propose a method to detect the attacked subcarriers, and then neutralize the jamming attack using a multiplicative signal. Thanks to studying the frequency aspect of the attacked signal, the proposed method is independent of the jammers transmitted power. Simulation results evaluate the detection performance of the proposed method with various numbers of OFDM subcarriers.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03035",
        "abstract url": "https://arxiv.org/abs/2408.03035",
        "title": "Training-Free Condition Video Diffusion Models for single frame Spatial-Semantic Echocardiogram Synthesis",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "medical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Conditional video diffusion models (CDM) have shown promising results for video synthesis, potentially enabling the generation of realistic echocardiograms to address the problem of data scarcity. However, current CDMs require a paired segmentation map and echocardiogram dataset. We present a new method called Free-Echo for generating realistic echocardiograms from a single end-diastolic segmentation map without additional training data. Our method is based on the 3D-Unet with Temporal Attention Layers model and is conditioned on the segmentation map using a training-free conditioning method based on SDEdit. We evaluate our model on two public echocardiogram datasets, CAMUS and EchoNet-Dynamic. We show that our model can generate plausible echocardiograms that are spatially aligned with the input segmentation map, achieving performance comparable to training-based CDMs. Our work opens up new possibilities for generating echocardiograms from a single segmentation map, which can be used for data augmentation, domain adaptation, and other applications in medical imaging. Our code is available at \\url{https://github.com/gungui98/echo-free}",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "Accepted to MICCAI 2024"
    },
    {
        "paper id": "2408.03078",
        "abstract url": "https://arxiv.org/abs/2408.03078",
        "title": "BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical Applications",
        "rating": "-3",
        "keywords": [
            [
                "3D",
                "Depth"
            ],
            [
                "SLAM"
            ],
            [
                "Surgical",
                "surgery",
                "Endoscopic"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Endoscopic surgery relies on two-dimensional views, posing challenges for surgeons in depth perception and instrument manipulation. While Simultaneous Localization and Mapping (SLAM) has emerged as a promising solution to address these limitations, its implementation in endoscopic procedures presents significant challenges due to hardware limitations, such as the use of a monocular camera and the absence of odometry sensors. This study presents a robust deep learning-based SLAM approach that combines state-of-the-art and newly developed models. It consists of three main parts: the Monocular Pose Estimation Module that introduces a novel unsupervised method based on the CycleGAN architecture, the Monocular Depth Estimation Module that leverages the novel Zoe architecture, and the 3D Reconstruction Module which uses information from the previous models to create a coherent surgical map. The performance of the procedure was rigorously evaluated using three publicly available datasets (Hamlyn, EndoSLAM, and SCARED) and benchmarked against two state-of-the-art methods, EndoSFMLearner and EndoDepth. The integration of Zoe in the MDEM demonstrated superior performance compared to state-of-the-art depth estimation algorithms in endoscopy, whereas the novel approach in the MPEM exhibited competitive performance and the lowest inference time. The results showcase the robustness of our approach in laparoscopy, gastroscopy, and colonoscopy, three different scenarios in endoscopic surgery. The proposed SLAM approach has the potential to improve the accuracy and efficiency of endoscopic procedures by providing surgeons with enhanced depth perception and 3D reconstruction capabilities.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "comment": "15 pages, 7 figures"
    },
    {
        "paper id": "2408.03121",
        "abstract url": "https://arxiv.org/abs/2408.03121",
        "title": "Flexible Type-Based Resource Estimation in Quantum Circuit Description Languages",
        "rating": "-3",
        "keywords": [
            [
                "depth"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "We introduce a type system for the Quipper language designed to derive upper bounds on the size of the circuits produced by the typed program. This size can be measured according to various metrics, including width, depth and gate count, but also variations thereof obtained by considering only some wire types or some gate kinds. The key ingredients for achieving this level of flexibility are effects and refinement types, both relying on indices, that is, generic arithmetic expressions whose operators are interpreted differently depending on the target metric. The approach is shown to be correct through logical predicates, under reasonable assumptions about the chosen resource metric. This approach is empirically evaluated through the QuRA tool, showing that, in many cases, inferring tight bounds is possible in a fully automatic way.",
        "subjects": [
            "cs.PL",
            "cs.CC",
            "cs.LO",
            "quant-ph"
        ],
        "comment": "26 pages, 13 figures"
    },
    {
        "paper id": "2408.03166",
        "abstract url": "https://arxiv.org/abs/2408.03166",
        "title": "CADRL: Category-aware Dual-agent Reinforcement Learning for Explainable Recommendations over Knowledge Graphs",
        "rating": "-3",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "recommendation"
            ]
        ],
        "abstract": "Knowledge graphs (KGs) have been widely adopted to mitigate data sparsity and address cold-start issues in recommender systems. While existing KGs-based recommendation methods can predict user preferences and demands, they fall short in generating explicit recommendation paths and lack explainability. As a step beyond the above methods, recent advancements utilize reinforcement learning (RL) to find suitable items for a given user via explainable recommendation paths. However, the performance of these solutions is still limited by the following two points. (1) Lack of ability to capture contextual dependencies from neighboring information. (2) The excessive reliance on short recommendation paths due to efficiency concerns. To surmount these challenges, we propose a category-aware dual-agent reinforcement learning (CADRL) model for explainable recommendations over KGs. Specifically, our model comprises two components: (1) a category-aware gated graph neural network that jointly captures context-aware item representations from neighboring entities and categories, and (2) a dual-agent RL framework where two agents efficiently traverse long paths to search for suitable items. Finally, experimental results show that CADRL outperforms state-of-the-art models in terms of both effectiveness and efficiency on large-scale datasets.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03249",
        "abstract url": "https://arxiv.org/abs/2408.03249",
        "title": "Multi-User Mobile Augmented Reality for Cardiovascular Surgical Planning",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "Surgical"
            ]
        ],
        "abstract": "Collaborative planning for congenital heart diseases typically involves creating physical heart models through 3D printing, which are then examined by both surgeons and cardiologists. Recent developments in mobile augmented reality (AR) technologies have presented a viable alternative, known for their ease of use and portability. However, there is still a lack of research examining the utilization of multi-user mobile AR environments to support collaborative planning for cardiovascular surgeries. We created ARCollab, an iOS AR app designed for enabling multiple surgeons and cardiologists to interact with a patient's 3D heart model in a shared environment. ARCollab enables surgeons and cardiologists to import heart models, manipulate them through gestures and collaborate with other users, eliminating the need for fabricating physical heart models. Our evaluation of ARCollab's usability and usefulness in enhancing collaboration, conducted with three cardiothoracic surgeons and two cardiologists, marks the first human evaluation of a multi-user mobile AR tool for surgical planning. ARCollab is open-source, available at https://github.com/poloclub/arcollab.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03393",
        "abstract url": "https://arxiv.org/abs/2408.03393",
        "title": "Biomedical Image Segmentation: A Systematic Literature Review of Deep Learning Based Object Detection Methods",
        "rating": "-3",
        "keywords": [
            [
                "Biomedical",
                "diagnosis"
            ],
            [
                "tabular"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Biomedical image segmentation plays a vital role in diagnosis of diseases across various organs. Deep learning-based object detection methods are commonly used for such segmentation. There exists an extensive research in this topic. However, there is no standard review on this topic. Existing surveys often lack a standardized approach or focus on broader segmentation techniques. In this paper, we conducted a systematic literature review (SLR), collected and analysed 148 articles that explore deep learning object detection methods for biomedical image segmentation. We critically analyzed these methods, identified the key challenges, and discussed the future directions. From the selected articles we extracted the results including the deep learning models, targeted imaging modalities, targeted diseases, and the metrics for the analysis of the methods. The results have been presented in tabular and/or charted forms. The results are presented in three major categories including two stage detection models, one stage detection models and point-based detection models. Each article is individually analyzed along with its pros and cons. Finally, we discuss open challenges, potential benefits, and future research directions. This SLR aims to provide the research community with a quick yet deeper understanding of these segmentation models, ultimately facilitating the development of more powerful solutions for biomedical image analysis.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03429",
        "abstract url": "https://arxiv.org/abs/2408.03429",
        "title": "MarQSim: Reconciling Determinism and Randomness in Compiler Optimization for Quantum Simulation",
        "rating": "-3",
        "keywords": [
            [
                "Graph"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum simulation, fundamental in quantum algorithm design, extends far beyond its foundational roots, powering diverse quantum computing applications. However, optimizing the compilation of quantum Hamiltonian simulation poses significant challenges. Existing approaches fall short in reconciling deterministic and randomized compilation, lack appropriate intermediate representations, and struggle to guarantee correctness. Addressing these challenges, we present MarQSim, a novel compilation framework. MarQSim leverages a Markov chain-based approach, encapsulated in the Hamiltonian Term Transition Graph, adeptly reconciling deterministic and randomized compilation benefits. We rigorously prove its algorithmic efficiency and correctness criteria. Furthermore, we formulate a Min-Cost Flow model that can tune transition matrices to enforce correctness while accommodating various optimization objectives. Experimental results demonstrate MarQSim's superiority in generating more efficient quantum circuits for simulating various quantum Hamiltonians while maintaining precision.",
        "subjects": [
            "quant-ph",
            "cs.ET"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03498",
        "abstract url": "https://arxiv.org/abs/2408.03498",
        "title": "Grasp Failure Constraints for Fast and Reliable Pick-and-Place Using Multi-Suction-Cup Grippers",
        "rating": "-3",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "industrial"
            ]
        ],
        "abstract": "Multi-suction-cup grippers are frequently employed to perform pick-and-place robotic tasks, especially in industrial settings where grasping a wide range of light to heavy objects in limited amounts of time is a common requirement. However, most existing works focus on using one or two suction cups to grasp only irregularly shaped but light objects. There is a lack of research on robust manipulation of heavy objects using larger arrays of suction cups, which introduces challenges in modeling and predicting grasp failure. This paper presents a general approach to modeling grasp strength in multi-suction-cup grippers, introducing new constraints usable for trajectory planning and optimization to achieve fast and reliable pick-and-place maneuvers. The primary modeling challenge is the accurate prediction of the distribution of loads at each suction cup while grasping objects. To solve for this load distribution, we find minimum spring potential energy configurations through a simple quadratic program. This results in a computationally efficient analytical solution that can be integrated to formulate grasp failure constraints in time-optimal trajectory planning. Finally, we present experimental results to validate the efficiency and accuracy of the proposed model.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03100",
        "abstract url": "https://arxiv.org/abs/2408.03100",
        "title": "Huge Ensembles Part I: Design of Ensemble Weather Forecasts using Spherical Fourier Neural Operators",
        "rating": "-3.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Studying low-likelihood high-impact extreme weather events in a warming world is a significant and challenging task for current ensemble forecasting systems. While these systems presently use up to 100 members, larger ensembles could enrich the sampling of internal variability. They may capture the long tails associated with climate hazards better than traditional ensemble sizes. Due to computational constraints, it is infeasible to generate huge ensembles (comprised of 1,000-10,000 members) with traditional, physics-based numerical models. In this two-part paper, we replace traditional numerical simulations with machine learning (ML) to generate hindcasts of huge ensembles. In Part I, we construct an ensemble weather forecasting system based on Spherical Fourier Neural Operators (SFNO), and we discuss important design decisions for constructing such an ensemble. The ensemble represents model uncertainty through perturbed-parameter techniques, and it represents initial condition uncertainty through bred vectors, which sample the fastest growing modes of the forecast. Using the European Centre for Medium-Range Weather Forecasts Integrated Forecasting System (IFS) as a baseline, we develop an evaluation pipeline composed of mean, spectral, and extreme diagnostics. Using large-scale, distributed SFNOs with 1.1 billion learned parameters, we achieve calibrated probabilistic forecasts. As the trajectories of the individual members diverge, the ML ensemble mean spectra degrade with lead time, consistent with physical expectations. However, the individual ensemble members' spectra stay constant with lead time. Therefore, these members simulate realistic weather states, and the ML ensemble thus passes a crucial spectral test in the literature. The IFS and ML ensembles have similar Extreme Forecast Indices, and we show that the ML extreme weather forecasts are reliable and discriminating.",
        "subjects": [
            "physics.ao-ph",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03446",
        "abstract url": "https://arxiv.org/abs/2408.03446",
        "title": "Optimizing NOMA Transmissions to Advance Federated Learning in Vehicular Networks",
        "rating": "-4",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "Federated Learning"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "Diverse critical data, such as location information and driving patterns, can be collected by IoT devices in vehicular networks to improve driving experiences and road safety. However, drivers are often reluctant to share their data due to privacy concerns. The Federated Vehicular Network (FVN) is a promising technology that tackles these concerns by transmitting model parameters instead of raw data, thereby protecting the privacy of drivers. Nevertheless, the performance of Federated Learning (FL) in a vehicular network depends on the joining ratio, which is restricted by the limited available wireless resources. To address these challenges, this paper proposes to apply Non-Orthogonal Multiple Access (NOMA) to improve the joining ratio in a FVN. Specifically, a vehicle selection and transmission power control algorithm is developed to exploit the power domain differences in the received signal to ensure the maximum number of vehicles capable of joining the FVN. Our simulation results demonstrate that the proposed NOMA-based strategy increases the joining ratio and significantly enhances the performance of the FVN.",
        "subjects": [
            "cs.NI",
            "eess.SP"
        ],
        "comment": "The paper is accepted by IEEE Globecom 2024"
    },
    {
        "paper id": "2408.03059",
        "abstract url": "https://arxiv.org/abs/2408.03059",
        "title": "Learning to Turn: Diffusion Imitation for Robust Row Turning in Under-Canopy Robots",
        "rating": "-5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "vehicle"
            ],
            [
                "navigation"
            ],
            [
                "agricultural"
            ]
        ],
        "abstract": "Under-canopy agricultural robots require robust navigation capabilities to enable full autonomy but struggle with tight row turning between crop rows due to degraded GPS reception, visual aliasing, occlusion, and complex vehicle dynamics. We propose an imitation learning approach using diffusion policies to learn row turning behaviors from demonstrations provided by human operators or privileged controllers. Simulation experiments in a corn field environment show potential in learning this task with only visual observations and velocity states. However, challenges remain in maintaining control within rows and handling varied initial conditions, highlighting areas for future improvement.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted as Extended Abstract to the IEEE ICRA@40 2024"
    },
    {
        "paper id": "2408.02982",
        "abstract url": "https://arxiv.org/abs/2408.02982",
        "title": "Practical Design of Probabilistic Constellation Shaping for Physical Layer Security in Visible Light Communications",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper studies a practical design of probabilistic constellation shaping (PCS) for physical layer security in visible light communications (VLC). In particular, we consider a wiretap VLC channel employing a probabilistically shaped $M$-ary pulse amplitude modulation (PAM) constellation. Considering the requirements for reliability of the legitimate user's channel, flickering-free transmission, and symmetric constellation distribution, the optimal constellation distributions to maximize modulation-constrained secrecy capacity or the bit error rate (BER) of eavesdropper's channel are investigated for both scenarios of known and unknown eavesdropper's channel state information (CSI). To formulate the constraint on the channel reliability, tractable closed-form expressions for the upper bound and approximate BER of $M$-ary PAM under an arbitrary symbol probability are derived. The design problem is shown to be non-convex due to the non-convex BER constraint. By proving that the upper bound BER is a concave function of the constellation distribution, a suboptimal solution based on the convex-concave procedure (CCCP) is presented. Our findings reveal that while the uniform signaling can only satisfy the BER constraint when the optical power is beyond a certain value, the proposed PCS design works in the entire region of the optical power. Some insights into the optimal constellation distribution with respect to the emitted optical power are also discussed.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2408.02990",
        "abstract url": "https://arxiv.org/abs/2408.02990",
        "title": "Joint Design of Probabilistic Constellation Shaping and Precoding for Multi-user VLC Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper proposes a joint design of probabilistic constellation shaping (PCS) and precoding to enhance the sum-rate performance of multi-user visible light communications (VLC) broadcast channels subject to signal amplitude constraint. In the proposed design, the transmission probabilities of bipolar $M$-pulse amplitude modulation ($M$-PAM) symbols for each user and the transmit precoding matrix are jointly optimized to improve the sum-rate performance. The joint design problem is shown to be a complex non-convex problem due to the non-convexity of the objective function. To tackle the problem, the firefly algorithm (FA), a nature-inspired heuristic optimization approach, is employed to solve a local optima to the original non-convex optimization problem. The FA-based approach, however, suffers from high computational complexity. Therefore, we propose a low-complexity design based on zero-forcing (ZF) precoding, which is solved using an alternating optimization (AO) approach. Simulation results reveal that the proposed joint design with PCS significantly improves the sum-rate performance compared to the conventional design with uniform signaling. Some insights into the optimal symbol distributions of the two joint design approaches are also provided.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03007",
        "abstract url": "https://arxiv.org/abs/2408.03007",
        "title": "Congestion or No Congestion: Packet Loss Identification and Prediction Using Machine Learning",
        "rating": "-10",
        "keywords": [],
        "abstract": "Packet losses in the network significantly impact network performance. Most TCP variants reduce the transmission rate when detecting packet losses, assuming network congestion, resulting in lower throughput and affecting bandwidth-intensive applications like immersive applications. However, not all packet losses are due to congestion; some occur due to wireless link issues, which we refer to as non-congestive packet losses. In today's hybrid Internet, packets of a single flow may traverse wired and wireless segments of a network to reach their destination. TCP should not react to non-congestive packet losses the same way as it does to congestive losses. However, TCP currently can not differentiate between these types of packet losses and lowers its transmission rate irrespective of packet loss type, resulting in lower throughput for wireless clients. To address this challenge, we use machine learning techniques to distinguish between these types of packet losses at end hosts, utilizing easily available features at the host. Our results demonstrate that Random Forest and K-Nearest Neighbor classifiers perform better in predicting the type of packet loss, offering a promising solution to enhance network performance.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "to be published in IEEE PlatCon-2024"
    },
    {
        "paper id": "2408.03008",
        "abstract url": "https://arxiv.org/abs/2408.03008",
        "title": "Faster and simpler online/sliding rightmost Lempel-Ziv factorizations",
        "rating": "-10",
        "keywords": [],
        "abstract": "We tackle the problems of computing the rightmost variant of the Lempel-Ziv factorizations in the online/sliding model. Previous best bounds for this problem are O(n log n) time with O(n) space, due to Amir et al. [IPL 2002] for the online model, and due to Larsson [CPM 2014] for the sliding model. In this paper, we present faster O(n log n/log log n)-time solutions to both of the online/sliding models. Our algorithms are built on a simple data structure named BP-linked trees, and on a slightly improved version of the range minimum/maximum query (RmQ/RMQ) data structure on a dynamic list of integers. We also present other applications of our algorithms.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03021",
        "abstract url": "https://arxiv.org/abs/2408.03021",
        "title": "The State of FaaS: An analysis of public Functions-as-a-Service providers",
        "rating": "-10",
        "keywords": [],
        "abstract": "Serverless computing is a growing and maturing field that is the focus of much research, industry interest and adoption. Previous works exploring Functions-as-a-Service providers have focused primarily on the most well known providers AWS Lambda, Google Cloud Functions and Microsoft Azure Functions without exploring other providers in similar detail. In this work, we conduct the first detailed review of ten currently publicly available FaaS platforms exploring everything from their history, to their features and pricing to where they sit within the overall public FaaS landscape, before making a number of observations as to the state of the FaaS.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "This paper has been accepted to the 17th IEEE International Conference on Cloud Computing (CLOUD). This paper contains 10 pages and 2 figures"
    },
    {
        "paper id": "2408.03037",
        "abstract url": "https://arxiv.org/abs/2408.03037",
        "title": "Causal Vector-valued Witsenhausen Counterexamples with Feedback",
        "rating": "-10",
        "keywords": [],
        "abstract": "We study the continuous vector-valued Witsenhausen counterexample through the lens of empirical coordination coding. We characterize the region of achievable pairs of costs in three scenarios: (i) causal encoding and causal decoding, (ii) causal encoding and causal decoding with channel feedback, and (iii) causal encoding and noncausal decoding with channel feedback. In these vector-valued versions of the problem, the optimal coding schemes must rely on a time-sharing strategy, since the region of achievable pairs of costs might not be convex in the scalar version of the problem. We examine the role of the channel feedback when the encoder is causal and the decoder is either causal or non-causal, and we show that feedback improves the performance, only when the decoder is non-causal.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "Accepted by IEEE Information Theory Workshop (ITW) 2024"
    },
    {
        "paper id": "2408.03063",
        "abstract url": "https://arxiv.org/abs/2408.03063",
        "title": "Social Behavior as a Key to Learning-based Multi-Agent Pathfinding Dilemmas",
        "rating": "-10",
        "keywords": [],
        "abstract": "The Multi-agent Path Finding (MAPF) problem involves finding collision-free paths for a team of agents in a known, static environment, with important applications in warehouse automation, logistics, or last-mile delivery. To meet the needs of these large-scale applications, current learning-based methods often deploy the same fully trained, decentralized network to all agents to improve scalability. However, such parameter sharing typically results in homogeneous behaviors among agents, which may prevent agents from breaking ties around symmetric conflict (e.g., bottlenecks) and might lead to live-/deadlocks. In this paper, we propose SYLPH, a novel learning-based MAPF framework aimed to mitigate the adverse effects of homogeneity by allowing agents to learn and dynamically select different social behaviors (akin to individual, dynamic roles), without affecting the scalability offered by parameter sharing. Specifically, SYLPH agents learn to select their Social Value Orientation (SVO) given the situation at hand, quantifying their own level of selfishness/altruism, as well as an SVO-conditioned MAPF policy dictating their movement actions. To these ends, each agent first determines the most influential other agent in the system by predicting future conflicts/interactions with other agents. Each agent selects its own SVO towards that agent, and trains its decentralized MAPF policy to enact this SVO until another agent becomes more influential. To further allow agents to consider each others' social preferences, each agent gets access to the SVO value of their neighbors. As a result of this hierarchical decision-making and exchange of social preferences, SYLPH endows agents with the ability to reason about the MAPF task through more latent spaces and nuanced contexts, leading to varied responses that can help break ties around symmetric conflicts. [...]",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Submitted to Springer's Artificial Intelligence Journal"
    },
    {
        "paper id": "2408.03077",
        "abstract url": "https://arxiv.org/abs/2408.03077",
        "title": "Model-free optimal controller for discrete-time Markovian jump linear systems: A Q-learning approach",
        "rating": "-10",
        "keywords": [],
        "abstract": "This research paper introduces a model-free optimal controller for discrete-time Markovian jump linear systems (MJLSs), employing principles from the methodology of reinforcement learning (RL). While Q-learning methods have demonstrated efficacy in determining optimal controller gains for deterministic systems, their application to systems with Markovian switching remains unexplored. To address this research gap, we propose a Q-function involving the Markovian mode. Subsequently, a Q-learning algorithm is proposed to learn the unknown kernel matrix using raw input-state information from the system. Notably, the study proves the convergence of the proposed Q-learning optimal controller gains to the model-based optimal controller gains after proving the convergence of a value iteration algorithm as the first step. Addition of excitation noise to input which is required to ensure the leaning performance does not lead to any bias. Unlike the conventional optimal controller, the proposed method does not require any knowledge on system dynamics and eliminates the need for solving coupled algebraic Riccati equations arising in optimal control of MJLSs. Finally, the efficiency of the proposed method is demonstrated through a simulation study.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03095",
        "abstract url": "https://arxiv.org/abs/2408.03095",
        "title": "TestART: Improving LLM-based Unit Test via Co-evolution of Automated Generation and Repair Iteration",
        "rating": "-10",
        "keywords": [],
        "abstract": "Unit test is crucial for detecting bugs in individual program units but consumes time and effort. The existing automated unit test generation methods are mainly based on search-based software testing (SBST) and language models to liberate developers. Recently, large language models (LLMs) have demonstrated remarkable reasoning and generation capabilities. However, several problems limit their ability to generate high-quality test cases: (1) LLMs may generate invalid test cases under insufficient context, resulting in compilation errors; (2) Lack of test and coverage feedback information may cause runtime errors and low coverage rates. (3) The repetitive suppression problem causes LLMs to get stuck into the repetition loop of self-repair or re-generation attempts. In this paper, we propose TestART, a novel unit test generation method that leverages the strengths of LLMs while overcoming the limitations mentioned. TestART improves LLM-based unit test via co-evolution of automated generation and repair iteration. TestART leverages the template-based repair technique to fix bugs in LLM-generated test cases, using prompt injection to guide the next-step automated generation and avoid repetition suppression. Furthermore, TestART extracts coverage information from the passed test cases and utilizes it as testing feedback to enhance the sufficiency of the final test case. This synergy between generation and repair elevates the quality, effectiveness, and readability of the produced test cases significantly beyond previous methods. In comparative experiments, the pass rate of TestART-generated test cases is 78.55%, which is approximately 18% higher than both the ChatGPT-4.0 model and the same ChatGPT-3.5-based method ChatUniTest. It also achieves an impressive line coverage rate of 90.96% on the focal methods that passed the test, exceeding EvoSuite by 3.4%.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03113",
        "abstract url": "https://arxiv.org/abs/2408.03113",
        "title": "Codes Correcting Two Bursts of Exactly $b$ Deletions",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we explore constructions for codes that correct two bursts of deletions, with each burst having length exactly $b$. Previously, the best known construction, derived using the syndrome compression technique, achieved a redundancy of at most $7\\log n+O\\left(\\log n/\\log\\log n\\right)$. In this work, we present new constructions for all $q\\ge 2$ that achieve redundancy at most $7\\log n+O(\\log\\log n)$ when $b>1$. Additionally, for $b=1$, we provide a new construction of $q$-ary two-deletion correcting codes with redundancy $5\\log n+O(\\log\\log n)$ for all $q>2$.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03116",
        "abstract url": "https://arxiv.org/abs/2408.03116",
        "title": "Centralization in Attester-Proposer Separation",
        "rating": "-10",
        "keywords": [],
        "abstract": "We show that Execution Tickets and Execution Auctions dramatically increase centralization in the market for block proposals, even without multi-block MEV concerns. Previous analyses have insufficiently or incorrectly modeled the interaction between ahead-of-time auctions and just-in-time (JIT) auctions. We study a model where bidders compete in an execution auction ahead of time, and then the winner holds a JIT auction to resell the proposal rights when the slot arrives. During the execution auction, bidders only know the distribution of their valuations. Bidders then draw values from their distributions and compete in the JIT auction. We show that a bidder who wins the execution auction is substantially advantaged in the JIT auction since they can set a reserve price higher than their own realized value for the slot to increase their revenue. As a result, there is a strong centralizing force in the execution auction, which allows the ex-ante strongest bidder to win the execution auction every time, and similarly gives them the strongest incentive to buy up all the tickets. Similar results trivially apply if the resale market is imperfect, since that only reinforces the advantages of the ex-ante strong buyer. To reiterate, these results do not require the bidders to employ multi-block MEV strategies, although if they did, it would likely amplify the centralizing effects.",
        "subjects": [
            "econ.TH",
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03170",
        "abstract url": "https://arxiv.org/abs/2408.03170",
        "title": "typedKanren: Statically Typed Relational Programming with Exhaustive Matching in Haskell",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present a statically typed embedding of relational programming (specifically a dialect of miniKanren with disequality constraints) in Haskell. Apart from handling types, our dialect extends standard relational combinator repertoire with a variation of relational matching that supports static exhaustiveness checks. To hide the boilerplate definitions and support comfortable logic programming with user-defined data types we use generic programming via GHC.Generics as well as metaprogramming via Template Haskell. We demonstrate our dialect on several examples and compare its performance against some other known implementations of miniKanren.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03188",
        "abstract url": "https://arxiv.org/abs/2408.03188",
        "title": "DaVE -- A Curated Database of Visualization Examples",
        "rating": "-10",
        "keywords": [],
        "abstract": "Visualization, from simple line plots to complex high-dimensional visual analysis systems, has established itself throughout numerous domains to explore, analyze, and evaluate data. Applying such visualizations in the context of simulation science where High-Performance Computing (HPC) produces ever-growing amounts of data that is more complex, potentially multidimensional, and multimodal, takes up resources and a high level of technological experience often not available to domain experts. In this work, we present DaVE -- a curated database of visualization examples, which aims to provide state-of-the-art and advanced visualization methods that arise in the context of HPC applications. Based on domain- or data-specific descriptors entered by the user, DaVE provides a list of appropriate visualization techniques, each accompanied by descriptions, examples, references, and resources. Sample code, adaptable container templates, and recipes for easy integration in HPC applications can be downloaded for easy access to high-fidelity visualizations. While the database is currently filled with a limited number of entries based on a broad evaluation of needs and challenges of current HPC users, DaVE is designed to be easily extended by experts from both the visualization and HPC communities.",
        "subjects": [
            "cs.DC",
            "cs.GR",
            "cs.HC"
        ],
        "comment": "4 + 1 pages, 4 figures, to be published at VIS 2024"
    },
    {
        "paper id": "2408.03221",
        "abstract url": "https://arxiv.org/abs/2408.03221",
        "title": "DRL-Assisted Dynamic QoT-Aware Service Provisioning in Multi-Band Elastic Optical Networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "We propose a DRL-assisted approach for service provisioning in multi-band elastic optical networks. Our simulation environment uses an accurate QoT estimator based on the GN/EGN model. Results show that the proposed approach reduces request blocking by 50% compared with heuristics from the literature.",
        "subjects": [
            "cs.NI",
            "eess.SP"
        ],
        "comment": "This paper has been accepted by 50th European Conference on Optical Communications (ECOC 2O24)"
    },
    {
        "paper id": "2408.03237",
        "abstract url": "https://arxiv.org/abs/2408.03237",
        "title": "MacWilliams Identities for Rank metric codes over Galois rings",
        "rating": "-10",
        "keywords": [],
        "abstract": "The purpose of this paper is to study rank metric codes over Galois rings. In particular, we prove MacWilliams identities for finite chain rings relating the sequences of $q$-binomial moments of a code and its dual and for Gabidulin analogues of free codes over Galois rings we prove the corresponding MacWilliams identity as a functional expression relating the weight enumerator of the dual with the MacWilliams transform of the weight enumerator of the original code plus one extra term attached to the zero divisors.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03244",
        "abstract url": "https://arxiv.org/abs/2408.03244",
        "title": "Modular assurance of an Autonomous Ferry using Contract-Based Design and Simulation-based Verification Principles",
        "rating": "-10",
        "keywords": [],
        "abstract": "With the introduction of autonomous technology into our society, e.g. autonomous shipping, it is important to assess and assure the safety of autonomous systems in a real-world context. Simulation-based testing is a common approach to attempt to verify performance of autonomous systems, but assurance also requires formal evidence. This paper introduces the Assurance of Digital Assets (ADA) framework, a structured method for the assurance of digital assets, i.e. novel, complex, or intelligent systems enabled by digital technologies, using contract-based design. Results are shown for an autonomous ferry assurance case, focusing on collision avoidance during the ferry's transit. Further, we discuss the role of simulation-based testing in verifying compliance to contract specifications, to build the necessary evidence for an assurance case.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "12 pages, 3 figures, final draft submitted to ICMASS/MTEC 2024 conference"
    },
    {
        "paper id": "2408.03260",
        "abstract url": "https://arxiv.org/abs/2408.03260",
        "title": "Employing Vector Field Techniques on the Analysis of Memristor Cellular Nonlinear Networks Cell Dynamics",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper introduces an innovative graphical analysis tool for investigating the dynamics of Memristor Cellular Nonlinear Networks (M-CNNs) featuring 2nd-order processing elements, known as M-CNN cells. In the era of specialized hardware catering to the demands of intelligent autonomous systems, the integration of memristors within Cellular Nonlinear Networks (CNNs) has emerged as a promising paradigm due to their exceptional characteristics. However, the standard Dynamic Route Map (DRM) analysis, applicable to 1st-order systems, fails to address the intricacies of 2nd-order M-CNN cell dynamics, as well the 2nd-order DRM (DRM2) exhibits limitations on the graphical illustration of local dynamical properties of the M-CNN cells, e.g. state derivative's magnitude. To address this limitation, we propose a novel integration of M-CNN cell vector field into the cell's phase portrait, enhancing the analysis efficacy and enabling efficient M-CNN cell design. A comprehensive exploration of M-CNN cell dynamics is presented, showcasing the utility of the proposed graphical tool for various scenarios, including bistable and monostable behavior, and demonstrating its superior ability to reveal subtle variations in cell behavior. Through this work, we offer a refined perspective on the analysis and design of M-CNNs, paving the way for advanced applications in edge computing and specialized hardware.",
        "subjects": [
            "cs.ET"
        ],
        "comment": "Presented at the 18th IEEE International Workshop on Cellular Nanoscale Networks and their Applications (CNNA'23) and the 8th Memristor and Memristive Symposium"
    },
    {
        "paper id": "2408.03262",
        "abstract url": "https://arxiv.org/abs/2408.03262",
        "title": "Towards Fixing Panic Bugs for Real-world Rust Programs",
        "rating": "-10",
        "keywords": [],
        "abstract": "The Rust programming language has garnered significant attention due to its robust safety features and memory management capabilities. Despite its guaranteed memory safety, Rust programs still suffer from runtime errors that are unmanageable, i.e., panic errors. Notably, over half of the bugs in rustc, Rust's own compiler, are attributable to crash stemming from panic errors. However, understanding root causes and resolving these panics often requires substantial effort due to the limited information provided, and the stack backtrace could be intricate, often omitting the actual fault locations. Although numerous automated program repair techniques exist, we observe that the prevailing fix patterns do not readily apply to Rust programs due to natural differences in language mechanisms. To tackle the above challenges, this paper introduces a systematic study aimed at fixing Rust panic bugs. We commence by assembling a dataset, namely Panic4R, which includes 102 real panic bugs and their fixes from the top 500 most downloaded open-source crates. By analyzing Rust's implementation, we identify Rust-specific patterns for fixing panic bugs, which can aid in understanding and providing guidance for generating patches. Finally, we design and implement the first automated fixing tool, PanicKiller, for Rust panic bugs, which effectively generates correct patches on the real-world large-scale dataset, and has already assisted in the resolution of 28 panic bugs in open-source projects. Each resolved issue has been validated by the developers and merged into the respective codebases.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03269",
        "abstract url": "https://arxiv.org/abs/2408.03269",
        "title": "Connections Beyond Data: Exploring Homophily With Visualizations",
        "rating": "-10",
        "keywords": [],
        "abstract": "Homophily refers to the tendency of individuals to associate with others who are similar to them in characteristics, such as, race, ethnicity, age, gender, or interests. In this paper, we investigate if individuals exhibit racial homophily when viewing visualizations, using mass shooting data in the United States as the example topic. We conducted a crowdsourced experiment (N=450) where each participant was shown a visualization displaying the counts of mass shooting victims, highlighting the counts for one of three racial groups (White, Black, or Hispanic). Participants were assigned to view visualizations highlighting their own race or a different race to assess the influence of racial concordance on changes in affect (emotion) and attitude towards gun control. While we did not find evidence of homophily, the results showed a significant negative shift in affect across all visualization conditions. Notably, political ideology significantly impacted changes in affect, with more liberal views correlating with a more negative affect change. Our findings underscore the complexity of reactions to mass shooting visualizations and suggest that future research should consider various methodological improvements to better assess homophily effects.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03285",
        "abstract url": "https://arxiv.org/abs/2408.03285",
        "title": "JetUnit: Rendering Diverse Force Feedback in Virtual Reality Using Water Jets",
        "rating": "-10",
        "keywords": [],
        "abstract": "We propose JetUnit, a water-based VR haptic system designed to produce force feedback with a wide spectrum of intensities and frequencies through water jets. The key challenge in designing this system lies in optimizing parameters to enable the haptic device to generate force feedback that closely replicates the most intense force produced by direct water jets while ensuring the user remains dry. In this paper, we present the key design parameters of the JetUnit wearable device determined through a set of quantitative experiments and a perception study. We further conducted a user study to assess the impact of integrating our haptic solutions into virtual reality experiences. The results revealed that, by adhering to the design principles of JetUnit, the water-based haptic system is capable of delivering diverse force feedback sensations, significantly enhancing the immersive experience in virtual reality.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03295",
        "abstract url": "https://arxiv.org/abs/2408.03295",
        "title": "Left of Fab: Securing Design and Collaboration in the Semiconductor Value Chain",
        "rating": "-10",
        "keywords": [],
        "abstract": "The purpose of this paper is to fill a gap in the general understanding -- and academic scrutiny -- of current and emerging workflows for designing and fabricating integrated circuits. The approach is to compare the IC design workflow with that for printed circuit boards, then to discern a classification for threats. The need to define and secure workflows is amplified by both U.S. investment in the semiconductor manufacturing and market forces affecting GPU production for AI applications. The origin of this knowledge gap can be the proprietary nature of solution spaces, but it can be the lack of demand for teaching and learning for engineers and technicians in this domain. This paper presents a framework for understanding the security of design workflows in a vendor- and tool-agnostic way.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03308",
        "abstract url": "https://arxiv.org/abs/2408.03308",
        "title": "Potential and Limitation of High-Frequency Cores and Caches",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper explores the potential of cryogenic computing and superconducting electronics as promising alternatives to traditional semiconductor devices. As semiconductor devices face challenges such as increased leakage currents and reduced performance at higher temperatures, these novel technologies offer high performance and low power computation. Cryogenic computing operates at ultra-low temperatures near 77 K, leading to lower leakage currents and improved electron mobility. On the other hand, superconducting electronics, operating near 0 K, allow electrons to flow without resistance, offering the potential for ultra-low-power, high-speed computation. This study presents a comprehensive performance modeling and analysis of these technologies and provides insights into their potential benefits and limitations. We implement models of in-order and out-of-order cores operating at high clock frequencies associated with superconducting electronics and cryogenic computing in gem5. We evaluate the performance of these components using workloads representative of real-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the potential speedups achievable by these components and the limitations posed by cache bandwidth. This work provides valuable insights into the performance implications and design trade-offs associated with cryogenic and superconducting technologies, laying the foundation for future research in this field using gem5.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03395",
        "abstract url": "https://arxiv.org/abs/2408.03395",
        "title": "An Analysis of Automated Use Case Component Extraction from Scenarios using ChatGPT",
        "rating": "-10",
        "keywords": [],
        "abstract": "Mobile applications (apps) are often developed by only a small number of developers with limited resources, especially in the early years of the app's development. In this setting, many requirements acquisition activities, such as interviews, are challenging or lower priority than development and release activities. Moreover, in this early period, requirements are frequently changing as mobile apps evolve to compete in the marketplace. As app development companies move to standardize their development processes, however, they will shift to documenting and analyzing requirements. One low-cost source of requirements post-deployment are user-authored scenarios describing how they interact with an app. We propose a method for extracting use case components from user-authored scenarios using large language models (LLMs). The method consists of a series of prompts that were developed to improve precision and recall on a ground truth dataset of 50 scenarios independently labeled with UC components. Our results reveal that LLMs require additional domain knowledge to extract UC components, and that refining prompts to include this knowledge improves the quality of the extracted UC components.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03416",
        "abstract url": "https://arxiv.org/abs/2408.03416",
        "title": "The AI-Native Software Development Lifecycle: A Theoretical and Practical New Methodology",
        "rating": "-10",
        "keywords": [],
        "abstract": "As AI continues to advance and impact every phase of the software development lifecycle (SDLC), a need for a new way of building software will emerge. By analyzing the factors that influence the current state of the SDLC and how those will change with AI we propose a new model of development. This white paper proposes the emergence of a fully AI-native SDLC, where AI is integrated seamlessly into every phase of development, from planning to deployment. We introduce the V-Bounce model, an adaptation of the traditional V-model that incorporates AI from end to end. The V-Bounce model leverages AI to dramatically reduce time spent in implementation phases, shifting emphasis towards requirements gathering, architecture design, and continuous validation. This model redefines the role of humans from primary implementers to primarily validators and verifiers with AI acting as an implementation engine.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "10 pages, 6 figures"
    },
    {
        "paper id": "2408.03443",
        "abstract url": "https://arxiv.org/abs/2408.03443",
        "title": "Combinatorial Nullstellensatz Techniques",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present different techniques for applying Combinatorial Nullstellensatz to polynomials over finite fields. For examples, we generalize theorems from Noga Alon's paper on the subject, and present a few of our own.",
        "subjects": [
            "cs.DM",
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03447",
        "abstract url": "https://arxiv.org/abs/2408.03447",
        "title": "Control of SIR Epidemics: Sacrificing Optimality for Feasibility",
        "rating": "-10",
        "keywords": [],
        "abstract": "We study the impact of parameter estimation and state measurement errors on a control framework for optimally mitigating the spread of epidemics. We capture the epidemic spreading process using a susceptible-infected-removed (SIR) epidemic model and consider isolation as the control strategy. We use a control strategy to remove (isolate) a portion of the infected population. Our goal is to maintain the daily infected population below a certain level, while minimizing the resource captured by the isolation rate. Distinct from existing works on leveraging control strategies in epidemic spreading, we propose a parameter estimation strategy and further characterize the parameter estimation error bound. In order to deal with uncertainties, we propose a robust control strategy by overestimating the seriousness of the epidemic and study the feasibility of the system. Compared to the optimal control strategy, we establish that the proposed strategy under parameter estimation and measurement errors will sacrifice optimality to flatten the curve.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2209.01508"
    },
    {
        "paper id": "2408.03451",
        "abstract url": "https://arxiv.org/abs/2408.03451",
        "title": "Molecular Absorption-Aware User Assignment, Spectrum, and Power Allocation in Dense THz Networks with Multi-Connectivity",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper develops a unified framework to maximize the network sum-rate in a multi-user, multi-BS downlink terahertz (THz) network by optimizing user associations, number and bandwidth of sub-bands in a THz transmission window (TW), bandwidth of leading and trailing edge-bands in a TW, sub-band assignment, and power allocations. The proposed framework incorporates multi-connectivity and captures the impact of molecular absorption coefficient variations in a TW, beam-squint, molecular absorption noise, and link blockages. To make the problem tractable, we first propose a convex approximation of the molecular absorption coefficient using curve fitting in a TW, determine the feasible bandwidths of the leading and trailing edge-bands, and then derive closed-form optimal solution for the number of sub-bands considering beam-squint constraints. We then decompose joint user associations, sub-band assignment, and power allocation problem into two sub-problems, i.e., \\textbf{(i)} joint user association and sub-band assignment, and \\textbf{(ii)} power allocation. To solve the former problem, we analytically prove the unimodularity of the constraint matrix which enables us to relax the integer constraint without loss of optimality. To solve power allocation sub-problem, a fractional programming (FP)-based centralized solution as well as an alternating direction method of multipliers (ADMM)-based light-weight distributed solution is proposed. The overall problem is then solved using alternating optimization until convergence. Complexity analysis of the algorithms and numerical convergence are presented. Numerical findings validate the effectiveness of the proposed algorithms and extract useful insights about the interplay of the density of base stations (BSs), Average order of multi-connectivity (AOM), molecular absorption, {hardware impairment}, {imperfect CSI}, and link blockages.",
        "subjects": [
            "cs.IT",
            "cs.ET",
            "eess.SP"
        ],
        "comment": "This paper has been accepted for publication in IEEE journals"
    },
    {
        "paper id": "2408.03452",
        "abstract url": "https://arxiv.org/abs/2408.03452",
        "title": "Matrix-Free Finite Volume Kernels on a Dataflow Architecture",
        "rating": "-10",
        "keywords": [],
        "abstract": "Fast and accurate numerical simulations are crucial for designing large-scale geological carbon storage projects ensuring safe long-term CO2 containment as a climate change mitigation strategy. These simulations involve solving numerous large and complex linear systems arising from the implicit Finite Volume (FV) discretization of PDEs governing subsurface fluid flow. Compounded with highly detailed geomodels, solving linear systems is computationally and memory expensive, and accounts for the majority of the simulation time. Modern memory hierarchies are insufficient to meet the latency and bandwidth needs of large-scale numerical simulations. Therefore, exploring algorithms that can leverage alternative and balanced paradigms, such as dataflow and in-memory computing is crucial. This work introduces a matrix-free algorithm to solve FV-based linear systems using a dataflow architecture to significantly minimize memory latency and bandwidth bottlenecks. Our implementation achieves two orders of magnitude speedup compared to a GPGPU-based reference implementation, and up to 1.2 PFlops on a single dataflow device.",
        "subjects": [
            "cs.MS",
            "physics.comp-ph"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2304.11274"
    },
    {
        "paper id": "2408.03482",
        "abstract url": "https://arxiv.org/abs/2408.03482",
        "title": "Beyond App Markets: Demystifying Underground Mobile App Distribution Via Telegram",
        "rating": "-10",
        "keywords": [],
        "abstract": "The thriving mobile app ecosystem encompasses a wide range of functionalities. However, within this ecosystem, a subset of apps provides illicit services such as gambling and pornography to pursue economic gains, collectively referred to as \"underground economy apps\". While previous studies have examined these apps' characteristics and identification methods, investigations into their distribution via platforms beyond app markets (like Telegram) remain scarce, which has emerged as a crucial channel for underground activities and cybercrime due to the robust encryption and user anonymity. This study provides the first comprehensive exploration of the underground mobile app ecosystem on Telegram. Overcoming the complexities of the Telegram environment, we build a novel dataset and analyze the prevalence, promotional strategies, and characteristics of these apps. Our findings reveal that these apps reach approximately 1% of Telegram's user base, primarily catering to gambling and pornography services. We uncover sophisticated promotional strategies involving complex networks of apps, websites, users, and channels, and identify significant gaps in Telegram's content moderation capabilities. Our analysis also exposes the misuse of iOS features for app distribution and the prevalence of malicious behaviors in these apps. This research not only enhances our understanding of the underground app ecosystem but also provides valuable insights for developing effective regulatory measures and protecting users from potential risks associated with these covert operations. Our findings provide implications for platform regulators, app market operators, law enforcement agencies, and cybersecurity professionals in combating the proliferation of underground apps on encrypted messaging platforms.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03488",
        "abstract url": "https://arxiv.org/abs/2408.03488",
        "title": "Recomposition: A New Technique for Efficient Compositional Verification",
        "rating": "-10",
        "keywords": [],
        "abstract": "Compositional verification algorithms are well-studied in the context of model checking. Properly selecting components for verification is important for efficiency, yet has received comparatively less attention. In this paper, we address this gap with a novel compositional verification framework that focuses on component selection as an explicit, first-class concept. The framework decomposes a system into components, which we then recompose into new components for efficient verification. At the heart of our technique is the recomposition map that determines how recomposition is performed; the component selection problem thus reduces to finding a good recomposition map. However, the space of possible recomposition maps can be large. We therefore propose heuristics to find a small portfolio of recomposition maps, which we then run in parallel. We implemented our techniques in a model checker for the TLA+ language. In our experiments, we show that our tool achieves competitive performance with TLC-a well-known model checker for TLA+-on a benchmark suite of distributed protocols.",
        "subjects": [
            "cs.FL",
            "cs.LO"
        ],
        "comment": "10 pages"
    },
    {
        "paper id": "2408.03504",
        "abstract url": "https://arxiv.org/abs/2408.03504",
        "title": "Sample Complexity of Low-rank Tensor Recovery from Uniformly Random Entries",
        "rating": "-10",
        "keywords": [],
        "abstract": "We show that a generic tensor $T\\in \\mathbb{F}^{n\\times n\\times \\dots\\times n}$ of order $k$ and CP rank $d$ can be uniquely recovered from $n\\log n+dn\\log \\log n +o(n\\log \\log n) $ uniformly random entries with high probability if $d$ and $k$ are constant and $\\mathbb{F}\\in \\{\\mathbb{R},\\mathbb{C}\\}$. The bound is tight up to the coefficient of the second leading term and improves on the existing $O(n^{\\frac{k}{2}}{\\rm polylog}(n))$ upper bound for order $k$ tensors. The bound is obtained by showing that the projection of the Segre variety to a random axis-parallel linear subspace preserves $d$-identifiability with high probability if the dimension of the subspace is $n\\log n+dn\\log \\log n +o(n\\log \\log n) $ and $n$ is sufficiently large.",
        "subjects": [
            "math.CO",
            "cs.IT",
            "math.AG",
            "math.PR"
        ],
        "comment": null
    },
    {
        "paper id": "2408.03509",
        "abstract url": "https://arxiv.org/abs/2408.03509",
        "title": "Establishing Software Engineering Design Competence with Soft Skills",
        "rating": "-10",
        "keywords": [],
        "abstract": "For a long time, it has been recognized that the software industry has a demand for students who are well grounded in design competencies and who are ready to contribute to a project with little additional training. In response to the industry needs, an engineering design course has been developed for senior level students enrolled in the software engineering program in Canada. The goals of the course are to provide a realistic design experience, introduce students to industry culture, improve their time management skills, challenge them technically and intellectually, improve their communication skills, raise student level of professionalism, hone their soft skills, and raise awareness of human factors in software engineering. This work discusses the details of how this design course has been developed and delivered, and the learning outcomes that has been obtained.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "3 pages"
    }
]