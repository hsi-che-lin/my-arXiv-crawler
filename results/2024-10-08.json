[
    {
        "paper id": "2410.05804",
        "abstract url": "https://arxiv.org/abs/2410.05804",
        "title": "CASA: Class-Agnostic Shared Attributes in Vision-Language Models for Efficient Incremental Object Detection",
        "rating": "3",
        "keywords": [
            [
                "parameter-efficient",
                "efficient fine-tuning"
            ],
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Incremental object detection (IOD) is challenged by background shift, where background categories in sequential data may include previously learned or future classes. Inspired by the vision-language foundation models such as CLIP, these models capture shared attributes from extensive image-text paired data during pre-training. We propose a novel method utilizing attributes in vision-language foundation models for incremental object detection. Our method constructs a Class-Agnostic Shared Attribute base (CASA) to capture common semantic information among incremental classes. Specifically, we utilize large language models to generate candidate textual attributes and select the most relevant ones based on current training data, recording their significance in an attribute assignment matrix. For subsequent tasks, we freeze the retained attributes and continue selecting from the remaining candidates while updating the attribute assignment matrix accordingly. Furthermore, we employ OWL-ViT as our baseline, preserving the original parameters of the pre-trained foundation model. Our method adds only 0.7% to parameter storage through parameter-efficient fine-tuning to significantly enhance the scalability and adaptability of IOD. Extensive two-phase and multi-phase experiments on the COCO dataset demonstrate the state-of-the-art performance of our proposed method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05963",
        "abstract url": "https://arxiv.org/abs/2410.05963",
        "title": "Training-Free Open-Ended Object Detection and Segmentation via Attention as Prompts",
        "rating": "2.5",
        "keywords": [
            [
                "Vision-Language",
                "VLM"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Existing perception models achieve great success by learning from large amounts of labeled data, but they still struggle with open-world scenarios. To alleviate this issue, researchers introduce open-set perception tasks to detect or segment unseen objects in the training set. However, these models require predefined object categories as inputs during inference, which are not available in real-world scenarios. Recently, researchers pose a new and more practical problem, \\textit{i.e.}, open-ended object detection, which discovers unseen objects without any object categories as inputs. In this paper, we present VL-SAM, a training-free framework that combines the generalized object recognition model (\\textit{i.e.,} Vision-Language Model) with the generalized object localization model (\\textit{i.e.,} Segment-Anything Model), to address the open-ended object detection and segmentation task. Without additional training, we connect these two generalized models with attention maps as the prompts. Specifically, we design an attention map generation module by employing head aggregation and a regularized attention flow to aggregate and propagate attention maps across all heads and layers in VLM, yielding high-quality attention maps. Then, we iteratively sample positive and negative points from the attention maps with a prompt generation module and send the sampled points to SAM to segment corresponding objects. Experimental results on the long-tail instance segmentation dataset (LVIS) show that our method surpasses the previous open-ended method on the object detection task and can provide additional instance segmentation masks. Besides, VL-SAM achieves favorable performance on the corner case object detection dataset (CODA), demonstrating the effectiveness of VL-SAM in real-world applications. Moreover, VL-SAM exhibits good model generalization that can incorporate various VLMs and SAMs.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by NeurIPS 2024"
    },
    {
        "paper id": "2410.05928",
        "abstract url": "https://arxiv.org/abs/2410.05928",
        "title": "Beyond Captioning: Task-Specific Prompting for Improved VLM Performance in Mathematical Reasoning",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language",
                "VLM"
            ],
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Vision-Language Models (VLMs) have transformed tasks requiring visual and reasoning abilities, such as image retrieval and Visual Question Answering (VQA). Despite their success, VLMs face significant challenges with tasks involving geometric reasoning, algebraic problem-solving, and counting. These limitations stem from difficulties effectively integrating multiple modalities and accurately interpreting geometry-related tasks. Various works claim that introducing a captioning pipeline before VQA tasks enhances performance. We incorporated this pipeline for tasks involving geometry, algebra, and counting. We found that captioning results are not generalizable, specifically with larger VLMs primarily trained on downstream QnA tasks showing random performance on math-related challenges. However, we present a promising alternative: task-based prompting, enriching the prompt with task-specific guidance. This approach shows promise and proves more effective than direct captioning methods for math-heavy problems.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05964",
        "abstract url": "https://arxiv.org/abs/2410.05964",
        "title": "STNet: Deep Audio-Visual Fusion Network for Robust Speaker Tracking",
        "rating": "2",
        "keywords": [
            [
                "Audio-Visual"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Audio-visual speaker tracking aims to determine the location of human targets in a scene using signals captured by a multi-sensor platform, whose accuracy and robustness can be improved by multi-modal fusion methods. Recently, several fusion methods have been proposed to model the correlation in multiple modalities. However, for the speaker tracking problem, the cross-modal interaction between audio and visual signals hasn't been well exploited. To this end, we present a novel Speaker Tracking Network (STNet) with a deep audio-visual fusion model in this work. We design a visual-guided acoustic measurement method to fuse heterogeneous cues in a unified localization space, which employs visual observations via a camera model to construct the enhanced acoustic map. For feature fusion, a cross-modal attention module is adopted to jointly model multi-modal contexts and interactions. The correlated information between audio and visual features is further interacted in the fusion model. Moreover, the STNet-based tracker is applied to multi-speaker cases by a quality-aware module, which evaluates the reliability of multi-modal observations to achieve robust tracking in complex scenarios. Experiments on the AV16.3 and CAV3D datasets show that the proposed STNet-based tracker outperforms uni-modal methods and state-of-the-art audio-visual speaker trackers.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06154",
        "abstract url": "https://arxiv.org/abs/2410.06154",
        "title": "GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models",
        "rating": "2",
        "keywords": [
            [
                "Vision Language",
                "VLMs"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task description, querying it for suitable VLM prompts (e.g., for zero-shot classification with CLIP). These prompts are ranked according to a purity measure obtained through a fitness function. In each respective optimization step, the ranked prompts are fed as in-context examples (with their accuracies) to equip the LLM with the knowledge of the type of text prompts preferred by the downstream VLM. Furthermore, we also explicitly steer the LLM generation process in each optimization step by specifically adding an offset difference vector of the embeddings from the positive and negative solutions found by the LLM, in previous optimization steps, to the intermediate layer of the network for the next generation step. This offset vector steers the LLM generation toward the type of language preferred by the downstream VLM, resulting in enhanced performance on the downstream vision tasks. We comprehensively evaluate our GLOV on 16 diverse datasets using two families of VLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models -- showing that the discovered solutions can enhance the recognition performance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these models.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Code: https://github.com/jmiemirza/GLOV"
    },
    {
        "paper id": "2410.06234",
        "abstract url": "https://arxiv.org/abs/2410.06234",
        "title": "TEOChat: A Large Vision-Language Assistant for Temporal Earth Observation Data",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Large vision and language assistants have enabled new capabilities for interpreting natural images. These approaches have recently been adapted to earth observation data, but they are only able to handle single image inputs, limiting their use for many real-world tasks. In this work, we develop a new vision and language assistant called TEOChat that can engage in conversations about temporal sequences of earth observation data. To train TEOChat, we curate an instruction-following dataset composed of many single image and temporal tasks including building change and damage assessment, semantic change detection, and temporal scene classification. We show that TEOChat can perform a wide variety of spatial and temporal reasoning tasks, substantially outperforming previous vision and language assistants, and even achieving comparable or better performance than specialist models trained to perform these specific tasks. Furthermore, TEOChat achieves impressive zero-shot performance on a change detection and change question answering dataset, outperforms GPT-4o and Gemini 1.5 Pro on multiple temporal tasks, and exhibits stronger single image capabilities than a comparable single EO image instruction-following model. We publicly release our data, models, and code at https://github.com/ermongroup/TEOChat .",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06338",
        "abstract url": "https://arxiv.org/abs/2410.06338",
        "title": "Are Large Language Models State-of-the-art Quality Estimators for Machine Translation of User-generated Content?",
        "rating": "2",
        "keywords": [
            [
                "parameter-efficient",
                "PEFT",
                "efficient fine-tuning"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper investigates whether large language models (LLMs) are state-of-the-art quality estimators for machine translation of user-generated content (UGC) that contains emotional expressions, without the use of reference translations. To achieve this, we employ an existing emotion-related dataset with human-annotated errors and calculate quality evaluation scores based on the Multi-dimensional Quality Metrics. We compare the accuracy of several LLMs with that of our fine-tuned baseline models, under in-context learning and parameter-efficient fine-tuning (PEFT) scenarios. We find that PEFT of LLMs leads to better performance in score prediction with human interpretable explanations than fine-tuned models. However, a manual analysis of LLM outputs reveals that they still have problems such as refusal to reply to a prompt and unstable output while evaluating machine translation of UGC.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06441",
        "abstract url": "https://arxiv.org/abs/2410.06441",
        "title": "Addax: Utilizing Zeroth-Order Gradients to Improve Memory Efficiency and Performance of SGD for Fine-Tuning Language Models",
        "rating": "2",
        "keywords": [
            [
                "Memory Efficiency"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Fine-tuning language models (LMs) with the Adam optimizer often demands excessive memory, limiting accessibility. The \"in-place\" version of Stochastic Gradient Descent (IP-SGD) and Memory-Efficient Zeroth-order Optimizer (MeZO) have been proposed to address this. However, IP-SGD still requires substantial memory, and MeZO suffers from slow convergence and degraded final performance due to its zeroth-order nature. This paper introduces Addax, a novel method that improves both memory efficiency and performance of IP-SGD by integrating it with MeZO. Specifically, Addax computes zeroth- or first-order gradients of data points in the minibatch based on their memory consumption, combining these gradient estimates to update directions. By computing zeroth-order gradients for data points that require more memory and first-order gradients for others, Addax overcomes the slow convergence of MeZO and the excessive memory requirement of IP-SGD. Additionally, the zeroth-order gradient acts as a regularizer for the first-order gradient, further enhancing the model's final performance. Theoretically, we establish the convergence of Addax under mild assumptions, demonstrating faster convergence and less restrictive hyper-parameter choices than MeZO. Our experiments with diverse LMs and tasks show that Addax consistently outperforms MeZO regarding accuracy and convergence speed while having a comparable memory footprint. When fine-tuning OPT-13B with one A100 GPU, on average, Addax outperforms MeZO in accuracy/F1 score by 14% and runs 15x faster while using memory similar to MeZO. In our experiments on the larger OPT-30B model, on average, Addax outperforms MeZO in terms of accuracy/F1 score by >16 and runs 30x faster on a single H100 GPU. Moreover, Addax surpasses the performance of standard fine-tuning approaches, such as IP-SGD and Adam, in most tasks with significantly less memory requirement.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05695",
        "abstract url": "https://arxiv.org/abs/2410.05695",
        "title": "Unlocking the Boundaries of Thought: A Reasoning Granularity Framework to Quantify and Optimize Chain-of-Thought",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Chain-of-Thought (CoT) reasoning has emerged as a promising approach for enhancing the performance of large language models (LLMs) on complex reasoning tasks. Recently, a series of studies attempt to explain the mechanisms underlying CoT, aiming to deepen the understanding of its efficacy. Nevertheless, the existing research faces two major challenges: (1) a lack of quantitative metrics to assess CoT capabilities and (2) a dearth of guidance on optimizing CoT performance. Motivated by this, in this work, we introduce a novel reasoning granularity framework (RGF) to address these challenges. To solve the lack of quantification, we first define a reasoning granularity (RG) to quantify the upper bound of CoT and establish a combination law for RG, enabling a practical quantitative approach applicable to various real-world CoT tasks. To address the lack of optimization, we propose three categories of RGs. We further optimize these categories with combination laws focused on RG promotion and reasoning path optimization for CoT improvement. Through extensive experiments on 25 models and 4 tasks, the study validates the existence and rationality of the proposed framework. Furthermore, it explains the effectiveness of 10 CoT strategies and guides optimization from two perspectives. We hope this work can provide a comprehensive understanding of the boundaries and optimization strategies for reasoning in LLMs. Our code and data are available at https://github.com/LightChen233/reasoning-granularity.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at NeurIPS2024 (Oral)"
    },
    {
        "paper id": "2410.05698",
        "abstract url": "https://arxiv.org/abs/2410.05698",
        "title": "A Two-Step Approach for Data-Efficient French Pronunciation Learning",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Recent studies have addressed intricate phonological phenomena in French, relying on either extensive linguistic knowledge or a significant amount of sentence-level pronunciation data. However, creating such resources is expensive and non-trivial. To this end, we propose a novel two-step approach that encompasses two pronunciation tasks: grapheme-to-phoneme and post-lexical processing. We then investigate the efficacy of the proposed approach with a notably limited amount of sentence-level pronunciation data. Our findings demonstrate that the proposed two-step approach effectively mitigates the lack of extensive labeled data, and serves as a feasible solution for addressing French phonological phenomena even under resource-constrained environments.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Accepted at EMNLP 2024 Main"
    },
    {
        "paper id": "2410.05714",
        "abstract url": "https://arxiv.org/abs/2410.05714",
        "title": "Enhancing Temporal Modeling of Video LLMs via Time Gating",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Video Large Language Models (Video LLMs) have achieved impressive performance on video-and-language tasks, such as video question answering. However, most existing Video LLMs neglect temporal information in video data, leading to struggles with temporal-aware video understanding. To address this gap, we propose a Time Gating Video LLM (TG-Vid) designed to enhance temporal modeling through a novel Time Gating module (TG). The TG module employs a time gating mechanism on its sub-modules, comprising gating spatial attention, gating temporal attention, and gating MLP. This architecture enables our model to achieve a robust understanding of temporal information within videos. Extensive evaluation of temporal-sensitive video benchmarks (i.e., MVBench, TempCompass, and NExT-QA) demonstrates that our TG-Vid model significantly outperforms the existing Video LLMs. Further, comprehensive ablation studies validate that the performance gains are attributed to the designs of our TG module. Our code is available at https://github.com/LaVi-Lab/TG-Vid.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": "EMNLP 2024 Findings (Short)"
    },
    {
        "paper id": "2410.05724",
        "abstract url": "https://arxiv.org/abs/2410.05724",
        "title": "Exploring rhythm formant analysis for Indic language classification",
        "rating": "1.5",
        "keywords": [
            [
                "eess.AS"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "This paper reports a preliminary study on quantitative frequency domain rhythm cues for classifying five Indian languages: Bengali, Kannada, Malayalam, Marathi, and Tamil. We employ rhythm formant (R-formants) analysis, a technique introduced by Gibbon that utilizes low-frequency spectral analysis of amplitude modulation and frequency modulation envelopes to characterize speech rhythm. Various measures are computed from the LF spectrum, including R-formants, discrete cosine transform-based measures, and spectral measures. Results show that threshold-based and spectral features outperform directly computed R-formants. Temporal pattern of rhythm derived from LF spectrograms provides better language-discriminating cues. Combining all derived features we achieve an accuracy of 69.21% and a weighted F1 score of 69.18% in classifying the five languages. This study demonstrates the potential of RFA in characterizing speech rhythm for Indian language classification.",
        "subjects": [
            "eess.AS",
            "eess.SP"
        ],
        "comment": "Submitted to ICASSP 2025"
    },
    {
        "paper id": "2410.05748",
        "abstract url": "https://arxiv.org/abs/2410.05748",
        "title": "Label Confidence Weighted Learning for Target-level Sentence Simplification",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Multi-level sentence simplification generates simplified sentences with varying language proficiency levels. We propose Label Confidence Weighted Learning (LCWL), a novel approach that incorporates a label confidence weighting scheme in the training loss of the encoder-decoder model, setting it apart from existing confidence-weighting methods primarily designed for classification. Experimentation on English grade-level simplification dataset shows that LCWL outperforms state-of-the-art unsupervised baselines. Fine-tuning the LCWL model on in-domain data and combining with Symmetric Cross Entropy (SCE) consistently delivers better simplifications compared to strong supervised methods. Our results highlight the effectiveness of label confidence weighting techniques for text simplification tasks with encoder-decoder architectures.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to EMNLP 2024"
    },
    {
        "paper id": "2410.05801",
        "abstract url": "https://arxiv.org/abs/2410.05801",
        "title": "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Recent Retrieval Augmented Generation (RAG) aims to enhance Large Language Models (LLMs) by incorporating extensive knowledge retrieved from external sources. However, such approach encounters some challenges: Firstly, the original queries may not be suitable for precise retrieval, resulting in erroneous contextual knowledge; Secondly, the language model can easily generate inconsistent answer with external references due to their knowledge boundary limitation. To address these issues, we propose the chain-of-verification (CoV-RAG) to enhance the external retrieval correctness and internal generation consistency. Specifically, we integrate the verification module into the RAG, engaging in scoring, judgment, and rewriting. To correct external retrieval errors, CoV-RAG retrieves new knowledge using a revised query. To correct internal generation errors, we unify QA and verification tasks with a Chain-of-Thought (CoT) reasoning during training. Our comprehensive experiments across various LLMs demonstrate the effectiveness and adaptability compared with other strong baselines. Especially, our CoV-RAG can significantly surpass the state-of-the-art baselines using different LLM backbones.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Accepted to EMNLP 2024 Findings. 9 pages, 4 figures, 7 tables"
    },
    {
        "paper id": "2410.05817",
        "abstract url": "https://arxiv.org/abs/2410.05817",
        "title": "Probing Language Models on Their Knowledge Source",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Large Language Models (LLMs) often encounter conflicts between their learned, internal (parametric knowledge, PK) and external knowledge provided during inference (contextual knowledge, CK). Understanding how LLMs models prioritize one knowledge source over the other remains a challenge. In this paper, we propose a novel probing framework to explore the mechanisms governing the selection between PK and CK in LLMs. Using controlled prompts designed to contradict the model's PK, we demonstrate that specific model activations are indicative of the knowledge source employed. We evaluate this framework on various LLMs of different sizes and demonstrate that mid-layer activations, particularly those related to relations in the input, are crucial in predicting knowledge source selection, paving the way for more reliable models capable of handling knowledge conflicts effectively.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at BlackBoxNLP@EMNLP2024"
    },
    {
        "paper id": "2410.05851",
        "abstract url": "https://arxiv.org/abs/2410.05851",
        "title": "Communicating with Speakers and Listeners of Different Pragmatic Levels",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "This paper explores the impact of variable pragmatic competence on communicative success through simulating language learning and conversing between speakers and listeners with different levels of reasoning abilities. Through studying this interaction, we hypothesize that matching levels of reasoning between communication partners would create a more beneficial environment for communicative success and language learning. Our research findings indicate that learning from more explicit, literal language is advantageous, irrespective of the learner's level of pragmatic competence. Furthermore, we find that integrating pragmatic reasoning during language learning, not just during evaluation, significantly enhances overall communication performance. This paper provides key insights into the importance of aligning reasoning levels and incorporating pragmatic reasoning in optimizing communicative interactions.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "EMNLP 2024 main"
    },
    {
        "paper id": "2410.06016",
        "abstract url": "https://arxiv.org/abs/2410.06016",
        "title": "Variable Bitrate Residual Vector Quantization for Audio Coding",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec.",
        "subjects": [
            "cs.SD",
            "cs.LG",
            "eess.AS"
        ],
        "comment": "Submitted to ICASSP 2025"
    },
    {
        "paper id": "2410.06040",
        "abstract url": "https://arxiv.org/abs/2410.06040",
        "title": "QERA: an Analytical Framework for Quantization Error Reconstruction",
        "rating": "1.5",
        "keywords": [
            [
                "parameter-efficient",
                "efficient fine-tuning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "he growing number of parameters and computational demands of large language models (LLMs) present significant challenges for their efficient deployment. Recently, there is an increasing interest in quantizing weights to extremely low precision while offsetting the resulting error with low-rank, high-precision error reconstruction terms. The combination of quantization and low-rank approximation is now popular in both adapter-based, parameter-efficient fine-tuning methods such as LoftQ and low-precision inference techniques including ZeroQuant-V2. Usually, the low-rank terms are calculated via the singular value decomposition (SVD) of the weight quantization error, minimizing the Frobenius and spectral norms of the weight approximation error. Recent methods like LQ-LoRA and LQER introduced hand-crafted heuristics to minimize errors in layer outputs (activations) rather than weights, resulting improved quantization results. However, these heuristic methods lack an analytical solution to guide the design of quantization error reconstruction terms. In this paper, we revisit this problem and formulate an analytical framework, named Quantization Error Reconstruction Analysis (QERA), and offer a closed-form solution to the problem. We show QERA benefits both existing low-precision fine-tuning and inference methods -- QERA achieves a fine-tuned accuracy gain of $\u0394_{\\text{acc}}$ = 6.05% of 2-bit RoBERTa-base on GLUE compared to LoftQ; and obtains $\u0394_{\\text{acc}}$ = 2.97% higher post-training quantization accuracy of 4-bit Llama-3.1-70B on average than ZeroQuant-V2 and $\u0394_{\\text{ppl}}$ = - 0.28 lower perplexity on WikiText2 than LQER.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06089",
        "abstract url": "https://arxiv.org/abs/2410.06089",
        "title": "TOWER: Tree Organized Weighting for Evaluating Complex Instructions",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Evaluating the ability of large language models (LLMs) to follow complex human-written instructions is essential for their deployment in real-world applications. While benchmarks like Chatbot Arena use human judges to assess model performance, they are resource-intensive and time-consuming. Alternative methods using LLMs as judges, such as AlpacaEval, MT Bench, WildBench, and InFoBench offer improvements but still do not capture that certain complex instruction aspects are more important than others to follow. To address this gap, we propose a novel evaluation metric, \\textsc{TOWER}, that incorporates human-judged importance into the assessment of complex instruction following. We show that human annotators agree with tree-based representations of these complex instructions nearly as much as they agree with other human annotators. We release tree-based annotations of the InFoBench dataset and the corresponding evaluation code to facilitate future research.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Accepted to EMNLP 2024"
    },
    {
        "paper id": "2410.06131",
        "abstract url": "https://arxiv.org/abs/2410.06131",
        "title": "Towards Unsupervised Eye-Region Segmentation for Eye Tracking",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Finding the eye and parsing out the parts (e.g. pupil and iris) is a key prerequisite for image-based eye tracking, which has become an indispensable module in today's head-mounted VR/AR devices. However, a typical route for training a segmenter requires tedious handlabeling. In this work, we explore an unsupervised way. First, we utilize priors of human eye and extract signals from the image to establish rough clues indicating the eye-region structure. Upon these sparse and noisy clues, a segmentation network is trained to gradually identify the precise area for each part. To achieve accurate parsing of the eye-region, we first leverage the pretrained foundation model Segment Anything (SAM) in an automatic way to refine the eye indications. Then, the learning process is designed in an end-to-end manner following progressive and prior-aware principle. Experiments show that our unsupervised approach can easily achieve 90% (the pupil and iris) and 85% (the whole eye-region) of the performances under supervised learning.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "ECCV2024 ICVSE workshop"
    },
    {
        "paper id": "2410.06328",
        "abstract url": "https://arxiv.org/abs/2410.06328",
        "title": "Auto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Recent advancements in prompt engineering strategies, such as Chain-of-Thought (CoT) and Self-Discover, have demonstrated significant potential in improving the reasoning abilities of Large Language Models (LLMs). However, these state-of-the-art (SOTA) prompting strategies rely on single or fixed set of static seed reasoning modules like \\emph{\"think step by step\"} or \\emph{\"break down this problem\"} intended to simulate human approach to problem-solving. This constraint limits the flexibility of models in tackling diverse problems effectively. In this paper, we introduce Auto-Evolve, a novel framework that enables LLMs to self-create dynamic reasoning modules and downstream action plan, resulting in significant improvements over current SOTA methods. We evaluate Auto-Evolve on the challenging BigBench-Hard (BBH) dataset with Claude 2.0, Claude 3 Sonnet, Mistral Large, and GPT 4, where it consistently outperforms the SOTA prompt strategies. Auto-Evolve outperforms CoT by up to 10.4\\% and on an average by 7\\% across these four models. Our framework introduces two innovations: a) Auto-Evolve dynamically generates reasoning modules for each task while aligning with human reasoning paradigm, thus eliminating the need for predefined templates. b) We introduce an iterative refinement component, that incrementally refines instruction guidance for LLMs and helps boost performance by average 2.8\\% compared to doing it in a single step.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted at EMNLP 2024"
    },
    {
        "paper id": "2410.06396",
        "abstract url": "https://arxiv.org/abs/2410.06396",
        "title": "MLissard: Multilingual Long and Simple Sequential Reasoning Benchmarks",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training. For example, state-of-the-art LLMs can find common items in two lists with up to 20 items but fail when lists have 80 items. In this paper, we introduce MLissard, a multilingual benchmark designed to evaluate models' abilities to process and generate texts of varied lengths and offers a mechanism for controlling sequence complexity. Our evaluation of open-source and proprietary models show a consistent decline in performance across all models and languages as the complexity of the sequence increases. Surprisingly, the use of in-context examples in languages other than English helps increase extrapolation performance significantly. The datasets and code are available at https://github.com/unicamp-dl/Lissard",
        "subjects": [
            "cs.CL"
        ],
        "comment": "GenBench Workshop by EMNLP 2024: Camera-ready version"
    },
    {
        "paper id": "2410.06431",
        "abstract url": "https://arxiv.org/abs/2410.06431",
        "title": "Functional-level Uncertainty Quantification for Calibrated Fine-tuning on LLMs",
        "rating": "1.5",
        "keywords": [
            [
                "parameter-efficient",
                "PEFT"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "From common-sense reasoning to domain-specific tasks, parameter-efficient fine tuning (PEFT) methods for large language models (LLMs) have showcased significant performance improvements on downstream tasks. However, fine-tuned LLMs often struggle with overconfidence in uncertain predictions, particularly due to sparse training data. This overconfidence reflects poor epistemic uncertainty calibration, which arises from limitations in the model's ability to generalize with limited data. Existing PEFT uncertainty quantification methods for LLMs focus on the post fine-tuning stage and thus have limited capability in calibrating epistemic uncertainty. To address these limitations, we propose Functional-Level Uncertainty Quantification for Calibrated Fine-Tuning (UQ4CT), which captures and calibrates functional-level epistemic uncertainty during the fine-tuning stage via a mixture-of-expert framework. We show that UQ4CT reduces Expected Calibration Error (ECE) by more than $25\\%$ while maintaining high accuracy across $5$ benchmarks. Furthermore, UQ4CT maintains superior ECE performance with high accuracy under distribution shift, showcasing improved generalizability.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06458",
        "abstract url": "https://arxiv.org/abs/2410.06458",
        "title": "LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Instruction following is a key capability for LLMs. However, recent studies have shown that LLMs often struggle with instructions containing multiple constraints (e.g. a request to create a social media post \"in a funny tone\" with \"no hashtag\"). Despite this, most evaluations focus solely on synthetic data. To address this, we introduce RealInstruct, the first benchmark designed to evaluate LLMs' ability to follow real-world multi-constrained instructions by leveraging queries real users asked AI assistants. We also investigate model-based evaluation as a cost-effective alternative to human annotation for this task. Our findings reveal that even the proprietary GPT-4 model fails to meet at least one constraint on over 21% of instructions, highlighting the limitations of state-of-the-art models. To address the performance gap between open-source and proprietary models, we propose the Decompose, Critique and Refine (DeCRIM) self-correction pipeline, which enhances LLMs' ability to follow constraints. DeCRIM works by decomposing the original instruction into a list of constraints and using a Critic model to decide when and where the LLM's response needs refinement. Our results show that DeCRIM improves Mistral's performance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback. Moreover, we demonstrate that with strong feedback, open-source LLMs with DeCRIM can outperform GPT-4 on both benchmarks.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "To appear at EMNLP 2024"
    },
    {
        "paper id": "2410.06459",
        "abstract url": "https://arxiv.org/abs/2410.06459",
        "title": "Mamba-based Segmentation Model for Speaker Diarization",
        "rating": "1.5",
        "keywords": [
            [
                "cs.SD",
                "eess.AS"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Mamba is a newly proposed architecture which behaves like a recurrent neural network (RNN) with attention-like capabilities. These properties are promising for speaker diarization, as attention-based models have unsuitable memory requirements for long-form audio, and traditional RNN capabilities are too limited. In this paper, we propose to assess the potential of Mamba for diarization by comparing the state-of-the-art neural segmentation of the pyannote pipeline with our proposed Mamba-based variant. Mamba's stronger processing capabilities allow usage of longer local windows, which significantly improve diarization quality by making the speaker embedding extraction more reliable. We find Mamba to be a superior alternative to both traditional RNN and the tested attention-based model. Our proposed Mamba-based system achieves state-of-the-art performance on three widely used diarization datasets.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": "5 pages, 4 figures. Submitted to ICASSP 2025. Code at https://github.com/nttcslab-sp/mamba-diarization"
    },
    {
        "paper id": "2410.06496",
        "abstract url": "https://arxiv.org/abs/2410.06496",
        "title": "On the Similarity of Circuits across Languages: a Case Study on the Subject-verb Agreement Task",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Several algorithms implemented by language models have recently been successfully reversed-engineered. However, these findings have been concentrated on specific tasks and models, leaving it unclear how universal circuits are across different settings. In this paper, we study the circuits implemented by Gemma 2B for solving the subject-verb agreement task across two different languages, English and Spanish. We discover that both circuits are highly consistent, being mainly driven by a particular attention head writing a `subject number' signal to the last residual stream, which is read by a small set of neurons in the final MLPs. Notably, this subject number signal is represented as a direction in the residual stream space, and is language-independent. We demonstrate that this direction has a causal effect on the model predictions, effectively flipping the Spanish predicted verb number by intervening with the direction found in English. Finally, we present evidence of similar behavior in other models within the Gemma 1 and Gemma 2 families.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at EMNLP 2024 Findings"
    },
    {
        "paper id": "2410.06519",
        "abstract url": "https://arxiv.org/abs/2410.06519",
        "title": "SEGMENT+: Long Text Processing with Short-Context Language Models",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "There is a growing interest in expanding the input capacity of language models (LMs) across various domains. However, simply increasing the context window does not guarantee robust performance across diverse long-input processing tasks, such as understanding extensive documents and extracting detailed information from lengthy and noisy data. In response, we introduce SEGMENT+, a general framework that enables LMs to handle extended inputs within limited context windows efficiently. SEGMENT+ utilizes structured notes and a filtering module to manage information flow, resulting in a system that is both controllable and interpretable. Our extensive experiments across various model sizes, focusing on long-document question-answering and Needle-in-a-Haystack tasks, demonstrate the effectiveness of SEGMENT+ in improving performance.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "EMNLP 2024"
    },
    {
        "paper id": "2410.06524",
        "abstract url": "https://arxiv.org/abs/2410.06524",
        "title": "Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Recent advancements of large language models (LLMs) have led to claims of AI surpassing humans in natural language processing (NLP) tasks such as textual understanding and reasoning. This work investigates these assertions by introducing CAIMIRA, a novel framework rooted in item response theory (IRT) that enables quantitative assessment and comparison of problem-solving abilities of question-answering (QA) agents: humans and AI systems. Through analysis of over 300,000 responses from ~70 AI systems and 155 humans across thousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in knowledge domains and reasoning skills. Humans outperform AI systems in knowledge-grounded abductive and conceptual reasoning, while state-of-the-art LLMs like GPT-4 and LLaMA show superior performance on targeted information retrieval and fact-based reasoning, particularly when information gaps are well-defined and addressable through pattern matching or data retrieval. These findings highlight the need for future QA tasks to focus on questions that challenge not only higher-order reasoning and scientific thinking, but also demand nuanced linguistic interpretation and cross-contextual knowledge application, helping advance AI developments that better emulate or complement human cognitive abilities in real-world problem-solving.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "To appear at EMNLP 2024 (Main)"
    },
    {
        "paper id": "2410.06535",
        "abstract url": "https://arxiv.org/abs/2410.06535",
        "title": "Happy: A Debiased Learning Framework for Continual Generalized Category Discovery",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Constantly discovering novel concepts is crucial in evolving environments. This paper explores the underexplored task of Continual Generalized Category Discovery (C-GCD), which aims to incrementally discover new classes from unlabeled data while maintaining the ability to recognize previously learned classes. Although several settings are proposed to study the C-GCD task, they have limitations that do not reflect real-world scenarios. We thus study a more practical C-GCD setting, which includes more new classes to be discovered over a longer period, without storing samples of past classes. In C-GCD, the model is initially trained on labeled data of known classes, followed by multiple incremental stages where the model is fed with unlabeled data containing both old and new classes. The core challenge involves two conflicting objectives: discover new classes and prevent forgetting old ones. We delve into the conflicts and identify that models are susceptible to prediction bias and hardness bias. To address these issues, we introduce a debiased learning framework, namely Happy, characterized by Hardness-aware prototype sampling and soft entropy regularization. For the prediction bias, we first introduce clustering-guided initialization to provide robust features. In addition, we propose soft entropy regularization to assign appropriate probabilities to new classes, which can significantly enhance the clustering performance of new classes. For the harness bias, we present the hardness-aware prototype sampling, which can effectively reduce the forgetting issue for previously seen classes, especially for difficult classes. Experimental results demonstrate our method proficiently manages the conflicts of C-GCD and achieves remarkable performance across various datasets, e.g., 7.5% overall gains on ImageNet-100. Our code is publicly available at https://github.com/mashijie1028/Happy-CGCD.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at NeurIPS 2024"
    },
    {
        "paper id": "2410.06547",
        "abstract url": "https://arxiv.org/abs/2410.06547",
        "title": "TuringQ: Benchmarking AI Comprehension in Theory of Computation",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "We present TuringQ, the first benchmark designed to evaluate the reasoning capabilities of large language models (LLMs) in the theory of computation. TuringQ consists of 4,006 undergraduate and graduate-level question-answer pairs, categorized into four difficulty levels and covering seven core theoretical areas. We evaluate several open-source LLMs, as well as GPT-4, using Chain of Thought prompting and expert human assessment. Additionally, we propose an automated LLM-based evaluation system that demonstrates competitive accuracy when compared to human evaluation. Fine-tuning a Llama3-8B model on TuringQ shows measurable improvements in reasoning ability and out-of-domain tasks such as algebra. TuringQ serves as both a benchmark and a resource for enhancing LLM performance in complex computational reasoning tasks. Our analysis offers insights into LLM capabilities and advances in AI comprehension of theoretical computer science.",
        "subjects": [
            "cs.CL",
            "cs.FL"
        ],
        "comment": "Accepted to EMNLP Findings 2024"
    },
    {
        "paper id": "2410.05700",
        "abstract url": "https://arxiv.org/abs/2410.05700",
        "title": "Log-concave Sampling over a Convex Body with a Barrier: a Robust and Unified Dikin Walk",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We consider the problem of sampling from a $d$-dimensional log-concave distribution $\u03c0(\u03b8) \\propto \\exp(-f(\u03b8))$ for $L$-Lipschitz $f$, constrained to a convex body with an efficiently computable self-concordant barrier function, contained in a ball of radius $R$ with a $w$-warm start. We propose a \\emph{robust} sampling framework that computes spectral approximations to the Hessian of the barrier functions in each iteration. We prove that for polytopes that are described by $n$ hyperplanes, sampling with the Lee-Sidford barrier function mixes within $\\widetilde O((d^2+dL^2R^2)\\log(w/\u03b4))$ steps with a per step cost of $\\widetilde O(nd^{\u03c9-1})$, where $\u03c9\\approx 2.37$ is the fast matrix multiplication exponent. Compared to the prior work of Mangoubi and Vishnoi, our approach gives faster mixing time as we are able to design a generalized soft-threshold Dikin walk beyond log-barrier. We further extend our result to show how to sample from a $d$-dimensional spectrahedron, the constrained set of a semidefinite program, specified by the set $\\{x\\in \\mathbb{R}^d: \\sum_{i=1}^d x_i A_i \\succeq C \\}$ where $A_1,\\ldots,A_d, C$ are $n\\times n$ real symmetric matrices. We design a walk that mixes in $\\widetilde O((nd+dL^2R^2)\\log(w/\u03b4))$ steps with a per iteration cost of $\\widetilde O(n^\u03c9+n^2d^{3\u03c9-5})$. We improve the mixing time bound of prior best Dikin walk due to Narayanan and Rakhlin that mixes in $\\widetilde O((n^2d^3+n^2dL^2R^2)\\log(w/\u03b4))$ steps.",
        "subjects": [
            "cs.DS",
            "cs.LG",
            "stat.ML"
        ],
        "comment": "NeurIPS 2024"
    },
    {
        "paper id": "2410.05721",
        "abstract url": "https://arxiv.org/abs/2410.05721",
        "title": "Mero Nagarikta: Advanced Nepali Citizenship Data Extractor with Deep Learning-Powered Text Detection and OCR",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Transforming text-based identity documents, such as Nepali citizenship cards, into a structured digital format poses several challenges due to the distinct characteristics of the Nepali script and minor variations in print alignment and contrast across different cards. This work proposes a robust system using YOLOv8 for accurate text object detection and an OCR algorithm based on Optimized PyTesseract. The system, implemented within the context of a mobile application, allows for the automated extraction of important textual information from both the front and the back side of Nepali citizenship cards, including names, citizenship numbers, and dates of birth. The final YOLOv8 model was accurate, with a mean average precision of 99.1% for text detection on the front and 96.1% on the back. The tested PyTesseract optimized for Nepali characters outperformed the standard OCR regarding flexibility and accuracy, extracting text from images with clean and noisy backgrounds and various contrasts. Using preprocessing steps such as converting the images into grayscale, removing noise from the images, and detecting edges further improved the system's OCR accuracy, even for low-quality photos. This work expands the current body of research in multilingual OCR and document analysis, especially for low-resource languages such as Nepali. It emphasizes the effectiveness of combining the latest object detection framework with OCR models that have been fine-tuned for practical applications.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "13 pages, 8 figures"
    },
    {
        "paper id": "2410.05739",
        "abstract url": "https://arxiv.org/abs/2410.05739",
        "title": "Array2BR: An End-to-End Noise-immune Binaural Audio Synthesis from Microphone-array Signals",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Telepresence technology aims to provide an immersive virtual presence for remote conference applications, and it is extremely important to synthesize high-quality binaural audio signals for this aim. Because the ambient noise is often inevitable in practical application scenarios, it is highly desired that binaural audio signals without noise can be obtained from microphone-array signals directly. For this purpose, this paper proposes a new end-to-end noise-immune binaural audio synthesis framework from microphone-array signals, abbreviated as Array2BR, and experimental results show that binaural cues can be correctly mapped and noise can be well suppressed simultaneously using the proposed framework. Compared with existing methods, the proposed method achieved better performance in terms of both objective and subjective metric scores.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05746",
        "abstract url": "https://arxiv.org/abs/2410.05746",
        "title": "Wolf2Pack: The AutoFusion Framework for Dynamic Parameter Fusion",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "In the rapidly evolving field of deep learning, specialized models have driven significant advancements in tasks such as computer vision and natural language processing. However, this specialization leads to a fragmented ecosystem where models lack the adaptability for broader applications. To overcome this, we introduce AutoFusion, an innovative framework that fuses distinct model parameters(with the same architecture) for multi-task learning without pre-trained checkpoints. Using an unsupervised, end-to-end approach, AutoFusion dynamically permutes model parameters at each layer, optimizing the combination through a loss-minimization process that does not require labeled data. We validate AutoFusion's effectiveness through experiments on commonly used benchmark datasets, demonstrating superior performance over established methods like Weight Interpolation, Git Re-Basin, and ZipIt. Our framework offers a scalable and flexible solution for model integration, positioning it as a powerful tool for future research and practical applications.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2410.05760",
        "abstract url": "https://arxiv.org/abs/2410.05760",
        "title": "Training-free Diffusion Model Alignment with Sampling Demons",
        "rating": "1",
        "keywords": [
            [
                "Visual-Language",
                "VLM"
            ],
            [
                "Diffusion",
                "text-to-image"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Aligning diffusion models with user preferences has been a key challenge. Existing methods for aligning diffusion models either require retraining or are limited to differentiable reward functions. To address these limitations, we propose a stochastic optimization approach, dubbed Demon, to guide the denoising process at inference time without backpropagation through reward functions or model retraining. Our approach works by controlling noise distribution in denoising steps to concentrate density on regions corresponding to high rewards through stochastic optimization. We provide comprehensive theoretical and empirical evidence to support and validate our approach, including experiments that use non-differentiable sources of rewards such as Visual-Language Model (VLM) APIs and human judgements. To the best of our knowledge, the proposed approach is the first inference-time, backpropagation-free preference alignment method for diffusion models. Our method can be easily integrated with existing diffusion models without further training. Our experiments show that the proposed approach significantly improves the average aesthetics scores for text-to-image generation.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "math.OC",
            "stat.ML"
        ],
        "comment": "36 pages"
    },
    {
        "paper id": "2410.05762",
        "abstract url": "https://arxiv.org/abs/2410.05762",
        "title": "Guided Self-attention: Find the Generalized Necessarily Distinct Vectors for Grain Size Grading",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the development of steel materials, metallographic analysis has become increasingly important. Unfortunately, grain size analysis is a manual process that requires experts to evaluate metallographic photographs, which is unreliable and time-consuming. To resolve this problem, we propose a novel classifi-cation method based on deep learning, namely GSNets, a family of hybrid models which can effectively introduce guided self-attention for classifying grain size. Concretely, we build our models from three insights:(1) Introducing our novel guided self-attention module can assist the model in finding the generalized necessarily distinct vectors capable of retaining intricate rela-tional connections and rich local feature information; (2) By improving the pixel-wise linear independence of the feature map, the highly condensed semantic representation will be captured by the model; (3) Our novel triple-stream merging module can significantly improve the generalization capability and efficiency of the model. Experiments show that our GSNet yields a classifi-cation accuracy of 90.1%, surpassing the state-of-the-art Swin Transformer V2 by 1.9% on the steel grain size dataset, which comprises 3,599 images with 14 grain size levels. Furthermore, we intuitively believe our approach is applicable to broader ap-plications like object detection and semantic segmentation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05767",
        "abstract url": "https://arxiv.org/abs/2410.05767",
        "title": "Grounding is All You Need? Dual Temporal Grounding for Video Dialog",
        "rating": "1",
        "keywords": [
            [
                "visual-language"
            ],
            [
                "trajectory"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "In the realm of video dialog response generation, the understanding of video content and the temporal nuances of conversation history are paramount. While a segment of current research leans heavily on large-scale pretrained visual-language models and often overlooks temporal dynamics, another delves deep into spatial-temporal relationships within videos but demands intricate object trajectory pre-extractions and sidelines dialog temporal dynamics. This paper introduces the Dual Temporal Grounding-enhanced Video Dialog model (DTGVD), strategically designed to merge the strengths of both dominant approaches. It emphasizes dual temporal relationships by predicting dialog turn-specific temporal regions, filtering video content accordingly, and grounding responses in both video and dialog contexts. One standout feature of DTGVD is its heightened attention to chronological interplay. By recognizing and acting upon the dependencies between different dialog turns, it captures more nuanced conversational dynamics. To further bolster the alignment between video and dialog temporal dynamics, we've implemented a list-wise contrastive learning strategy. Within this framework, accurately grounded turn-clip pairings are designated as positive samples, while less precise pairings are categorized as negative. This refined classification is then funneled into our holistic end-to-end response generation mechanism. Evaluations using AVSD@DSTC-7 and AVSD@DSTC-8 datasets underscore the superiority of our methodology.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05770",
        "abstract url": "https://arxiv.org/abs/2410.05770",
        "title": "Efficient Few-shot Learning for Multi-label Classification of Scientific Documents with Many Classes",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Scientific document classification is a critical task and often involves many classes. However, collecting human-labeled data for many classes is expensive and usually leads to label-scarce scenarios. Moreover, recent work has shown that sentence embedding model fine-tuning for few-shot classification is efficient, robust, and effective. In this work, we propose FusionSent (Fusion-based Sentence Embedding Fine-tuning), an efficient and prompt-free approach for few-shot classification of scientific documents with many classes. FusionSent uses available training examples and their respective label texts to contrastively fine-tune two different sentence embedding models. Afterward, the parameters of both fine-tuned models are fused to combine the complementary knowledge from the separate fine-tuning steps into a single model. Finally, the resulting sentence embedding model is frozen to embed the training instances, which are then used as input features to train a classification head. Our experiments show that FusionSent significantly outperforms strong baselines by an average of $6.0$ $F_{1}$ points across multiple scientific document classification datasets. In addition, we introduce a new dataset for multi-label classification of scientific documents, which contains 183,565 scientific articles and 130 classes from the arXiv category taxonomy. Code and data are available at https://github.com/sebischair/FusionSent.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to the 7th International Conference on Natural Language and Speech Processing (ICNLSP 2024)"
    },
    {
        "paper id": "2410.05771",
        "abstract url": "https://arxiv.org/abs/2410.05771",
        "title": "Cefdet: Cognitive Effectiveness Network Based on Fuzzy Inference for Action Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Action detection and understanding provide the foundation for the generation and interaction of multimedia content. However, existing methods mainly focus on constructing complex relational inference networks, overlooking the judgment of detection effectiveness. Moreover, these methods frequently generate detection results with cognitive abnormalities. To solve the above problems, this study proposes a cognitive effectiveness network based on fuzzy inference (Cefdet), which introduces the concept of \"cognition-based detection\" to simulate human cognition. First, a fuzzy-driven cognitive effectiveness evaluation module (FCM) is established to introduce fuzzy inference into action detection. FCM is combined with human action features to simulate the cognition-based detection process, which clearly locates the position of frames with cognitive abnormalities. Then, a fuzzy cognitive update strategy (FCS) is proposed based on the FCM, which utilizes fuzzy logic to re-detect the cognition-based detection results and effectively update the results with cognitive abnormalities. Experimental results demonstrate that Cefdet exhibits superior performance against several mainstream algorithms on the public datasets, validating its effectiveness and superiority.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The paper has been accepted by ACM MM. If you find this work helpful, please consider citing our paper. Zhe Luo, Weina Fu, Shuai Liu, Saeed Anwar, Muhammad Saqib, Sambit Bakshi, Khan Muhammad (2024) Cefdet: Cognitive Effectiveness Network Based on Fuzzy Inference for Action Detection, 32nd ACM International Conference on Multimedia, online first, 10.1145/3664647.3681226"
    },
    {
        "paper id": "2410.05774",
        "abstract url": "https://arxiv.org/abs/2410.05774",
        "title": "ActionAtlas: A VideoQA Benchmark for Domain-specialized Action Recognition",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Our world is full of varied actions and moves across specialized domains that we, as humans, strive to identify and understand. Within any single domain, actions can often appear quite similar, making it challenging for deep models to distinguish them accurately. To evaluate the effectiveness of multimodal foundation models in helping us recognize such actions, we present ActionAtlas v1.0, a multiple-choice video question answering benchmark featuring short videos across various sports. Each video in the dataset is paired with a question and four or five choices. The question pinpoints specific individuals, asking which choice \"best\" describes their action within a certain temporal context. Overall, the dataset includes 934 videos showcasing 580 unique actions across 56 sports, with a total of 1896 actions within choices. Unlike most existing video question answering benchmarks that only cover simplistic actions, often identifiable from a single frame, ActionAtlas focuses on intricate movements and rigorously tests the model's capability to discern subtle differences between moves that look similar within each domain. We evaluate open and proprietary foundation models on this benchmark, finding that the best model, GPT-4o, achieves a maximum accuracy of 45.52%. Meanwhile, Non-expert crowd workers, provided with action description for each choice, achieve 61.64% accuracy, where random chance is approximately 21%. Our findings with state-of-the-art models indicate that having a high frame sampling rate is important for accurately recognizing actions in ActionAtlas, a feature that some leading proprietary video models, such as Gemini, do not include in their default configuration.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05797",
        "abstract url": "https://arxiv.org/abs/2410.05797",
        "title": "CodeCipher: Learning to Obfuscate Source Code Against LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "While large code language models have made significant strides in AI-assisted coding tasks, there are growing concerns about privacy challenges. The user code is transparent to the cloud LLM service provider, inducing risks of unauthorized training, reading, and execution of the user code. In this paper, we propose CodeCipher, a novel method that perturbs privacy from code while preserving the original response from LLMs. CodeCipher transforms the LLM's embedding matrix so that each row corresponds to a different word in the original matrix, forming a token-to-token confusion mapping for obfuscating source code. The new embedding matrix is optimized by minimizing the task-specific loss function. To tackle the challenge of the discrete and sparse nature of word vector spaces, CodeCipher adopts a discrete optimization strategy that aligns the updated vector to the nearest valid token in the vocabulary before each gradient update. We demonstrate the effectiveness of our approach on three AI-assisted coding tasks including code completion, summarization, and translation. Results show that our model successfully confuses the privacy in source code while preserving the original LLM's performance.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05800",
        "abstract url": "https://arxiv.org/abs/2410.05800",
        "title": "Core Tokensets for Data-efficient Sequential Training of Transformers",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Deep networks are frequently tuned to novel tasks and continue learning from ongoing data streams. Such sequential training requires consolidation of new and past information, a challenge predominantly addressed by retaining the most important data points - formally known as coresets. Traditionally, these coresets consist of entire samples, such as images or sentences. However, recent transformer architectures operate on tokens, leading to the famous assertion that an image is worth 16x16 words. Intuitively, not all of these tokens are equally informative or memorable. Going beyond coresets, we thus propose to construct a deeper-level data summary on the level of tokens. Our respectively named core tokensets both select the most informative data points and leverage feature attribution to store only their most relevant features. We demonstrate that core tokensets yield significant performance retention in incremental image classification, open-ended visual question answering, and continual image captioning with significantly reduced memory. In fact, we empirically find that a core tokenset of 1\\% of the data performs comparably to at least a twice as large and up to 10 times larger coreset.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05802",
        "abstract url": "https://arxiv.org/abs/2410.05802",
        "title": "Gradual Learning: Optimizing Fine-Tuning with Partially Mastered Knowledge in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "During the pretraining phase, large language models (LLMs) acquire vast amounts of knowledge from extensive text corpora. Nevertheless, in later stages such as fine-tuning and inference, the model may encounter knowledge not covered in the initial training, which can lead to hallucinations and degraded performance. This issue has a profound impact on the model's capabilities, as it will inevitably face out-of-scope knowledge after pretraining. Furthermore, fine-tuning is often required to adapt LLMs to domain-specific tasks. However, this phenomenon limits the model's ability to learn and integrate new information during fine-tuning. The effectiveness of fine-tuning largely depends on the type of knowledge involved. Existing research suggests that fine-tuning the model on partially mastered knowledge-for instance, question-answer pairs where the model has a chance of providing correct responses under non-greedy decoding-can enable the model to acquire new knowledge while mitigating hallucination. Notably, this approach can still lead to the forgetting of fully mastered knowledge, constraining the fine-tuning dataset to a narrower range and limiting the model's overall potential for improvement. Given the model's intrinsic reasoning abilities and the interconnectedness of different knowledge areas, it is likely that as the model's capacity to utilize existing knowledge improves during fine-tuning, previously unmastered knowledge may become more understandable. To explore this hypothesis, we conducted experiments and, based on the results, proposed a two-stage fine-tuning strategy. This approach not only improves the model's overall test accuracy and knowledge retention but also preserves its accuracy on previously mastered content. When fine-tuning on the WikiQA dataset, our method increases the amount of knowledge acquired by the model in this stage by 24%.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05849",
        "abstract url": "https://arxiv.org/abs/2410.05849",
        "title": "ModalPrompt:Dual-Modality Guided Prompt for Continual Learning of Large Multimodal Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Large Multimodal Models (LMMs) exhibit remarkable multi-tasking ability by learning mixed datasets jointly. However, novel tasks would be encountered sequentially in dynamic world, and continually fine-tuning LMMs often leads to performance degrades. To handle the challenges of catastrophic forgetting, existing methods leverage data replay or model expansion, both of which are not specially developed for LMMs and have their inherent limitations. In this paper, we propose a novel dual-modality guided prompt learning framework (ModalPrompt) tailored for multimodal continual learning to effectively learn new tasks while alleviating forgetting of previous knowledge. Concretely, we learn prototype prompts for each task and exploit efficient prompt selection for task identifiers and prompt fusion for knowledge transfer based on image-text supervision. Extensive experiments demonstrate the superiority of our approach, e.g., ModalPrompt achieves +20% performance gain on LMMs continual learning benchmarks with $\\times$ 1.42 inference speed refraining from growing training cost in proportion to the number of tasks. The code will be made publically available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05864",
        "abstract url": "https://arxiv.org/abs/2410.05864",
        "title": "From Tokens to Words: On the Inner Lexicon of LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Natural language is composed of words, but modern LLMs process sub-words as input. A natural question raised by this discrepancy is whether LLMs encode words internally, and if so how. We present evidence that LLMs engage in an intrinsic detokenization process, where sub-word sequences are combined into coherent word representations. Our experiments show that this process takes place primarily within the early and middle layers of the model. They also show that it is robust to non-morphemic splits, typos and perhaps importantly-to out-of-vocabulary words: when feeding the inner representation of such words to the model as input vectors, it can \"understand\" them despite never seeing them during training. Our findings suggest that LLMs maintain a latent vocabulary beyond the tokenizer's scope. These insights provide a practical, finetuning-free application for expanding the vocabulary of pre-trained models. By enabling the addition of new vocabulary words, we reduce input length and inference iterations, which reduces both space and model latency, with little to no loss in model accuracy.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05873",
        "abstract url": "https://arxiv.org/abs/2410.05873",
        "title": "MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "English-centric large language models (LLMs) often show strong multilingual capabilities. However, the multilingual performance of these models remains unclear and is not thoroughly evaluated for many languages. Most benchmarks for multilinguality focus on classic NLP tasks, or cover a minimal number of languages. We introduce MEXA, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages the fact that English-centric LLMs use English as a kind of pivot language in their intermediate layers. It computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model performance in other languages. We conduct studies using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves a statistically significant average Pearson correlation of 0.90 with three established downstream tasks across nine models and two parallel datasets. This suggests that MEXA is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs. Leaderboard: https://huggingface.co/spaces/cis-lmu/Mexa, Code: https://github.com/cisnlp/Mexa.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05881",
        "abstract url": "https://arxiv.org/abs/2410.05881",
        "title": "Edit Distances and Their Applications to Downstream Tasks in Research and Commercial Contexts",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The tutorial describes the concept of edit distances applied to research and commercial contexts. We use Translation Edit Rate (TER), Levenshtein, Damerau-Levenshtein, Longest Common Subsequence and $n$-gram distances to demonstrate the frailty of statistical metrics when comparing text sequences. Our discussion disassembles them into their essential components. We discuss the centrality of four editing actions: insert, delete, replace and move words, and show their implementations in openly available packages and toolkits. The application of edit distances in downstream tasks often assumes that these accurately represent work done by post-editors and real errors that need to be corrected in MT output. We discuss how imperfect edit distances are in capturing the details of this error correction work and the implications for researchers and for commercial applications, of these uses of edit distances. In terms of commercial applications, we discuss their integration in computer-assisted translation tools and how the perception of the connection between edit distances and post-editor effort affects the definition of translator rates.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Tutorial @ 16th AMTA Conference, 2024"
    },
    {
        "paper id": "2410.05890",
        "abstract url": "https://arxiv.org/abs/2410.05890",
        "title": "Ordering-Based Causal Discovery for Linear and Nonlinear Relations",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Identifying causal relations from purely observational data typically requires additional assumptions on relations and/or noise. Most current methods restrict their analysis to datasets that are assumed to have pure linear or nonlinear relations, which is often not reflective of real-world datasets that contain a combination of both. This paper presents CaPS, an ordering-based causal discovery algorithm that effectively handles linear and nonlinear relations. CaPS introduces a novel identification criterion for topological ordering and incorporates the concept of \"parent score\" during the post-processing optimization stage. These scores quantify the strength of the average causal effect, helping to accelerate the pruning process and correct inaccurate predictions in the pruning step. Experimental results demonstrate that our proposed solutions outperform state-of-the-art baselines on synthetic data with varying ratios of linear and nonlinear relations. The results obtained from real-world data also support the competitiveness of CaPS. Code and datasets are available at https://github.com/E2real/CaPS.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "NeurIPS 2024 poster"
    },
    {
        "paper id": "2410.05903",
        "abstract url": "https://arxiv.org/abs/2410.05903",
        "title": "Automatic Summarization of Long Documents",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "A vast amount of textual data is added to the internet daily, making utilization and interpretation of such data difficult and cumbersome. As a result, automatic text summarization is crucial for extracting relevant information, saving precious reading time. Although many transformer-based models excel in summarization, they are constrained by their input size, preventing them from processing texts longer than their context size. This study introduces three novel algorithms that allow any LLM to efficiently overcome its input size limitation, effectively utilizing its full potential without any architectural modifications. We test our algorithms on texts with more than 70,000 words, and our experiments show a significant increase in BERTScore with competitive ROUGE scores.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "9 pages (including bibliography) with 6 figures. ACL 2023 proceedings format"
    },
    {
        "paper id": "2410.05915",
        "abstract url": "https://arxiv.org/abs/2410.05915",
        "title": "Give me a hint: Can LLMs take a hint to solve math problems?",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "While many state-of-the-art LLMs have shown poor logical and basic mathematical reasoning, recent works try to improve their problem-solving abilities using prompting techniques. We propose giving \"hints\" to improve the language model's performance on advanced mathematical problems, taking inspiration from how humans approach math pedagogically. We also test the model's adversarial robustness to wrong hints. We demonstrate the effectiveness of our approach by evaluating various LLMs, presenting them with a diverse set of problems of different difficulties and topics from the MATH dataset and comparing against techniques such as one-shot, few-shot, and chain of thought prompting.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05938",
        "abstract url": "https://arxiv.org/abs/2410.05938",
        "title": "EMMA: Empowering Multi-modal Mamba with Structural and Hierarchical Alignment",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Mamba-based architectures have shown to be a promising new direction for deep learning models owing to their competitive performance and sub-quadratic deployment speed. However, current Mamba multi-modal large language models (MLLM) are insufficient in extracting visual features, leading to imbalanced cross-modal alignment between visual and textural latents, negatively impacting performance on multi-modal tasks. In this work, we propose Empowering Multi-modal Mamba with Structural and Hierarchical Alignment (EMMA), which enables the MLLM to extract fine-grained visual information. Specifically, we propose a pixel-wise alignment module to autoregressively optimize the learning and processing of spatial image-level features along with textual tokens, enabling structural alignment at the image level. In addition, to prevent the degradation of visual information during the cross-model alignment process, we propose a multi-scale feature fusion (MFF) module to combine multi-scale visual features from intermediate layers, enabling hierarchical alignment at the feature level. Extensive experiments are conducted across a variety of multi-modal benchmarks. Our model shows lower latency than other Mamba-based MLLMs and is nearly four times faster than transformer-based MLLMs of similar scale during inference. Due to better cross-modal alignment, our model exhibits lower degrees of hallucination and enhanced sensitivity to visual details, which manifests in superior performance across diverse multi-modal benchmarks. Code will be provided.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05940",
        "abstract url": "https://arxiv.org/abs/2410.05940",
        "title": "TouchInsight: Uncertainty-aware Rapid Touch and Text Input for Mixed Reality from Egocentric Vision",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "While passive surfaces offer numerous benefits for interaction in mixed reality, reliably detecting touch input solely from head-mounted cameras has been a long-standing challenge. Camera specifics, hand self-occlusion, and rapid movements of both head and fingers introduce considerable uncertainty about the exact location of touch events. Existing methods have thus not been capable of achieving the performance needed for robust interaction. In this paper, we present a real-time pipeline that detects touch input from all ten fingers on any physical surface, purely based on egocentric hand tracking. Our method TouchInsight comprises a neural network to predict the moment of a touch event, the finger making contact, and the touch location. TouchInsight represents locations through a bivariate Gaussian distribution to account for uncertainties due to sensing inaccuracies, which we resolve through contextual priors to accurately infer intended user input. We first evaluated our method offline and found that it locates input events with a mean error of 6.3 mm, and accurately detects touch events (F1=0.99) and identifies the finger used (F1=0.96). In an online evaluation, we then demonstrate the effectiveness of our approach for a core application of dexterous touch input: two-handed text entry. In our study, participants typed 37.0 words per minute with an uncorrected error rate of 2.9% on average.",
        "subjects": [
            "cs.CV",
            "cs.HC"
        ],
        "comment": "Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology (UIST'24)"
    },
    {
        "paper id": "2410.05951",
        "abstract url": "https://arxiv.org/abs/2410.05951",
        "title": "Hyper Adversarial Tuning for Boosting Adversarial Robustness of Pretrained Large Vision Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Large vision models have been found vulnerable to adversarial examples, emphasizing the need for enhancing their adversarial robustness. While adversarial training is an effective defense for deep convolutional models, it often faces scalability issues with large vision models due to high computational costs. Recent approaches propose robust fine-tuning methods, such as adversarial tuning of low-rank adaptation (LoRA) in large vision models, but they still struggle to match the accuracy of full parameter adversarial fine-tuning. The integration of various defense mechanisms offers a promising approach to enhancing the robustness of large vision models, yet this paradigm remains underexplored. To address this, we propose hyper adversarial tuning (HyperAT), which leverages shared defensive knowledge among different methods to improve model robustness efficiently and effectively simultaneously. Specifically, adversarial tuning of each defense method is formulated as a learning task, and a hypernetwork generates LoRA specific to this defense. Then, a random sampling and tuning strategy is proposed to extract and facilitate the defensive knowledge transfer between different defenses. Finally, diverse LoRAs are merged to enhance the adversarial robustness. Experiments on various datasets and model architectures demonstrate that HyperAT significantly enhances the adversarial robustness of pretrained large vision models without excessive computational overhead, establishing a new state-of-the-art benchmark.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05969",
        "abstract url": "https://arxiv.org/abs/2410.05969",
        "title": "Deep neural network-based detection of counterfeit products from smartphone images",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Counterfeit products such as drugs and vaccines as well as luxury items such as high-fashion handbags, watches, jewelry, garments, and cosmetics, represent significant direct losses of revenue to legitimate manufacturers and vendors, as well as indirect costs to societies at large. We present the world's first purely computer-vision-based system to combat such counterfeiting-one that does not require special security tags or other alterations to the products or modifications to supply chain tracking. Our deep neural network system shows high accuracy on branded garments from our first manufacturer tested (99.71% after 3.06% rejections) using images captured under natural, weakly controlled conditions, such as in retail stores, customs checkpoints, warehouses, and outdoors. Our system, suitably transfer trained on a small number of fake and genuine articles, should find application in additional product categories as well, for example fashion accessories, perfume boxes, medicines, and more.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05970",
        "abstract url": "https://arxiv.org/abs/2410.05970",
        "title": "PDF-WuKong: A Large Multimodal Model for Efficient Long PDF Reading with End-to-End Sparse Sampling",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Document understanding is a challenging task to process and comprehend large amounts of textual and visual information. Recent advances in Large Language Models (LLMs) have significantly improved the performance of this task. However, existing methods typically focus on either plain text or a limited number of document images, struggling to handle long PDF documents with interleaved text and images, especially in academic papers. In this paper, we introduce PDF-WuKong, a multimodal large language model (MLLM) which is designed to enhance multimodal question-answering (QA) for long PDF documents. PDF-WuKong incorporates a sparse sampler that operates on both text and image representations, significantly improving the efficiency and capability of the MLLM. The sparse sampler is integrated with the MLLM's image encoder and selects the paragraphs or diagrams most pertinent to user queries for processing by the language model. To effectively train and evaluate our model, we construct PaperPDF, a dataset consisting of a broad collection of academic papers sourced from arXiv, multiple strategies are proposed to generate automatically 1M QA pairs along with their corresponding evidence sources. Experimental results demonstrate the superiority and high efficiency of our approach over other models on the task of long multimodal PDF understanding, surpassing proprietary products by an average of 8.6% on F1. Our code and dataset will be released at https://github.com/yh-hust/PDF-Wukong.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05983",
        "abstract url": "https://arxiv.org/abs/2410.05983",
        "title": "Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Retrieval-augmented generation (RAG) empowers large language models (LLMs) to utilize external knowledge sources. The increasing capacity of LLMs to process longer input sequences opens up avenues for providing more retrieved information, to potentially enhance the quality of generated outputs. It is plausible to assume that a larger retrieval set would contain more relevant information (higher recall), that might result in improved performance. However, our empirical findings demonstrate that for many long-context LLMs, the quality of generated output initially improves first, but then subsequently declines as the number of retrieved passages increases. This paper investigates this phenomenon, identifying the detrimental impact of retrieved \"hard negatives\" as a key contributor. To mitigate this and enhance the robustness of long-context LLM-based RAG, we propose both training-free and training-based approaches. We first showcase the effectiveness of retrieval reordering as a simple yet powerful training-free optimization. Furthermore, we explore training-based methods, specifically RAG-specific implicit LLM fine-tuning and RAG-oriented fine-tuning with intermediate reasoning, demonstrating their capacity for substantial performance gains. Finally, we conduct a systematic analysis of design choices for these training-based methods, including data distribution, retriever selection, and training context length.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "34 pages"
    },
    {
        "paper id": "2410.05984",
        "abstract url": "https://arxiv.org/abs/2410.05984",
        "title": "Are Minimal Radial Distortion Solvers Necessary for Relative Pose Estimation?",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Estimating the relative pose between two cameras is a fundamental step in many applications such as Structure-from-Motion. The common approach to relative pose estimation is to apply a minimal solver inside a RANSAC loop. Highly efficient solvers exist for pinhole cameras. Yet, (nearly) all cameras exhibit radial distortion. Not modeling radial distortion leads to (significantly) worse results. However, minimal radial distortion solvers are significantly more complex than pinhole solvers, both in terms of run-time and implementation efforts. This paper compares radial distortion solvers with a simple-to-implement approach that combines an efficient pinhole solver with sampled radial distortion parameters. Extensive experiments on multiple datasets and RANSAC variants show that this simple approach performs similarly or better than the most accurate minimal distortion solvers at faster run-times while being significantly more accurate than faster non-minimal solvers. We clearly show that complex radial distortion solvers are not necessary in practice. Code and benchmark are available at https://github.com/kocurvik/rd.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05986",
        "abstract url": "https://arxiv.org/abs/2410.05986",
        "title": "The USTC-NERCSLIP Systems for the CHiME-8 MMCSG Challenge",
        "rating": "1",
        "keywords": [
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "In the two-person conversation scenario with one wearing smart glasses, transcribing and displaying the speaker's content in real-time is an intriguing application, providing a priori information for subsequent tasks such as translation and comprehension. Meanwhile, multi-modal data captured from the smart glasses is scarce. Therefore, we propose utilizing simulation data with multiple overlap rates and a one-to-one matching training strategy to narrow down the deviation for the model training between real and simulated data. In addition, combining IMU unit data in the model can assist the audio to achieve better real-time speech recognition performance.",
        "subjects": [
            "eess.AS",
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05991",
        "abstract url": "https://arxiv.org/abs/2410.05991",
        "title": "Vector Grimoire: Codebook-based Shape Generation under Raster Image Supervision",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Scalable Vector Graphics (SVG) is a popular format on the web and in the design industry. However, despite the great strides made in generative modeling, SVG has remained underexplored due to the discrete and complex nature of such data. We introduce GRIMOIRE, a text-guided SVG generative model that is comprised of two modules: A Visual Shape Quantizer (VSQ) learns to map raster images onto a discrete codebook by reconstructing them as vector shapes, and an Auto-Regressive Transformer (ART) models the joint probability distribution over shape tokens, positions and textual descriptions, allowing us to generate vector graphics from natural language. Unlike existing models that require direct supervision from SVG data, GRIMOIRE learns shape image patches using only raster image supervision which opens up vector generative modeling to significantly more data. We demonstrate the effectiveness of our method by fitting GRIMOIRE for closed filled shapes on the MNIST and for outline strokes on icon and font data, surpassing previous image-supervised methods in generative quality and vector-supervised approach in flexibility.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05993",
        "abstract url": "https://arxiv.org/abs/2410.05993",
        "title": "Aria: An Open Multimodal Native Mixture-of-Experts Model",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Information comes in diverse modalities. Multimodal native AI models are essential to integrate real-world information and deliver comprehensive understanding. While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill this gap, we introduce Aria, an open multimodal native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. Aria is a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual token and text token, respectively. It outperforms Pixtral-12B and Llama3.2-11B, and is competitive against the best proprietary models on various multimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline, which progressively equips the model with strong capabilities in language understanding, multimodal understanding, long context window, and instruction following. We open-source the model weights along with a codebase that facilitates easy adoptions and adaptations of Aria in real-world applications.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05997",
        "abstract url": "https://arxiv.org/abs/2410.05997",
        "title": "An Eye for an Ear: Zero-shot Audio Description Leveraging an Image Captioner using Audiovisual Distribution Alignment",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Multimodal large language models have fueled progress in image captioning. These models, fine-tuned on vast image datasets, exhibit a deep understanding of semantic concepts. In this work, we show that this ability can be re-purposed for audio captioning, where the joint image-language decoder can be leveraged to describe auditory content associated with image sequences within videos featuring audiovisual content. This can be achieved via multimodal alignment. Yet, this multimodal alignment task is non-trivial due to the inherent disparity between audible and visible elements in real-world videos. Moreover, multimodal representation learning often relies on contrastive learning, facing the challenge of the so-called modality gap which hinders smooth integration between modalities. In this work, we introduce a novel methodology for bridging the audiovisual modality gap by matching the distributions of tokens produced by an audio backbone and those of an image captioner. Our approach aligns the audio token distribution with that of the image tokens, enabling the model to perform zero-shot audio captioning in an unsupervised fashion while keeping the initial image captioning component unaltered. This alignment allows for the use of either audio or audiovisual input by combining or substituting the image encoder with the aligned audio encoder. Our method achieves significantly improved performances in zero-shot audio captioning, compared to existing approaches.",
        "subjects": [
            "eess.AS",
            "cs.CV",
            "cs.LG",
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06001",
        "abstract url": "https://arxiv.org/abs/2410.06001",
        "title": "TapType: Ten-finger text entry on everyday surfaces via Bayesian inference",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Despite the advent of touchscreens, typing on physical keyboards remains most efficient for entering text, because users can leverage all fingers across a full-size keyboard for convenient typing. As users increasingly type on the go, text input on mobile and wearable devices has had to compromise on full-size typing. In this paper, we present TapType, a mobile text entry system for full-size typing on passive surfaces--without an actual keyboard. From the inertial sensors inside a band on either wrist, TapType decodes and relates surface taps to a traditional QWERTY keyboard layout. The key novelty of our method is to predict the most likely character sequences by fusing the finger probabilities from our Bayesian neural network classifier with the characters' prior probabilities from an n-gram language model. In our online evaluation, participants on average typed 19 words per minute with a character error rate of 0.6% after 30 minutes of training. Expert typists thereby consistently achieved more than 25 WPM at a similar error rate. We demonstrate applications of TapType in mobile use around smartphones and tablets, as a complement to interaction in situated Mixed Reality outside visual control, and as an eyes-free mobile text input method using an audio feedback-only interface.",
        "subjects": [
            "cs.HC",
            "cs.CV"
        ],
        "comment": "In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems"
    },
    {
        "paper id": "2410.06003",
        "abstract url": "https://arxiv.org/abs/2410.06003",
        "title": "Is the MMI Criterion Necessary for Interpretability? Degenerating Non-causal Features to Plain Noise for Self-Rationalization",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "An important line of research in the field of explainability is to extract a small subset of crucial rationales from the full input. The most widely used criterion for rationale extraction is the maximum mutual information (MMI) criterion. However, in certain datasets, there are spurious features non-causally correlated with the label and also get high mutual information, complicating the loss landscape of MMI. Although some penalty-based methods have been developed to penalize the spurious features (e.g., invariance penalty, intervention penalty, etc) to help MMI work better, these are merely remedial measures. In the optimization objectives of these methods, spurious features are still distinguished from plain noise, which hinders the discovery of causal rationales. This paper aims to develop a new criterion that treats spurious features as plain noise, allowing the model to work on datasets rich in spurious features as if it were working on clean datasets, thereby making rationale extraction easier. We theoretically observe that removing either plain noise or spurious features from the input does not alter the conditional distribution of the remaining components relative to the task label. However, significant changes in the conditional distribution occur only when causal features are eliminated. Based on this discovery, the paper proposes a criterion for \\textbf{M}aximizing the \\textbf{R}emaining \\textbf{D}iscrepancy (MRD). Experiments on six widely used datasets show that our MRD criterion improves rationale quality (measured by the overlap with human-annotated rationales) by up to $10.4\\%$ as compared to several recent competitive MMI variants. Code: \\url{https://github.com/jugechengzi/Rationalization-MRD}.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted at NeurIPS 2024. arXiv admin note: text overlap with arXiv:2309.13391"
    },
    {
        "paper id": "2410.06019",
        "abstract url": "https://arxiv.org/abs/2410.06019",
        "title": "Unveiling Transformer Perception by Exploring Input Manifolds",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces a general method for the exploration of equivalence classes in the input space of Transformer models. The proposed approach is based on sound mathematical theory which describes the internal layers of a Transformer architecture as sequential deformations of the input manifold. Using eigendecomposition of the pullback of the distance metric defined on the output space through the Jacobian of the model, we are able to reconstruct equivalence classes in the input space and navigate across them. We illustrate how this method can be used as a powerful tool for investigating how a Transformer sees the input space, facilitating local and task-agnostic explainability in Computer Vision and Natural Language Processing tasks.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "11 pages, 4 figures"
    },
    {
        "paper id": "2410.06020",
        "abstract url": "https://arxiv.org/abs/2410.06020",
        "title": "QT-DoG: Quantization-aware Training for Domain Generalization",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Domain Generalization (DG) aims to train models that perform well not only on the training (source) domains but also on novel, unseen target data distributions. A key challenge in DG is preventing overfitting to source domains, which can be mitigated by finding flatter minima in the loss landscape. In this work, we propose Quantization-aware Training for Domain Generalization (QT-DoG) and demonstrate that weight quantization effectively leads to flatter minima in the loss landscape, thereby enhancing domain generalization. Unlike traditional quantization methods focused on model compression, QT-DoG exploits quantization as an implicit regularizer by inducing noise in model weights, guiding the optimization process toward flatter minima that are less sensitive to perturbations and overfitting. We provide both theoretical insights and empirical evidence demonstrating that quantization inherently encourages flatter minima, leading to better generalization across domains. Moreover, with the benefit of reducing the model size through quantization, we demonstrate that an ensemble of multiple quantized models further yields superior accuracy than the state-of-the-art DG approaches with no computational or memory overheads. Our extensive experiments demonstrate that QT-DoG generalizes across various datasets, architectures, and quantization algorithms, and can be combined with other DG methods, establishing its versatility and robustness.",
        "subjects": [
            "cs.LG",
            "cs.CV",
            "cs.RO"
        ],
        "comment": "Code will be released soon"
    },
    {
        "paper id": "2410.06044",
        "abstract url": "https://arxiv.org/abs/2410.06044",
        "title": "HyperDet: Generalizable Detection of Synthesized Images by Generating and Merging A Mixture of Hyper LoRAs",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The emergence of diverse generative vision models has recently enabled the synthesis of visually realistic images, underscoring the critical need for effectively detecting these generated images from real photos. Despite advances in this field, existing detection approaches often struggle to accurately identify synthesized images generated by different generative models. In this work, we introduce a novel and generalizable detection framework termed HyperDet, which innovatively captures and integrates shared knowledge from a collection of functionally distinct and lightweight expert detectors. HyperDet leverages a large pretrained vision model to extract general detection features while simultaneously capturing and enhancing task-specific features. To achieve this, HyperDet first groups SRM filters into five distinct groups to efficiently capture varying levels of pixel artifacts based on their different functionality and complexity. Then, HyperDet utilizes a hypernetwork to generate LoRA model weights with distinct embedding parameters. Finally, we merge the LoRA networks to form an efficient model ensemble. Also, we propose a novel objective function that balances the pixel and semantic artifacts effectively. Extensive experiments on the UnivFD and Fake2M datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance. Moreover, our work paves a new way to establish generalizable domain-specific fake image detectors based on pretrained large vision models.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06045",
        "abstract url": "https://arxiv.org/abs/2410.06045",
        "title": "Extracting Finite State Machines from Transformers",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Fueled by the popularity of the transformer architecture in deep learning, several works have investigated what formal languages a transformer can learn. Nonetheless, existing results remain hard to compare and a fine-grained understanding of the trainability of transformers on regular languages is still lacking. We investigate transformers trained on regular languages from a mechanistic interpretability perspective. Using an extension of the $L^*$ algorithm, we extract Moore machines from transformers. We empirically find tighter lower bounds on the trainability of transformers, when a finite number of symbols determine the state. Additionally, our mechanistic insight allows us to characterise the regular languages a one-layer transformer can learn with good length generalisation. However, we also identify failure cases where the determining symbols get misrecognised due to saturation of the attention mechanism.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Accepted for Workshop on Mechanistic Interpretability ICML 2024"
    },
    {
        "paper id": "2410.06097",
        "abstract url": "https://arxiv.org/abs/2410.06097",
        "title": "Decoding Decoded: Understanding Hyperparameter Effects in Open-Ended Text Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Decoding strategies for large language models (LLMs) are a critical but often underexplored aspect of text generation tasks. Since LLMs produce probability distributions over the entire vocabulary, various decoding methods have been developed to transform these probabilities into coherent and fluent text, each with its own set of hyperparameters. In this study, we present a large-scale, comprehensive analysis of how hyperparameter selection affects text quality in open-ended text generation across multiple LLMs, datasets, and evaluation metrics. Through an extensive sensitivity analysis, we provide practical guidelines for hyperparameter tuning and demonstrate the substantial influence of these choices on text quality. Using three established datasets, spanning factual domains (e.g., news) and creative domains (e.g., fiction), we show that hyperparameter tuning significantly impacts generation quality, though its effects vary across models and tasks. We offer in-depth insights into these effects, supported by both human evaluations and a synthesis of widely-used automatic evaluation metrics.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06104",
        "abstract url": "https://arxiv.org/abs/2410.06104",
        "title": "RefineStyle: Dynamic Convolution Refinement for StyleGAN",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In StyleGAN, convolution kernels are shaped by both static parameters shared across images and dynamic modulation factors $w^+\\in\\mathcal{W}^+$ specific to each image. Therefore, $\\mathcal{W}^+$ space is often used for image inversion and editing. However, pre-trained model struggles with synthesizing out-of-domain images due to the limited capabilities of $\\mathcal{W}^+$ and its resultant kernels, necessitating full fine-tuning or adaptation through a complex hypernetwork. This paper proposes an efficient refining strategy for dynamic kernels. The key idea is to modify kernels by low-rank residuals, learned from input image or domain guidance. These residuals are generated by matrix multiplication between two sets of tokens with the same number, which controls the complexity. We validate the refining scheme in image inversion and domain adaptation. In the former task, we design grouped transformer blocks to learn these token sets by one- or two-stage training. In the latter task, token sets are directly optimized to support synthesis in the target domain while preserving original content. Extensive experiments show that our method achieves low distortions for image inversion and high quality for out-of-domain editing.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by PRCV2024"
    },
    {
        "paper id": "2410.06109",
        "abstract url": "https://arxiv.org/abs/2410.06109",
        "title": "Continuous Contrastive Learning for Long-Tailed Semi-Supervised Recognition",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Long-tailed semi-supervised learning poses a significant challenge in training models with limited labeled data exhibiting a long-tailed label distribution. Current state-of-the-art LTSSL approaches heavily rely on high-quality pseudo-labels for large-scale unlabeled data. However, these methods often neglect the impact of representations learned by the neural network and struggle with real-world unlabeled data, which typically follows a different distribution than labeled data. This paper introduces a novel probabilistic framework that unifies various recent proposals in long-tail learning. Our framework derives the class-balanced contrastive loss through Gaussian kernel density estimation. We introduce a continuous contrastive learning method, CCL, extending our framework to unlabeled data using reliable and smoothed pseudo-labels. By progressively estimating the underlying label distribution and optimizing its alignment with model predictions, we tackle the diverse distribution of unlabeled data in real-world scenarios. Extensive experiments across multiple datasets with varying unlabeled data distributions demonstrate that CCL consistently outperforms prior state-of-the-art methods, achieving over 4% improvement on the ImageNet-127 dataset. Our source code is available at https://github.com/zhouzihao11/CCL",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted at NeurIPS 2024"
    },
    {
        "paper id": "2410.06118",
        "abstract url": "https://arxiv.org/abs/2410.06118",
        "title": "Optimizing the Training Schedule of Multilingual NMT using Reinforcement Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Multilingual NMT is a viable solution for translating low-resource languages (LRLs) when data from high-resource languages (HRLs) from the same language family is available. However, the training schedule, i.e. the order of presentation of languages, has an impact on the quality of such systems. Here, in a many-to-one translation setting, we propose to apply two algorithms that use reinforcement learning to optimize the training schedule of NMT: (1) Teacher-Student Curriculum Learning and (2) Deep Q Network. The former uses an exponentially smoothed estimate of the returns of each action based on the loss on monolingual or multilingual development subsets, while the latter estimates rewards using an additional neural network trained from the history of actions selected in different states of the system, together with the rewards received. On a 8-to-1 translation dataset with LRLs and HRLs, our second method improves BLEU and COMET scores with respect to both random selection of monolingual batches and shuffled multilingual batches, by adjusting the number of presentations of LRL vs. HRL batches.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06124",
        "abstract url": "https://arxiv.org/abs/2410.06124",
        "title": "Learning AND-OR Templates for Professional Photograph Parsing and Guidance",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Since the development of photography art, many so-called \"templates\" have been formed, namely visual styles summarized from a series of themed and stylized photography works. In this paper, we propose to analysize and and summarize these 'templates' in photography by learning composite templates of photography images. We present a framework for learning a hierarchical reconfigurable image template from photography images to learn and characterize the \"templates\" used in these photography images. Using this method, we measured the artistic quality of photography on the photos and conducted photography guidance. In addition, we also utilized the \"templates\" for guidance in several image generation tasks. Experimental results show that the learned templates can well describe the photography techniques and styles, whereas the proposed approach can assess the quality of photography images as human being does.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06134",
        "abstract url": "https://arxiv.org/abs/2410.06134",
        "title": "Adaptive Label Smoothing for Out-of-Distribution Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Out-of-distribution (OOD) detection, which aims to distinguish unknown classes from known classes, has received increasing attention recently. A main challenge within is the unavailable of samples from the unknown classes in the training process, and an effective strategy is to improve the performance for known classes. Using beneficial strategies such as data augmentation and longer training is thus a way to improve OOD detection. However, label smoothing, an effective method for classifying known classes, degrades the performance of OOD detection, and this phenomenon is under exploration. In this paper, we first analyze that the limited and predefined learning target in label smoothing results in the smaller maximal probability and logit, which further leads to worse OOD detection performance. To mitigate this issue, we then propose a novel regularization method, called adaptive label smoothing (ALS), and the core is to push the non-true classes to have same probabilities whereas the maximal probability is neither fixed nor limited. Extensive experimental results in six datasets with two backbones suggest that ALS contributes to classifying known samples and discerning unknown samples with clear margins. Our code will be available to the public.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06153",
        "abstract url": "https://arxiv.org/abs/2410.06153",
        "title": "AgentSquare: Automatic LLM Agent Search in Modular Design Space",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in Large Language Models (LLMs) have led to a rapid growth of agentic systems capable of handling a wide range of complex tasks. However, current research largely relies on manual, task-specific design, limiting their adaptability to novel tasks. In this paper, we introduce a new research problem: Modularized LLM Agent Search (MoLAS). We propose a modular design space that abstracts existing LLM agent designs into four fundamental modules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory. Building on this design space, we present a novel LLM agent search framework called AgentSquare, which introduces two core mechanisms, i.e., module evolution and recombination, to efficiently search for optimized LLM agents. To further accelerate the process, we design a performance predictor that uses in-context surrogate models to skip unpromising agent designs. Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Moreover, AgentSquare can generate interpretable design insights, enabling a deeper understanding of agentic architecture and its impact on task performance. We believe that the modular design space and AgentSquare search framework offer a platform for fully exploiting the potential of prior successful designs and consolidating the collective efforts of research community. Code repo is available at https://github.com/tsinghua-fib-lab/AgentSquare.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "26 pages"
    },
    {
        "paper id": "2410.06169",
        "abstract url": "https://arxiv.org/abs/2410.06169",
        "title": "Quadratic Is Not What You Need For Multimodal Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In the past year, the capabilities of Multimodal Large Language Models (MLLMs) have significantly improved across various aspects. However, constrained by the quadratic growth of computation in LLMs as the number of tokens increases, efficiency has become a bottleneck for further scaling MLLMs. Although recent efforts have been made to prune visual tokens or use more lightweight LLMs to reduce computation, the problem of quadratic growth in computation with the increase of visual tokens still persists. To address this, we propose a novel approach: instead of reducing the input visual tokens for LLMs, we focus on pruning vision-related computations within the LLMs. After pruning, the computation growth in the LLM is no longer quadratic with the increase of visual tokens, but linear. Surprisingly, we found that after applying such extensive pruning, the capabilities of MLLMs are comparable with the original one and even superior on some benchmarks with only 25% of the computation. This finding opens up the possibility for MLLMs to incorporate much denser visual tokens. Additionally, based on this finding, we further analyzed some architectural design deficiencies in existing MLLMs and proposed promising improvements. To the best of our knowledge, this is the first study to investigate the computational redundancy in the LLM's vision component of MLLMs. Code and checkpoints will be released soon.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06172",
        "abstract url": "https://arxiv.org/abs/2410.06172",
        "title": "Multimodal Situational Safety",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating impressive capabilities as multimodal assistants that interact with both humans and their environments. However, this increased sophistication introduces significant safety concerns. In this paper, we present the first evaluation and analysis of a novel safety challenge termed Multimodal Situational Safety, which explores how safety considerations vary based on the specific situation in which the user or agent is engaged. We argue that for an MLLM to respond safely, whether through language or action, it often needs to assess the safety implications of a language query within its corresponding visual context. To evaluate this capability, we develop the Multimodal Situational Safety benchmark (MSSBench) to assess the situational safety performance of current MLLMs. The dataset comprises 1,820 language query-image pairs, half of which the image context is safe, and the other half is unsafe. We also develop an evaluation framework that analyzes key safety aspects, including explicit safety reasoning, visual understanding, and, crucially, situational safety reasoning. Our findings reveal that current MLLMs struggle with this nuanced safety problem in the instruction-following setting and struggle to tackle these situational safety challenges all at once, highlighting a key area for future research. Furthermore, we develop multi-agent pipelines to coordinately solve safety challenges, which shows consistent improvement in safety over the original MLLM response. Code and data: mssbench.github.io.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06173",
        "abstract url": "https://arxiv.org/abs/2410.06173",
        "title": "Manual Verbalizer Enrichment for Few-Shot Text Classification",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "With the continuous development of pre-trained language models, prompt-based training becomes a well-adopted paradigm that drastically improves the exploitation of models for many natural language processing tasks. Prompting also shows great performance compared to traditional fine-tuning when adapted to zero-shot or few-shot scenarios where the number of annotated data is limited. In this framework, the role of verbalizers is essential, as an interpretation from masked word distributions into output predictions. In this work, we propose \\acrshort{mave}, an approach for verbalizer construction by enrichment of class labels using neighborhood relation in the embedding space of words for the text classification task. In addition, we elaborate a benchmarking procedure to evaluate typical baselines of verbalizers for document classification in few-shot learning contexts. Our model achieves state-of-the-art results while using significantly fewer resources. We show that our approach is particularly effective in cases with extremely limited supervision data.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06195",
        "abstract url": "https://arxiv.org/abs/2410.06195",
        "title": "Entering Real Social World! Benchmarking the Theory of Mind and Socialization Capabilities of LLMs from a First-person Perspective",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In the social world, humans possess the capability to infer and reason about others mental states (such as emotions, beliefs, and intentions), known as the Theory of Mind (ToM). Simultaneously, humans own mental states evolve in response to social situations, a capability we refer to as socialization. Together, these capabilities form the foundation of human social interaction. In the era of artificial intelligence (AI), especially with the development of large language models (LLMs), we raise an intriguing question: How do LLMs perform in terms of ToM and socialization capabilities? And more broadly, can these AI models truly enter and navigate the real social world? Existing research evaluating LLMs ToM and socialization capabilities by positioning LLMs as passive observers from a third person perspective, rather than as active participants. However, compared to the third-person perspective, observing and understanding the world from an egocentric first person perspective is a natural approach for both humans and AI agents. The ToM and socialization capabilities of LLMs from a first person perspective, a crucial attribute for advancing embodied AI agents, remain unexplored. To answer the aforementioned questions and bridge the research gap, we introduce EgoSocialArena, a novel framework designed to evaluate and investigate the ToM and socialization capabilities of LLMs from a first person perspective. It encompasses two evaluation environments: static environment and interactive environment, with seven scenarios: Daily Life, Counterfactual, New World, Blackjack, Number Guessing, and Limit Texas Hold em, totaling 2,195 data entries. With EgoSocialArena, we have conducted a comprehensive evaluation of nine advanced LLMs and observed some key insights regarding the future development of LLMs as well as the capabilities levels of the most advanced LLMs currently available.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "15 pages, 5 figures"
    },
    {
        "paper id": "2410.06203",
        "abstract url": "https://arxiv.org/abs/2410.06203",
        "title": "Integrating Planning into Single-Turn Long-Form Text Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Generating high-quality, in-depth textual documents, such as academic papers, news articles, Wikipedia entries, and books, remains a significant challenge for Large Language Models (LLMs). In this paper, we propose to use planning to generate long form content. To achieve our goal, we generate intermediate steps via an auxiliary task that teaches the LLM to plan, reason and structure before generating the final text. Our main novelty lies in a single auxiliary task that does not require multiple rounds of prompting or planning. To overcome the scarcity of training data for these intermediate steps, we leverage LLMs to generate synthetic intermediate writing data such as outlines, key information and summaries from existing full articles. Our experiments demonstrate on two datasets from different domains, namely the scientific news dataset SciNews and Wikipedia datasets in KILT-Wiki and FreshWiki, that LLMs fine-tuned with the auxiliary task generate higher quality documents. We observed +2.5% improvement in ROUGE-Lsum, and a strong 3.60 overall win/loss ratio via human SxS evaluation, with clear wins in organization, relevance, and verifiability.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06205",
        "abstract url": "https://arxiv.org/abs/2410.06205",
        "title": "Round and Round We Go! What makes Rotary Positional Encodings useful?",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Positional Encodings (PEs) are a critical component of Transformer-based Large Language Models (LLMs), providing the attention mechanism with important sequence-position information. One of the most popular types of encoding used today in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries and keys based on their relative distance. A common belief is that RoPE is useful because it helps to decay token dependency as relative distance increases. In this work, we argue that this is unlikely to be the core reason. We study the internals of a trained Gemma 7B model to understand how RoPE is being used at a mechanical level. We find that Gemma learns to use RoPE to construct robust \"positional\" attention patterns by exploiting the highest frequencies. We also find that, in general, Gemma greatly prefers to use the lowest frequencies of RoPE, which we suspect are used to carry semantic information. We mathematically prove interesting behaviours of RoPE and conduct experiments to verify our findings, proposing a modification of RoPE that fixes some highlighted issues and improves performance. We believe that this work represents an interesting step in better understanding PEs in LLMs, which we believe holds crucial value for scaling LLMs to large sizes and context lengths.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06215",
        "abstract url": "https://arxiv.org/abs/2410.06215",
        "title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Recent approaches using LLMs as annotators reduce human effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents - or teachers - is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid and scalable testing for such agents and their modules, we introduce DataEnvGym, a testbed of teacher environments for data generation agents. DataEnvGym frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides student feedback. The agent's goal is to improve student performance. Students are iteratively trained and evaluated on generated data, with their feedback (in the form of errors or weak skills) being reported to the agent after each iteration. DataEnvGym includes multiple teacher environment instantiations across 3 levels of structure in the state representation and action space. More structured environments are based on inferred skills and offer more interpretability and curriculum control. We support 3 diverse tasks (math, code, and VQA) and test multiple students and teachers. Example agents in our teaching environments can iteratively improve students across tasks and settings. Moreover, we show that environments teach different skill levels and test variants of key modules, pointing to future work in improving data generation agents, engines, and feedback mechanisms.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Project Page: https://DataEnvGym.github.io"
    },
    {
        "paper id": "2410.06236",
        "abstract url": "https://arxiv.org/abs/2410.06236",
        "title": "SD-$\u03c0$XL: Generating Low-Resolution Quantized Imagery via Score Distillation",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Low-resolution quantized imagery, such as pixel art, is seeing a revival in modern applications ranging from video game graphics to digital design and fabrication, where creativity is often bound by a limited palette of elemental units. Despite their growing popularity, the automated generation of quantized images from raw inputs remains a significant challenge, often necessitating intensive manual input. We introduce SD-$\u03c0$XL, an approach for producing quantized images that employs score distillation sampling in conjunction with a differentiable image generator. Our method enables users to input a prompt and optionally an image for spatial conditioning, set any desired output size $H \\times W$, and choose a palette of $n$ colors or elements. Each color corresponds to a distinct class for our generator, which operates on an $H \\times W \\times n$ tensor. We adopt a softmax approach, computing a convex sum of elements, thus rendering the process differentiable and amenable to backpropagation. We show that employing Gumbel-softmax reparameterization allows for crisp pixel art effects. Unique to our method is the ability to transform input images into low-resolution, quantized versions while retaining their key semantic features. Our experiments validate SD-$\u03c0$XL's performance in creating visually pleasing and faithful representations, consistently outperforming the current state-of-the-art. Furthermore, we showcase SD-$\u03c0$XL's practical utility in fabrication through its applications in interlocking brick mosaic, beading and embroidery design.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "To be presented at SIGGRAPH Asia 2024 (conference track). Main paper is 8 pages + 2 figure-only pages + references. Supplementary is 11 pages + references"
    },
    {
        "paper id": "2410.06270",
        "abstract url": "https://arxiv.org/abs/2410.06270",
        "title": "MC-MoE: Mixture Compressor for Mixture-of-Experts LLMs Gains More",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Mixture-of-Experts large language models (MoE-LLMs) marks a significant step forward of language models, however, they encounter two critical challenges in practice: 1) expert parameters lead to considerable memory consumption and loading latency; and 2) the current activated experts are redundant, as many tokens may only require a single expert. Motivated by these issues, we investigate the MoE-LLMs and make two key observations: a) different experts exhibit varying behaviors on activation reconstruction error, routing scores, and activated frequencies, highlighting their differing importance, and b) not all tokens are equally important -- only a small subset is critical. Building on these insights, we propose MC-MoE, a training-free Mixture-Compressor for MoE-LLMs, which leverages the significance of both experts and tokens to achieve an extreme compression. First, to mitigate storage and loading overheads, we introduce Pre-Loading Mixed-Precision Quantization, which formulates the adaptive bit-width allocation as a Linear Programming problem, where the objective function balances multi-factors reflecting the importance of each expert. Additionally, we develop Online Dynamic Pruning, which identifies important tokens to retain and dynamically select activated experts for other tokens during inference to optimize efficiency while maintaining performance. Our MC-MoE integrates static quantization and dynamic pruning to collaboratively achieve extreme compression for MoE-LLMs with less accuracy loss, ensuring an optimal trade-off between performance and efficiency. Extensive experiments confirm the effectiveness of our approach. For instance, at 2.54 bits, MC-MoE compresses 76.6% of the model, with only a 3.8% average accuracy loss. During dynamic inference, we further reduce activated parameters by 15%, with a performance drop of less than 0.6%.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2410.06271",
        "abstract url": "https://arxiv.org/abs/2410.06271",
        "title": "Probing the Robustness of Theory of Mind in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "With the success of ChatGPT and other similarly sized SotA LLMs, claims of emergent human like social reasoning capabilities, especially Theory of Mind (ToM), in these models have appeared in the scientific literature. On the one hand those ToM-capabilities have been successfully tested using tasks styled similar to those used in psychology (Kosinski, 2023). On the other hand, follow up studies showed that those capabilities vanished when the tasks were slightly altered (Ullman, 2023). In this work we introduce a novel dataset of 68 tasks for probing ToM in LLMs, including potentially challenging variations which are assigned to 10 complexity classes. This way it is providing novel insights into the challenges LLMs face with those task variations. We evaluate the ToM performance of four SotA open source LLMs on our dataset and the dataset introduced by (Kosinski, 2023). The overall low goal accuracy across all evaluated models indicates only a limited degree of ToM capabilities. The LLMs' performance on simple complexity class tasks from both datasets are similar. Whereas we find a consistent tendency in all tested LLMs to perform poorly on tasks that require the realization that an agent has knowledge of automatic state changes in its environment, even when those are spelled out to the model. For task complications that change the relationship between objects by replacing prepositions, we notice a performance drop in all models, with the strongest impact on the mixture-of-experts model. With our dataset of tasks grouped by complexity we offer directions for further research on how to stabilize and advance ToM capabilities in LLM.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06277",
        "abstract url": "https://arxiv.org/abs/2410.06277",
        "title": "Is Pontryagin's Maximum Principle all you need? Solving optimal control problems with PMP-inspired neural networks",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Calculus of Variations is the mathematics of functional optimization, i.e., when the solutions are functions over a time interval. This is particularly important when the time interval is unknown like in minimum-time control problems, so that forward in time solutions are not possible. Calculus of Variations offers a robust framework for learning optimal control and inference. How can this framework be leveraged to design neural networks to solve challenges in control and inference? We propose the Pontryagin's Maximum Principle Neural Network (PMP-net) that is tailored to estimate control and inference solutions, in accordance with the necessary conditions outlined by Pontryagin's Maximum Principle. We assess PMP-net on two classic optimal control and inference problems: optimal linear filtering and minimum-time control. Our findings indicate that PMP-net can be effectively trained in an unsupervised manner to solve these problems without the need for ground-truth data, successfully deriving the classical \"Kalman filter\" and \"bang-bang\" control solution. This establishes a new approach for addressing general, possibly yet unsolved, optimal control problems.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "math.OC"
        ],
        "comment": "16 pages, 5 figures, under review at ICLR 2025"
    },
    {
        "paper id": "2410.06287",
        "abstract url": "https://arxiv.org/abs/2410.06287",
        "title": "Non-Halting Queries: Exploiting Fixed Points in LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We introduce a new vulnerability that exploits fixed points in autoregressive models and use it to craft queries that never halt, i.e. an LLM output that does not terminate. More precisely, for what we call non-halting queries, the LLM never samples the end-of-string token (<eos>). We rigorously analyze the conditions under which the non-halting anomaly presents itself. In particular, at temperature zero, we prove that if a repeating (cyclic) sequence of tokens is observed at the output beyond the context size, then the LLM does not halt. We demonstrate the non-halting anomaly in a number of experiments performed in base (unaligned) models where repeating tokens immediately lead to a non-halting cyclic behavior as predicted by the analysis. Further, we develop a simple recipe that takes the same fixed points observed in the base model and creates a prompt structure to target aligned models. We study the recipe behavior in bypassing alignment in a number of LLMs including GPT-4o, llama-3-8b-instruct, and gemma-2-9b-it where all models are forced into a non-halting state. Further, we demonstrate the recipe's success in sending most major models released over the past year into a non-halting state with the same simple prompt even at higher temperatures. Further, we study direct inversion based techniques to craft new short prompts to induce the non-halting state. Our experiments with the gradient search based inversion technique ARCA show that non-halting is prevalent across models and may be easily induced with a few input tokens. While its impact on the reliability of hosted systems can be mitigated by configuring a hard maximum token limit in the sampler, the non-halting anomaly still manages to break alignment. This underlines the need for further studies and stronger forms of alignment against non-halting anomalies.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06293",
        "abstract url": "https://arxiv.org/abs/2410.06293",
        "title": "Accelerated Preference Optimization for Large Language Model Alignment",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal tool for aligning large language models (LLMs) with human preferences. Direct Preference Optimization (DPO), one of the most popular approaches, formulates RLHF as a policy optimization problem without explicitly estimating the reward function. It overcomes the stability and efficiency issues of two-step approaches, which typically involve first estimating the reward function and then optimizing the policy via proximal policy optimization (PPO). Since RLHF is essentially an optimization problem, and it is well-known that momentum techniques can accelerate optimization both theoretically and empirically, a natural question arises: Can RLHF be accelerated by momentum? This paper answers this question in the affirmative. In detail, we first show that the iterative preference optimization method can be viewed as a proximal point method. Based on this observation, we propose a general Accelerated Preference Optimization (APO) framework, which unifies many existing preference optimization algorithms and employs Nesterov's momentum technique to speed up the alignment of LLMs. Theoretically, we demonstrate that APO can achieve a faster convergence rate than the standard iterative preference optimization methods, including DPO and Self-Play Preference Optimization (SPPO). Empirically, we show the superiority of APO over DPO, iterative DPO, and other strong baselines for RLHF on the AlpacaEval 2.0 benchmark.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "44 pages, 10 tables"
    },
    {
        "paper id": "2410.06300",
        "abstract url": "https://arxiv.org/abs/2410.06300",
        "title": "Amortized SHAP values via sparse Fourier function approximation",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "SHAP values are a popular local feature-attribution method widely used in interpretable and explainable AI. We tackle the problem of efficiently computing these values. We cover both the model-agnostic (black-box) setting, where one only has query access to the model and also the case of (ensembles of) trees where one has access to the structure of the tree. For both the black-box and the tree setting we propose a two-stage approach for estimating SHAP values. Our algorithm's first step harnesses recent results showing that many real-world predictors have a spectral bias that allows us to either exactly represent (in the case of ensembles of decision trees), or efficiently approximate them (in the case of neural networks) using a compact Fourier representation. In the second step of the algorithm, we use the Fourier representation to exactly compute SHAP values. The second step is computationally very cheap because firstly, the representation is compact and secondly, we prove that there exists a closed-form expression for SHAP values for the Fourier basis functions. Furthermore, the expression we derive effectively linearizes the computation into a simple summation and is amenable to parallelization on multiple cores or a GPU. Since the function approximation (first step) is only done once, it allows us to produce Shapley values in an amortized way. We show speedups compared to relevant baseline methods equal levels of accuracy for both the tree and black-box settings. Moreover, this approach introduces a reliable and fine-grained continuous trade-off between computation and accuracy through the sparsity of the Fourier approximation, a feature previously unavailable in all black-box methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Submitted as a conference paper to ICLR 2025"
    },
    {
        "paper id": "2410.06304",
        "abstract url": "https://arxiv.org/abs/2410.06304",
        "title": "Fine-grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Hallucinations in large language models (LLMs) pose significant challenges in tasks requiring complex multi-step reasoning, such as mathematical problem-solving. Existing approaches primarily detect the presence of hallucinations but lack a nuanced understanding of their types and manifestations. In this paper, we first introduce a comprehensive taxonomy that categorizes the common hallucinations in mathematical reasoning task into six types: fabrication, factual inconsistency, context inconsistency, instruction inconsistency, logical inconsistency, and logical error. We then propose FG-PRM (Fine-Grained Process Reward Model), an augmented model designed to detect and mitigate hallucinations in a fine-grained, step-level manner. To address the limitations of manually labeling training data, we propose an automated method for generating fine-grained hallucination data using LLMs. By injecting hallucinations into reasoning steps of correct solutions, we create a diverse and balanced synthetic dataset for training FG-PRM, which consists of six specialized Process Reward Models (PRMs), each tailored to detect a specific hallucination type. Our FG-PRM demonstrates superior performance across two key tasks: 1) Fine-grained hallucination detection: classifying hallucination types for each reasoning step; and 2) Verification: ranking multiple LLM-generated outputs to select the most accurate solution, mitigating reasoning hallucinations. Our experiments show that FG-PRM outperforms ChatGPT-3.5 and Claude-3 on fine-grained hallucination detection and substantially boosts the performance of LLMs on GSM8K and MATH benchmarks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06306",
        "abstract url": "https://arxiv.org/abs/2410.06306",
        "title": "Benchmarking of a new data splitting method on volcanic eruption data",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, a novel method for data splitting is presented: an iterative procedure divides the input dataset of volcanic eruption, chosen as the proposed use case, into two parts using a dissimilarity index calculated on the cumulative histograms of these two parts. The Cumulative Histogram Dissimilarity (CHD) index is introduced as part of the design. Based on the obtained results the proposed model in this case, compared to both Random splitting and K-means implemented over different configurations, achieves the best performance, with a slightly higher number of epochs. However, this demonstrates that the model can learn more deeply from the input dataset, which is attributable to the quality of the splitting. In fact, each model was trained with early stopping, suitable in case of overfitting, and the higher number of epochs in the proposed method demonstrates that early stopping did not detect overfitting, and consequently, the learning was optimal.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "To be sumbitted to IEEE IGARSS 2025"
    },
    {
        "paper id": "2410.06314",
        "abstract url": "https://arxiv.org/abs/2410.06314",
        "title": "Temporal Image Caption Retrieval Competition -- Description and Results",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Multimodal models, which combine visual and textual information, have recently gained significant recognition. This paper addresses the multimodal challenge of Text-Image retrieval and introduces a novel task that extends the modalities to include temporal data. The Temporal Image Caption Retrieval Competition (TICRC) presented in this paper is based on the Chronicling America and Challenging America projects, which offer access to an extensive collection of digitized historic American newspapers spanning 274 years. In addition to the competition results, we provide an analysis of the delivered dataset and the process of its creation.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06327",
        "abstract url": "https://arxiv.org/abs/2410.06327",
        "title": "Towards a GENEA Leaderboard -- an Extended, Living Benchmark for Evaluating and Advancing Conversational Motion Synthesis",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Current evaluation practices in speech-driven gesture generation lack standardisation and focus on aspects that are easy to measure over aspects that actually matter. This leads to a situation where it is impossible to know what is the state of the art, or to know which method works better for which purpose when comparing two publications. In this position paper, we review and give details on issues with existing gesture-generation evaluation, and present a novel proposal for remedying them. Specifically, we announce an upcoming living leaderboard to benchmark progress in conversational motion synthesis. Unlike earlier gesture-generation challenges, the leaderboard will be updated with large-scale user studies of new gesture-generation systems multiple times per year, and systems on the leaderboard can be submitted to any publication venue that their authors prefer. By evolving the leaderboard evaluation data and tasks over time, the effort can keep driving progress towards the most important end goals identified by the community. We actively seek community involvement across the entire evaluation pipeline: from data and tasks for the evaluation, via tooling, to the systems evaluated. In other words, our proposal will not only make it easier for researchers to perform good evaluations, but their collective input and contributions will also help drive the future of gesture-generation research.",
        "subjects": [
            "cs.HC",
            "cs.CV",
            "cs.GR",
            "cs.LG"
        ],
        "comment": "15 pages, 2 figures, project page: https://genea-workshop.github.io/leaderboard/"
    },
    {
        "paper id": "2410.06373",
        "abstract url": "https://arxiv.org/abs/2410.06373",
        "title": "Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "This paper delves into the interplay between vision backbones and optimizers, unvealing an inter-dependent phenomenon termed \\textit{\\textbf{b}ackbone-\\textbf{o}ptimizer \\textbf{c}oupling \\textbf{b}ias} (BOCB). We observe that canonical CNNs, such as VGG and ResNet, exhibit a marked co-dependency with SGD families, while recent architectures like ViTs and ConvNeXt share a tight coupling with the adaptive learning rate ones. We further show that BOCB can be introduced by both optimizers and certain backbone designs and may significantly impact the pre-training and downstream fine-tuning of vision models. Through in-depth empirical analysis, we summarize takeaways on recommended optimizers and insights into robust vision backbone architectures. We hope this work can inspire the community to question long-held assumptions on backbones and optimizers, stimulate further explorations, and thereby contribute to more robust vision systems. The source code and models are publicly available at https://bocb-ai.github.io/.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Preprint V1. Online project at https://bocb-ai.github.io/"
    },
    {
        "paper id": "2410.06388",
        "abstract url": "https://arxiv.org/abs/2410.06388",
        "title": "Evaluating the Impact of Warning Modalities and False Alarms in Pedestrian Crossing Alert System",
        "rating": "1",
        "keywords": [
            [
                "audio-visual"
            ]
        ],
        "abstract": "With the steadily increasing pedestrian fatalities, pedestrian safety is a growing concern, especially in urban environments. Advanced Driver Assistance Systems (ADAS) have been developed to mitigate road user risks by predicting potential pedestrian crossings and issuing timely driver alerts. However, there is limited understanding of how drivers respond to different modalities of alerts, particularly in the presence of false alarms. In this study, we utilized a full-scale driving simulator to compare the effectiveness of different alert modalities, audio-visual (AV), visual-tactile (VT), and audio-visual-tactile (AVT), in alerting drivers to various pedestrian jaywalking events. Our findings reveal that, compared to no alerts, multimodal alerts significantly increased the number of vehicles stopped for pedestrians and the distance to pedestrians when stopped. However, the false alarms negatively impacted driver trust, with some drivers exhibiting excessive caution, alert fatigue and anxiety, even including one instance where a driver fully stopped when no pedestrian was present.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "15 pages"
    },
    {
        "paper id": "2410.06405",
        "abstract url": "https://arxiv.org/abs/2410.06405",
        "title": "Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the Importance of 2D Representation, Positions, and Objects",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The Abstraction and Reasoning Corpus (ARC) is a popular benchmark focused on visual reasoning in the evaluation of Artificial Intelligence systems. In its original framing, an ARC task requires solving a program synthesis problem over small 2D images using a few input-output training pairs. In this work, we adopt the recently popular data-driven approach to the ARC and ask whether a Vision Transformer (ViT) can learn the implicit mapping, from input image to output image, that underlies the task. We show that a ViT -- otherwise a state-of-the-art model for images -- fails dramatically on most ARC tasks even when trained on one million examples per task. This points to an inherent representational deficiency of the ViT architecture that makes it incapable of uncovering the simple structured mappings underlying the ARC tasks. Building on these insights, we propose ViTARC, a ViT-style architecture that unlocks some of the visual reasoning capabilities required by the ARC. Specifically, we use a pixel-level input representation, design a spatially-aware tokenization scheme, and introduce a novel object-based positional encoding that leverages automatic segmentation, among other enhancements. Our task-specific ViTARC models achieve a test solve rate close to 100% on more than half of the 400 public ARC tasks strictly through supervised learning from input-output grids. This calls attention to the importance of imbuing the powerful (Vision) Transformer with the correct inductive biases for abstract visual reasoning that are critical even when the training data is plentiful and the mapping is noise-free. Hence, ViTARC provides a strong foundation for future research in visual reasoning using transformer-based architectures.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06418",
        "abstract url": "https://arxiv.org/abs/2410.06418",
        "title": "MIRACLE 3D: Memory-efficient Integrated Robust Approach for Continual Learning on Point Clouds via Shape Model construction",
        "rating": "1",
        "keywords": [
            [
                "Memory-efficient"
            ],
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we introduce a novel framework for memory-efficient and privacy-preserving continual learning in 3D object classification. Unlike conventional memory-based approaches in continual learning that require storing numerous exemplars, our method constructs a compact shape model for each class, retaining only the mean shape along with a few key modes of variation. This strategy not only enables the generation of diverse training samples while drastically reducing memory usage but also enhances privacy by eliminating the need to store original data. To further improve model robustness against input variations, an issue common in 3D domains due to the absence of strong backbones and limited training data, we incorporate Gradient Mode Regularization. This technique enhances model stability and broadens classification margins, resulting in accuracy improvements. We validate our approach through extensive experiments on the ModelNet40, ShapeNet, and ScanNet datasets, where we achieve state-of-the-art performance. Notably, our method consumes only 15% of the memory required by competing methods on the ModelNet40 and ShapeNet, while achieving comparable performance on the challenging ScanNet dataset with just 8.5% of the memory. These results underscore the scalability, effectiveness, and privacy-preserving strengths of our framework for 3D object classification.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06424",
        "abstract url": "https://arxiv.org/abs/2410.06424",
        "title": "Restructuring Vector Quantization with the Rotation Trick",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors -- often referred to as the codebook -- and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows around the vector quantization layer rather than through it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Our code is available at https://github.com/cfifty/rotation_trick.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06427",
        "abstract url": "https://arxiv.org/abs/2410.06427",
        "title": "NLP Case Study on Predicting the Before and After of the Ukraine-Russia and Hamas-Israel Conflicts",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We propose a method to predict toxicity and other textual attributes through the use of natural language processing (NLP) techniques for two recent events: the Ukraine-Russia and Hamas-Israel conflicts. This article provides a basis for exploration in future conflicts with hopes to mitigate risk through the analysis of social media before and after a conflict begins. Our work compiles several datasets from Twitter and Reddit for both conflicts in a before and after separation with an aim of predicting a future state of social media for avoidance. More specifically, we show that: (1) there is a noticeable difference in social media discussion leading up to and following a conflict and (2) social media discourse on platforms like Twitter and Reddit is useful in identifying future conflicts before they arise. Our results show that through the use of advanced NLP techniques (both supervised and unsupervised) toxicity and other attributes about language before and after a conflict is predictable with a low error of nearly 1.2 percent for both conflicts.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "The clusters created using topic modeling can be viewed at https://naturallang.com/conflict/conflict.html"
    },
    {
        "paper id": "2410.06428",
        "abstract url": "https://arxiv.org/abs/2410.06428",
        "title": "Stress Detection on Code-Mixed Texts in Dravidian Languages using Machine Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Stress is a common feeling in daily life, but it can affect mental well-being in some situations, the development of robust detection models is imperative. This study introduces a methodical approach to the stress identification in code-mixed texts for Dravidian languages. The challenge encompassed two datasets, targeting Tamil and Telugu languages respectively. This proposal underscores the importance of using uncleaned text as a benchmark to refine future classification methodologies, incorporating diverse preprocessing techniques. Random Forest algorithm was used, featuring three textual representations: TF-IDF, Uni-grams of words, and a composite of (1+2+3)-Grams of characters. The approach achieved a good performance for both linguistic categories, achieving a Macro F1-score of 0.734 in Tamil and 0.727 in Telugu, overpassing results achieved with different complex techniques such as FastText and Transformer models. The results underscore the value of uncleaned data for mental state detection and the challenges classifying code-mixed texts for stress, indicating the potential for improved performance through cleaning data, other preprocessing techniques, or more complex models.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.HC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06465",
        "abstract url": "https://arxiv.org/abs/2410.06465",
        "title": "On the Solution of Linearized Inverse Scattering Problems in Near-Field Microwave Imaging by Operator Inversion and Matched Filtering",
        "rating": "1",
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "Microwave imaging is commonly based on the solution of linearized inverse scattering problems by matched-filtering algorithms, i.e., by applying the adjoint of the forward scattering operator to the observation data. A more rigorous approach is the explicit inversion of the forward scattering operator, which is performed in this work for quasi-monostatic imaging scenarios based on a planar plane-wave representation according to the Weyl-identity and hierarchical acceleration algorithms. The inversion is achieved by a regularized iterative linear system of equations solver, where irregular observations as well as full probe correction are supported. In the spatial image generation low-pass filtering can be considered in order to reduce imaging artifacts. A corresponding spectral back-projection algorithm and a spatial back-projection algorithm together with improved focusing operators are also introduced and the resulting image generation algorithms are analyzed and compared for a variety of examples, comprising both simulated and measured observation data.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "Submitted to IEEE Transactions on Microwave Theory and Techniques on 1st October 2024, and currently under revision"
    },
    {
        "paper id": "2410.06468",
        "abstract url": "https://arxiv.org/abs/2410.06468",
        "title": "Does Spatial Cognition Emerge in Frontier Models?",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Not yet. We present SPACE, a benchmark that systematically evaluates spatial cognition in frontier models. Our benchmark builds on decades of research in cognitive science. It evaluates large-scale mapping abilities that are brought to bear when an organism traverses physical environments, smaller-scale reasoning about object shapes and layouts, and cognitive infrastructure such as spatial attention and memory. For many tasks, we instantiate parallel presentations via text and images, allowing us to benchmark both large language models and large multimodal models. Results suggest that contemporary frontier models fall short of the spatial intelligence of animals, performing near chance level on a number of classic tests of animal cognition.",
        "subjects": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06474",
        "abstract url": "https://arxiv.org/abs/2410.06474",
        "title": "Flipping-based Policy for Chance-Constrained Markov Decision Processes",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Safe reinforcement learning (RL) is a promising approach for many real-world decision-making problems where ensuring safety is a critical necessity. In safe RL research, while expected cumulative safety constraints (ECSCs) are typically the first choices, chance constraints are often more pragmatic for incorporating safety under uncertainties. This paper proposes a \\textit{flipping-based policy} for Chance-Constrained Markov Decision Processes (CCMDPs). The flipping-based policy selects the next action by tossing a potentially distorted coin between two action candidates. The probability of the flip and the two action candidates vary depending on the state. We establish a Bellman equation for CCMDPs and further prove the existence of a flipping-based policy within the optimal solution sets. Since solving the problem with joint chance constraints is challenging in practice, we then prove that joint chance constraints can be approximated into Expected Cumulative Safety Constraints (ECSCs) and that there exists a flipping-based policy in the optimal solution sets for constrained MDPs with ECSCs. As a specific instance of practical implementations, we present a framework for adapting constrained policy optimization to train a flipping-based policy. This framework can be applied to other safe RL algorithms. We demonstrate that the flipping-based policy can improve the performance of the existing safe RL algorithms under the same limits of safety constraints on Safety Gym benchmarks.",
        "subjects": [
            "cs.LG",
            "math.OC"
        ],
        "comment": "Accepted to NeurIPS 2024"
    },
    {
        "paper id": "2410.06513",
        "abstract url": "https://arxiv.org/abs/2410.06513",
        "title": "MotionRL: Align Text-to-Motion Generation to Human Preferences with Multi-Reward Reinforcement Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We introduce MotionRL, the first approach to utilize Multi-Reward Reinforcement Learning (RL) for optimizing text-to-motion generation tasks and aligning them with human preferences. Previous works focused on improving numerical performance metrics on the given datasets, often neglecting the variability and subjectivity of human feedback. In contrast, our novel approach uses reinforcement learning to fine-tune the motion generator based on human preferences prior knowledge of the human perception model, allowing it to generate motions that better align human preferences. In addition, MotionRL introduces a novel multi-objective optimization strategy to approximate Pareto optimality between text adherence, motion quality, and human preferences. Extensive experiments and user studies demonstrate that MotionRL not only allows control over the generated results across different objectives but also significantly enhances performance across these metrics compared to other algorithms.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06520",
        "abstract url": "https://arxiv.org/abs/2410.06520",
        "title": "A Novel LLM-based Two-stage Summarization Approach for Long Dialogues",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Long document summarization poses a significant challenge in natural language processing due to input lengths that exceed the capacity of most state-of-the-art pre-trained language models. This study proposes a hierarchical framework that segments and condenses information from long documents, subsequently fine-tuning the processed text with an abstractive summarization model. Unsupervised topic segmentation methods identify semantically appropriate breakpoints. The condensation stage utilizes an unsupervised generation model to generate condensed data, and our current experiments employ ChatGPT(v3.5). The summarization stage fine-tunes the abstractive summarization model on the condensed data to generate the final results. This framework enables long documents to be processed on models even when the document length exceeds the model's maximum input size. The exclusion of the entire document from the summarization model reduces the time and computational resources required for training, making the framework suitable for contexts with constrained local computational resources.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06527",
        "abstract url": "https://arxiv.org/abs/2410.06527",
        "title": "The Sampling-Gaussian for stereo matching",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The soft-argmax operation is widely adopted in neural network-based stereo matching methods to enable differentiable regression of disparity. However, network trained with soft-argmax is prone to being multimodal due to absence of explicit constraint to the shape of the probability distribution. Previous methods leverages Laplacian distribution and cross-entropy for training but failed to effectively improve the accuracy and even compromises the efficiency of the network. In this paper, we conduct a detailed analysis of the previous distribution-based methods and propose a novel supervision method for stereo matching, Sampling-Gaussian. We sample from the Gaussian distribution for supervision. Moreover, we interpret the training as minimizing the distance in vector space and propose a combined loss of L1 loss and cosine similarity loss. Additionally, we leveraged bilinear interpolation to upsample the cost volume. Our method can be directly applied to any soft-argmax-based stereo matching method without a reduction in efficiency. We have conducted comprehensive experiments to demonstrate the superior performance of our Sampling-Gaussian. The experimental results prove that we have achieved better accuracy on five baseline methods and two datasets. Our method is easy to implement, and the code is available online.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "TL;DR: A novel Gaussian distribution-based supervision method for stereo matching. Implemented with five baseline methods and achieves notable improvement. Main content, 10 pages. conference submission"
    },
    {
        "paper id": "2410.06541",
        "abstract url": "https://arxiv.org/abs/2410.06541",
        "title": "Chip-Tuning: Classify Before Language Models Say",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The rapid development in the performance of large language models (LLMs) is accompanied by the escalation of model size, leading to the increasing cost of model training and inference. Previous research has discovered that certain layers in LLMs exhibit redundancy, and removing these layers brings only marginal loss in model performance. In this paper, we adopt the probing technique to explain the layer redundancy in LLMs and demonstrate that language models can be effectively pruned with probing classifiers. We propose chip-tuning, a simple and effective structured pruning framework specialized for classification problems. Chip-tuning attaches tiny probing classifiers named chips to different layers of LLMs, and trains chips with the backbone model frozen. After selecting a chip for classification, all layers subsequent to the attached layer could be removed with marginal performance loss. Experimental results on various LLMs and datasets demonstrate that chip-tuning significantly outperforms previous state-of-the-art baselines in both accuracy and pruning ratio, achieving a pruning ratio of up to 50%. We also find that chip-tuning could be applied on multimodal models, and could be combined with model finetuning, proving its excellent compatibility.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07245",
        "abstract url": "https://arxiv.org/abs/2410.07245",
        "title": "AAAI Workshop on AI Planning for Cyber-Physical Systems -- CAIPI24",
        "rating": "1",
        "keywords": [
            [
                "cs.AI"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "The workshop 'AI-based Planning for Cyber-Physical Systems', which took place on February 26, 2024, as part of the 38th Annual AAAI Conference on Artificial Intelligence in Vancouver, Canada, brought together researchers to discuss recent advances in AI planning methods for Cyber-Physical Systems (CPS). CPS pose a major challenge due to their complexity and data-intensive nature, which often exceeds the capabilities of traditional planning algorithms. The workshop highlighted new approaches such as neuro-symbolic architectures, large language models (LLMs), deep reinforcement learning and advances in symbolic planning. These techniques are promising when it comes to managing the complexity of CPS and have potential for real-world applications.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "This is the Proceedings of the AAAI Workshop on AI Planning for Cyber-Physical Systems - CAIPI24, which was held in Vancouver, CA, February 26, 2024"
    },
    {
        "paper id": "2410.08037",
        "abstract url": "https://arxiv.org/abs/2410.08037",
        "title": "Composite Learning Units: Generalized Learning Beyond Parameter Updates to Transform LLMs into Adaptive Reasoners",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Human learning thrives on the ability to learn from mistakes, adapt through feedback, and refine understanding-processes often missing in static machine learning models. In this work, we introduce Composite Learning Units (CLUs) designed to transform reasoners, such as Large Language Models (LLMs), into learners capable of generalized, continuous learning without conventional parameter updates while enhancing their reasoning abilities through continual interaction and feedback. CLUs are built on an architecture that allows a reasoning model to maintain and evolve a dynamic knowledge repository: a General Knowledge Space for broad, reusable insights and a Prompt-Specific Knowledge Space for task-specific learning. Through goal-driven interactions, CLUs iteratively refine these knowledge spaces, enabling the system to adapt dynamically to complex tasks, extract nuanced insights, and build upon past experiences autonomously. We demonstrate CLUs' effectiveness through a cryptographic reasoning task, where they continuously evolve their understanding through feedback to uncover hidden transformation rules. While conventional models struggle to grasp underlying logic, CLUs excel by engaging in an iterative, goal-oriented process. Specialized components-handling knowledge retrieval, prompt generation, and feedback analysis-work together within a reinforcing feedback loop. This approach allows CLUs to retain the memory of past failures and successes, adapt autonomously, and apply sophisticated reasoning effectively, continually learning from mistakes while also building on breakthroughs.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05690",
        "abstract url": "https://arxiv.org/abs/2410.05690",
        "title": "Long-Context Linear System Identification",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper addresses the problem of long-context linear system identification, where the state $x_t$ of a dynamical system at time $t$ depends linearly on previous states $x_s$ over a fixed context window of length $p$. We establish a sample complexity bound that matches the i.i.d. parametric rate up to logarithmic factors for a broad class of systems, extending previous works that considered only first-order dependencies. Our findings reveal a learning-without-mixing phenomenon, indicating that learning long-context linear autoregressive models is not hindered by slow mixing properties potentially associated with extended context windows. Additionally, we extend these results to (i) shared low-rank representations, where rank-regularized estimators improve rates with respect to dimensionality, and (ii) misspecified context lengths in strictly stable systems, where shorter contexts offer statistical advantages.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "eess.SY",
            "math.ST"
        ],
        "comment": "30 pages, 4 figures"
    },
    {
        "paper id": "2410.05728",
        "abstract url": "https://arxiv.org/abs/2410.05728",
        "title": "Reducing fuzzy relation equations via concept lattices",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper has taken into advantage the relationship between Fuzzy Relation Equations (FRE) and Concept Lattices in order to introduce a procedure to reduce a FRE, without losing information. Specifically, attribute reduction theory in property-oriented and object-oriented concept lattices has been considered in order to present a mechanism for detecting redundant equations. As a first consequence, the computation of the whole solution set of a solvable FRE is reduced. Moreover, we will also introduce a novel method for computing approximate solutions of unsolvable FRE related to a (real) dataset with uncertainty/imprecision data.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05734",
        "abstract url": "https://arxiv.org/abs/2410.05734",
        "title": "Diminishing Exploration: A Minimalist Approach to Piecewise Stationary Multi-Armed Bandits",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The piecewise-stationary bandit problem is an important variant of the multi-armed bandit problem that further considers abrupt changes in the reward distributions. The main theme of the problem is the trade-off between exploration for detecting environment changes and exploitation of traditional bandit algorithms. While this problem has been extensively investigated, existing works either assume knowledge about the number of change points $M$ or require extremely high computational complexity. In this work, we revisit the piecewise-stationary bandit problem from a minimalist perspective. We propose a novel and generic exploration mechanism, called diminishing exploration, which eliminates the need for knowledge about $M$ and can be used in conjunction with an existing change detection-based algorithm to achieve near-optimal regret scaling. Simulation results show that despite oblivious of $M$, equipping existing algorithms with the proposed diminishing exploration generally achieves better empirical regret than the traditional uniform exploration.",
        "subjects": [
            "cs.LG",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05752",
        "abstract url": "https://arxiv.org/abs/2410.05752",
        "title": "Exploring the Meaningfulness of Nearest Neighbor Search in High-Dimensional Space",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Dense high dimensional vectors are becoming increasingly vital in fields such as computer vision, machine learning, and large language models (LLMs), serving as standard representations for multimodal data. Now the dimensionality of these vector can exceed several thousands easily. Despite the nearest neighbor search (NNS) over these dense high dimensional vectors have been widely used for retrieval augmented generation (RAG) and many other applications, the effectiveness of NNS in such a high-dimensional space remains uncertain, given the possible challenge caused by the \"curse of dimensionality.\" To address above question, in this paper, we conduct extensive NNS studies with different distance functions, such as $L_1$ distance, $L_2$ distance and angular-distance, across diverse embedding datasets, of varied types, dimensionality and modality. Our aim is to investigate factors influencing the meaningfulness of NNS. Our experiments reveal that high-dimensional text embeddings exhibit increased resilience as dimensionality rises to higher levels when compared to random vectors. This resilience suggests that text embeddings are less affected to the \"curse of dimensionality,\" resulting in more meaningful NNS outcomes for practical use. Additionally, the choice of distance function has minimal impact on the relevance of NNS. Our study shows the effectiveness of the embedding-based data representation method and can offer opportunity for further optimization of dense vector-related applications.",
        "subjects": [
            "cs.LG",
            "cs.DB",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05753",
        "abstract url": "https://arxiv.org/abs/2410.05753",
        "title": "Pathwise Gradient Variance Reduction with Control Variates in Variational Inference",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Variational inference in Bayesian deep learning often involves computing the gradient of an expectation that lacks a closed-form solution. In these cases, pathwise and score-function gradient estimators are the most common approaches. The pathwise estimator is often favoured for its substantially lower variance compared to the score-function estimator, which typically requires variance reduction techniques. However, recent research suggests that even pathwise gradient estimators could benefit from variance reduction. In this work, we review existing control-variates-based variance reduction methods for pathwise gradient estimators to assess their effectiveness. Notably, these methods often rely on integrand approximations and are applicable only to simple variational families. To address this limitation, we propose applying zero-variance control variates to pathwise gradient estimators. This approach offers the advantage of requiring minimal assumptions about the variational distribution, other than being able to sample from it.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "stat.CO",
            "stat.ME"
        ],
        "comment": "9 (+16 appendix) pages"
    },
    {
        "paper id": "2410.05754",
        "abstract url": "https://arxiv.org/abs/2410.05754",
        "title": "Simple Relative Deviation Bounds for Covariance and Gram Matrices",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We provide non-asymptotic, relative deviation bounds for the eigenvalues of empirical covariance and gram matrices in general settings. Unlike typical uniform bounds, which may fail to capture the behavior of smaller eigenvalues, our results provide sharper control across the spectrum. Our analysis is based on a general-purpose theorem that allows one to convert existing uniform bounds into relative ones. The theorems and techniques emphasize simplicity and should be applicable across various settings.",
        "subjects": [
            "math.PR",
            "cs.LG",
            "math.ST",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05757",
        "abstract url": "https://arxiv.org/abs/2410.05757",
        "title": "Temperature Optimization for Bayesian Deep Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The Cold Posterior Effect (CPE) is a phenomenon in Bayesian Deep Learning (BDL), where tempering the posterior to a cold temperature often improves the predictive performance of the posterior predictive distribution (PPD). Although the term `CPE' suggests colder temperatures are inherently better, the BDL community increasingly recognizes that this is not always the case. Despite this, there remains no systematic method for finding the optimal temperature beyond grid search. In this work, we propose a data-driven approach to select the temperature that maximizes test log-predictive density, treating the temperature as a model parameter and estimating it directly from the data. We empirically demonstrate that our method performs comparably to grid search, at a fraction of the cost, across both regression and classification tasks. Finally, we highlight the differing perspectives on CPE between the BDL and Generalized Bayes communities: while the former primarily focuses on predictive performance of the PPD, the latter emphasizes calibrated uncertainty and robustness to model misspecification; these distinct objectives lead to different temperature preferences.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "stat.CO",
            "stat.ME"
        ],
        "comment": "11 pages (+5 reference, +16 appendix)"
    },
    {
        "paper id": "2410.05799",
        "abstract url": "https://arxiv.org/abs/2410.05799",
        "title": "SeeClear: Semantic Distillation Enhances Pixel Condensation for Video Super-Resolution",
        "rating": "0.5",
        "keywords": [
            [
                "Diffusion",
                "Super-Resolution"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Diffusion-based Video Super-Resolution (VSR) is renowned for generating perceptually realistic videos, yet it grapples with maintaining detail consistency across frames due to stochastic fluctuations. The traditional approach of pixel-level alignment is ineffective for diffusion-processed frames because of iterative disruptions. To overcome this, we introduce SeeClear--a novel VSR framework leveraging conditional video generation, orchestrated by instance-centric and channel-wise semantic controls. This framework integrates a Semantic Distiller and a Pixel Condenser, which synergize to extract and upscale semantic details from low-resolution frames. The Instance-Centric Alignment Module (InCAM) utilizes video-clip-wise tokens to dynamically relate pixels within and across frames, enhancing coherency. Additionally, the Channel-wise Texture Aggregation Memory (CaTeGory) infuses extrinsic knowledge, capitalizing on long-standing semantic textures. Our method also innovates the blurring diffusion process with the ResShift mechanism, finely balancing between sharpness and diffusion effects. Comprehensive experiments confirm our framework's advantage over state-of-the-art diffusion-based VSR techniques. The code is available: https://github.com/Tang1705/SeeClear-NeurIPS24.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to NeurIPS 2024"
    },
    {
        "paper id": "2410.05807",
        "abstract url": "https://arxiv.org/abs/2410.05807",
        "title": "Extended convexity and smoothness and their applications in deep learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The underlying mechanism by which simple gradient-based iterative algorithms can effectively handle the non-convex problem of deep model training remains incompletely understood within the traditional convex and non-convex analysis frameworks, which often require the Lipschitz smoothness of the gradient and strong convexity. In this paper, we introduce $\\mathcal{H}(\u03c6)$-convexity and $\\mathcal{H}(\u03a6)$-smoothness, which broaden the existing concepts of smoothness and convexity, and delineate their fundamental properties. Building on these concepts, we introduce the high-order gradient descent and high-order stochastic gradient descent methods, which serve as extensions to the traditional gradient descent and stochastic gradient descent methods, respectively. Furthermore, we establish descent lemmas for the $\\mathcal{H}(\u03c6)$-convex and $\\mathcal{H}(\u03a6)$-smooth objective functions when utilizing these four methods. On the basis of these findings, we develop the gradient structure control algorithm to address non-convex optimization objectives, encompassing both the functions represented by machine learning models and common loss functions in deep learning. The effectiveness of the proposed methodology is empirically validated through experiments.",
        "subjects": [
            "cs.LG",
            "cs.DS",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05810",
        "abstract url": "https://arxiv.org/abs/2410.05810",
        "title": "Uncertainty-Aware Fairness-Adaptive Classification Trees",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In an era where artificial intelligence and machine learning algorithms increasingly impact human life, it is crucial to develop models that account for potential discrimination in their predictions. This paper tackles this problem by introducing a new classification tree algorithm using a novel splitting criterion that incorporates fairness adjustments into the tree-building process. The proposed method integrates a fairness-aware impurity measure that balances predictive accuracy with fairness across protected groups. By ensuring that each splitting node considers both the gain in classification error and the fairness, our algorithm encourages splits that mitigate discrimination. Importantly, in penalizing unfair splits, we account for the uncertainty in the fairness metric by utilizing its confidence interval instead of relying on its point estimate. Experimental results on benchmark and synthetic datasets illustrate that our method effectively reduces discriminatory predictions compared to traditional classification trees, without significant loss in overall accuracy.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "14 pages, 3 figures"
    },
    {
        "paper id": "2410.05827",
        "abstract url": "https://arxiv.org/abs/2410.05827",
        "title": "Towards an Operational Responsible AI Framework for Learning Analytics in Higher Education",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "Universities are increasingly adopting data-driven strategies to enhance student success, with AI applications like Learning Analytics (LA) and Predictive Learning Analytics (PLA) playing a key role in identifying at-risk students, personalising learning, supporting teachers, and guiding educational decision-making. However, concerns are rising about potential harms these systems may pose, such as algorithmic biases leading to unequal support for minority students. While many have explored the need for Responsible AI in LA, existing works often lack practical guidance for how institutions can operationalise these principles. In this paper, we propose a novel Responsible AI framework tailored specifically to LA in Higher Education (HE). We started by mapping 11 established Responsible AI frameworks, including those by leading tech companies, to the context of LA in HE. This led to the identification of seven key principles such as transparency, fairness, and accountability. We then conducted a systematic review of the literature to understand how these principles have been applied in practice. Drawing from these findings, we present a novel framework that offers practical guidance to HE institutions and is designed to evolve with community input, ensuring its relevance as LA systems continue to develop.",
        "subjects": [
            "cs.CY",
            "cs.AI"
        ],
        "comment": "16 pages, 1 figure, submitted to LAK 25"
    },
    {
        "paper id": "2410.05838",
        "abstract url": "https://arxiv.org/abs/2410.05838",
        "title": "Time Transfer: On Optimal Learning Rate and Batch Size In The Infinite Data Limit",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "One of the main challenges in optimal scaling of large language models (LLMs) is the prohibitive cost of hyperparameter tuning, particularly learning rate $\u03b7$ and batch size $B$. While techniques like $\u03bc$P (Yang et al., 2022) provide scaling rules for optimal $\u03b7$ transfer in the infinite model size limit, the optimal scaling behavior in the infinite data size limit ($T \\to \\infty$) remains unknown. We fill in this gap by observing for the first time an interplay of three optimal $\u03b7$ scaling regimes: $\u03b7\\propto \\sqrt{T}$, $\u03b7\\propto 1$, and $\u03b7\\propto 1/\\sqrt{T}$ with transitions controlled by $B$ and its relation to the time-evolving critical batch size $B_\\mathrm{crit} \\propto T$. Furthermore, we show that the optimal batch size is positively correlated with $B_\\mathrm{crit}$: keeping it fixed becomes suboptimal over time even if learning rate is scaled optimally. Surprisingly, our results demonstrate that the observed optimal $\u03b7$ and $B$ dynamics are preserved with $\u03bc$P model scaling, challenging the conventional view of $B_\\mathrm{crit}$ dependence solely on loss value. Complementing optimality, we examine the sensitivity of loss to changes in learning rate, where we find the sensitivity to decrease with $T \\to \\infty$ and to remain constant with $\u03bc$P model scaling. We hope our results make the first step towards a unified picture of the joint optimal data and model scaling.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05856",
        "abstract url": "https://arxiv.org/abs/2410.05856",
        "title": "Stochastic Bandits for Egalitarian Assignment",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study EgalMAB, an egalitarian assignment problem in the context of stochastic multi-armed bandits. In EgalMAB, an agent is tasked with assigning a set of users to arms. At each time step, the agent must assign exactly one arm to each user such that no two users are assigned to the same arm. Subsequently, each user obtains a reward drawn from the unknown reward distribution associated with its assigned arm. The agent's objective is to maximize the minimum expected cumulative reward among all users over a fixed horizon. This problem has applications in areas such as fairness in job and resource allocations, among others. We design and analyze a UCB-based policy EgalUCB and establish upper bounds on the cumulative regret. In complement, we establish an almost-matching policy-independent impossibility result.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05860",
        "abstract url": "https://arxiv.org/abs/2410.05860",
        "title": "MelissaDL x Breed: Towards Data-Efficient On-line Supervised Training of Multi-parametric Surrogates with Active Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Artificial intelligence is transforming scientific computing with deep neural network surrogates that approximate solutions to partial differential equations (PDEs). Traditional off-line training methods face issues with storage and I/O efficiency, as the training dataset has to be computed with numerical solvers up-front. Our previous work, the Melissa framework, addresses these problems by enabling data to be created \"on-the-fly\" and streamed directly into the training process. In this paper we introduce a new active learning method to enhance data-efficiency for on-line surrogate training. The surrogate is direct and multi-parametric, i.e., it is trained to predict a given timestep directly with different initial and boundary conditions parameters. Our approach uses Adaptive Multiple Importance Sampling guided by training loss statistics, in order to focus NN training on the difficult areas of the parameter space. Preliminary results for 2D heat PDE demonstrate the potential of this method, called Breed, to improve the generalization capabilities of surrogates while reducing computational overhead.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05870",
        "abstract url": "https://arxiv.org/abs/2410.05870",
        "title": "Heuristics for Partially Observable Stochastic Contingent Planning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Acting to complete tasks in stochastic partially observable domains is an important problem in artificial intelligence, and is often formulated as a goal-based POMDP. Goal-based POMDPs can be solved using the RTDP-BEL algorithm, that operates by running forward trajectories from the initial belief to the goal. These trajectories can be guided by a heuristic, and more accurate heuristics can result in significantly faster convergence. In this paper, we develop a heuristic function that leverages the structured representation of domain models. We compute, in a relaxed space, a plan to achieve the goal, while taking into account the value of information, as well as the stochastic effects. We provide experiments showing that while our heuristic is slower to compute, it requires an order of magnitude less trajectories before convergence. Overall, it thus speeds up RTDP-BEL, particularly in problems where significant information gathering is needed.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05871",
        "abstract url": "https://arxiv.org/abs/2410.05871",
        "title": "A second-order-like optimizer with adaptive gradient scaling for deep learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In this empirical article, we introduce INNAprop, an optimization algorithm that combines the INNA method with the RMSprop adaptive gradient scaling. It leverages second-order information and rescaling while keeping the memory requirements of standard DL methods as AdamW or SGD with momentum.After having recalled our geometrical motivations, we provide quite extensive experiments. On image classification (CIFAR-10, ImageNet) and language modeling (GPT-2), INNAprop consistently matches or outperforms AdamW both in training speed and accuracy, with minimal hyperparameter tuning in large-scale settings. Our code is publicly available at \\url{https://github.com/innaprop/innaprop}.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05880",
        "abstract url": "https://arxiv.org/abs/2410.05880",
        "title": "Improved Sample Complexity for Private Nonsmooth Nonconvex Optimization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study differentially private (DP) optimization algorithms for stochastic and empirical objectives which are neither smooth nor convex, and propose methods that return a Goldstein-stationary point with sample complexity bounds that improve on existing works. We start by providing a single-pass $(\u03b5,\u03b4)$-DP algorithm that returns an $(\u03b1,\u03b2)$-stationary point as long as the dataset is of size $\\widetilde\u03a9\\left(1/\u03b1\u03b2^{3}+d/\u03b5\u03b1\u03b2^{2}+d^{3/4}/\u03b5^{1/2}\u03b1\u03b2^{5/2}\\right)$, which is $\u03a9(\\sqrt{d})$ times smaller than the algorithm of Zhang et al. [2024] for this task, where $d$ is the dimension. We then provide a multi-pass polynomial time algorithm which further improves the sample complexity to $\\widetilde\u03a9\\left(d/\u03b2^2+d^{3/4}/\u03b5\u03b1^{1/2}\u03b2^{3/2}\\right)$, by designing a sample efficient ERM algorithm, and proving that Goldstein-stationary points generalize from the empirical loss to the population loss.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "math.OC",
            "stat.ML"
        ],
        "comment": "25 pages"
    },
    {
        "paper id": "2410.05902",
        "abstract url": "https://arxiv.org/abs/2410.05902",
        "title": "Mini-Batch Kernel $k$-means",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We present the first mini-batch kernel $k$-means algorithm, offering an order of magnitude improvement in running time compared to the full batch algorithm. A single iteration of our algorithm takes $\\widetilde{O}(kb^2)$ time, significantly faster than the $O(n^2)$ time required by the full batch kernel $k$-means, where $n$ is the dataset size and $b$ is the batch size. Extensive experiments demonstrate that our algorithm consistently achieves a 10-100x speedup with minimal loss in quality, addressing the slow runtime that has limited kernel $k$-means adoption in practice. We further complement these results with a theoretical analysis under an early stopping condition, proving that with a batch size of $\\widetilde\u03a9(\\max \\{\u03b3^{4}, \u03b3^{2}\\} \\cdot \u03b5^{-2})$, the algorithm terminates in $O(\u03b3^2/\u03b5)$ iterations with high probability, where $\u03b3$ bounds the norm of points in feature space and $\u03b5$ is a termination threshold. Our analysis holds for any reasonable center initialization, and when using $k$-means++ initialization, the algorithm achieves an approximation ratio of $O(\\log k)$ in expectation. For normalized kernels, such as Gaussian or Laplacian it holds that $\u03b3=1$. Taking $\u03b5= O(1)$ and $b=\u0398(\\log n)$, the algorithm terminates in $O(1)$ iterations, with each iteration running in $\\widetilde{O}(k)$ time.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.DS"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2304.00419"
    },
    {
        "paper id": "2410.05910",
        "abstract url": "https://arxiv.org/abs/2410.05910",
        "title": "Digital Labor and the Inconspicuous Production of Artificial Intelligence",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Digital platforms capitalize on users' labor, often disguising essential contributions as casual activities or consumption, regardless of users' recognition of their efforts. Data annotation, content creation, and engagement with advertising are all aspects of this hidden productivity. Despite playing a crucial role in driving AI development, such tasks remain largely unrecognized and undercompensated. This chapter exposes the systemic devaluation of these activities in the digital economy, by drawing on historical theories about unrecognized labor, from housework to audience labor. This approach advocates for a broader understanding of digital labor by introducing the concept of ''inconspicuous production.'' It moves beyond the traditional notion of ''invisible work'' to highlight the hidden elements inherent in all job types, especially in light of growing automation and platform-based employment.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "SAGE Handbook of Digital Labour., In press"
    },
    {
        "paper id": "2410.05920",
        "abstract url": "https://arxiv.org/abs/2410.05920",
        "title": "FINALLY: fast and universal speech enhancement with studio-like quality",
        "rating": "0.5",
        "keywords": [
            [
                "speech enhancement"
            ],
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In this paper, we address the challenge of speech enhancement in real-world recordings, which often contain various forms of distortion, such as background noise, reverberation, and microphone artifacts. We revisit the use of Generative Adversarial Networks (GANs) for speech enhancement and theoretically show that GANs are naturally inclined to seek the point of maximum density within the conditional clean speech distribution, which, as we argue, is essential for the speech enhancement task. We study various feature extractors for perceptual loss to facilitate the stability of adversarial training, developing a methodology for probing the structure of the feature space. This leads us to integrate WavLM-based perceptual loss into MS-STFT adversarial training pipeline, creating an effective and stable training procedure for the speech enhancement model. The resulting speech enhancement model, which we refer to as FINALLY, builds upon the HiFi++ architecture, augmented with a WavLM encoder and a novel training pipeline. Empirical results on various datasets confirm our model's ability to produce clear, high-quality speech at 48 kHz, achieving state-of-the-art performance in the field of speech enhancement.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "eess.AS"
        ],
        "comment": "Accepted to NeurIPS 2024"
    },
    {
        "paper id": "2410.05930",
        "abstract url": "https://arxiv.org/abs/2410.05930",
        "title": "Fortify Your Foundations: Practical Privacy and Security for Foundation Model Deployments In The Cloud",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Foundation Models (FMs) display exceptional performance in tasks such as natural language processing and are being applied across a growing range of disciplines. Although typically trained on large public datasets, FMs are often fine-tuned or integrated into Retrieval-Augmented Generation (RAG) systems, which rely on private data. This access, along with their size and costly training, heightens the risk of intellectual property theft. Moreover, multimodal FMs may expose sensitive information. In this work, we examine the FM threat model and discuss the practicality and comprehensiveness of various approaches for securing against them, such as ML-based methods and trusted execution environments (TEEs). We demonstrate that TEEs offer an effective balance between strong security properties, usability, and performance. Specifically, we present a solution achieving less than 10\\% overhead versus bare metal for the full Llama2 7B and 13B inference pipelines running inside \\intel\\ SGX and \\intel\\ TDX. We also share our configuration files and insights from our implementation. To our knowledge, our work is the first to show the practicality of TEEs for securing FMs.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05937",
        "abstract url": "https://arxiv.org/abs/2410.05937",
        "title": "Athanor: Local Search over Abstract Constraint Specifications",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Local search is a common method for solving combinatorial optimisation problems. We focus on general-purpose local search solvers that accept as input a constraint model - a declarative description of a problem consisting of a set of decision variables under a set of constraints. Existing approaches typically take as input models written in solver-independent constraint modelling languages like MiniZinc. The Athanor solver we describe herein differs in that it begins from a specification of a problem in the abstract constraint specification language Essence, which allows problems to be described without commitment to low-level modelling decisions through its support for a rich set of abstract types. The advantage of proceeding from Essence is that the structure apparent in a concise, abstract specification of a problem can be exploited to generate high quality neighbourhoods automatically, avoiding the difficult task of identifying that structure in an equivalent constraint model. Based on the twin benefits of neighbourhoods derived from high level types and the scalability derived by searching directly over those types, our empirical results demonstrate strong performance in practice relative to existing solution methods.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "48 pages"
    },
    {
        "paper id": "2410.05942",
        "abstract url": "https://arxiv.org/abs/2410.05942",
        "title": "Single Point-Based Distributed Zeroth-Order Optimization with a Non-Convex Stochastic Objective Function",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Zero-order (ZO) optimization is a powerful tool for dealing with realistic constraints. On the other hand, the gradient-tracking (GT) technique proved to be an efficient method for distributed optimization aiming to achieve consensus. However, it is a first-order (FO) method that requires knowledge of the gradient, which is not always possible in practice. In this work, we introduce a zero-order distributed optimization method based on a one-point estimate of the gradient tracking technique. We prove that this new technique converges with a single noisy function query at a time in the non-convex setting. We then establish a convergence rate of $O(\\frac{1}{\\sqrt[3]{K}})$ after a number of iterations K, which competes with that of $O(\\frac{1}{\\sqrt[4]{K}})$ of its centralized counterparts. Finally, a numerical example validates our theoretical results.",
        "subjects": [
            "cs.LG",
            "math.OC"
        ],
        "comment": "In this version, we slightly modify the proof of Theorem 3.7 in the original publication. We remove the expectation in the proof that was added by error. The original publication can be found at: https://proceedings.mlr.press/v202/mhanna23a.html"
    },
    {
        "paper id": "2410.05952",
        "abstract url": "https://arxiv.org/abs/2410.05952",
        "title": "Active Evaluation Acquisition for Efficient LLM Benchmarking",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "As large language models (LLMs) become increasingly versatile, numerous large scale benchmarks have been developed to thoroughly assess their capabilities. These benchmarks typically consist of diverse datasets and prompts to evaluate different aspects of LLM performance. However, comprehensive evaluations on hundreds or thousands of prompts incur tremendous costs in terms of computation, money, and time. In this work, we investigate strategies to improve evaluation efficiency by selecting a subset of examples from each benchmark using a learned policy. Our approach models the dependencies across test examples, allowing accurate prediction of the evaluation outcomes for the remaining examples based on the outcomes of the selected ones. Consequently, we only need to acquire the actual evaluation outcomes for the selected subset. We rigorously explore various subset selection policies and introduce a novel RL-based policy that leverages the captured dependencies. Empirical results demonstrate that our approach significantly reduces the number of evaluation prompts required while maintaining accurate performance estimates compared to previous methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05966",
        "abstract url": "https://arxiv.org/abs/2410.05966",
        "title": "FLOPS: Forward Learning with OPtimal Sampling",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Given the limitations of backpropagation, perturbation-based gradient computation methods have recently gained focus for learning with only forward passes, also referred to as queries. Conventional forward learning consumes enormous queries on each data point for accurate gradient estimation through Monte Carlo sampling, which hinders the scalability of those algorithms. However, not all data points deserve equal queries for gradient estimation. In this paper, we study the problem of improving the forward learning efficiency from a novel perspective: how to reduce the gradient estimation variance with minimum cost? For this, we propose to allocate the optimal number of queries over each data in one batch during training to achieve a good balance between estimation accuracy and computational efficiency. Specifically, with a simplified proxy objective and a reparameterization technique, we derive a novel plug-and-play query allocator with minimal parameters. Theoretical results are carried out to verify its optimality. We conduct extensive experiments for fine-tuning Vision Transformers on various datasets and further deploy the allocator to two black-box applications: prompt tuning and multimodal alignment for foundation models. All findings demonstrate that our proposed allocator significantly enhances the scalability of forward-learning algorithms, paving the way for real-world applications.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05975",
        "abstract url": "https://arxiv.org/abs/2410.05975",
        "title": "ConML: A Universal Meta-Learning Framework with Task-Level Contrastive Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Meta-learning enables learning systems to adapt quickly to new tasks, similar to humans. To emulate this human-like rapid learning and enhance alignment and discrimination abilities, we propose ConML, a universal meta-learning framework that can be applied to various meta-learning algorithms without relying on specific model architectures nor target models. The core of ConML is task-level contrastive learning, which extends contrastive learning from the representation space in unsupervised learning to the model space in meta-learning. By leveraging task identity as an additional supervision signal during meta-training, we contrast the outputs of the meta-learner in the model space, minimizing inner-task distance (between models trained on different subsets of the same task) and maximizing inter-task distance (between models from different tasks). We demonstrate that ConML integrates seamlessly with optimization-based, metric-based, and amortization-based meta-learning algorithms, as well as in-context learning, resulting in performance improvements across diverse few-shot learning tasks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05980",
        "abstract url": "https://arxiv.org/abs/2410.05980",
        "title": "Generalizing to any diverse distribution: uniformity, gentle finetuning and rebalancing",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "As training datasets grow larger, we aspire to develop models that generalize well to any diverse test distribution, even if the latter deviates significantly from the training data. Various approaches like domain adaptation, domain generalization, and robust optimization attempt to address the out-of-distribution challenge by posing assumptions about the relation between training and test distribution. Differently, we adopt a more conservative perspective by accounting for the worst-case error across all sufficiently diverse test distributions within a known domain. Our first finding is that training on a uniform distribution over this domain is optimal. We also interrogate practical remedies when uniform samples are unavailable by considering methods for mitigating non-uniformity through finetuning and rebalancing. Our theory provides a mathematical grounding for previous observations on the role of entropy and rebalancing for o.o.d. generalization and foundation model training. We also provide new empirical evidence across tasks involving o.o.d. shifts which illustrate the broad applicability of our perspective.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05988",
        "abstract url": "https://arxiv.org/abs/2410.05988",
        "title": "Utilizing Lyapunov Exponents in designing deep neural networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Training large deep neural networks is resource intensive. This study investigates whether Lyapunov exponents can accelerate this process by aiding in the selection of hyperparameters. To study this I formulate an optimization problem using neural networks with different activation functions in the hidden layers. By initializing model weights with different random seeds, I calculate the Lyapunov exponent while performing traditional gradient descent on these model weights. The findings demonstrate that variations in the learning rate can induce chaotic changes in model weights. I also show that activation functions with more negative Lyapunov exponents exhibit better convergence properties. Additionally, the study also demonstrates that Lyapunov exponents can be utilized to select effective initial model weights for deep neural networks, potentially enhancing the optimization process.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06012",
        "abstract url": "https://arxiv.org/abs/2410.06012",
        "title": "Generalized Sparse Additive Model with Unknown Link Function",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Generalized additive models (GAM) have been successfully applied to high dimensional data analysis. However, most existing methods cannot simultaneously estimate the link function, the component functions and the variable interaction. To alleviate this problem, we propose a new sparse additive model, named generalized sparse additive model with unknown link function (GSAMUL), in which the component functions are estimated by B-spline basis and the unknown link function is estimated by a multi-layer perceptron (MLP) network. Furthermore, $\\ell_{2,1}$-norm regularizer is used for variable selection. The proposed GSAMUL can realize both variable selection and hidden interaction. We integrate this estimation into a bilevel optimization problem, where the data is split into training set and validation set. In theory, we provide the guarantees about the convergence of the approximate procedure. In applications, experimental evaluations on both synthetic and real world data sets consistently validate the effectiveness of the proposed approach.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06017",
        "abstract url": "https://arxiv.org/abs/2410.06017",
        "title": "Evacuation patterns and socioeconomic stratification in the context of wildfires in Chile",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Climate change is altering the frequency and intensity of wildfires, leading to increased evacuation events that disrupt human mobility and socioeconomic structures. These disruptions affect access to resources, employment, and housing, amplifying existing vulnerabilities within communities. Understanding the interplay between climate change, wildfires, evacuation patterns, and socioeconomic factors is crucial for developing effective mitigation and adaptation strategies. To contribute to this challenge, we use high-definition mobile phone records to analyse evacuation patterns during the wildfires in Valpara\u00edso, Chile, that took place between February 2-3, 2024. This data allows us to track the movements of individuals in the disaster area, providing insight into how people respond to large-scale evacuations in the context of severe wildfires. We apply a causal inference approach that combines regression discontinuity and difference-in-differences methodologies to observe evacuation behaviours during wildfires, with a focus on socioeconomic stratification. This approach allows us to isolate the impact of the wildfires on different socioeconomic groups by comparing the evacuation patterns of affected populations before and after the event, while accounting for underlying trends and discontinuities at the threshold of the disaster. We find that many people spent nights away from home, with those in the lowest socioeconomic segment stayed away the longest. In general, people reduced their travel distance during the evacuation, and the lowest socioeconomic group moved the least. Initially, movements became more random, as people sought refuge in a rush, but eventually gravitated towards areas with similar socioeconomic status. Our results show that socioeconomic differences play a role in evacuation dynamics, providing useful insights for response planning.",
        "subjects": [
            "physics.soc-ph",
            "cs.CY"
        ],
        "comment": "31 pages, 15 figures, 11 tables"
    },
    {
        "paper id": "2410.06030",
        "abstract url": "https://arxiv.org/abs/2410.06030",
        "title": "Data Quality Issues in Vulnerability Detection Datasets",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Vulnerability detection is a crucial yet challenging task to identify potential weaknesses in software for cyber security. Recently, deep learning (DL) has made great progress in automating the detection process. Due to the complex multi-layer structure and a large number of parameters, a DL model requires massive labeled (vulnerable or secure) source code to gain knowledge to effectively distinguish between vulnerable and secure code. In the literature, many datasets have been created to train DL models for this purpose. However, these datasets suffer from several issues that will lead to low detection accuracy of DL models. In this paper, we define three critical issues (i.e., data imbalance, low vulnerability coverage, biased vulnerability distribution) that can significantly affect the model performance and three secondary issues (i.e., errors in source code, mislabeling, noisy historical data) that also affect the performance but can be addressed through a dedicated pre-processing procedure. In addition, we conduct a study of 14 papers along with 54 datasets for vulnerability detection to confirm these defined issues. Furthermore, we discuss good practices to use existing datasets and to create new ones.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": "2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&P;PW)"
    },
    {
        "paper id": "2410.06074",
        "abstract url": "https://arxiv.org/abs/2410.06074",
        "title": "Scalable Mechanistic Neural Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose Scalable Mechanistic Neural Network (S-MNN), an enhanced neural network framework designed for scientific machine learning applications involving long temporal sequences. By reformulating the original Mechanistic Neural Network (MNN) (Pervez et al., 2024), we reduce the computational time and space complexities from cubic and quadratic with respect to the sequence length, respectively, to linear. This significant improvement enables efficient modeling of long-term dynamics without sacrificing accuracy or interpretability. Extensive experiments demonstrate that S-MNN matches the original MNN in precision while substantially reducing computational resources. Consequently, S-MNN can drop-in replace the original MNN in applications, providing a practical and efficient tool for integrating mechanistic bottlenecks into neural network models of complex dynamical systems.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06086",
        "abstract url": "https://arxiv.org/abs/2410.06086",
        "title": "The GDPR's Rules on Data Breaches: Analysing Their Rationales and Effects",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "The General Data Protection Regulation (GDPR) requires an organisation that suffers a data breach to notify the competent Data Protection Authority. The organisation must also inform the relevant individuals, when a data breach threatens their rights and freedoms. This paper focuses on the following question: given the goals of the GDPR's data breach notification obligation, and we assess the obligation in the light of those goals. We refer to insights from information security and economics, and present them in a reader-friendly way for lawyers. Our main conclusion is that the GDPR's data breach rules are likely to contribute to the goals. For instance, the data breach notification obligation can nudge organisations towards better security; such an obligation enables regulators to perform their duties; and such an obligation improves transparency and accountability. However, the paper also warns that we should not have unrealistic expectations of the possibilities for people to protect their interests after a data breach notice. Likewise, we should not have high expectations of people switching to other service providers after receiving a data breach notification. Lastly, the paper calls for Data Protection Authorities to publish more information about reported data breaches. Such information can help to analyse security threats.",
        "subjects": [
            "cs.CR",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06101",
        "abstract url": "https://arxiv.org/abs/2410.06101",
        "title": "Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Reinforcement learning (RL) has emerged as a pivotal technique for fine-tuning large language models (LLMs) on specific tasks. However, prevailing RL fine-tuning methods predominantly rely on PPO and its variants. Though these algorithms are effective in general RL settings, they often exhibit suboptimal performance and vulnerability to distribution collapse when applied to the fine-tuning of LLMs. In this paper, we propose CORY, extending the RL fine-tuning of LLMs to a sequential cooperative multi-agent reinforcement learning framework, to leverage the inherent coevolution and emergent capabilities of multi-agent systems. In CORY, the LLM to be fine-tuned is initially duplicated into two autonomous agents: a pioneer and an observer. The pioneer generates responses based on queries, while the observer generates responses using both the queries and the pioneer's responses. The two agents are trained together. During training, the agents exchange roles periodically, fostering cooperation and coevolution between them. Experiments evaluate CORY's performance by fine-tuning GPT-2 and Llama-2 under subjective and objective reward functions on the IMDB Review and GSM8K datasets, respectively. Results show that CORY outperforms PPO in terms of policy optimality, resistance to distribution collapse, and training robustness, thereby underscoring its potential as a superior methodology for refining LLMs in real-world applications.",
        "subjects": [
            "cs.AI",
            "cs.MA"
        ],
        "comment": "28 pages, 26 images"
    },
    {
        "paper id": "2410.06107",
        "abstract url": "https://arxiv.org/abs/2410.06107",
        "title": "Towards AI-Native Software Engineering (SE 3.0): A Vision and a Challenge Roadmap",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The rise of AI-assisted software engineering (SE 2.0), powered by Foundation Models (FMs) and FM-powered copilots, has shown promise in improving developer productivity. However, it has also exposed inherent limitations, such as cognitive overload on developers and inefficiencies. We propose a shift towards Software Engineering 3.0 (SE 3.0), an AI-native approach characterized by intent-first, conversation-oriented development between human developers and AI teammates. SE 3.0 envisions AI systems evolving beyond task-driven copilots into intelligent collaborators, capable of deeply understanding and reasoning about software engineering principles and intents. We outline the key components of the SE 3.0 technology stack, which includes Teammate.next for adaptive and personalized AI partnership, IDE.next for intent-first conversation-oriented development, Compiler.next for multi-objective code synthesis, and Runtime.next for SLA-aware execution with edge-computing support. Our vision addresses the inefficiencies and cognitive strain of SE 2.0 by fostering a symbiotic relationship between human developers and AI, maximizing their complementary strengths. We also present a roadmap of challenges that must be overcome to realize our vision of SE 3.0. This paper lays the foundation for future discussions on the role of AI in the next era of software engineering.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06108",
        "abstract url": "https://arxiv.org/abs/2410.06108",
        "title": "ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Robotic planning and execution in open-world environments is a complex problem due to the vast state spaces and high variability of task embodiment. Recent advances in perception algorithms, combined with Large Language Models (LLMs) for planning, offer promising solutions to these challenges, as the common sense reasoning capabilities of LLMs provide a strong heuristic for efficiently searching the action space. However, prior work fails to address the possibility of hallucinations from LLMs, which results in failures to execute the planned actions largely due to logical fallacies at high- or low-levels. To contend with automation failure due to such hallucinations, we introduce ConceptAgent, a natural language-driven robotic platform designed for task execution in unstructured environments. With a focus on scalability and reliability of LLM-based planning in complex state and action spaces, we present innovations designed to limit these shortcomings, including 1) Predicate Grounding to prevent and recover from infeasible actions, and 2) an embodied version of LLM-guided Monte Carlo Tree Search with self reflection. In simulation experiments, ConceptAgent achieved a 19% task completion rate across three room layouts and 30 easy level embodied tasks outperforming other state-of-the-art LLM-driven reasoning baselines that scored 10.26% and 8.11% on the same benchmark. Additionally, ablation studies on moderate to hard embodied tasks revealed a 20% increase in task completion from the baseline agent to the fully enhanced ConceptAgent, highlighting the individual and combined contributions of Predicate Grounding and LLM-guided Tree Search to enable more robust automation in complex state and action spaces.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06112",
        "abstract url": "https://arxiv.org/abs/2410.06112",
        "title": "SwiftQueue: Optimizing Low-Latency Applications with Swift Packet Queuing",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Low Latency, Low Loss, and Scalable Throughput (L4S), as an emerging router-queue management technique, has seen steady deployment in the industry. An L4S-enabled router assigns each packet to the queue based on the packet header marking. Currently, L4S employs per-flow queue selection, i.e. all packets of a flow are marked the same way and thus use the same queues, even though each packet is marked separately. However, this may hurt tail latency and latency-sensitive applications because transient congestion and queue buildups may only affect a fraction of packets in a flow. We present SwiftQueue, a new L4S queue-selection strategy in which a sender uses a novel per-packet latency predictor to pinpoint which packets likely have latency spikes or drops. The insight is that many packet-level latency variations result from complex interactions among recent packets at shared router queues. Yet, these intricate packet-level latency patterns are hard to learn efficiently by traditional models. Instead, SwiftQueue uses a custom Transformer, which is well-studied for its expressiveness on sequential patterns, to predict the next packet's latency based on the latencies of recently received ACKs. Based on the predicted latency of each outgoing packet, SwiftQueue's sender dynamically marks the L4S packet header to assign packets to potentially different queues, even within the same flow. Using real network traces, we show that SwiftQueue is 45-65% more accurate in predicting latency and its variations than state-of-art methods. Based on its latency prediction, SwiftQueue reduces the tail latency for L4S-enabled flows by 36-45%, compared with the existing L4S queue-selection method.",
        "subjects": [
            "cs.NI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06120",
        "abstract url": "https://arxiv.org/abs/2410.06120",
        "title": "Uncertainty estimation via ensembles of deep learning models and dropout layers for seismic traces",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep learning models have demonstrated remarkable success in various fields, including seismology. However, one major challenge in deep learning is the presence of mislabeled examples. Additionally, accurately estimating model uncertainty is another challenge in machine learning. In this study, we develop Convolutional Neural Networks (CNNs) to classify seismic waveforms based on first-motion polarity. We trained multiple CNN models with different settings. We also constructed ensembles of networks to estimate uncertainty. The results showed that each training setting achieved satisfactory performances, with the ensemble method outperforming individual networks in uncertainty estimation. We observe that the uncertainty estimation ability of the ensembles of networks can be enhanced using dropout layers. In addition, comparisons among different training settings revealed that the use of dropout improved the robustness of networks to mislabeled examples.",
        "subjects": [
            "cs.LG",
            "physics.data-an"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06121",
        "abstract url": "https://arxiv.org/abs/2410.06121",
        "title": "Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA",
        "rating": "0.5",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Retrieval-Augmented Generation (RAG) is widely used to inject external non-parametric knowledge into large language models (LLMs). Recent works suggest that Knowledge Graphs (KGs) contain valuable external knowledge for LLMs. Retrieving information from KGs differs from extracting it from document sets. Most existing approaches seek to directly retrieve relevant subgraphs, thereby eliminating the need for extensive SPARQL annotations, traditionally required by semantic parsing methods. In this paper, we model the subgraph retrieval task as a conditional generation task handled by small language models. Specifically, we define a subgraph identifier as a sequence of relations, each represented as a special token stored in the language models. Our base generative subgraph retrieval model, consisting of only 220M parameters, achieves competitive retrieval performance compared to state-of-the-art models relying on 7B parameters, demonstrating that small language models are capable of performing the subgraph retrieval task. Furthermore, our largest 3B model, when plugged with an LLM reader, sets new SOTA end-to-end performance on both the WebQSP and CWQ benchmarks. Our model and data will be made available online: https://github.com/hwy9855/GSR.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by EMNLP 2024 Findings"
    },
    {
        "paper id": "2410.06128",
        "abstract url": "https://arxiv.org/abs/2410.06128",
        "title": "Zero-Shot Learning of Causal Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "With the increasing acquisition of datasets over time, we now have access to precise and varied descriptions of the world, capturing all sorts of phenomena. These datasets can be seen as empirical observations of unknown causal generative processes, which can commonly be described by Structural Causal Models (SCMs). Recovering these causal generative processes from observations poses formidable challenges, and often require to learn a specific generative model for each dataset. In this work, we propose to learn a \\emph{single} model capable of inferring in a zero-shot manner the causal generative processes of datasets. Rather than learning a specific SCM for each dataset, we enable the Fixed-Point Approach (FiP) proposed in~\\cite{scetbon2024fip}, to infer the generative SCMs conditionally on their empirical representations. More specifically, we propose to amortize the learning of a conditional version of FiP to infer generative SCMs from observations and causal structures on synthetically generated datasets. We show that our model is capable of predicting in zero-shot the true generative SCMs, and as a by-product, of (i) generating new dataset samples, and (ii) inferring intervened ones. Our experiments demonstrate that our amortized procedure achieves performances on par with SoTA methods trained specifically for each dataset on both in and out-of-distribution problems. To the best of our knowledge, this is the first time that SCMs are inferred in a zero-shot manner from observations, paving the way for a paradigmatic shift towards the assimilation of causal knowledge across datasets.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06171",
        "abstract url": "https://arxiv.org/abs/2410.06171",
        "title": "Stochastic Kernel Regularisation Improves Generalisation in Deep Kernel Machines",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent work developed convolutional deep kernel machines, achieving 92.7% test accuracy on CIFAR-10 using a ResNet-inspired architecture, which is SOTA for kernel methods. However, this still lags behind neural networks, which easily achieve over 94% test accuracy with similar architectures. In this work we introduce several modifications to improve the convolutional deep kernel machine's generalisation, including stochastic kernel regularisation, which adds noise to the learned Gram matrices during training. The resulting model achieves 94.5% test accuracy on CIFAR-10. This finding has important theoretical and practical implications, as it demonstrates that the ability to perform well on complex tasks like image classification is not unique to neural networks. Instead, other approaches including deep kernel methods can achieve excellent performance on such tasks, as long as they have the capacity to learn representations from data.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "Neurips 2024 Camera Ready Version (without checklist)"
    },
    {
        "paper id": "2410.06176",
        "abstract url": "https://arxiv.org/abs/2410.06176",
        "title": "SC-Bench: A Large-Scale Dataset for Smart Contract Auditing",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "There is a huge demand to ensure the compliance of smart contracts listed on blockchain platforms to safety and economic standards. Today, manual efforts in the form of auditing are commonly used to achieve this goal. ML-based automated techniques have the promise to alleviate human efforts and the resulting monetary costs. However, unlike other domains where ML techniques have had huge successes, no systematic ML techniques have been proposed or applied to smart contract auditing. We present SC-Bench, the first dataset for automated smart-contract auditing research. SC-Bench consists of 5,377 real-world smart contracts running on Ethereum, a widely used blockchain platform, and 15,975 violations of standards on Ehereum called ERCs. Out of these violations, 139 are real violations programmers made. The remaining are errors we systematically injected to reflect the violations of different ERC rules. We evaluate SC-Bench using GPT-4 by prompting it with both the contracts and ERC rules. In addition, we manually identify each violated rule and the corresponding code site (i.e., oracle) and prompt GPT-4 with the information asking for a True-or-False question. Our results show that without the oracle, GPT-4 can only detect 0.9% violations, and with the oracle, it detects 22.9% violations. These results show the potential room for improvement in ML-based techniques for smart-contract auditing.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06187",
        "abstract url": "https://arxiv.org/abs/2410.06187",
        "title": "A column generation algorithm with dynamic constraint aggregation for minimum sum-of-squares clustering",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The minimum sum-of-squares clustering problem (MSSC), also known as $k$-means clustering, refers to the problem of partitioning $n$ data points into $k$ clusters, with the objective of minimizing the total sum of squared Euclidean distances between each point and the center of its assigned cluster. We propose an efficient algorithm for solving large-scale MSSC instances, which combines column generation (CG) with dynamic constraint aggregation (DCA) to effectively reduce the number of constraints considered in the CG master problem. DCA was originally conceived to reduce degeneracy in set partitioning problems by utilizing an aggregated restricted master problem obtained from a partition of the set partitioning constraints into disjoint clusters. In this work, we explore the use of DCA within a CG algorithm for MSSC exact solution. Our method is fine-tuned by a series of ablation studies on DCA design choices, and is demonstrated to significantly outperform existing state-of-the-art exact approaches available in the literature.",
        "subjects": [
            "math.OC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06191",
        "abstract url": "https://arxiv.org/abs/2410.06191",
        "title": "Benign Overfitting for Regression with Trained Two-Layer ReLU Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We study the least-square regression problem with a two-layer fully-connected neural network, with ReLU activation function, trained by gradient flow. Our first result is a generalization result, that requires no assumptions on the underlying regression function or the noise other than that they are bounded. We operate in the neural tangent kernel regime, and our generalization result is developed via a decomposition of the excess risk into estimation and approximation errors, viewing gradient flow as an implicit regularizer. This decomposition in the context of neural networks is a novel perspective of gradient descent, and helps us avoid uniform convergence traps. In this work, we also establish that under the same setting, the trained network overfits to the data. Together, these results, establishes the first result on benign overfitting for finite-width ReLU networks for arbitrary regression functions.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "65 pages"
    },
    {
        "paper id": "2410.06212",
        "abstract url": "https://arxiv.org/abs/2410.06212",
        "title": "Solving robust MDPs as a sequence of static RL problems",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Designing control policies whose performance level is guaranteed to remain above a given threshold in a span of environments is a critical feature for the adoption of reinforcement learning (RL) in real-world applications. The search for such robust policies is a notoriously difficult problem, related to the so-called dynamic model of transition function uncertainty, where the environment dynamics are allowed to change at each time step. But in practical cases, one is rather interested in robustness to a span of static transition models throughout interaction episodes. The static model is known to be harder to solve than the dynamic one, and seminal algorithms, such as robust value iteration, as well as most recent works on deep robust RL, build upon the dynamic model. In this work, we propose to revisit the static model. We suggest an analysis of why solving the static model under some mild hypotheses is a reasonable endeavor, based on an equivalence with the dynamic model, and formalize the general intuition that robust MDPs can be solved by tackling a series of static problems. We introduce a generic meta-algorithm called IWOCS, which incrementally identifies worst-case transition models so as to guide the search for a robust policy. Discussion on IWOCS sheds light on new ways to decouple policy optimization and adversarial transition functions and opens new perspectives for analysis. We derive a deep RL version of IWOCS and demonstrate it is competitive with state-of-the-art algorithms on classical benchmarks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "12 pages"
    },
    {
        "paper id": "2410.06213",
        "abstract url": "https://arxiv.org/abs/2410.06213",
        "title": "RL, but don't do anything I wouldn't do",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In reinforcement learning, if the agent's reward differs from the designers' true utility, even only rarely, the state distribution resulting from the agent's policy can be very bad, in theory and in practice. When RL policies would devolve into undesired behavior, a common countermeasure is KL regularization to a trusted policy (\"Don't do anything I wouldn't do\"). All current cutting-edge language models are RL agents that are KL-regularized to a \"base policy\" that is purely predictive. Unfortunately, we demonstrate that when this base policy is a Bayesian predictive model of a trusted policy, the KL constraint is no longer reliable for controlling the behavior of an advanced RL agent. We demonstrate this theoretically using algorithmic information theory, and while systems today are too weak to exhibit this theorized failure precisely, we RL-finetune a language model and find evidence that our formal results are plausibly relevant in practice. We also propose a theoretical alternative that avoids this problem by replacing the \"Don't do anything I wouldn't do\" principle with \"Don't do anything I mightn't do\".",
        "subjects": [
            "cs.LG"
        ],
        "comment": "10 pages, 7 page appendix, 4 figures"
    },
    {
        "paper id": "2410.06214",
        "abstract url": "https://arxiv.org/abs/2410.06214",
        "title": "Fair-OBNC: Correcting Label Noise for Fairer Datasets",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Data used by automated decision-making systems, such as Machine Learning models, often reflects discriminatory behavior that occurred in the past. These biases in the training data are sometimes related to label noise, such as in COMPAS, where more African-American offenders are wrongly labeled as having a higher risk of recidivism when compared to their White counterparts. Models trained on such biased data may perpetuate or even aggravate the biases with respect to sensitive information, such as gender, race, or age. However, while multiple label noise correction approaches are available in the literature, these focus on model performance exclusively. In this work, we propose Fair-OBNC, a label noise correction method with fairness considerations, to produce training datasets with measurable demographic parity. The presented method adapts Ordering-Based Noise Correction, with an adjusted criterion of ordering, based both on the margin of error of an ensemble, and the potential increase in the observed demographic parity of the dataset. We evaluate Fair-OBNC against other different pre-processing techniques, under different scenarios of controlled label noise. Our results show that the proposed method is the overall better alternative within the pool of label correction methods, being capable of attaining better reconstructions of the original labels. Models trained in the corrected data have an increase, on average, of 150% in demographic parity, when compared to models trained in data with noisy labels, across the considered levels of label noise.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06219",
        "abstract url": "https://arxiv.org/abs/2410.06219",
        "title": "Gaussian Variational Schemes on Bounded and Unbounded Domains",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "A machine-learnable variational scheme using Gaussian radial basis functions (GRBFs) is presented and used to approximate linear problems on bounded and unbounded domains. In contrast to standard mesh-free methods, which use GRBFs to discretize strong-form differential equations, this work exploits the relationship between integrals of GRBFs, their derivatives, and polynomial moments to produce exact quadrature formulae which enable weak-form expressions. Combined with trainable GRBF means and covariances, this leads to a flexible, generalized Galerkin variational framework which is applied in the infinite-domain setting where the scheme is conforming, as well as the bounded-domain setting where it is not. Error rates for the proposed GRBF scheme are derived in each case, and examples are presented demonstrating utility of this approach as a surrogate modeling technique.",
        "subjects": [
            "math.NA",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06225",
        "abstract url": "https://arxiv.org/abs/2410.06225",
        "title": "A Timeline and Analysis for Representation Plasticity in Large Language Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The ability to steer AI behavior is crucial to preventing its long term dangerous and catastrophic potential. Representation Engineering (RepE) has emerged as a novel, powerful method to steer internal model behaviors, such as \"honesty\", at a top-down level. Understanding the steering of representations should thus be placed at the forefront of alignment initiatives. Unfortunately, current efforts to understand plasticity at this level are highly neglected. This paper aims to bridge the knowledge gap and understand how LLM representation stability, specifically for the concept of \"honesty\", and model plasticity evolve by applying steering vectors extracted at different fine-tuning stages, revealing differing magnitudes of shifts in model behavior. The findings are pivotal, showing that while early steering exhibits high plasticity, later stages have a surprisingly responsive critical window. This pattern is observed across different model architectures, signaling that there is a general pattern of model plasticity that can be used for effective intervention. These insights greatly contribute to the field of AI transparency, addressing a pressing lack of efficiency limiting our ability to effectively steer model behavior.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06237",
        "abstract url": "https://arxiv.org/abs/2410.06237",
        "title": "BUMBLE: Unifying Reasoning and Acting with Vision-Language Models for Building-wide Mobile Manipulation",
        "rating": "0.5",
        "keywords": [
            [
                "Vision-Language",
                "VLM"
            ],
            [
                "RGBD"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "To operate at a building scale, service robots must perform very long-horizon mobile manipulation tasks by navigating to different rooms, accessing different floors, and interacting with a wide and unseen range of everyday objects. We refer to these tasks as Building-wide Mobile Manipulation. To tackle these inherently long-horizon tasks, we introduce BUMBLE, a unified Vision-Language Model (VLM)-based framework integrating open-world RGBD perception, a wide spectrum of gross-to-fine motor skills, and dual-layered memory. Our extensive evaluation (90+ hours) indicates that BUMBLE outperforms multiple baselines in long-horizon building-wide tasks that require sequencing up to 12 ground truth skills spanning 15 minutes per trial. BUMBLE achieves 47.1% success rate averaged over 70 trials in different buildings, tasks, and scene layouts from different starting rooms and floors. Our user study demonstrates 22% higher satisfaction with our method than state-of-the-art mobile manipulation methods. Finally, we demonstrate the potential of using increasingly-capable foundation models to push performance further. For more information, see https://robin-lab.cs.utexas.edu/BUMBLE/",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "7 Figures, 2 Tables, 11 Pages"
    },
    {
        "paper id": "2410.06265",
        "abstract url": "https://arxiv.org/abs/2410.06265",
        "title": "SHADE: Deep Density-based Clustering",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Detecting arbitrarily shaped clusters in high-dimensional noisy data is challenging for current clustering methods. We introduce SHADE (Structure-preserving High-dimensional Analysis with Density-based Exploration), the first deep clustering algorithm that incorporates density-connectivity into its loss function. Similar to existing deep clustering algorithms, SHADE supports high-dimensional and large data sets with the expressive power of a deep autoencoder. In contrast to most existing deep clustering methods that rely on a centroid-based clustering objective, SHADE incorporates a novel loss function that captures density-connectivity. SHADE thereby learns a representation that enhances the separation of density-connected clusters. SHADE detects a stable clustering and noise points fully automatically without any user input. It outperforms existing methods in clustering quality, especially on data that contain non-Gaussian clusters, such as video data. Moreover, the embedded space of SHADE is suitable for visualization and interpretation of the clustering results as the individual shapes of the clusters are preserved.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Short version accepted at ICDM 2024"
    },
    {
        "paper id": "2410.06267",
        "abstract url": "https://arxiv.org/abs/2410.06267",
        "title": "Can metacognition predict your success in solving problems? An exploratory case study in programming",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Metacognition has been recognized as an essential skill for academic success and for performance in solving problems. During learning or problem-solving, metacognitive skills facilitate a range of cognitive and affective processes, leading collectively to improved performance. This study explores the predictive potential of metacognition in the second introductory programming course. A two-dimensional model has been proposed, consisting of metacognitive awareness and metacognitive behavior. To evaluate the predictive capacity of metacognition empirically, an exploratory case study with 194 participants from two institutions was conducted in the second introductory programming course. A latent approach was employed to examine the associations between metacognition and performance in object-oriented programming. Our findings indicate that both metacognitive dimensions have a positive effect on programming. Likewise, the results of the structural equation modeling show that 27% of variance in programming performance is explained by metacognitive behavior. Following the results, metacognition has the potential to be considered as one of the important predictors of performance in introductory programming.",
        "subjects": [
            "cs.CY",
            "cs.HC"
        ],
        "comment": "Preprint submitted to ACM Koli Calling 2024 conference"
    },
    {
        "paper id": "2410.06272",
        "abstract url": "https://arxiv.org/abs/2410.06272",
        "title": "The Mystery of Compositional Generalization in Graph-based Generative Commonsense Reasoning",
        "rating": "0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "While LLMs have emerged as performant architectures for reasoning tasks, their compositional generalization capabilities have been questioned. In this work, we introduce a Compositional Generalization Challenge for Graph-based Commonsense Reasoning (CGGC) that goes beyond previous evaluations that are based on sequences or tree structures - and instead involves a reasoning graph: It requires models to generate a natural sentence based on given concepts and a corresponding reasoning graph, where the presented graph involves a previously unseen combination of relation types. To master this challenge, models need to learn how to reason over relation tupels within the graph, and how to compose them when conceptualizing a verbalization. We evaluate seven well-known LLMs using in-context learning and find that performant LLMs still struggle in compositional generalization. We investigate potential causes of this gap by analyzing the structures of reasoning graphs, and find that different structures present varying levels of difficulty for compositional generalization. Arranging the order of demonstrations according to the structures' difficulty shows that organizing samples in an easy-to-hard schema enhances the compositional generalization ability of LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted Findings at EMNLP 2024"
    },
    {
        "paper id": "2410.06273",
        "abstract url": "https://arxiv.org/abs/2410.06273",
        "title": "PREDICT: Preference Reasoning by Evaluating Decomposed preferences Inferred from Candidate Trajectories",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Accommodating human preferences is essential for creating AI agents that deliver personalized and effective interactions. Recent work has shown the potential for LLMs to infer preferences from user interactions, but they often produce broad and generic preferences, failing to capture the unique and individualized nature of human preferences. This paper introduces PREDICT, a method designed to enhance the precision and adaptability of inferring preferences. PREDICT incorporates three key elements: (1) iterative refinement of inferred preferences, (2) decomposition of preferences into constituent components, and (3) validation of preferences across multiple trajectories. We evaluate PREDICT on two distinct environments: a gridworld setting and a new text-domain environment (PLUME). PREDICT more accurately infers nuanced human preferences improving over existing baselines by 66.2\\% (gridworld environment) and 41.0\\% (PLUME).",
        "subjects": [
            "cs.AI",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06290",
        "abstract url": "https://arxiv.org/abs/2410.06290",
        "title": "Score Design for Multi-Criteria Incentivization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "We present a framework for designing scores to summarize performance metrics. Our design has two multi-criteria objectives: (1) improving on scores should improve all performance metrics, and (2) achieving pareto-optimal scores should achieve pareto-optimal metrics. We formulate our design to minimize the dimensionality of scores while satisfying the objectives. We give algorithms to design scores, which are provably minimal under mild assumptions on the structure of performance metrics. This framework draws motivation from real-world practices in hospital rating systems, where misaligned scores and performance metrics lead to unintended consequences.",
        "subjects": [
            "cs.CY",
            "cs.CG",
            "cs.LG"
        ],
        "comment": "A condensed version of this paper appeared at Foundations of Responsible Computing (FORC) 2024"
    },
    {
        "paper id": "2410.06299",
        "abstract url": "https://arxiv.org/abs/2410.06299",
        "title": "A Taxonomy of Collectible Card Games from a Game-Playing AI Perspective",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Collectible card games are challenging, widely played games that have received increasing attention from the AI research community in recent years. Despite important breakthroughs, the field still poses many unresolved challenges. This work aims to help further research on the genre by proposing a taxonomy of collectible card games by analyzing their rules, mechanics, and game modes from the perspective of game-playing AI research. To achieve this, we studied a set of popular games and provided a thorough discussion about their characteristics.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "16 pages, accepted at the International Conference on Entertainment Computing (ICEC) 2024"
    },
    {
        "paper id": "2410.06303",
        "abstract url": "https://arxiv.org/abs/2410.06303",
        "title": "Compositional Risk Minimization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In this work, we tackle a challenging and extreme form of subpopulation shift, which is termed compositional shift. Under compositional shifts, some combinations of attributes are totally absent from the training distribution but present in the test distribution. We model the data with flexible additive energy distributions, where each energy term represents an attribute, and derive a simple alternative to empirical risk minimization termed compositional risk minimization (CRM). We first train an additive energy classifier to predict the multiple attributes and then adjust this classifier to tackle compositional shifts. We provide an extensive theoretical analysis of CRM, where we show that our proposal extrapolates to special affine hulls of seen attribute combinations. Empirical evaluations on benchmark datasets confirms the improved robustness of CRM compared to other methods from the literature designed to tackle various forms of subpopulation shifts.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Preprint. Under Review"
    },
    {
        "paper id": "2410.06307",
        "abstract url": "https://arxiv.org/abs/2410.06307",
        "title": "Model Predictive Control is Almost Optimal for Restless Bandit",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider the discrete time infinite horizon average reward restless markovian bandit (RMAB) problem. We propose a \\emph{model predictive control} based non-stationary policy with a rolling computational horizon $\u03c4$. At each time-slot, this policy solves a $\u03c4$ horizon linear program whose first control value is kept as a control for the RMAB. Our solution requires minimal assumptions and quantifies the loss in optimality in terms of $\u03c4$ and the number of arms, $N$. We show that its sub-optimality gap is $O(1/\\sqrt{N})$ in general, and $\\exp(-\u03a9(N))$ under a local-stability condition. Our proof is based on a framework from dynamic control known as \\emph{dissipativity}. Our solution easy to implement and performs very well in practice when compared to the state of the art. Further, both our solution and our proof methodology can easily be generalized to more general constrained MDP settings and should thus, be of great interest to the burgeoning RMAB community.",
        "subjects": [
            "math.OC",
            "cs.LG",
            "math.PR",
            "stat.ML"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2410.06308",
        "abstract url": "https://arxiv.org/abs/2410.06308",
        "title": "Quantifying Training Difficulty and Accelerating Convergence in Neural Network-Based PDE Solvers",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Neural network-based methods have emerged as powerful tools for solving partial differential equations (PDEs) in scientific and engineering applications, particularly when handling complex domains or incorporating empirical data. These methods leverage neural networks as basis functions to approximate PDE solutions. However, training such networks can be challenging, often resulting in limited accuracy. In this paper, we investigate the training dynamics of neural network-based PDE solvers with a focus on the impact of initialization techniques. We assess training difficulty by analyzing the eigenvalue distribution of the kernel and apply the concept of effective rank to quantify this difficulty, where a larger effective rank correlates with faster convergence of the training error. Building upon this, we discover through theoretical analysis and numerical experiments that two initialization techniques, partition of unity (PoU) and variance scaling (VS), enhance the effective rank, thereby accelerating the convergence of training error. Furthermore, comprehensive experiments using popular PDE-solving frameworks, such as PINN, Deep Ritz, and the operator learning framework DeepOnet, confirm that these initialization techniques consistently speed up convergence, in line with our theoretical findings.",
        "subjects": [
            "math.NA",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06317",
        "abstract url": "https://arxiv.org/abs/2410.06317",
        "title": "Learning in complex action spaces without policy gradients",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Conventional wisdom suggests that policy gradient methods are better suited to complex action spaces than action-value methods. However, foundational studies have shown equivalences between these paradigms in small and finite action spaces (O'Donoghue et al., 2017; Schulman et al., 2017a). This raises the question of why their computational applicability and performance diverge as the complexity of the action space increases. We hypothesize that the apparent superiority of policy gradients in such settings stems not from intrinsic qualities of the paradigm, but from universal principles that can also be applied to action-value methods to serve similar functionality. We identify three such principles and provide a framework for incorporating them into action-value methods. To support our hypothesis, we instantiate this framework in what we term QMLE, for Q-learning with maximum likelihood estimation. Our results show that QMLE can be applied to complex action spaces with a controllable computational cost that is comparable to that of policy gradient methods, all without using policy gradients. Furthermore, QMLE demonstrates strong performance on the DeepMind Control Suite, even when compared to the state-of-the-art methods such as DMPO and D4PG.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06324",
        "abstract url": "https://arxiv.org/abs/2410.06324",
        "title": "Differentiation Through Black-Box Quadratic Programming Solvers",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In recent years, many deep learning approaches have incorporated layers that solve optimization problems (e.g., linear, quadratic, and semidefinite programs). Integrating these optimization problems as differentiable layers requires computing the derivatives of the optimization problem's solution with respect to its objective and constraints. This has so far prevented the use of state-of-the-art black-box numerical solvers within neural networks, as they lack a differentiable interface. To address this issue for one of the most common convex optimization problems -- quadratic programming (QP) -- we introduce dQP, a modular framework that enables plug-and-play differentiation for any QP solver, allowing seamless integration into neural networks and bi-level optimization tasks. Our solution is based on the core theoretical insight that knowledge of the active constraint set at the QP optimum allows for explicit differentiation. This insight reveals a unique relationship between the computation of the solution and its derivative, enabling efficient differentiation of any solver, that only requires the primal solution. Our implementation, which will be made publicly available, interfaces with an existing framework that supports over 15 state-of-the-art QP solvers, providing each with a fully differentiable backbone for immediate use as a differentiable layer in learning setups. To demonstrate the scalability and effectiveness of dQP, we evaluate it on a large benchmark dataset of QPs with varying structures. We compare dQP with existing differentiable QP methods, demonstrating its advantages across a range of problems, from challenging small and dense problems to large-scale sparse ones, including a novel bi-level geometry optimization problem.",
        "subjects": [
            "cs.LG",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06332",
        "abstract url": "https://arxiv.org/abs/2410.06332",
        "title": "Boolean Nearest Neighbor Language in the Knowledge Compilation Map",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The Boolean Nearest Neighbor (BNN) representation of Boolean functions was recently introduced by Hajnal, Liu and Turan. A BNN representation of $f$ is a pair $(P,N)$ of sets of Boolean vectors (called positive and negative prototypes) where $f(x)=1$ for every positive prototype $x \\in P$, $f(x)=0$ for all every negative prototype $x \\in N$, and the value $f(x)$ for $x \\not\\in P \\cup N$ is determined by the type of the closest prototype. The main aim of this paper is to determine the position of the BNN language in the Knowledge Compilation Map (KCM). To this end, we derive results which compare the succinctness of the BNN language to several standard languages from KCM, and determine the complexity status of most standard queries and transformations for BNN inputs.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "19 pages, 5 figures, 2 tables"
    },
    {
        "paper id": "2410.06348",
        "abstract url": "https://arxiv.org/abs/2410.06348",
        "title": "Harnessing the Power of Noise: A Survey of Techniques and Applications",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Noise, traditionally considered a nuisance in computational systems, is reconsidered for its unexpected and counter-intuitive benefits across a wide spectrum of domains, including nonlinear information processing, signal processing, image processing, machine learning, network science, and natural language processing. Through a comprehensive review of both historical and contemporary research, this survey presents a dual perspective on noise, acknowledging its potential to both disrupt and enhance performance. Particularly, we highlight how noise-enhanced training strategies can lead to models that better generalize from noisy data, positioning noise not just as a challenge to overcome but as a strategic tool for improvement. This work calls for a shift in how we perceive noise, proposing that it can be a spark for innovation and advancement in the information era.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06349",
        "abstract url": "https://arxiv.org/abs/2410.06349",
        "title": "Robust Domain Generalisation with Causal Invariant Bayesian Neural Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep neural networks can obtain impressive performance on various tasks under the assumption that their training domain is identical to their target domain. Performance can drop dramatically when this assumption does not hold. One explanation for this discrepancy is the presence of spurious domain-specific correlations in the training data that the network exploits. Causal mechanisms, in the other hand, can be made invariant under distribution changes as they allow disentangling the factors of distribution underlying the data generation. Yet, learning causal mechanisms to improve out-of-distribution generalisation remains an under-explored area. We propose a Bayesian neural architecture that disentangles the learning of the the data distribution from the inference process mechanisms. We show theoretically and experimentally that our model approximates reasoning under causal interventions. We demonstrate the performance of our method, outperforming point estimate-counterparts, on out-of-distribution image recognition tasks where the data distribution acts as strong adversarial confounders.",
        "subjects": [
            "cs.LG",
            "stat.ME"
        ],
        "comment": "16 pages, 10 pages for main paper and 6 pages for references and appendix, 8 figures"
    },
    {
        "paper id": "2410.06352",
        "abstract url": "https://arxiv.org/abs/2410.06352",
        "title": "Tree-Based Leakage Inspection and Control in Concept Bottleneck Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "As AI models grow larger, the demand for accountability and interpretability has become increasingly critical for understanding their decision-making processes. Concept Bottleneck Models (CBMs) have gained attention for enhancing interpretability by mapping inputs to intermediate concepts before making final predictions. However, CBMs often suffer from information leakage, where additional input data, not captured by the concepts, is used to improve task performance, complicating the interpretation of downstream predictions. In this paper, we introduce a novel approach for training both joint and sequential CBMs that allows us to identify and control leakage using decision trees. Our method quantifies leakage by comparing the decision paths of hard CBMs with their soft, leaky counterparts. Specifically, we show that soft leaky CBMs extend the decision paths of hard CBMs, particularly in cases where concept information is incomplete. Using this insight, we develop a technique to better inspect and manage leakage, isolating the subsets of data most affected by this. Through synthetic and real-world experiments, we demonstrate that controlling leakage in this way not only improves task accuracy but also yields more informative and transparent explanations.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06364",
        "abstract url": "https://arxiv.org/abs/2410.06364",
        "title": "SpaLLM: Unified Compressive Adaptation of Large Language Models with Sketching",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Compressive adaptation approaches, such as QLoRA, are widely popular alternatives for reducing memory requirements during fine-tuning of large language models (LLMs) while producing models capable of handling various downstream tasks. The key idea is to employ a \"two-tower\" architecture: compressing pre-trained LLM parameters into compact representations and fine-tuning the additive full-precision adapter, which typically has few tunable parameters in low-rank format. However, the strict algebraic assumptions, such as low-rank assumption, and the complexity of composing two-tower architectures are some of the known shortcomings, resulting in a poor accuracy-efficiency trade-off. In response to these known limitations, we propose SpaLLM (Sketched Parameter Adaptation of LLMs), a novel compressive adaptation approach for LLMs. This method is also the first to illustrate parameter-sharing compression methods for LLM fine-tuning, which, unlike QLoRA, are free from strict low-rank algebraic assumptions on adapters. Furthermore, our proposal unifies model compression and adaptation into a single, streamlined process, eliminating the need for two-tower architectures. SpaLLM sketches pre-trained LLM weights into lookup tables and directly fine-tunes the values in these tables. This approach simplifies LLMs' compressive adaptation workflow, potentially improves multi-user serving efficiency, and delivers significantly better accuracy for both natural language understanding and generation tasks. Moreover, by avoiding the \"two-tower\" architecture, our framework only requires one compressed matrix multiplication per layer during inference, demonstrating superior inference efficiency compared to previous methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06372",
        "abstract url": "https://arxiv.org/abs/2410.06372",
        "title": "Cooperative and Asynchronous Transformer-based Mission Planning for Heterogeneous Teams of Mobile Robots",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Coordinating heterogeneous teams of mobile robots for tasks such as search and rescue is highly challenging. This is due to the complexities of perception, decision making and planning in such environments, with agents' non-synchronous operation, constrained communication, and limited computational resources. This paper presents the Cooperative and Asynchronous Transformer-based Mission Planning (CATMiP) framework, which leverages multi-agent reinforcement learning (MARL) to effectively coordinate agents with heterogeneous sensing, motion, and actuation capabilities. The framework introduces a Class-based Macro-Action Decentralized Partially Observable Markov Decision Process (CMD-POMDP) model to handle asynchronous decision-making among different agent classes via macro-actions. It also extends the Multi-Agent Transformer (MAT) architecture to facilitate distributed, ad hoc communication among the agents. CATMiP easily adapts to mission complexities and communication constraints, and scales to varying environment sizes and team compositions. Simulations demonstrate its scalability and ability to achieve cooperative mission objectives with two classes of explorer and rescuer agents, even under severe communication constraints. The code is available at https://github.com/mylad13/CATMiP.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "8 pages, 7 figures. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2410.06376",
        "abstract url": "https://arxiv.org/abs/2410.06376",
        "title": "Riemannian Optimization for Non-convex Euclidean Distance Geometry with Global Recovery Guarantees",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The problem of determining the configuration of points from partial distance information, known as the Euclidean Distance Geometry (EDG) problem, is fundamental to many tasks in the applied sciences. In this paper, we propose two algorithms grounded in the Riemannian optimization framework to address the EDG problem. Our approach formulates the problem as a low-rank matrix completion task over the Gram matrix, using partial measurements represented as expansion coefficients of the Gram matrix in a non-orthogonal basis. For the first algorithm, under a uniform sampling with replacement model for the observed distance entries, we demonstrate that, with high probability, a Riemannian gradient-like algorithm on the manifold of rank-$r$ matrices converges linearly to the true solution, given initialization via a one-step hard thresholding. This holds provided the number of samples, $m$, satisfies $m \\geq \\mathcal{O}(n^{7/4}r^2 \\log(n))$. With a more refined initialization, achieved through resampled Riemannian gradient-like descent, we further improve this bound to $m \\geq \\mathcal{O}(nr^2 \\log(n))$. Our analysis for the first algorithm leverages a non-self-adjoint operator and depends on deriving eigenvalue bounds for an inner product matrix of restricted basis matrices, leveraging sparsity properties for tighter guarantees than previously established. The second algorithm introduces a self-adjoint surrogate for the sampling operator. This algorithm demonstrates strong numerical performance on both synthetic and real data. Furthermore, we show that optimizing over manifolds of higher-than-rank-$r$ matrices yields superior numerical results, consistent with recent literature on overparameterization in the EDG problem.",
        "subjects": [
            "math.OC",
            "cs.LG"
        ],
        "comment": "38 pages, 4 figures, 5 tables"
    },
    {
        "paper id": "2410.06378",
        "abstract url": "https://arxiv.org/abs/2410.06378",
        "title": "Covering Numbers for Deep ReLU Networks with Applications to Function Approximation and Nonparametric Regression",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Covering numbers of families of (deep) ReLU networks have been used to characterize their approximation-theoretic performance, upper-bound the prediction error they incur in nonparametric regression, and quantify their classification capacity. These results are based on covering number upper bounds obtained through the explicit construction of coverings. Lower bounds on covering numbers do not seem to be available in the literature. The present paper fills this gap by deriving tight (up to a multiplicative constant) lower and upper bounds on the covering numbers of fully-connected networks with bounded weights, sparse networks with bounded weights, and fully-connected networks with quantized weights. Thanks to the tightness of the bounds, a fundamental understanding of the impact of sparsity, quantization, bounded vs. unbounded weights, and network output truncation can be developed. Furthermore, the bounds allow to characterize the fundamental limits of neural network transformation, including network compression, and lead to sharp upper bounds on the prediction error in nonparametric regression through deep networks. Specifically, we can remove a $\\log^6(n)$-factor in the best-known sample complexity rate in the estimation of Lipschitz functions through deep networks thereby establishing optimality. Finally, we identify a systematic relation between optimal nonparametric regression and optimal approximation through deep networks, unifying numerous results in the literature and uncovering general underlying principles.",
        "subjects": [
            "stat.ML",
            "cs.AI",
            "cs.IT",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06399",
        "abstract url": "https://arxiv.org/abs/2410.06399",
        "title": "Adaptive Random Fourier Features Training Stabilized By Resampling With Applications in Image Regression",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper presents an enhanced adaptive random Fourier features (ARFF) training algorithm for shallow neural networks, building upon the work introduced in \"Adaptive Random Fourier Features with Metropolis Sampling\", Kammonen et al., Foundations of Data Science, 2(3):309--332, 2020. This improved method uses a particle filter type resampling technique to stabilize the training process and reduce sensitivity to parameter choices. With resampling, the Metropolis test may also be omitted, reducing the number of hyperparameters and reducing the computational cost per iteration, compared to ARFF. We present comprehensive numerical experiments demonstrating the efficacy of our proposed algorithm in function regression tasks, both as a standalone method and as a pre-training step before gradient-based optimization, here Adam. Furthermore, we apply our algorithm to a simple image regression problem, showcasing its utility in sampling frequencies for the random Fourier features (RFF) layer of coordinate-based multilayer perceptrons (MLPs). In this context, we use the proposed algorithm to sample the parameters of the RFF layer in an automated manner.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "42 pages"
    },
    {
        "paper id": "2410.06407",
        "abstract url": "https://arxiv.org/abs/2410.06407",
        "title": "A Skewness-Based Criterion for Addressing Heteroscedastic Noise in Causal Discovery",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Real-world data often violates the equal-variance assumption (homoscedasticity), making it essential to account for heteroscedastic noise in causal discovery. In this work, we explore heteroscedastic symmetric noise models (HSNMs), where the effect $Y$ is modeled as $Y = f(X) + \u03c3(X)N$, with $X$ as the cause and $N$ as independent noise following a symmetric distribution. We introduce a novel criterion for identifying HSNMs based on the skewness of the score (i.e., the gradient of the log density) of the data distribution. This criterion establishes a computationally tractable measurement that is zero in the causal direction but nonzero in the anticausal direction, enabling the causal direction discovery. We extend this skewness-based criterion to the multivariate setting and propose SkewScore, an algorithm that handles heteroscedastic noise without requiring the extraction of exogenous noise. We also conduct a case study on the robustness of SkewScore in a bivariate model with a latent confounder, providing theoretical insights into its performance. Empirical studies further validate the effectiveness of the proposed method.",
        "subjects": [
            "cs.LG",
            "stat.ME",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06415",
        "abstract url": "https://arxiv.org/abs/2410.06415",
        "title": "Biased AI can Influence Political Decision-Making",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "As modern AI models become integral to everyday tasks, concerns about their inherent biases and their potential impact on human decision-making have emerged. While bias in models are well-documented, less is known about how these biases influence human decisions. This paper presents two interactive experiments investigating the effects of partisan bias in AI language models on political decision-making. Participants interacted freely with either a biased liberal, conservative, or unbiased control model while completing political decision-making tasks. We found that participants exposed to politically biased models were significantly more likely to adopt opinions and make decisions aligning with the AI's bias, regardless of their personal political partisanship. However, we also discovered that prior knowledge about AI could lessen the impact of the bias, highlighting the possible importance of AI education for robust bias mitigation. Our findings not only highlight the critical effects of interacting with biased AI and its ability to impact public discourse and political conduct, but also highlights potential techniques for mitigating these risks in the future.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06420",
        "abstract url": "https://arxiv.org/abs/2410.06420",
        "title": "ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments",
        "rating": "0.5",
        "keywords": [
            [
                "Vision Language"
            ],
            [
                "healthcare"
            ],
            [
                "cs.CV",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "The global shortage of healthcare workers has demanded the development of smart healthcare assistants, which can help monitor and alert healthcare workers when necessary. We examine the healthcare knowledge of existing Large Vision Language Models (LVLMs) via the Visual Question Answering (VQA) task in hospital settings through expert annotated open-ended questions. We introduce the Emergency Room Visual Question Answering (ERVQA) dataset, consisting of <image, question, answer> triplets covering diverse emergency room scenarios, a seminal benchmark for LVLMs. By developing a detailed error taxonomy and analyzing answer trends, we reveal the nuanced nature of the task. We benchmark state-of-the-art open-source and closed LVLMs using traditional and adapted VQA metrics: Entailment Score and CLIPScore Confidence. Analyzing errors across models, we infer trends based on properties like decoder type, model size, and in-context examples. Our findings suggest the ERVQA dataset presents a highly complex task, highlighting the need for specialized, domain-specific solutions.",
        "subjects": [
            "cs.CL",
            "cs.CV"
        ],
        "comment": "Accepted at EMNLP 2024"
    },
    {
        "paper id": "2410.06423",
        "abstract url": "https://arxiv.org/abs/2410.06423",
        "title": "FAIREDU: A Multiple Regression-Based Method for Enhancing Fairness in Machine Learning Models for Educational Applications",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Fairness in artificial intelligence and machine learning (AI/ML) models is becoming critically important, especially as decisions made by these systems impact diverse groups. In education, a vital sector for all countries, the widespread application of AI/ML systems raises specific concerns regarding fairness. Current research predominantly focuses on fairness for individual sensitive features, which limits the comprehensiveness of fairness assessments. This paper introduces FAIREDU, a novel and effective method designed to improve fairness across multiple sensitive features. Through extensive experiments, we evaluate FAIREDU effectiveness in enhancing fairness without compromising model performance. The results demonstrate that FAIREDU addresses intersectionality across features such as gender, race, age, and other sensitive features, outperforming state-of-the-art methods with minimal effect on model accuracy. The paper also explores potential future research directions to enhance further the method robustness and applicability to various machine-learning models and datasets.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06481",
        "abstract url": "https://arxiv.org/abs/2410.06481",
        "title": "Leaf Stripping on Uniform Attachment Trees",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this note we analyze the performance of a simple root-finding algorithm in uniform attachment trees. The leaf-stripping algorithm recursively removes all leaves of the tree for a carefully chosen number of rounds. We show that, with probability $1 - \u03b5$, the set of remaining vertices contains the root and has a size only depending on $\u03b5$ but not on the size of the tree.",
        "subjects": [
            "math.PR",
            "cs.DS",
            "cs.LG",
            "stat.ML"
        ],
        "comment": "9 pages, 5 figures"
    },
    {
        "paper id": "2410.06491",
        "abstract url": "https://arxiv.org/abs/2410.06491",
        "title": "Honesty to Subterfuge: In-Context Reinforcement Learning Can Make Honest Models Reward Hack",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Previous work has shown that training \"helpful-only\" LLMs with reinforcement learning on a curriculum of gameable environments can lead models to generalize to egregious specification gaming, such as editing their own reward function or modifying task checklists to appear more successful. We show that gpt-4o, gpt-4o-mini, o1-preview, and o1-mini - frontier models trained to be helpful, harmless, and honest - can engage in specification gaming without training on a curriculum of tasks, purely from in-context iterative reflection (which we call in-context reinforcement learning, \"ICRL\"). We also show that using ICRL to generate highly-rewarded outputs for expert iteration (compared to the standard expert iteration reinforcement learning algorithm) may increase gpt-4o-mini's propensity to learn specification-gaming policies, generalizing (in very rare cases) to the most egregious strategy where gpt-4o-mini edits its own reward function. Our results point toward the strong ability of in-context reflection to discover rare specification-gaming strategies that models might not exhibit zero-shot or with normal training, highlighting the need for caution when relying on alignment of LLMs in zero-shot settings.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": "20 pages, 9 figures"
    },
    {
        "paper id": "2410.06494",
        "abstract url": "https://arxiv.org/abs/2410.06494",
        "title": "Conformal Prediction: A Data Perspective",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Conformal prediction (CP), a distribution-free uncertainty quantification (UQ) framework, reliably provides valid predictive inference for black-box models. CP constructs prediction sets that contain the true output with a specified probability. However, modern data science diverse modalities, along with increasing data and model complexity, challenge traditional CP methods. These developments have spurred novel approaches to address evolving scenarios. This survey reviews the foundational concepts of CP and recent advancements from a data-centric perspective, including applications to structured, unstructured, and dynamic data. We also discuss the challenges and opportunities CP faces in large-scale data and models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "35 pages, journal, survey"
    },
    {
        "paper id": "2410.07263",
        "abstract url": "https://arxiv.org/abs/2410.07263",
        "title": "Memory-augmented Transformers can implement Linear First-Order Optimization Methods",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We show that memory-augmented Transformers (Memformers) can implement linear first-order optimization methods such as conjugate gradient descent, momentum methods, and more generally, methods that linearly combine past gradients. Building on prior work that demonstrates how Transformers can simulate preconditioned gradient descent, we provide theoretical and empirical evidence that Memformers can learn more advanced optimization algorithms. Specifically, we analyze how memory registers in Memformers store suitable intermediate attention values allowing them to implement algorithms such as conjugate gradient. Our results show that Memformers can efficiently learn these methods by training on random linear regression tasks, even learning methods that outperform conjugate gradient. This work extends our knowledge about the algorithmic capabilities of Transformers, showing how they can learn complex optimization methods.",
        "subjects": [
            "cs.LG",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07265",
        "abstract url": "https://arxiv.org/abs/2410.07265",
        "title": "A Survey: Collaborative Hardware and Software Design in the Era of Large Language Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The rapid development of large language models (LLMs) has significantly transformed the field of artificial intelligence, demonstrating remarkable capabilities in natural language processing and moving towards multi-modal functionality. These models are increasingly integrated into diverse applications, impacting both research and industry. However, their development and deployment present substantial challenges, including the need for extensive computational resources, high energy consumption, and complex software optimizations. Unlike traditional deep learning systems, LLMs require unique optimization strategies for training and inference, focusing on system-level efficiency. This paper surveys hardware and software co-design approaches specifically tailored to address the unique characteristics and constraints of large language models. This survey analyzes the challenges and impacts of LLMs on hardware and algorithm research, exploring algorithm optimization, hardware design, and system-level innovations. It aims to provide a comprehensive understanding of the trade-offs and considerations in LLM-centric computing systems, guiding future advancements in AI. Finally, we summarize the existing efforts in this space and outline future directions toward realizing production-grade co-design methodologies for the next generation of large language models and AI systems.",
        "subjects": [
            "cs.AR",
            "cs.AI",
            "cs.LG",
            "cs.SE"
        ],
        "comment": "Accepted by IEEE Circuits and Systems Magazine"
    },
    {
        "paper id": "2410.05694",
        "abstract url": "https://arxiv.org/abs/2410.05694",
        "title": "DiffusionGuard: A Robust Defense Against Malicious Diffusion-based Image Editing",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "Image Editing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advances in diffusion models have introduced a new era of text-guided image manipulation, enabling users to create realistic edited images with simple textual prompts. However, there is significant concern about the potential misuse of these methods, especially in creating misleading or harmful content. Although recent defense strategies, which introduce imperceptible adversarial noise to induce model failure, have shown promise, they remain ineffective against more sophisticated manipulations, such as editing with a mask. In this work, we propose DiffusionGuard, a robust and effective defense method against unauthorized edits by diffusion-based image editing models, even in challenging setups. Through a detailed analysis of these models, we introduce a novel objective that generates adversarial noise targeting the early stage of the diffusion process. This approach significantly improves the efficiency and effectiveness of adversarial noises. We also introduce a mask-augmentation technique to enhance robustness against various masks during test time. Finally, we introduce a comprehensive benchmark designed to evaluate the effectiveness and robustness of methods in protecting against privacy threats in realistic scenarios. Through extensive experiments, we show that our method achieves stronger protection and improved mask robustness with lower computational costs compared to the strongest baseline. Additionally, our method exhibits superior transferability and better resilience to noise removal techniques compared to all baseline methods. Our source code is publicly available at https://github.com/choi403/DiffusionGuard.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Preprint. Under review"
    },
    {
        "paper id": "2410.05710",
        "abstract url": "https://arxiv.org/abs/2410.05710",
        "title": "PixLens: A Novel Framework for Disentangled Evaluation in Diffusion-Based Image Editing with Object Detection + SAM",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "Image Editing"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Evaluating diffusion-based image-editing models is a crucial task in the field of Generative AI. Specifically, it is imperative to assess their capacity to execute diverse editing tasks while preserving the image content and realism. While recent developments in generative models have opened up previously unheard-of possibilities for image editing, conducting a thorough evaluation of these models remains a challenging and open task. The absence of a standardized evaluation benchmark, primarily due to the inherent need for a post-edit reference image for evaluation, further complicates this issue. Currently, evaluations often rely on established models such as CLIP or require human intervention for a comprehensive understanding of the performance of these image editing models. Our benchmark, PixLens, provides a comprehensive evaluation of both edit quality and latent representation disentanglement, contributing to the advancement and refinement of existing methodologies in the field.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "35 pages (17 main paper, 18 appendix), 22 figures"
    },
    {
        "paper id": "2410.05735",
        "abstract url": "https://arxiv.org/abs/2410.05735",
        "title": "CUBE360: Learning Cubic Field Representation for Monocular 360 Depth Estimation for Virtual Reality",
        "rating": "0",
        "keywords": [
            [
                "RGB-D",
                "Depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Panoramic images provide comprehensive scene information and are suitable for VR applications. Obtaining corresponding depth maps is essential for achieving immersive and interactive experiences. However, panoramic depth estimation presents significant challenges due to the severe distortion caused by equirectangular projection (ERP) and the limited availability of panoramic RGB-D datasets. Inspired by the recent success of neural rendering, we propose a novel method, named $\\mathbf{CUBE360}$, that learns a cubic field composed of multiple MPIs from a single panoramic image for $\\mathbf{continuous}$ depth estimation at any view direction. Our CUBE360 employs cubemap projection to transform an ERP image into six faces and extract the MPIs for each, thereby reducing the memory consumption required for MPI processing of high-resolution data. Additionally, this approach avoids the computational complexity of handling the uneven pixel distribution inherent to equirectangular projectio. An attention-based blending module is then employed to learn correlations among the MPIs of cubic faces, constructing a cubic field representation with color and density information at various depth levels. Furthermore, a novel sampling strategy is introduced for rendering novel views from the cubic field at both cubic and planar scales. The entire pipeline is trained using photometric loss calculated from rendered views within a self-supervised learning approach, enabling training on 360 videos without depth annotations. Experiments on both synthetic and real-world datasets demonstrate the superior performance of CUBE360 compared to prior SSL methods. We also highlight its effectiveness in downstream applications, such as VR roaming and visual effects, underscoring CUBE360's potential to enhance immersive experiences.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05772",
        "abstract url": "https://arxiv.org/abs/2410.05772",
        "title": "Comparative Analysis of Novel View Synthesis and Photogrammetry for 3D Forest Stand Reconstruction and extraction of individual tree parameters",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "NeRF",
                "Radiance Fields"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Accurate and efficient 3D reconstruction of trees is crucial for forest resource assessments and management. Close-Range Photogrammetry (CRP) is commonly used for reconstructing forest scenes but faces challenges like low efficiency and poor quality. Recently, Novel View Synthesis (NVS) technologies, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have shown promise for 3D plant reconstruction with limited images. However, existing research mainly focuses on small plants in orchards or individual trees, leaving uncertainty regarding their application in larger, complex forest stands. In this study, we collected sequential images of forest plots with varying complexity and performed dense reconstruction using NeRF and 3DGS. The resulting point clouds were compared with those from photogrammetry and laser scanning. Results indicate that NVS methods significantly enhance reconstruction efficiency. Photogrammetry struggles with complex stands, leading to point clouds with excessive canopy noise and incorrectly reconstructed trees, such as duplicated trunks. NeRF, while better for canopy regions, may produce errors in ground areas with limited views. The 3DGS method generates sparser point clouds, particularly in trunk areas, affecting diameter at breast height (DBH) accuracy. All three methods can extract tree height information, with NeRF yielding the highest accuracy; however, photogrammetry remains superior for DBH accuracy. These findings suggest that NVS methods have significant potential for 3D reconstruction of forest stands, offering valuable support for complex forest resource inventory and visualization tasks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "31page,15figures"
    },
    {
        "paper id": "2410.05814",
        "abstract url": "https://arxiv.org/abs/2410.05814",
        "title": "CALoR: Towards Comprehensive Model Inversion Defense",
        "rating": "0",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Model Inversion Attacks (MIAs) aim at recovering privacy-sensitive training data from the knowledge encoded in the released machine learning models. Recent advances in the MIA field have significantly enhanced the attack performance under multiple scenarios, posing serious privacy risks of Deep Neural Networks (DNNs). However, the development of defense strategies against MIAs is relatively backward to resist the latest MIAs and existing defenses fail to achieve further trade-off between model utility and model robustness. In this paper, we provide an in-depth analysis from the perspective of intrinsic vulnerabilities of MIAs, comprehensively uncovering the weaknesses inherent in the basic pipeline, which are partially investigated in the previous defenses. Building upon these new insights, we propose a robust defense mechanism, integrating Confidence Adaptation and Low-Rank compression(CALoR). Our method includes a novel robustness-enhanced classification loss specially-designed for model inversion defenses and reveals the extraordinary effectiveness of compressing the classification header. With CALoR, we can mislead the optimization objective, reduce the leaked information and impede the backpropagation of MIAs, thus mitigating the risk of privacy leakage. Extensive experimental results demonstrate that our method achieves state-of-the-art (SOTA) defense performance against MIAs and exhibits superior generalization to existing defenses across various scenarios.",
        "subjects": [
            "cs.CR",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "26 pages"
    },
    {
        "paper id": "2410.05820",
        "abstract url": "https://arxiv.org/abs/2410.05820",
        "title": "IncSAR: A Dual Fusion Incremental Learning Framework for SAR Target Recognition",
        "rating": "0",
        "keywords": [
            [
                "Radar"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deep learning techniques have been successfully applied in Synthetic Aperture Radar (SAR) target recognition in static scenarios relying on predefined datasets. However, in real-world scenarios, models must incrementally learn new information without forgetting previously learned knowledge. Models' tendency to forget old knowledge when learning new tasks, known as catastrophic forgetting, remains an open challenge. In this paper, an incremental learning framework, called IncSAR, is proposed to mitigate catastrophic forgetting in SAR target recognition. IncSAR comprises a Vision Transformer (ViT) and a custom-designed Convolutional Neural Network (CNN) in individual branches combined through a late-fusion strategy. A denoising module, utilizing the properties of Robust Principal Component Analysis (RPCA), is introduced to alleviate the speckle noise present in SAR images. Moreover, a random projection layer is employed to enhance the linear separability of features, and a Linear Discriminant Analysis (LDA) approach is proposed to decorrelate the extracted class prototypes. Experimental results on the MSTAR and OpenSARShip benchmark datasets demonstrate that IncSAR outperforms state-of-the-art approaches, leading to an improvement from $98.05\\%$ to $99.63\\%$ in average accuracy and from $3.05\\%$ to $0.33\\%$ in performance dropping rate.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05821",
        "abstract url": "https://arxiv.org/abs/2410.05821",
        "title": "A Zero-Shot approach to the Conversational Tree Search Task",
        "rating": "0",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In sensitive domains, such as legal or medial domains, the correctness of information given to users is critical. To address this, the recently introduced task Conversational Tree Search (CTS) provides a graph-based framework for controllable task-oriented dialog in sensitive domains. However, a big drawback of state-of-the-art CTS agents is their long training time, which is especially problematic as a new agent must be trained every time the associated domain graph is updated. The goal of this paper is to eliminate the need for training CTS agents altogether. To achieve this, we implement a novel LLM-based method for zero-shot, controllable CTS agents. We show that these agents significantly outperform state-of-the-art CTS agents (p<0.0001; Barnard Exact test) in simulation. This generalizes to all available CTS domains. Finally, we perform user evaluation to test the agent performance in the wild, showing that our policy significantly (p<0.05; Barnard Exact) improves task-success compared to the state-of-the-art Reinforcement Learning-based CTS agent.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06028",
        "abstract url": "https://arxiv.org/abs/2410.06028",
        "title": "SpecTrack: Learned Multi-Rotation Tracking via Speckle Imaging",
        "rating": "0",
        "keywords": [
            [
                "robotics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Precision pose detection is increasingly demanded in fields such as personal fabrication, Virtual Reality (VR), and robotics due to its critical role in ensuring accurate positioning information. However, conventional vision-based systems used in these systems often struggle with achieving high precision and accuracy, particularly when dealing with complex environments or fast-moving objects. To address these limitations, we investigate Laser Speckle Imaging (LSI), an emerging optical tracking method that offers promising potential for improving pose estimation accuracy. Specifically, our proposed LSI-Based Tracking (SpecTrack) leverages the captures from a lensless camera and a retro-reflector marker with a coded aperture to achieve multi-axis rotational pose estimation with high precision. Our extensive trials using our in-house built testbed have shown that SpecTrack achieves an accuracy of 0.31\u00b0 (std=0.43\u00b0), significantly outperforming state-of-the-art approaches and improving accuracy up to 200%.",
        "subjects": [
            "cs.ET",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06055",
        "abstract url": "https://arxiv.org/abs/2410.06055",
        "title": "AP-LDM: Attentive and Progressive Latent Diffusion Model for Training-Free High-Resolution Image Generation",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Latent diffusion models (LDMs), such as Stable Diffusion, often experience significant structural distortions when directly generating high-resolution (HR) images that exceed their original training resolutions. A straightforward and cost-effective solution is to adapt pre-trained LDMs for HR image generation; however, existing methods often suffer from poor image quality and long inference time. In this paper, we propose an Attentive and Progressive LDM (AP-LDM), a novel, training-free framework aimed at enhancing HR image quality while accelerating the generation process. AP-LDM decomposes the denoising process of LDMs into two stages: (i) attentive training-resolution denoising, and (ii) progressive high-resolution denoising. The first stage generates a latent representation of a higher-quality training-resolution image through the proposed attentive guidance, which utilizes a novel parameter-free self-attention mechanism to enhance the structural consistency. The second stage progressively performs upsampling in pixel space, alleviating the severe artifacts caused by latent space upsampling. Leveraging the effective initialization from the first stage enables denoising at higher resolutions with significantly fewer steps, enhancing overall efficiency. Extensive experimental results demonstrate that AP-LDM significantly outperforms state-of-the-art methods, delivering up to a 5x speedup in HR image generation, thereby highlighting its substantial advantages for real-world applications. Code is available at https://github.com/kmittle/AP-LDM.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06072",
        "abstract url": "https://arxiv.org/abs/2410.06072",
        "title": "Training-free LLM-generated Text Detection by Mining Token Probability Sequences",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in generating high-quality texts across diverse domains. However, the potential misuse of LLMs has raised significant concerns, underscoring the urgent need for reliable detection of LLM-generated texts. Conventional training-based detectors often struggle with generalization, particularly in cross-domain and cross-model scenarios. In contrast, training-free methods, which focus on inherent discrepancies through carefully designed statistical features, offer improved generalization and interpretability. Despite this, existing training-free detection methods typically rely on global text sequence statistics, neglecting the modeling of local discriminative features, thereby limiting their detection efficacy. In this work, we introduce a novel training-free detector, termed \\textbf{Lastde} that synergizes local and global statistics for enhanced detection. For the first time, we introduce time series analysis to LLM-generated text detection, capturing the temporal dynamics of token probability sequences. By integrating these local statistics with global ones, our detector reveals significant disparities between human and LLM-generated texts. We also propose an efficient alternative, \\textbf{Lastde++} to enable real-time detection. Extensive experiments on six datasets involving cross-domain, cross-model, and cross-lingual detection scenarios, under both white-box and black-box settings, demonstrated that our method consistently achieves state-of-the-art performance. Furthermore, our approach exhibits greater robustness against paraphrasing attacks compared to existing baseline methods.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06126",
        "abstract url": "https://arxiv.org/abs/2410.06126",
        "title": "$\\textit{X}^2$-DFD: A framework for e${X}$plainable and e${X}$tendable Deepfake Detection",
        "rating": "0",
        "keywords": [
            [
                "Deepfake"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Detecting deepfakes has become an important task. Most existing detection methods provide only real/fake predictions without offering human-comprehensible explanations. Recent studies leveraging MLLMs for deepfake detection have shown improvements in explainability. However, the performance of pre-trained MLLMs (e.g., LLaVA) remains limited due to a lack of understanding of their capabilities for this task and strategies to enhance them. In this work, we empirically assess the strengths and weaknesses of MLLMs specifically in deepfake detection via forgery features analysis. Building on these assessments, we propose a novel framework called ${X}^2$-DFD, consisting of three core modules. The first module, Model Feature Assessment (MFA), measures the detection capabilities of forgery features intrinsic to MLLMs, and gives a descending ranking of these features. The second module, Strong Feature Strengthening (SFS), enhances the detection and explanation capabilities by fine-tuning the MLLM on a dataset constructed based on the top-ranked features. The third module, Weak Feature Supplementing (WFS), improves the fine-tuned MLLM's capabilities on lower-ranked features by integrating external dedicated deepfake detectors. To verify the effectiveness of this framework, we further present a practical implementation, where an automated forgery features generation, evaluation, and ranking procedure is designed for MFA module; an automated generation procedure of the fine-tuning dataset containing real and fake images with explanations based on top-ranked features is developed for SFS model; an external conventional deepfake detector focusing on blending artifact, which corresponds to a low detection capability in the pre-trained MLLM, is integrated for WFS module. Experiments show that our approach enhances both detection and explanation performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06140",
        "abstract url": "https://arxiv.org/abs/2410.06140",
        "title": "Estimating the Number of HTTP/3 Responses in QUIC Using Deep Learning",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "QUIC, a new and increasingly used transport protocol, enhances TCP by providing better security, performance, and features like stream multiplexing. These features, however, also impose challenges for network middle-boxes that need to monitor and analyze web traffic. This paper proposes a novel solution for estimating the number of HTTP/3 responses in a given QUIC connection by an observer. This estimation reveals server behavior, client-server interactions, and data transmission efficiency, which is crucial for various applications such as designing a load balancing solution and detecting HTTP/3 flood attacks. The proposed scheme transforms QUIC connection traces into a sequence of images and trains machine learning (ML) models to predict the number of responses. Then, by aggregating images of a QUIC connection, an observer can estimate the total number of responses. As the problem is formulated as a discrete regression problem, we introduce a dedicated loss function. The proposed scheme is evaluated on a dataset of over seven million images, generated from $100,000$ traces collected from over $44,000$ websites over a four-month period, from various vantage points. The scheme achieves up to 97\\% cumulative accuracy in both known and unknown web server settings and 92\\% accuracy in estimating the total number of responses in unseen QUIC traces.",
        "subjects": [
            "cs.LG",
            "cs.CV",
            "cs.NI"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2410.03728"
    },
    {
        "paper id": "2410.06158",
        "abstract url": "https://arxiv.org/abs/2410.06158",
        "title": "GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation",
        "rating": "0",
        "keywords": [
            [
                "Robot"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We present GR-2, a state-of-the-art generalist robot agent for versatile and generalizable robot manipulation. GR-2 is first pre-trained on a vast number of Internet videos to capture the dynamics of the world. This large-scale pre-training, involving 38 million video clips and over 50 billion tokens, equips GR-2 with the ability to generalize across a wide range of robotic tasks and environments during subsequent policy learning. Following this, GR-2 is fine-tuned for both video generation and action prediction using robot trajectories. It exhibits impressive multi-task learning capabilities, achieving an average success rate of 97.7% across more than 100 tasks. Moreover, GR-2 demonstrates exceptional generalization to new, previously unseen scenarios, including novel backgrounds, environments, objects, and tasks. Notably, GR-2 scales effectively with model size, underscoring its potential for continued growth and application. Project page: \\url{https://gr2-manipulation.github.io}.",
        "subjects": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Tech Report. Authors are listed in alphabetical order. Project page: https://gr2-manipulation.github.io"
    },
    {
        "paper id": "2410.06163",
        "abstract url": "https://arxiv.org/abs/2410.06163",
        "title": "Likelihood-based Differentiable Structure Learning",
        "rating": "0",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Existing approaches to differentiable structure learning of directed acyclic graphs (DAGs) rely on strong identifiability assumptions in order to guarantee that global minimizers of the acyclicity-constrained optimization problem identifies the true DAG. Moreover, it has been observed empirically that the optimizer may exploit undesirable artifacts in the loss function. We explain and remedy these issues by studying the behavior of differentiable acyclicity-constrained programs under general likelihoods with multiple global minimizers. By carefully regularizing the likelihood, it is possible to identify the sparsest model in the Markov equivalence class, even in the absence of an identifiable parametrization. We first study the Gaussian case in detail, showing how proper regularization of the likelihood defines a score that identifies the sparsest model. Assuming faithfulness, it also recovers the Markov equivalence class. These results are then generalized to general models and likelihoods, where the same claims hold. These theoretical results are validated empirically, showing how this can be done using standard gradient-based optimizers, thus paving the way for differentiable structure learning under general models and losses.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.ST",
            "stat.ME"
        ],
        "comment": "38 pages, 14 figures, to appear at NeurIPS 2024"
    },
    {
        "paper id": "2410.06166",
        "abstract url": "https://arxiv.org/abs/2410.06166",
        "title": "Temporal Reasoning Transfer from Text to Video",
        "rating": "0",
        "keywords": [
            [
                "Text to Video"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Video Large Language Models (Video LLMs) have shown promising capabilities in video comprehension, yet they struggle with tracking temporal changes and reasoning about temporal relationships. While previous research attributed this limitation to the ineffective temporal encoding of visual inputs, our diagnostic study reveals that video representations contain sufficient information for even small probing classifiers to achieve perfect accuracy. Surprisingly, we find that the key bottleneck in Video LLMs' temporal reasoning capability stems from the underlying LLM's inherent difficulty with temporal concepts, as evidenced by poor performance on textual temporal question-answering tasks. Building on this discovery, we introduce the Textual Temporal reasoning Transfer (T3). T3 synthesizes diverse temporal reasoning tasks in pure text format from existing image-text datasets, addressing the scarcity of video samples with complex temporal scenarios. Remarkably, without using any video data, T3 enhances LongVA-7B's temporal understanding, yielding a 5.3 absolute accuracy improvement on the challenging TempCompass benchmark, which enables our model to outperform ShareGPT4Video-8B trained on 28,000 video samples. Additionally, the enhanced LongVA-7B model achieves competitive performance on comprehensive video benchmarks. For example, it achieves a 49.7 accuracy on the Temporal Reasoning task of Video-MME, surpassing powerful large-scale models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. Further analysis reveals a strong correlation between textual and video temporal task performance, validating the efficacy of transferring temporal reasoning abilities from text to video domains.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": "Project page: https://video-t3.github.io"
    },
    {
        "paper id": "2410.06241",
        "abstract url": "https://arxiv.org/abs/2410.06241",
        "title": "BroadWay: Boost Your Text-to-Video Generation Model in a Training-free Way",
        "rating": "0",
        "keywords": [
            [
                "Text-to-Video"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The text-to-video (T2V) generation models, offering convenient visual creation, have recently garnered increasing attention. Despite their substantial potential, the generated videos may present artifacts, including structural implausibility, temporal inconsistency, and a lack of motion, often resulting in near-static video. In this work, we have identified a correlation between the disparity of temporal attention maps across different blocks and the occurrence of temporal inconsistencies. Additionally, we have observed that the energy contained within the temporal attention maps is directly related to the magnitude of motion amplitude in the generated videos. Based on these observations, we present BroadWay, a training-free method to improve the quality of text-to-video generation without introducing additional parameters, augmenting memory or sampling time. Specifically, BroadWay is composed of two principal components: 1) Temporal Self-Guidance improves the structural plausibility and temporal consistency of generated videos by reducing the disparity between the temporal attention maps across various decoder blocks. 2) Fourier-based Motion Enhancement enhances the magnitude and richness of motion by amplifying the energy of the map. Extensive experiments demonstrate that BroadWay significantly improves the quality of text-to-video generation with negligible additional cost.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06244",
        "abstract url": "https://arxiv.org/abs/2410.06244",
        "title": "Story-Adapter: A Training-free Iterative Framework for Long Story Visualization",
        "rating": "0",
        "keywords": [
            [
                "diffusion",
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Story visualization, the task of generating coherent images based on a narrative, has seen significant advancements with the emergence of text-to-image models, particularly diffusion models. However, maintaining semantic consistency, generating high-quality fine-grained interactions, and ensuring computational feasibility remain challenging, especially in long story visualization (i.e., up to 100 frames). In this work, we propose a training-free and computationally efficient framework, termed Story-Adapter, to enhance the generative capability of long stories. Specifically, we propose an iterative paradigm to refine each generated image, leveraging both the text prompt and all generated images from the previous iteration. Central to our framework is a training-free global reference cross-attention module, which aggregates all generated images from the previous iteration to preserve semantic consistency across the entire story, while minimizing computational costs with global embeddings. This iterative process progressively optimizes image generation by repeatedly incorporating text constraints, resulting in more precise and fine-grained interactions. Extensive experiments validate the superiority of Story-Adapter in improving both semantic consistency and generative capability for fine-grained interactions, particularly in long story scenarios. The project page and associated code can be accessed via https://jwmao1.github.io/storyadapter .",
        "subjects": [
            "cs.CV"
        ],
        "comment": "20 pages, 16 figures, The project page and associated code can be accessed via https://jwmao1.github.io/storyadapter"
    },
    {
        "paper id": "2410.06245",
        "abstract url": "https://arxiv.org/abs/2410.06245",
        "title": "HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Reconstructing 3D scenes from multiple viewpoints is a fundamental task in stereo vision. Recently, advances in generalizable 3D Gaussian Splatting have enabled high-quality novel view synthesis for unseen scenes from sparse input views by feed-forward predicting per-pixel Gaussian parameters without extra optimization. However, existing methods typically generate single-scale 3D Gaussians, which lack representation of both large-scale structure and texture details, resulting in mislocation and artefacts. In this paper, we propose a novel framework, HiSplat, which introduces a hierarchical manner in generalizable 3D Gaussian Splatting to construct hierarchical 3D Gaussians via a coarse-to-fine strategy. Specifically, HiSplat generates large coarse-grained Gaussians to capture large-scale structures, followed by fine-grained Gaussians to enhance delicate texture details. To promote inter-scale interactions, we propose an Error Aware Module for Gaussian compensation and a Modulating Fusion Module for Gaussian repair. Our method achieves joint optimization of hierarchical representations, allowing for novel view synthesis using only two-view reference images. Comprehensive experiments on various datasets demonstrate that HiSplat significantly enhances reconstruction quality and cross-dataset generalization compared to prior single-scale methods. The corresponding ablation study and analysis of different-scale 3D Gaussians reveal the mechanism behind the effectiveness. Project website: https://open3dvlab.github.io/HiSplat/",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06264",
        "abstract url": "https://arxiv.org/abs/2410.06264",
        "title": "Think While You Generate: Discrete Diffusion with Planned Denoising",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Discrete diffusion has achieved state-of-the-art performance, outperforming or approaching autoregressive models on standard benchmarks. In this work, we introduce Discrete Diffusion with Planned Denoising (DDPD), a novel framework that separates the generation process into two models: a planner and a denoiser. At inference time, the planner selects which positions to denoise next by identifying the most corrupted positions in need of denoising, including both initially corrupted and those requiring additional refinement. This plan-and-denoise approach enables more efficient reconstruction during generation by iteratively identifying and denoising corruptions in the optimal order. DDPD outperforms traditional denoiser-only mask diffusion methods, achieving superior results on language modeling benchmarks such as text8, OpenWebText, and token-based generation on ImageNet $256 \\times 256$. Notably, in language modeling, DDPD significantly reduces the performance gap between diffusion-based and autoregressive methods in terms of generative perplexity. Code is available at https://github.com/liusulin/DDPD.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06331",
        "abstract url": "https://arxiv.org/abs/2410.06331",
        "title": "Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing",
        "rating": "0",
        "keywords": [
            [
                "Knowledge Editing"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The locate-then-edit paradigm has shown significant promise for knowledge editing (KE) in Large Language Models (LLMs). While previous methods perform well on single-hop fact recall tasks, they consistently struggle with multi-hop factual recall tasks involving newly edited knowledge. In this paper, leveraging tools in mechanistic interpretability, we first identify that in multi-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper MLP layers, unlike single-hop tasks, which rely on earlier layers. This distinction explains the poor performance of current methods in multi-hop queries, as they primarily focus on editing shallow layers, leaving deeper layers unchanged. To address this, we propose IFMET, a novel locate-then-edit KE approach designed to edit both shallow and deep MLP layers. IFMET employs multi-hop editing prompts and supplementary sets to locate and modify knowledge across different reasoning stages. Experimental results demonstrate that IFMET significantly improves performance on multi-hop factual recall tasks, effectively overcoming the limitations of previous locate-then-edit methods.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "21 pages"
    },
    {
        "paper id": "2410.06369",
        "abstract url": "https://arxiv.org/abs/2410.06369",
        "title": "Communication-Efficient Federated Group Distributionally Robust Optimization",
        "rating": "0",
        "keywords": [
            [
                "Federated learning"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Federated learning faces challenges due to the heterogeneity in data volumes and distributions at different clients, which can compromise model generalization ability to various distributions. Existing approaches to address this issue based on group distributionally robust optimization (GDRO) often lead to high communication and sample complexity. To this end, this work introduces algorithms tailored for communication-efficient Federated Group Distributionally Robust Optimization (FGDRO). Our contributions are threefold: Firstly, we introduce the FGDRO-CVaR algorithm, which optimizes the average top-K losses while reducing communication complexity to $O(1/\u03b5^4)$, where $\u03b5$ denotes the desired precision level. Secondly, our FGDRO-KL algorithm is crafted to optimize KL regularized FGDRO, cutting communication complexity to $O(1/\u03b5^3)$. Lastly, we propose FGDRO-KL-Adam to utilize Adam-type local updates in FGDRO-KL, which not only maintains a communication cost of $O(1/\u03b5^3)$ but also shows potential to surpass SGD-type local steps in practical applications. The effectiveness of our algorithms has been demonstrated on a variety of real-world tasks, including natural language processing and computer vision.",
        "subjects": [
            "cs.LG",
            "cs.DC",
            "stat.ML"
        ],
        "comment": "Accepted to NeurIPS 2024"
    },
    {
        "paper id": "2410.06380",
        "abstract url": "https://arxiv.org/abs/2410.06380",
        "title": "Adver-City: Open-Source Multi-Modal Dataset for Collaborative Perception Under Adverse Weather Conditions",
        "rating": "0",
        "keywords": [
            [
                "LiDAR"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Adverse weather conditions pose a significant challenge to the widespread adoption of Autonomous Vehicles (AVs) by impacting sensors like LiDARs and cameras. Even though Collaborative Perception (CP) improves AV perception in difficult conditions, existing CP datasets lack adverse weather conditions. To address this, we introduce Adver-City, the first open-source synthetic CP dataset focused on adverse weather conditions. Simulated in CARLA with OpenCDA, it contains over 24 thousand frames, over 890 thousand annotations, and 110 unique scenarios across six different weather conditions: clear weather, soft rain, heavy rain, fog, foggy heavy rain and, for the first time in a synthetic CP dataset, glare. It has six object categories including pedestrians and cyclists, and uses data from vehicles and roadside units featuring LiDARs, RGB and semantic segmentation cameras, GNSS, and IMUs. Its scenarios, based on real crash reports, depict the most relevant road configurations for adverse weather and poor visibility conditions, varying in object density, with both dense and sparse scenes, allowing for novel testing conditions of CP models. Benchmarks run on the dataset show that weather conditions created challenging conditions for perception models, reducing multi-modal object detection performance by up to 19%, while object density affected LiDAR-based detection by up to 29%. The dataset, code and documentation are available at https://labs.cs.queensu.ca/quarrg/datasets/adver-city/.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2410.06392",
        "abstract url": "https://arxiv.org/abs/2410.06392",
        "title": "Counterfactual Causal Inference in Natural Language with Large Language Models",
        "rating": "0",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Causal structure discovery methods are commonly applied to structured data where the causal variables are known and where statistical testing can be used to assess the causal relationships. By contrast, recovering a causal structure from unstructured natural language data such as news articles contains numerous challenges due to the absence of known variables or counterfactual data to estimate the causal links. Large Language Models (LLMs) have shown promising results in this direction but also exhibit limitations. This work investigates LLM's abilities to build causal graphs from text documents and perform counterfactual causal inference. We propose an end-to-end causal structure discovery and causal inference method from natural language: we first use an LLM to extract the instantiated causal variables from text data and build a causal graph. We merge causal graphs from multiple data sources to represent the most exhaustive set of causes possible. We then conduct counterfactual inference on the estimated graph. The causal graph conditioning allows reduction of LLM biases and better represents the causal estimands. We use our method to show that the limitations of LLMs in counterfactual causal reasoning come from prediction errors and propose directions to mitigate them. We demonstrate the applicability of our method on real-world news articles.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "22 pages, 10 pages for the main paper, 12 pages for the references and appendix, 5 figures"
    },
    {
        "paper id": "2410.06437",
        "abstract url": "https://arxiv.org/abs/2410.06437",
        "title": "LocoVR: Multiuser Indoor Locomotion Dataset in Virtual Reality",
        "rating": "0",
        "keywords": [
            [
                "navigation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Understanding human locomotion is crucial for AI agents such as robots, particularly in complex indoor home environments. Modeling human trajectories in these spaces requires insight into how individuals maneuver around physical obstacles and manage social navigation dynamics. These dynamics include subtle behaviors influenced by proxemics - the social use of space, such as stepping aside to allow others to pass or choosing longer routes to avoid collisions. Previous research has developed datasets of human motion in indoor scenes, but these are often limited in scale and lack the nuanced social navigation dynamics common in home environments. To address this, we present LocoVR, a dataset of 7000+ two-person trajectories captured in virtual reality from over 130 different indoor home environments. LocoVR provides full body pose data and precise spatial information, along with rich examples of socially-motivated movement behaviors. For example, the dataset captures instances of individuals navigating around each other in narrow spaces, adjusting paths to respect personal boundaries in living areas, and coordinating movements in high-traffic zones like entryways and kitchens. Our evaluation shows that LocoVR significantly enhances model performance in three practical indoor tasks utilizing human trajectories, and demonstrates predicting socially-aware navigation patterns in home environments.",
        "subjects": [
            "cs.RO",
            "cs.CV",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06446",
        "abstract url": "https://arxiv.org/abs/2410.06446",
        "title": "Machine Unlearning in Forgettability Sequence",
        "rating": "0",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Machine unlearning (MU) is becoming a promising paradigm to achieve the \"right to be forgotten\", where the training trace of any chosen data points could be eliminated, while maintaining the model utility on general testing samples after unlearning. With the advancement of forgetting research, many fundamental open questions remain unanswered: do different samples exhibit varying levels of difficulty in being forgotten? Further, does the sequence in which samples are forgotten, determined by their respective difficulty levels, influence the performance of forgetting algorithms? In this paper, we identify key factor affecting unlearning difficulty and the performance of unlearning algorithms. We find that samples with higher privacy risks are more likely to be unlearning, indicating that the unlearning difficulty varies among different samples which motives a more precise unlearning mode. Built upon this insight, we propose a general unlearning framework, dubbed RSU, which consists of Ranking module and SeqUnlearn module.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06456",
        "abstract url": "https://arxiv.org/abs/2410.06456",
        "title": "From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning",
        "rating": "0",
        "keywords": [
            [
                "Vision Language",
                "VLMs"
            ],
            [
                "medical",
                "diagnosis"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Large vision language models (VLMs) combine large language models with vision encoders, demonstrating promise across various tasks. However, they often underperform in task-specific applications due to domain gaps between pre-training and fine-tuning. We introduce VITask, a novel framework that enhances task-specific adaptability of VLMs by integrating task-specific models (TSMs). VITask employs three key strategies: exemplar prompting (EP), response distribution alignment (RDA), and contrastive response tuning (CRT) to improve the task-specific performance of VLMs by adjusting their response distributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to adapt without TSMs during inference by learning from exemplar-prompted models. CRT further optimizes the ranking of correct image-response pairs, thereby reducing the risk of generating undesired responses. Experiments on 12 medical diagnosis datasets across 9 imaging modalities show that VITask outperforms both vanilla instruction-tuned VLMs and TSMs, showcasing its ability to integrate complementary features from both models effectively. Additionally, VITask offers practical advantages such as flexible TSM integration and robustness to incomplete instructions, making it a versatile and efficient solution for task-specific VLM tuning. Our code are available at https://github.com/baiyang4/VITask.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06475",
        "abstract url": "https://arxiv.org/abs/2410.06475",
        "title": "3D Representation Methods: A Survey",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "Voxel",
                "Point Cloud",
                "NeRF",
                "SDF"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The field of 3D representation has experienced significant advancements, driven by the increasing demand for high-fidelity 3D models in various applications such as computer graphics, virtual reality, and autonomous systems. This review examines the development and current state of 3D representation methods, highlighting their research trajectories, innovations, strength and weakness. Key techniques such as Voxel Grid, Point Cloud, Mesh, Signed Distance Function (SDF), Neural Radiance Field (NeRF), 3D Gaussian Splatting, Tri-Plane, and Deep Marching Tetrahedra (DMTet) are reviewed. The review also introduces essential datasets that have been pivotal in advancing the field, highlighting their characteristics and impact on research progress. Finally, we explore potential research directions that hold promise for further expanding the capabilities and applications of 3D representation methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Preliminary Draft"
    },
    {
        "paper id": "2410.06479",
        "abstract url": "https://arxiv.org/abs/2410.06479",
        "title": "LLM Compression with Neural Architecture Search",
        "rating": "0",
        "keywords": [
            [
                "Architecture Search",
                "NAS"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) exhibit remarkable reasoning abilities, allowing them to generalize across a wide range of downstream tasks, such as commonsense reasoning or instruction following. However, as LLMs scale, inference costs become increasingly prohibitive, accumulating significantly over their life cycle. This poses the question: Can we compress pre-trained LLMs to meet diverse size and latency requirements? We leverage Neural Architecture Search (NAS) to compress LLMs by pruning structural components, such as attention heads, neurons, and layers, aiming to achieve a Pareto-optimal balance between performance and efficiency. While NAS already achieved promising results on small language models in previous work, in this paper we propose various extensions that allow us to scale to LLMs. Compared to structural pruning baselines, we show that NAS improves performance up to 3.4% on MMLU with an on-device latency speedup.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06488",
        "abstract url": "https://arxiv.org/abs/2410.06488",
        "title": "HFH-Font: Few-shot Chinese Font Synthesis with Higher Quality, Faster Speed, and Higher Resolution",
        "rating": "0",
        "keywords": [
            [
                "diffusion",
                "super-resolution"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The challenge of automatically synthesizing high-quality vector fonts, particularly for writing systems (e.g., Chinese) consisting of huge amounts of complex glyphs, remains unsolved. Existing font synthesis techniques fall into two categories: 1) methods that directly generate vector glyphs, and 2) methods that initially synthesize glyph images and then vectorize them. However, the first category often fails to construct complete and correct shapes for complex glyphs, while the latter struggles to efficiently synthesize high-resolution (i.e., 1024 $\\times$ 1024 or higher) glyph images while preserving local details. In this paper, we introduce HFH-Font, a few-shot font synthesis method capable of efficiently generating high-resolution glyph images that can be converted into high-quality vector glyphs. More specifically, our method employs a diffusion model-based generative framework with component-aware conditioning to learn different levels of style information adaptable to varying input reference sizes. We also design a distillation module based on Score Distillation Sampling for 1-step fast inference, and a style-guided super-resolution module to refine and upscale low-resolution synthesis results. Extensive experiments, including a user study with professional font designers, have been conducted to demonstrate that our method significantly outperforms existing font synthesis approaches. Experimental results show that our method produces high-fidelity, high-resolution raster images which can be vectorized into high-quality vector fonts. Using our method, for the first time, large-scale Chinese vector fonts of a quality comparable to those manually created by professional font designers can be automatically generated.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to SIGGRAPH Asia 2024 (TOG). Code: https://github.com/grovessss/HFH-Font"
    },
    {
        "paper id": "2410.06508",
        "abstract url": "https://arxiv.org/abs/2410.06508",
        "title": "Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning",
        "rating": "0",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Monte Carlo Tree Search (MCTS) has recently emerged as a powerful technique for enhancing the reasoning capabilities of LLMs. Techniques such as SFT or DPO have enabled LLMs to distill high-quality behaviors from MCTS, improving their reasoning performance. However, existing distillation methods underutilize the rich trajectory information generated by MCTS, limiting the potential for improvements in LLM reasoning. In this paper, we propose AlphaLLM-CPL, a novel pairwise training framework that enables LLMs to self-improve through MCTS behavior distillation. AlphaLLM-CPL efficiently leverages MCTS trajectories via two key innovations: (1) AlphaLLM-CPL constructs stepwise trajectory pairs from child nodes sharing the same parent in the search tree, providing step-level information for more effective MCTS behavior distillation. (2) AlphaLLM-CPL introduces curriculum preference learning, dynamically adjusting the training sequence of trajectory pairs in each offline training epoch to prioritize critical learning steps and mitigate overfitting. Experimental results on mathematical reasoning tasks demonstrate that AlphaLLM-CPL significantly outperforms previous MCTS behavior distillation methods, substantially boosting the reasoning capabilities of LLMs.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06511",
        "abstract url": "https://arxiv.org/abs/2410.06511",
        "title": "TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The development of large language models (LLMs) has been instrumental in advancing state-of-the-art natural language processing applications. Training LLMs with billions of parameters and trillions of tokens require sophisticated distributed systems that enable composing and comparing several state-of-the-art techniques in order to efficiently scale across thousands of accelerators. However, existing solutions are complex, scattered across multiple libraries/repositories, lack interoperability, and are cumbersome to maintain. Thus, curating and empirically comparing training recipes require non-trivial engineering effort. This paper introduces TorchTitan, an open-source, PyTorch-native distributed training system that unifies state-of-the-art techniques, streamlining integration and reducing overhead. TorchTitan enables 3D parallelism in a modular manner with elastic scaling, providing comprehensive logging, checkpointing, and debugging tools for production-ready training. It also incorporates hardware-software co-designed solutions, leveraging features like Float8 training and SymmetricMemory. As a flexible test bed, TorchTitan facilitates custom recipe curation and comparison, allowing us to develop optimized training recipes for Llama 3.1 and provide guidance on selecting techniques for maximum efficiency based on our experiences. We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8 billion to 405 billion parameters, and showcase its exceptional performance, modular composability, and elastic scalability. By stacking training optimizations, we demonstrate accelerations of 65.08% with 1D parallelism at the 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at the 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at the 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized baselines.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07266",
        "abstract url": "https://arxiv.org/abs/2410.07266",
        "title": "Spiking GS: Towards High-Accuracy and Low-Cost Surface Reconstruction via Spiking Neuron-based Gaussian Splatting",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D Gaussian Splatting is capable of reconstructing 3D scenes in minutes. Despite recent advances in improving surface reconstruction accuracy, the reconstructed results still exhibit bias and suffer from inefficiency in storage and training. This paper provides a different observation on the cause of the inefficiency and the reconstruction bias, which is attributed to the integration of the low-opacity parts (LOPs) of the generated Gaussians. We show that LOPs consist of Gaussians with overall low-opacity (LOGs) and the low-opacity tails (LOTs) of Gaussians. We propose Spiking GS to reduce such two types of LOPs by integrating spiking neurons into the Gaussian Splatting pipeline. Specifically, we introduce global and local full-precision integrate-and-fire spiking neurons to the opacity and representation function of flattened 3D Gaussians, respectively. Furthermore, we enhance the density control strategy with spiking neurons' thresholds and an new criterion on the scale of Gaussians. Our method can represent more accurate reconstructed surfaces at a lower cost. The code is available at \\url{https://github.com/shippoT/Spiking_GS}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05707",
        "abstract url": "https://arxiv.org/abs/2410.05707",
        "title": "A First-Order Algorithm for Graph Learning from Smooth Signals Under Partial Observability",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Learning graph structures from smooth signals is a significant problem in data science and engineering. A common challenge in real-world scenarios is the availability of only partially observed nodes. While some studies have considered hidden nodes and proposed various optimization frameworks, existing methods often lack the practical efficiency needed for large-scale networks or fail to provide theoretical convergence guarantees. In this paper, we address the problem of inferring network topologies from smooth signals with partially observed nodes. We propose a first-order algorithmic framework that includes two variants: one based on column sparsity regularization and the other on a low-rank constraint. We establish theoretical convergence guarantees and demonstrate the linear convergence rate of our algorithms. Extensive experiments on both synthetic and real-world data show that our results align with theoretical predictions, exhibiting not only linear convergence but also superior speed compared to existing methods. To the best of our knowledge, this is the first work to propose a first-order algorithmic framework for inferring network structures from smooth signals under partial observability, offering both guaranteed linear convergence and practical effectiveness for large-scale networks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05733",
        "abstract url": "https://arxiv.org/abs/2410.05733",
        "title": "Private and Communication-Efficient Federated Learning based on Differentially Private Sketches",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning (FL) faces two primary challenges: the risk of privacy leakage due to parameter sharing and communication inefficiencies. To address these challenges, we propose DPSFL, a federated learning method that utilizes differentially private sketches. DPSFL compresses the local gradients of each client using a count sketch, thereby improving communication efficiency, while adding noise to the sketches to ensure differential privacy (DP). We provide a theoretical analysis of privacy and convergence for the proposed method. Gradient clipping is essential in DP learning to limit sensitivity and constrain the addition of noise. However, clipping introduces bias into the gradients, negatively impacting FL performance. To mitigate the impact of clipping, we propose an enhanced method, DPSFL-AC, which employs an adaptive clipping strategy. Experimental comparisons with existing techniques demonstrate the superiority of our methods concerning privacy preservation, communication efficiency, and model accuracy.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05740",
        "abstract url": "https://arxiv.org/abs/2410.05740",
        "title": "Learning to Race in Extreme Turning Scene with Active Exploration and Gaussian Process Regression-based MPC",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory",
                "vehicle"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Extreme cornering in racing often induces large side-slip angles, presenting a formidable challenge in vehicle control. To tackle this issue, this paper introduces an Active Exploration with Double GPR (AEDGPR) system. The system initiates by planning a minimum-time trajectory with a Gaussian Process Regression(GPR) compensated model. The planning results show that in the cornering section, the yaw angular velocity and side-slip angle are in opposite directions, indicating that the vehicle is drifting. In response, we develop a drift controller based on Model Predictive Control (MPC) and incorporate Gaussian Process Regression to correct discrepancies in the vehicle dynamics model. Moreover, the covariance from the GPR is employed to actively explore various cornering states, aiming to minimize trajectory tracking errors. The proposed algorithm is validated through simulations on the Simulink-Carsim platform and experiments using a 1/10 scale RC vehicle.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05750",
        "abstract url": "https://arxiv.org/abs/2410.05750",
        "title": "Polynomial Time Cryptanalytic Extraction of Deep Neural Networks in the Hard-Label Setting",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Deep neural networks (DNNs) are valuable assets, yet their public accessibility raises security concerns about parameter extraction by malicious actors. Recent work by Carlini et al. (crypto'20) and Canales-Mart\u00ednez et al. (eurocrypt'24) has drawn parallels between this issue and block cipher key extraction via chosen plaintext attacks. Leveraging differential cryptanalysis, they demonstrated that all the weights and biases of black-box ReLU-based DNNs could be inferred using a polynomial number of queries and computational time. However, their attacks relied on the availability of the exact numeric value of output logits, which allowed the calculation of their derivatives. To overcome this limitation, Chen et al. (asiacrypt'24) tackled the more realistic hard-label scenario, where only the final classification label (e.g., \"dog\" or \"car\") is accessible to the attacker. They proposed an extraction method requiring a polynomial number of queries but an exponential execution time. In addition, their approach was applicable only to a restricted set of architectures, could deal only with binary classifiers, and was demonstrated only on tiny neural networks with up to four neurons split among up to two hidden layers. This paper introduces new techniques that, for the first time, achieve cryptanalytic extraction of DNN parameters in the most challenging hard-label setting, using both a polynomial number of queries and polynomial time. We validate our approach by extracting nearly one million parameters from a DNN trained on the CIFAR-10 dataset, comprising 832 neurons in four hidden layers. Our results reveal the surprising fact that all the weights of a ReLU-based DNN can be efficiently determined by analyzing only the geometric shape of its decision boundaries.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05779",
        "abstract url": "https://arxiv.org/abs/2410.05779",
        "title": "LightRAG: Simple and Fast Retrieval-Augmented Generation",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs. However, existing RAG systems have significant limitations, including reliance on flat data representations and inadequate contextual awareness, which can lead to fragmented answers that fail to capture complex inter-dependencies. To address these challenges, we propose LightRAG, which incorporates graph structures into text indexing and retrieval processes. This innovative framework employs a dual-level retrieval system that enhances comprehensive information retrieval from both low-level and high-level knowledge discovery. Additionally, the integration of graph structures with vector representations facilitates efficient retrieval of related entities and their relationships, significantly improving response times while maintaining contextual relevance. This capability is further enhanced by an incremental update algorithm that ensures the timely integration of new data, allowing the system to remain effective and responsive in rapidly changing data environments. Extensive experimental validation demonstrates considerable improvements in retrieval accuracy and efficiency compared to existing approaches. We have made our LightRAG open-source and available at the link: https://github.com/HKUDS/LightRAG.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05782",
        "abstract url": "https://arxiv.org/abs/2410.05782",
        "title": "Reinforcement Learning From Imperfect Corrective Actions And Proxy Rewards",
        "rating": "-0.5",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In practice, reinforcement learning (RL) agents are often trained with a possibly imperfect proxy reward function, which may lead to a human-agent alignment issue (i.e., the learned policy either converges to non-optimal performance with low cumulative rewards, or achieves high cumulative rewards but in undesired manner). To tackle this issue, we consider a framework where a human labeler can provide additional feedback in the form of corrective actions, which expresses the labeler's action preferences although this feedback may possibly be imperfect as well. In this setting, to obtain a better-aligned policy guided by both learning signals, we propose a novel value-based deep RL algorithm called Iterative learning from Corrective actions and Proxy rewards (ICoPro), which cycles through three phases: (1) Solicit sparse corrective actions from a human labeler on the agent's demonstrated trajectories; (2) Incorporate these corrective actions into the Q-function using a margin loss to enforce adherence to labeler's preferences; (3) Train the agent with standard RL losses regularized with a margin loss to learn from proxy rewards and propagate the Q-values learned from human feedback. Moreover, another novel design in our approach is to integrate pseudo-labels from the target Q-network to reduce human labor and further stabilize training. We experimentally validate our proposition on a variety of tasks (Atari games and autonomous driving on highway). On the one hand, using proxy rewards with different levels of imperfection, our method can better align with human preferences and is more sample-efficient than baseline methods. On the other hand, facing corrective actions with different types of imperfection, our method can overcome the non-optimality of this feedback thanks to the guidance from proxy reward.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05785",
        "abstract url": "https://arxiv.org/abs/2410.05785",
        "title": "Contextual Bandits with Non-Stationary Correlated Rewards for User Association in MmWave Vehicular Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Millimeter wave (mmWave) communication has emerged as a propelling technology in vehicular communication. Usually, an appropriate decision on user association requires timely channel information between vehicles and base stations (BSs), which is challenging given a fast-fading mmWave vehicular channel. In this paper, relying solely on learning transmission rate, we propose a low-complexity semi-distributed contextual correlated upper confidence bound (SD-CC-UCB) algorithm to establish an up-to-date user association without explicit measurement of channel state information (CSI). Under a contextual multi-arm bandits framework, SD-CC-UCB learns and predicts the transmission rate given the location and velocity of the vehicle, which can adequately capture the intricate channel condition for a prompt decision on user association. Further, SD-CC-UCB efficiently identifies the set of candidate BSs which probably support supreme transmission rate by leveraging the correlated distributions of transmission rates on different locations. To further refine the learning transmission rate over the link to candidate BSs, each vehicle deploys the Thompson Sampling algorithm by taking the interference among vehicles and handover overhead into consideration. Numerical results show that our proposed algorithm achieves the network throughput within 100%-103% of a benchmark algorithm which requires perfect instantaneous CSI, demonstrating the effectiveness of SD-CC-UCB in vehicular communications.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "13 pages, 9 figures"
    },
    {
        "paper id": "2410.05786",
        "abstract url": "https://arxiv.org/abs/2410.05786",
        "title": "Enhanced Feature Based Granular Ball Twin Support Vector Machine",
        "rating": "-0.5",
        "keywords": [
            [
                "Support Vector Machine"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we propose enhanced feature based granular ball twin support vector machine (EF-GBTSVM). EF-GBTSVM employs the coarse granularity of granular balls (GBs) as input rather than individual data samples. The GBs are mapped to the feature space of the hidden layer using random projection followed by the utilization of a non-linear activation function. The concatenation of original and hidden features derived from the centers of GBs gives rise to an enhanced feature space, commonly referred to as the random vector functional link (RVFL) space. This space encapsulates nuanced feature information to GBs. Further, we employ twin support vector machine (TSVM) in the RVFL space for classification. TSVM generates the two non-parallel hyperplanes in the enhanced feature space, which improves the generalization performance of the proposed EF-GBTSVM model. Moreover, the coarser granularity of the GBs enables the proposed EF-GBTSVM model to exhibit robustness to resampling, showcasing reduced susceptibility to the impact of noise and outliers. We undertake a thorough evaluation of the proposed EF-GBTSVM model on benchmark UCI and KEEL datasets. This evaluation encompasses scenarios with and without the inclusion of label noise. Moreover, experiments using NDC datasets further emphasize the proposed model's ability to handle large datasets. Experimental results, supported by thorough statistical analyses, demonstrate that the proposed EF-GBTSVM model significantly outperforms the baseline models in terms of generalization capabilities, scalability, and robustness. The source code for the proposed EF-GBTSVM model, along with additional results and further details, can be accessed at https://github.com/mtanveer1/EF-GBTSVM.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05837",
        "abstract url": "https://arxiv.org/abs/2410.05837",
        "title": "A noise-corrected Langevin algorithm and sampling by half-denoising",
        "rating": "-0.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The Langevin algorithm is a classic method for sampling from a given pdf in a real space. In its basic version, it only requires knowledge of the gradient of the log-density, also called the score function. However, in deep learning, it is often easier to learn the so-called \"noisy score function\", i.e. the gradient of the log-density of noisy data, more precisely when Gaussian noise is added to the data. Such an estimate is biased and complicates the use of the Langevin method. Here, we propose a noise-corrected version of the Langevin algorithm, where the bias due to noisy data is removed, at least regarding first-order terms. Unlike diffusion models, our algorithm needs to know the noisy score function for one single noise level only. We further propose a simple special case which has an interesting intuitive interpretation of iteratively adding noise the data and then attempting to remove half of that noise.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05839",
        "abstract url": "https://arxiv.org/abs/2410.05839",
        "title": "Bottom-up Anytime Discovery of Generalised Multimodal Graph Patterns for Knowledge Graphs",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Vast amounts of heterogeneous knowledge are becoming publicly available in the form of knowledge graphs, often linking multiple sources of data that have never been together before, and thereby enabling scholars to answer many new research questions. It is often not known beforehand, however, which questions the data might have the answers to, potentially leaving many interesting and novel insights to remain undiscovered. To support scholars during this scientific workflow, we introduce an anytime algorithm for the bottom-up discovery of generalized multimodal graph patterns in knowledge graphs. Each pattern is a conjunction of binary statements with (data-) type variables, constants, and/or value patterns. Upon discovery, the patterns are converted to SPARQL queries and presented in an interactive facet browser together with metadata and provenance information, enabling scholars to explore, analyse, and share queries. We evaluate our method from a user perspective, with the help of domain experts in the humanities.",
        "subjects": [
            "cs.AI",
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05911",
        "abstract url": "https://arxiv.org/abs/2410.05911",
        "title": "Accelerating Error Correction Code Transformers",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Error correction codes (ECC) are crucial for ensuring reliable information transmission in communication systems. Choukroun & Wolf (2022b) recently introduced the Error Correction Code Transformer (ECCT), which has demonstrated promising performance across various transmission channels and families of codes. However, its high computational and memory demands limit its practical applications compared to traditional decoding algorithms. Achieving effective quantization of the ECCT presents significant challenges due to its inherently small architecture, since existing, very low-precision quantization techniques often lead to performance degradation in compact neural networks. In this paper, we introduce a novel acceleration method for transformer-based decoders. We first propose a ternary weight quantization method specifically designed for the ECCT, inducing a decoder with multiplication-free linear layers. We present an optimized self-attention mechanism to reduce computational complexity via codeaware multi-heads processing. Finally, we provide positional encoding via the Tanner graph eigendecomposition, enabling a richer representation of the graph connectivity. The approach not only matches or surpasses ECCT's performance but also significantly reduces energy consumption, memory footprint, and computational complexity. Our method brings transformer-based error correction closer to practical implementation in resource-constrained environments, achieving a 90% compression ratio and reducing arithmetic operation energy consumption by at least 224 times on modern hardware.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05985",
        "abstract url": "https://arxiv.org/abs/2410.05985",
        "title": "Asynchronous Stochastic Gradient Descent with Decoupled Backpropagation and Layer-Wise Updates",
        "rating": "-0.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The increasing size of deep learning models has created the need for more efficient alternatives to the standard error backpropagation algorithm, that make better use of asynchronous, parallel and distributed computing. One major shortcoming of backpropagation is the interlocking between the forward phase of the algorithm, which computes a global loss, and the backward phase where the loss is backpropagated through all layers to compute the gradients, which are used to update the network parameters. To address this problem, we propose a method that parallelises SGD updates across the layers of a model by asynchronously updating them from multiple threads. Furthermore, since we observe that the forward pass is often much faster than the backward pass, we use separate threads for the forward and backward pass calculations, which allows us to use a higher ratio of forward to backward threads than the usual 1:1 ratio, reducing the overall staleness of the parameters. Thus, our approach performs asynchronous stochastic gradient descent using separate threads for the loss (forward) and gradient (backward) computations and performs layer-wise partial updates to parameters in a distributed way. We show that this approach yields close to state-of-the-art results while running up to 2.97x faster than Hogwild! scaled on multiple devices (Locally-Partitioned-Asynchronous-Parallel SGD). We theoretically prove the convergence of the algorithm using a novel theoretical framework based on stochastic differential equations and the drift diffusion process, by modeling the asynchronous parameter updates as a stochastic process.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ],
        "comment": "16 pages, 4 figures"
    },
    {
        "paper id": "2410.06022",
        "abstract url": "https://arxiv.org/abs/2410.06022",
        "title": "Can Language Models Induce Grammatical Knowledge from Indirect Evidence?",
        "rating": "-0.5",
        "keywords": [
            [
                "Grammatical"
            ],
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "What kinds of and how much data is necessary for language models to induce grammatical knowledge to judge sentence acceptability? Recent language models still have much room for improvement in their data efficiency compared to humans. This paper investigates whether language models efficiently use indirect data (indirect evidence), from which they infer sentence acceptability. In contrast, humans use indirect evidence efficiently, which is considered one of the inductive biases contributing to efficient language acquisition. To explore this question, we introduce the Wug InDirect Evidence Test (WIDET), a dataset consisting of training instances inserted into the pre-training data and evaluation instances. We inject synthetic instances with newly coined wug words into pretraining data and explore the model's behavior on evaluation data that assesses grammatical acceptability regarding those words. We prepare the injected instances by varying their levels of indirectness and quantity. Our experiments surprisingly show that language models do not induce grammatical knowledge even after repeated exposure to instances with the same structure but differing only in lexical items from evaluation instances in certain language phenomena. Our findings suggest a potential direction for future research: developing models that use latent indirect evidence to induce grammatical knowledge.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "This paper is accepted at EMNLP 2024 Main"
    },
    {
        "paper id": "2410.06042",
        "abstract url": "https://arxiv.org/abs/2410.06042",
        "title": "Weighted Embeddings for Low-Dimensional Graph Representation",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Learning low-dimensional numerical representations from symbolic data, e.g., embedding the nodes of a graph into a geometric space, is an important concept in machine learning. While embedding into Euclidean space is common, recent observations indicate that hyperbolic geometry is better suited to represent hierarchical information and heterogeneous data (e.g., graphs with a scale-free degree distribution). Despite their potential for more accurate representations, hyperbolic embeddings also have downsides like being more difficult to compute and harder to use in downstream tasks. We propose embedding into a weighted space, which is closely related to hyperbolic geometry but mathematically simpler. We provide the embedding algorithm WEmbed and demonstrate, based on generated as well as over 2000 real-world graphs, that our weighted embeddings heavily outperform state-of-the-art Euclidean embeddings for heterogeneous graphs while using fewer dimensions. The running time of WEmbed and embedding quality for the remaining instances is on par with state-of-the-art Euclidean embedders.",
        "subjects": [
            "cs.LG",
            "cs.DS",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06051",
        "abstract url": "https://arxiv.org/abs/2410.06051",
        "title": "Gaussian-Based and Outside-the-Box Runtime Monitoring Join Forces",
        "rating": "-0.5",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Since neural networks can make wrong predictions even with high confidence, monitoring their behavior at runtime is important, especially in safety-critical domains like autonomous driving. In this paper, we combine ideas from previous monitoring approaches based on observing the activation values of hidden neurons. In particular, we combine the Gaussian-based approach, which observes whether the current value of each monitored neuron is similar to typical values observed during training, and the Outside-the-Box monitor, which creates clusters of the acceptable activation values, and, thus, considers the correlations of the neurons' values. Our experiments evaluate the achieved improvement.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06151",
        "abstract url": "https://arxiv.org/abs/2410.06151",
        "title": "Quality Diversity Imitation Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "robot"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Imitation learning (IL) has shown great potential in various applications, such as robot control. However, traditional IL methods are usually designed to learn only one specific type of behavior since demonstrations typically correspond to a single expert. In this work, we introduce the first generic framework for Quality Diversity Imitation Learning (QD-IL), which enables the agent to learn a broad range of skills from limited demonstrations. Our framework integrates the principles of quality diversity with adversarial imitation learning (AIL) methods, and can potentially improve any inverse reinforcement learning (IRL) method. Empirically, our framework significantly improves the QD performance of GAIL and VAIL on the challenging continuous control tasks derived from Mujoco environments. Moreover, our method even achieves 2x expert performance in the most challenging Humanoid environment.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "22 pages, conference paper"
    },
    {
        "paper id": "2410.06186",
        "abstract url": "https://arxiv.org/abs/2410.06186",
        "title": "The Last Iterate Advantage: Empirical Auditing and Principled Heuristic Analysis of Differentially Private SGD",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose a simple heuristic privacy analysis of noisy clipped stochastic gradient descent (DP-SGD) in the setting where only the last iterate is released and the intermediate iterates remain hidden. Namely, our heuristic assumes a linear structure for the model. We show experimentally that our heuristic is predictive of the outcome of privacy auditing applied to various training procedures. Thus it can be used prior to training as a rough estimate of the final privacy leakage. We also probe the limitations of our heuristic by providing some artificial counterexamples where it underestimates the privacy leakage. The standard composition-based privacy analysis of DP-SGD effectively assumes that the adversary has access to all intermediate iterates, which is often unrealistic. However, this analysis remains the state of the art in practice. While our heuristic does not replace a rigorous privacy analysis, it illustrates the large gap between the best theoretical upper bounds and the privacy auditing lower bounds and sets a target for further work to improve the theoretical privacy analyses. We also empirically support our heuristic and show existing privacy auditing attacks are bounded by our heuristic analysis in both vision and language tasks.",
        "subjects": [
            "cs.CR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06209",
        "abstract url": "https://arxiv.org/abs/2410.06209",
        "title": "LeanAgent: Lifelong Learning for Formal Theorem Proving",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have been successful in mathematical reasoning tasks such as formal theorem proving when integrated with interactive proof assistants like Lean. Existing approaches involve training or fine-tuning an LLM on a specific dataset to perform well on particular domains, such as undergraduate-level mathematics. These methods struggle with generalizability to advanced mathematics. A fundamental limitation is that these approaches operate on static domains, failing to capture how mathematicians often work across multiple domains and projects simultaneously or cyclically. We present LeanAgent, a novel lifelong learning framework for theorem proving that continuously generalizes to and improves on ever-expanding mathematical knowledge without forgetting previously learned knowledge. LeanAgent introduces several key innovations, including a curriculum learning strategy that optimizes the learning trajectory in terms of mathematical difficulty, a dynamic database for efficient management of evolving mathematical knowledge, and progressive training to balance stability and plasticity. LeanAgent successfully proves 162 theorems previously unproved by humans across 23 diverse Lean repositories, many from advanced mathematics. It performs up to 11$\\times$ better than the static LLM baseline, proving challenging theorems in domains like abstract algebra and algebraic topology while showcasing a clear progression of learning from basic concepts to advanced topics. In addition, we analyze LeanAgent's superior performance on key lifelong learning metrics. LeanAgent achieves exceptional scores in stability and backward transfer, where learning new tasks improves performance on previously learned tasks. This emphasizes LeanAgent's continuous generalizability and improvement, explaining its superior theorem proving performance.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06235",
        "abstract url": "https://arxiv.org/abs/2410.06235",
        "title": "Parameter Choice and Neuro-Symbolic Approaches for Deep Domain-Invariant Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "As artificial intelligence (AI) systems advance, we move towards broad AI: systems capable of performing well on diverse tasks, understanding context, and adapting rapidly to new scenarios. A central challenge for broad AI systems is to generalize over tasks in related domains and being robust to distribution shifts. Neuro-symbolic (NeSy) AI bridges the gap between symbolic and sub-symbolic paradigms to address these challenges, enabling adaptable, generalizable, and more interpretable systems. The development of broad AI requires advancements in domain adaptation (DA), enabling models trained on source domains to effectively generalize to unseen target domains. Traditional approaches often rely on parameter optimization and fine-tuning, which can be impractical due to high costs and risks of catastrophic forgetting. NeSy AI systems use multiple models and methods to generalize to unseen domains and maintain performance across varying conditions. We analyze common DA and NeSy approaches with a focus on deep domain-invariant learning, extending to real-world challenges such as adapting to continuously changing domains and handling large domain gaps. We showcase state-of-the-art model-selection methods for scenarios with limited samples and introduce domain-specific adaptations without gradient-based updates for cases where model tuning is infeasible. This work establishes a framework for scalable and generalizable broad AI systems applicable across various problem settings, demonstrating how symbolic reasoning and large language models can build universal computational graphs that generalize across domains and problems, contributing to more adaptable AI approaches for real-world applications.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "177 pages. Doctoral thesis"
    },
    {
        "paper id": "2410.06262",
        "abstract url": "https://arxiv.org/abs/2410.06262",
        "title": "SymDiff: Equivariant Diffusion via Stochastic Symmetrisation",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose SymDiff, a novel method for constructing equivariant diffusion models using the recently introduced framework of stochastic symmetrisation. SymDiff resembles a learned data augmentation that is deployed at sampling time, and is lightweight, computationally efficient, and easy to implement on top of arbitrary off-the-shelf models. Notably, in contrast to previous work, SymDiff typically does not require any neural network components that are intrinsically equivariant, avoiding the need for complex parameterizations and the use of higher-order geometric features. Instead, our method can leverage highly scalable modern architectures as drop-in replacements for these more constrained alternatives. We show that this additional flexibility yields significant empirical benefit on $\\mathrm{E}(3)$-equivariant molecular generation. To the best of our knowledge, this is the first application of symmetrisation to generative modelling, suggesting its potential in this domain more generally.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06296",
        "abstract url": "https://arxiv.org/abs/2410.06296",
        "title": "Conformal Structured Prediction",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Conformal prediction has recently emerged as a promising strategy for quantifying the uncertainty of a predictive model; these algorithms modify the model to output sets of labels that are guaranteed to contain the true label with high probability. However, existing conformal prediction algorithms have largely targeted classification and regression settings, where the structure of the prediction set has a simple form as a level set of the scoring function. However, for complex structured outputs such as text generation, these prediction sets might include a large number of labels and therefore be hard for users to interpret. In this paper, we propose a general framework for conformal prediction in the structured prediction setting, that modifies existing conformal prediction algorithms to output structured prediction sets that implicitly represent sets of labels. In addition, we demonstrate how our approach can be applied in domains where the prediction sets can be represented as a set of nodes in a directed acyclic graph; for instance, for hierarchical labels such as image classification, a prediction set might be a small subset of coarse labels implicitly representing the prediction set of all their more fine-descendants. We demonstrate how our algorithm can be used to construct prediction sets that satisfy a desired coverage guarantee in several domains.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "14 pages, 12 figures"
    },
    {
        "paper id": "2410.06339",
        "abstract url": "https://arxiv.org/abs/2410.06339",
        "title": "Filtered Randomized Smoothing: A New Defense for Robust Modulation Classification",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep Neural Network (DNN) based classifiers have recently been used for the modulation classification of RF signals. These classifiers have shown impressive performance gains relative to conventional methods, however, they are vulnerable to imperceptible (low-power) adversarial attacks. Some of the prominent defense approaches include adversarial training (AT) and randomized smoothing (RS). While AT increases robustness in general, it fails to provide resilience against previously unseen adaptive attacks. Other approaches, such as Randomized Smoothing (RS), which injects noise into the input, address this shortcoming by providing provable certified guarantees against arbitrary attacks, however, they tend to sacrifice accuracy. In this paper, we study the problem of designing robust DNN-based modulation classifiers that can provide provable defense against arbitrary attacks without significantly sacrificing accuracy. To this end, we first analyze the spectral content of commonly studied attacks on modulation classifiers for the benchmark RadioML dataset. We observe that spectral signatures of un-perturbed RF signals are highly localized, whereas attack signals tend to be spread out in frequency. To exploit this spectral heterogeneity, we propose Filtered Randomized Smoothing (FRS), a novel defense which combines spectral filtering together with randomized smoothing. FRS can be viewed as a strengthening of RS by leveraging the specificity (spectral Heterogeneity) inherent to the modulation classification problem. In addition to providing an approach to compute the certified accuracy of FRS, we also provide a comprehensive set of simulations on the RadioML dataset to show the effectiveness of FRS and show that it significantly outperforms existing defenses including AT and RS in terms of accuracy on both attacked and benign signals.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "cs.IT",
            "cs.NI",
            "eess.SP"
        ],
        "comment": "IEEE Milcom 2024"
    },
    {
        "paper id": "2410.06340",
        "abstract url": "https://arxiv.org/abs/2410.06340",
        "title": "FedGraph: A Research Library and Benchmark for Federated Graph Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated graph learning is an emerging field with significant practical challenges. While many algorithms have been proposed to enhance model accuracy, their system performance, crucial for real-world deployment, is often overlooked. To address this gap, we present FedGraph, a research library designed for practical distributed deployment and benchmarking in federated graph learning. FedGraph supports a range of state-of-the-art methods and includes profiling tools for system performance evaluation, focusing on communication and computation costs during training. FedGraph can then facilitate the development of practical applications and guide the design of future algorithms.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "https://github.com/FedGraph/fedgraph"
    },
    {
        "paper id": "2410.06347",
        "abstract url": "https://arxiv.org/abs/2410.06347",
        "title": "Solving Multi-Goal Robotic Tasks with Decision Transformer",
        "rating": "-0.5",
        "keywords": [
            [
                "robotics",
                "robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Artificial intelligence plays a crucial role in robotics, with reinforcement learning (RL) emerging as one of the most promising approaches for robot control. However, several key challenges hinder its broader application. First, many RL methods rely on online learning, which requires either real-world hardware or advanced simulation environments--both of which can be costly, time-consuming, and impractical. Offline reinforcement learning offers a solution, enabling models to be trained without ongoing access to physical robots or simulations. A second challenge is learning multi-goal tasks, where robots must achieve multiple objectives simultaneously. This adds complexity to the training process, as the model must generalize across different goals. At the same time, transformer architectures have gained significant popularity across various domains, including reinforcement learning. Yet, no existing methods effectively combine offline training, multi-goal learning, and transformer-based architectures. In this paper, we address these challenges by introducing a novel adaptation of the decision transformer architecture for offline multi-goal reinforcement learning in robotics. Our approach integrates goal-specific information into the decision transformer, allowing it to handle complex tasks in an offline setting. To validate our method, we developed a new offline reinforcement learning dataset using the Panda robotic platform in simulation. Our extensive experiments demonstrate that the decision transformer can outperform state-of-the-art online reinforcement learning methods.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06355",
        "abstract url": "https://arxiv.org/abs/2410.06355",
        "title": "Context-Aware Command Understanding for Tabletop Scenarios",
        "rating": "-0.5",
        "keywords": [
            [
                "robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper presents a novel hybrid algorithm designed to interpret natural human commands in tabletop scenarios. By integrating multiple sources of information, including speech, gestures, and scene context, the system extracts actionable instructions for a robot, identifying relevant objects and actions. The system operates in a zero-shot fashion, without reliance on predefined object models, enabling flexible and adaptive use in various environments. We assess the integration of multiple deep learning models, evaluating their suitability for deployment in real-world robotic setups. Our algorithm performs robustly across different tasks, combining language processing with visual grounding. In addition, we release a small dataset of video recordings used to evaluate the system. This dataset captures real-world interactions in which a human provides instructions in natural language to a robot, a contribution to future research on human-robot interaction. We discuss the strengths and limitations of the system, with particular focus on how it handles multimodal command interpretation, and its ability to be integrated into symbolic robotic frameworks for safe and explainable decision-making.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06397",
        "abstract url": "https://arxiv.org/abs/2410.06397",
        "title": "Provable Accuracy Bounds for Hybrid Dynamical Optimization and Sampling",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Analog dynamical accelerators (DXs) are a growing sub-field in computer architecture research, offering order-of-magnitude gains in power efficiency and latency over traditional digital methods in several machine learning, optimization, and sampling tasks. However, limited-capacity accelerators require hybrid analog/digital algorithms to solve real-world problems, commonly using large-neighborhood local search (LNLS) frameworks. Unlike fully digital algorithms, hybrid LNLS has no non-asymptotic convergence guarantees and no principled hyperparameter selection schemes, particularly limiting cross-device training and inference. In this work, we provide non-asymptotic convergence guarantees for hybrid LNLS by reducing to block Langevin Diffusion (BLD) algorithms. Adapting tools from classical sampling theory, we prove exponential KL-divergence convergence for randomized and cyclic block selection strategies using ideal DXs. With finite device variation, we provide explicit bounds on the 2-Wasserstein bias in terms of step duration, noise strength, and function parameters. Our BLD model provides a key link between established theory and novel computing platforms, and our theoretical results provide a closed-form expression linking device variation, algorithm hyperparameters, and performance.",
        "subjects": [
            "cs.LG",
            "cs.DS",
            "math.ST"
        ],
        "comment": "31 pages, 2 figures"
    },
    {
        "paper id": "2410.06400",
        "abstract url": "https://arxiv.org/abs/2410.06400",
        "title": "Reliable Heading Tracking for Pedestrian Road Crossing Prediction Using Commodity Devices",
        "rating": "-0.5",
        "keywords": [
            [
                "navigation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Pedestrian heading tracking enables applications in pedestrian navigation, traffic safety, and accessibility. Previous works, using inertial sensor fusion or machine learning, are limited in that they assume the phone is fixed in specific orientations, hindering their generalizability. We propose a new heading tracking algorithm, the Orientation-Heading Alignment (OHA), which leverages a key insight: people tend to carry smartphones in certain ways due to habits, such as swinging them while walking. For each smartphone attitude during this motion, OHA maps the smartphone orientation to the pedestrian heading and learns such mappings efficiently from coarse headings and smartphone orientations. To anchor our algorithm in a practical scenario, we apply OHA to a challenging task: predicting when pedestrians are about to cross the road to improve road user safety. In particular, using 755 hours of walking data collected since 2020 from 60 individuals, we develop a lightweight model that operates in real-time on commodity devices to predict road crossings. Our evaluation shows that OHA achieves 3.4 times smaller heading errors across nine scenarios than existing methods. Furthermore, OHA enables the early and accurate detection of pedestrian crossing behavior, issuing crossing alerts 0.35 seconds, on average, before pedestrians enter the road range.",
        "subjects": [
            "eess.SP",
            "cs.LG"
        ],
        "comment": "25 pages"
    },
    {
        "paper id": "2410.06406",
        "abstract url": "https://arxiv.org/abs/2410.06406",
        "title": "Topology-Agnostic Graph U-Nets for Scalar Field Prediction on Unstructured Meshes",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine-learned surrogate models to accelerate lengthy computer simulations are becoming increasingly important as engineers look to streamline the product design cycle. In many cases, these approaches offer the ability to predict relevant quantities throughout a geometry, but place constraints on the form of the input data. In a world of diverse data types, a preferred approach would not restrict the input to a particular structure. In this paper, we propose Topology-Agnostic Graph U-Net (TAG U-Net), a graph convolutional network that can be trained to input any mesh or graph structure and output a prediction of a target scalar field at each node. The model constructs coarsened versions of each input graph and performs a set of convolution and pooling operations to predict the node-wise outputs on the original graph. By training on a diverse set of shapes, the model can make strong predictions, even for shapes unlike those seen during training. A 3-D additive manufacturing dataset is presented, containing Laser Powder Bed Fusion simulation results for thousands of parts. The model is demonstrated on this dataset, and it performs well, predicting both 2-D and 3-D scalar fields with a median R-squared > 0.85 on test geometries. Code and datasets are available online.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "18 pages, 10 figures"
    },
    {
        "paper id": "2410.06460",
        "abstract url": "https://arxiv.org/abs/2410.06460",
        "title": "A Benchmark on Directed Graph Representation Learning in Hardware Designs",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "To keep pace with the rapid advancements in design complexity within modern computing systems, directed graph representation learning (DGRL) has become crucial, particularly for encoding circuit netlists, computational graphs, and developing surrogate models for hardware performance prediction. However, DGRL remains relatively unexplored, especially in the hardware domain, mainly due to the lack of comprehensive and user-friendly benchmarks. This study presents a novel benchmark comprising five hardware design datasets and 13 prediction tasks spanning various levels of circuit abstraction. We evaluate 21 DGRL models, employing diverse graph neural networks and graph transformers (GTs) as backbones, enhanced by positional encodings (PEs) tailored for directed graphs. Our results highlight that bidirected (BI) message passing neural networks (MPNNs) and robust PEs significantly enhance model performance. Notably, the top-performing models include PE-enhanced GTs interleaved with BI-MPNN layers and BI-Graph Isomorphism Network, both surpassing baselines across the 13 tasks. Additionally, our investigation into out-of-distribution (OOD) performance emphasizes the urgent need to improve OOD generalization in DGRL models. This benchmark, implemented with a modular codebase, streamlines the evaluation of DGRL models for both hardware and ML practitioners",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06462",
        "abstract url": "https://arxiv.org/abs/2410.06462",
        "title": "Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders",
        "rating": "-0.5",
        "keywords": [
            [
                "Attack"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The research builds and evaluates the adversarial potential to introduce copied code or hallucinated AI recommendations for malicious code in popular code repositories. While foundational large language models (LLMs) from OpenAI, Google, and Anthropic guard against both harmful behaviors and toxic strings, previous work on math solutions that embed harmful prompts demonstrate that the guardrails may differ between expert contexts. These loopholes would appear in mixture of expert's models when the context of the question changes and may offer fewer malicious training examples to filter toxic comments or recommended offensive actions. The present work demonstrates that foundational models may refuse to propose destructive actions correctly when prompted overtly but may unfortunately drop their guard when presented with a sudden change of context, like solving a computer programming challenge. We show empirical examples with trojan-hosting repositories like GitHub, NPM, NuGet, and popular content delivery networks (CDN) like jsDelivr which amplify the attack surface. In the LLM's directives to be helpful, example recommendations propose application programming interface (API) endpoints which a determined domain-squatter could acquire and setup attack mobile infrastructure that triggers from the naively copied code. We compare this attack to previous work on context-shifting and contrast the attack surface as a novel version of \"living off the land\" attacks in the malware literature. In the latter case, foundational language models can hijack otherwise innocent user prompts to recommend actions that violate their owners' safety policies when posed directly without the accompanying coding support request.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06472",
        "abstract url": "https://arxiv.org/abs/2410.06472",
        "title": "Enabling Novel Mission Operations and Interactions with ROSA: The Robot Operating System Agent",
        "rating": "-0.5",
        "keywords": [
            [
                "robotics",
                "Robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The advancement of robotic systems has revolutionized numerous industries, yet their operation often demands specialized technical knowledge, limiting accessibility for non-expert users. This paper introduces ROSA (Robot Operating System Agent), an AI-powered agent that bridges the gap between the Robot Operating System (ROS) and natural language interfaces. By leveraging state-of-the-art language models and integrating open-source frameworks, ROSA enables operators to interact with robots using natural language, translating commands into actions and interfacing with ROS through well-defined tools. ROSA's design is modular and extensible, offering seamless integration with both ROS1 and ROS2, along with safety mechanisms like parameter validation and constraint enforcement to ensure secure, reliable operations. While ROSA is originally designed for ROS, it can be extended to work with other robotics middle-wares to maximize compatibility across missions. ROSA enhances human-robot interaction by democratizing access to complex robotic systems, empowering users of all expertise levels with multi-modal capabilities such as speech integration and visual perception. Ethical considerations are thoroughly addressed, guided by foundational principles like Asimov's Three Laws of Robotics, ensuring that AI integration promotes safety, transparency, privacy, and accountability. By making robotic technology more user-friendly and accessible, ROSA not only improves operational efficiency but also sets a new standard for responsible AI use in robotics and potentially future mission operations. This paper introduces ROSA's architecture and showcases initial mock-up operations in JPL's Mars Yard, a laboratory, and a simulation using three different robots. The core ROSA library is available as open-source.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.HC"
        ],
        "comment": "Under review for IEEE Aerospace Conference, 20 pages, 20 figures"
    },
    {
        "paper id": "2410.06473",
        "abstract url": "https://arxiv.org/abs/2410.06473",
        "title": "Grounding Robot Policies with Visuomotor Language Guidance",
        "rating": "-0.5",
        "keywords": [
            [
                "Robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Recent advances in the fields of natural language processing and computer vision have shown great potential in understanding the underlying dynamics of the world from large-scale internet data. However, translating this knowledge into robotic systems remains an open challenge, given the scarcity of human-robot interactions and the lack of large-scale datasets of real-world robotic data. Previous robot learning approaches such as behavior cloning and reinforcement learning have shown great capabilities in learning robotic skills from human demonstrations or from scratch in specific environments. However, these approaches often require task-specific demonstrations or designing complex simulation environments, which limits the development of generalizable and robust policies for new settings. Aiming to address these limitations, we propose an agent-based framework for grounding robot policies to the current context, considering the constraints of a current robot and its environment using visuomotor-grounded language guidance. The proposed framework is composed of a set of conversational agents designed for specific roles -- namely, high-level advisor, visual grounding, monitoring, and robotic agents. Given a base policy, the agents collectively generate guidance at run time to shift the action distribution of the base policy towards more desirable future states. We demonstrate that our approach can effectively guide manipulation policies to achieve significantly higher success rates both in simulation and in real-world experiments without the need for additional human demonstrations or extensive exploration. Project videos at https://sites.google.com/view/motorcortex/home.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "19 pages, 6 figures, 1 table"
    },
    {
        "paper id": "2410.06490",
        "abstract url": "https://arxiv.org/abs/2410.06490",
        "title": "FedL2G: Learning to Guide Local Training in Heterogeneous Federated Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Data and model heterogeneity are two core issues in Heterogeneous Federated Learning (HtFL). In scenarios with heterogeneous model architectures, aggregating model parameters becomes infeasible, leading to the use of prototypes (i.e., class representative feature vectors) for aggregation and guidance. However, they still experience a mismatch between the extra guiding objective and the client's original local objective when aligned with global prototypes. Thus, we propose a Federated Learning-to-Guide (FedL2G) method that adaptively learns to guide local training in a federated manner and ensures the extra guidance is beneficial to clients' original tasks. With theoretical guarantees, FedL2G efficiently implements the learning-to-guide process using only first-order derivatives w.r.t. model parameters and achieves a non-convex convergence rate of O(1/T). We conduct extensive experiments on two data heterogeneity and six model heterogeneity settings using 14 heterogeneous model architectures (e.g., CNNs and ViTs) to demonstrate FedL2G's superior performance compared to six counterparts.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05712",
        "abstract url": "https://arxiv.org/abs/2410.05712",
        "title": "Advancing VR Simulators for Autonomous Vehicle-Pedestrian Interactions: A Focus on Multi-Entity Scenarios",
        "rating": "-1",
        "keywords": [
            [
                "Vehicle"
            ]
        ],
        "abstract": "Recent research has increasingly focused on how autonomous vehicles (AVs) communicate with pedestrians in complex traffic situations involving multiple vehicles and pedestrians. VR is emerging as an effective tool to simulate these multi-entity scenarios, offering a safe and controlled study environment. Despite its growing use, there is a lack of thorough investigation into the effectiveness of these VR simulations, leaving a notable gap in documented insights and lessons. This research undertook a retrospective analysis of two distinct VR-based studies: one focusing on multiple AV scenarios (N=32) and the other on multiple pedestrian scenarios (N=25). Central to our examination are the participants' sense of presence and their crossing behaviour. The findings highlighted key factors that either enhance or diminish the sense of presence in each simulation, providing considerations for future improvements. Furthermore, they underscore the influence of controlled scenarios on crossing behaviour and interactions with AVs, advocating for the exploration of more natural and interactive simulations that better reflect real-world AV and pedestrian dynamics. Through this study, we set a groundwork for advancing VR simulators to study complex interactions between AVs and pedestrians.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "Accepted to Transportation Research Part F: Traffic Psychology and Behaviour"
    },
    {
        "paper id": "2410.05715",
        "abstract url": "https://arxiv.org/abs/2410.05715",
        "title": "Demonstration Based Explainable AI for Learning from Demonstration Methods",
        "rating": "-1",
        "keywords": [
            [
                "robot",
                "navigation"
            ]
        ],
        "abstract": "Learning from Demonstration (LfD) is a powerful type of machine learning that can allow novices to teach and program robots to complete various tasks. However, the learning process for these systems may still be difficult for novices to interpret and understand, making effective teaching challenging. Explainable artificial intelligence (XAI) aims to address this challenge by explaining a system to the user. In this work, we investigate XAI within LfD by implementing an adaptive explanatory feedback system on an inverse reinforcement learning (IRL) algorithm. The feedback is implemented by demonstrating selected learnt trajectories to users. The system adapts to user teaching by categorizing and then selectively sampling trajectories shown to a user, to show a representative sample of both successful and unsuccessful trajectories. The system was evaluated through a user study with 26 participants teaching a robot a navigation task. The results of the user study demonstrated that the proposed explanatory feedback system can improve robot performance, teaching efficiency and user understanding of the robot.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 Pages, 9 Figures, 2 Tables"
    },
    {
        "paper id": "2410.05729",
        "abstract url": "https://arxiv.org/abs/2410.05729",
        "title": "Equi-GSPR: Equivariant SE(3) Graph Network Model for Sparse Point Cloud Registration",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Point cloud registration is a foundational task for 3D alignment and reconstruction applications. While both traditional and learning-based registration approaches have succeeded, leveraging the intrinsic symmetry of point cloud data, including rotation equivariance, has received insufficient attention. This prohibits the model from learning effectively, resulting in a requirement for more training data and increased model complexity. To address these challenges, we propose a graph neural network model embedded with a local Spherical Euclidean 3D equivariance property through SE(3) message passing based propagation. Our model is composed mainly of a descriptor module, equivariant graph layers, match similarity, and the final regression layers. Such modular design enables us to utilize sparsely sampled input points and initialize the descriptor by self-trained or pre-trained geometric feature descriptors easily. Experiments conducted on the 3DMatch and KITTI datasets exhibit the compelling and robust performance of our model compared to state-of-the-art approaches, while the model complexity remains relatively low at the same time.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "18 main body pages, and 9 pages for supplementary part"
    },
    {
        "paper id": "2410.05731",
        "abstract url": "https://arxiv.org/abs/2410.05731",
        "title": "Enhancing SPARQL Generation by Triplet-order-sensitive Pre-training",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "Semantic parsing that translates natural language queries to SPARQL is of great importance for Knowledge Graph Question Answering (KGQA) systems. Although pre-trained language models like T5 have achieved significant success in the Text-to-SPARQL task, their generated outputs still exhibit notable errors specific to the SPARQL language, such as triplet flips. To address this challenge and further improve the performance, we propose an additional pre-training stage with a new objective, Triplet Order Correction (TOC), along with the commonly used Masked Language Modeling (MLM), to collectively enhance the model's sensitivity to triplet order and SPARQL syntax. Our method achieves state-of-the-art performances on three widely-used benchmarks.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "accepted by CIKM 2024"
    },
    {
        "paper id": "2410.05773",
        "abstract url": "https://arxiv.org/abs/2410.05773",
        "title": "GLRT-Based Metric Learning for Remote Sensing Object Retrieval",
        "rating": "-1",
        "keywords": [
            [
                "Remote Sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the improvement in the quantity and quality of remote sensing images, content-based remote sensing object retrieval (CBRSOR) has become an increasingly important topic. However, existing CBRSOR methods neglect the utilization of global statistical information during both training and test stages, which leads to the overfitting of neural networks to simple sample pairs of samples during training and suboptimal metric performance. Inspired by the Neyman-Pearson theorem, we propose a generalized likelihood ratio test-based metric learning (GLRTML) approach, which can estimate the relative difficulty of sample pairs by incorporating global data distribution information during training and test phases. This guides the network to focus more on difficult samples during the training process, thereby encourages the network to learn more discriminative feature embeddings. In addition, GLRT is a more effective than traditional metric space due to the utilization of global data distribution information. Accurately estimating the distribution of embeddings is critical for GLRTML. However, in real-world applications, there is often a distribution shift between the training and target domains, which diminishes the effectiveness of directly using the distribution estimated on training data. To address this issue, we propose the clustering pseudo-labels-based fast parameter adaptation (CPLFPA) method. CPLFPA efficiently estimates the distribution of embeddings in the target domain by clustering target domain instances and re-estimating the distribution parameters for GLRTML. We reorganize datasets for CBRSOR tasks based on fine-grained ship remote sensing image slices (FGSRSI-23) and military aircraft recognition (MAR20) datasets. Extensive experiments on these datasets demonstrate the effectiveness of our proposed GLRTML and CPLFPA.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05778",
        "abstract url": "https://arxiv.org/abs/2410.05778",
        "title": "Song Emotion Classification of Lyrics with Out-of-Domain Data under Label Scarcity",
        "rating": "-1",
        "keywords": [
            [
                "Song"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Songs have been found to profoundly impact human emotions, with lyrics having significant power to stimulate emotional changes in the audience. There is a scarcity of large, high quality in-domain datasets for lyrics-based song emotion classification (Edmonds and Sedoc, 2021; Zhou, 2022). It has been noted that in-domain training datasets are often difficult to acquire (Zhang and Miao, 2023) and that label acquisition is often limited by cost, time, and other factors (Azad et al., 2018). We examine the novel usage of a large out-of-domain dataset as a creative solution to the challenge of training data scarcity in the emotional classification of song lyrics. We find that CNN models trained on a large Reddit comments dataset achieve satisfactory performance and generalizability to lyrical emotion classification, thus giving insights into and a promising possibility in leveraging large, publicly available out-of-domain datasets for domains whose in-domain data are lacking or costly to acquire.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05793",
        "abstract url": "https://arxiv.org/abs/2410.05793",
        "title": "Distributed Coordination for Multi-Vehicle Systems in the Presence of Misbehaving Vehicles",
        "rating": "-1",
        "keywords": [
            [
                "autonomous driving",
                "Vehicle"
            ]
        ],
        "abstract": "The coordination problem of multi-vehicle systems is of great interests in the area of autonomous driving and multi-vehicle control. This work mainly focuses on multi-task coordination problem of a group of vehicles with a bicycle model and some specific control objectives, including collision avoidance, connectivity maintenance and convergence to desired destinations. The basic idea is to develop a proper Lyapunov-like barrier function for all tasks and a distributed controller could be built in the presence of misbehaving vehicles. Control protocols are provided for both leader vehicle and follower vehicles. The simulation results demonstrate the effectiveness of proposed method.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "13 pages, 5 figures, accepted by The 15th Asia Conference on Mechanical and Aerospace Engineering (ACMAE 2024)"
    },
    {
        "paper id": "2410.05803",
        "abstract url": "https://arxiv.org/abs/2410.05803",
        "title": "Radio Map-assisted CSI Tracking with Uncertain Locations in Massive MIMO Networks",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "Massive multiple-input multiple-output (MIMO) systems offer significant potential to enhance wireless communication performance, yet efficient and accurate channel state information (CSI) tracking remains a key challenge, particularly in dynamic urban settings. To address this, we propose a radio mapassisted framework for CSI tracking and trajectory discovery, relying on sparse channel observations. The radio map is redefined as a mapping from spatial positions to deterministic channel covariance matrices, which captures the complex and time-varying characteristics of urban wireless environments. Leveraging these covariance maps, we develop a CSI tracking method that enables accurate estimation using only single-dimensional observations collected during user movement. Furthermore, we present an efficient algorithm that constructs and continuously refines the radio map through sequential sparse observations, even when location labels are uncertain. Numerical results based on real city maps and ray-tracing MIMO channel datasets show that the proposed framework significantly outperforms baseline methods in both accuracy and adaptability.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05805",
        "abstract url": "https://arxiv.org/abs/2410.05805",
        "title": "PostCast: Generalizable Postprocessing for Precipitation Nowcasting via Unsupervised Blurriness Modeling",
        "rating": "-1",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "radar"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Precipitation nowcasting plays a pivotal role in socioeconomic sectors, especially in severe convective weather warnings. Although notable progress has been achieved by approaches mining the spatiotemporal correlations with deep learning, these methods still suffer severe blurriness as the lead time increases, which hampers accurate predictions for extreme precipitation. To alleviate blurriness, researchers explore generative methods conditioned on blurry predictions. However, the pairs of blurry predictions and corresponding ground truth need to be generated in advance, making the training pipeline cumbersome and limiting the generality of generative models within blur modes that appear in training data. By rethinking the blurriness in precipitation nowcasting as a blur kernel acting on predictions, we propose an unsupervised postprocessing method to eliminate the blurriness without the requirement of training with the pairs of blurry predictions and corresponding ground truth. Specifically, we utilize blurry predictions to guide the generation process of a pre-trained unconditional denoising diffusion probabilistic model (DDPM) to obtain high-fidelity predictions with eliminated blurriness. A zero-shot blur kernel estimation mechanism and an auto-scale denoise guidance strategy are introduced to adapt the unconditional DDPM to any blurriness modes varying from datasets and lead times in precipitation nowcasting. Extensive experiments are conducted on 7 precipitation radar datasets, demonstrating the generality and superiority of our method.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05813",
        "abstract url": "https://arxiv.org/abs/2410.05813",
        "title": "Single Actuator Undulation Soft-bodied Robots Using A Precompressed Variable Thickness Flexible Beam",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Soft robots - due to their intrinsic flexibility of the body - can adaptively navigate unstructured environments. One of the most popular locomotion gaits that has been implemented in soft robots is undulation. The undulation motion in soft robots resembles the locomotion gait of stringy creatures such as snakes, eels, and C. Elegans. Typically, the implementation of undulation locomotion on a soft robot requires many actuators to control each segment of the stringy body. The added weight of multiple actuators limits the navigating performance of soft-bodied robots. In this paper, we propose a simple tendon-driven flexible beam with only one actuator (a DC motor) that can generate a mechanical traveling wave along the beam to support the undulation locomotion of soft robots. The beam will be precompressed along its axis by shortening the length of the two tendons to form an S-shape, thus pretensioning the tendons. The motor will wind and unwind the tendons to deform the flexible beam and generate traveling waves along the body of the robot. We experiment with different pre-tension to characterize the relationship between tendon pre-tension forces and the DC-motor winding/unwinding. Our proposal enables a simple implementation of undulation motion to support the locomotion of soft-bodied robots.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted to IROS 2024"
    },
    {
        "paper id": "2410.05824",
        "abstract url": "https://arxiv.org/abs/2410.05824",
        "title": "Multi-Session Client-Centered Treatment Outcome Evaluation in Psychotherapy",
        "rating": "-1",
        "keywords": [
            [
                "health",
                "clinical",
                "Psychological"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In psychotherapy, therapeutic outcome assessment, or treatment outcome evaluation, is essential for enhancing mental health care by systematically evaluating therapeutic processes and outcomes. Existing large language model approaches often focus on therapist-centered, single-session evaluations, neglecting the client's subjective experience and longitudinal progress across multiple sessions. To address these limitations, we propose IPAEval, a client-Informed Psychological Assessment-based Evaluation framework that automates treatment outcome evaluations from the client's perspective using clinical interviews. IPAEval integrates cross-session client-contextual assessment and session-focused client-dynamics assessment to provide a comprehensive understanding of therapeutic progress. Experiments on our newly developed TheraPhase dataset demonstrate that IPAEval effectively tracks symptom severity and treatment outcomes over multiple sessions, outperforming previous single-session models and validating the benefits of items-aware reasoning mechanisms.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2410.05828",
        "abstract url": "https://arxiv.org/abs/2410.05828",
        "title": "Effort Allocation for Deadline-Aware Task and Motion Planning: A Metareasoning Approach",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "In robot planning, tasks can often be achieved through multiple options, each consisting of several actions. This work specifically addresses deadline constraints in task and motion planning, aiming to find a plan that can be executed within the deadline despite uncertain planning and execution times. We propose an effort allocation problem, formulated as a Markov decision process (MDP), to find such a plan by leveraging metareasoning perspectives to allocate computational resources among the given options. We formally prove the NP-hardness of the problem by reducing it from the knapsack problem. Both a model-based approach, where transition models are learned from past experience, and a model-free approach, which overcomes the unavailability of prior data acquisition through reinforcement learning, are explored. For the model-based approach, we investigate Monte Carlo tree search (MCTS) to approximately solve the proposed MDP and further design heuristic schemes to tackle NP-hardness, leading to the approximate yet efficient algorithm called DP_Rerun. In experiments, DP_Rerun demonstrates promising performance comparable to MCTS while requiring negligible computation time.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "48 pages, 6 figures"
    },
    {
        "paper id": "2410.05829",
        "abstract url": "https://arxiv.org/abs/2410.05829",
        "title": "A GPT-based Decision Transformer for Multi-Vehicle Coordination at Unsignalized Intersections",
        "rating": "-1",
        "keywords": [
            [
                "Vehicle"
            ]
        ],
        "abstract": "In this paper, we explore the application of the Decision Transformer, a decision-making algorithm based on the Generative Pre-trained Transformer (GPT) architecture, to multi-vehicle coordination at unsignalized intersections. We formulate the coordination problem so as to find the optimal trajectories for multiple vehicles at intersections, modeling it as a sequence prediction task to fully leverage the power of GPTs as a sequence model. Through extensive experiments, we compare our approach to a reservation-based intersection management system. Our results show that the Decision Transformer can outperform the training data in terms of total travel time and can be generalized effectively to various scenarios, including noise-induced velocity variations, continuous interaction environments, and different vehicle numbers and road configurations.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages"
    },
    {
        "paper id": "2410.05869",
        "abstract url": "https://arxiv.org/abs/2410.05869",
        "title": "Unobserved Object Detection using Generative Models",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Can we detect an object that is not visible in an image? This study introduces the novel task of 2D and 3D unobserved object detection for predicting the location of objects that are occluded or lie outside the image frame. We adapt several state-of-the-art pre-trained generative models to solve this task, including 2D and 3D diffusion models and vision--language models, and show that they can be used to infer the presence of objects that are not directly observed. To benchmark this task, we propose a suite of metrics that captures different aspects of performance. Our empirical evaluations on indoor scenes from the RealEstate10k dataset with COCO object categories demonstrate results that motivate the use of generative models for the unobserved object detection task. The current work presents a promising step towards compelling applications like visual search and probabilistic planning that can leverage object detection beyond what can be directly observed.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "comment": "16 pages; 41 figures"
    },
    {
        "paper id": "2410.05883",
        "abstract url": "https://arxiv.org/abs/2410.05883",
        "title": "Improved PCRLB for radar tracking in clutter with geometry-dependent target measurement uncertainty and application to radar trajectory control",
        "rating": "-1",
        "keywords": [
            [
                "trajectory",
                "radar"
            ]
        ],
        "abstract": "In realistic radar tracking, target measurement uncertainty (TMU) in terms of both detection probability and measurement error covariance is significantly affected by the target-to-radar (T2R) geometry. However, existing posterior Cramer-Rao Lower Bounds (PCRLBs) rarely investigate the fundamental impact of T2R geometry on target measurement uncertainty and eventually on mean square error (MSE) of state estimate, inevitably resulting in over-conservative lower bound. To address this issue, this paper firstly derives the generalized model of target measurement error covariance for bistatic radar with moving receiver and transmitter illuminating any type of signal, along with its approximated solution to specify the impact of T2R geometry on error covariance. Based upon formulated TMU model, an improved PCRLB (IPCRLB) fully accounting for both measurement origin uncertainty and geometry-dependent TMU is then re-derived, both detection probability and measurement error covariance are treated as state-dependent parameters when differentiating log-likelihood with respect to target state. Compared to existing PCRLBs that partially or completely ignore the dependence of target measurement uncertainty on T2R geometry, proposed IPCRLB provides a much accurate (less-conservative) lower bound for radar tracking in clutter with geometry-dependent TMU. The new bound is then applied to radar trajectory control to effectively optimize T2R geometry and exhibits least uncertainty of acquired target measurement and more accurate state estimate for bistatic radar tracking in clutter, compared to state-of-the-art trajectory control methods.",
        "subjects": [
            "eess.SP",
            "math.OC"
        ],
        "comment": "15 pages,12 figures"
    },
    {
        "paper id": "2410.05907",
        "abstract url": "https://arxiv.org/abs/2410.05907",
        "title": "Privacy-Enhanced Over-the-Air Federated Learning via Client-Driven Power Balancing",
        "rating": "-1",
        "keywords": [
            [
                "Federated Learning"
            ]
        ],
        "abstract": "This paper introduces a novel privacy-enhanced over-the-air Federated Learning (OTA-FL) framework using client-driven power balancing (CDPB) to address privacy concerns in OTA-FL systems. In recent studies, a server determines the power balancing based on the continuous transmission of channel state information (CSI) from each client. Furthermore, they concentrate on fulfilling privacy requirements in every global iteration, which can heighten the risk of privacy exposure as the learning process extends. To mitigate these risks, we propose two CDPB strategies -- CDPB-n (noisy) and CDPB-i (idle) -- allowing clients to adjust transmission power independently, without sharing CSI. CDPB-n transmits noise during poor conditions, while CDPB-i pauses transmission until conditions improve. To further enhance privacy and learning efficiency, we show a mixed strategy, CDPB-mixed, which combines CDPB-n and CDPB-i. Our experimental results show that CDPB outperforms traditional approaches in terms of model accuracy and privacy guarantees, providing a practical solution for enhancing OTA-FL in resource-constrained environments.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "13pages"
    },
    {
        "paper id": "2410.05926",
        "abstract url": "https://arxiv.org/abs/2410.05926",
        "title": "Bayesian model of individual learning to control a motor imagery BCI",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "The cognitive mechanisms underlying subjects' self-regulation in Brain-Computer Interface (BCI) and neurofeedback (NF) training remain poorly understood. Yet, a mechanistic computational model of each individual learning trajectory is required to improve the reliability of BCI applications. The few existing attempts mostly rely on model-free (reinforcement learning) approaches. Hence, they cannot capture the strategy developed by each subject and neither finely predict their learning curve. In this study, we propose an alternative, model-based approach rooted in cognitive skill learning within the Active Inference framework. We show how BCI training may be framed as an inference problem under high uncertainties. We illustrate the proposed approach on a previously published synthetic Motor Imagery ERD laterality training. We show how simple changes in model parameters allow us to qualitatively match experimental results and account for various subject. In the near future, this approach may provide a powerful computational to model individual skill learning and thus optimize and finely characterize BCI training.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "9th International Graz Brain-Computer Interface Conference 2024, Sep 2024, Graz, Austria"
    },
    {
        "paper id": "2410.05931",
        "abstract url": "https://arxiv.org/abs/2410.05931",
        "title": "Construction of Musculoskeletal Simulation for Shoulder Complex with Ligaments and Its Validation via Model Predictive Control",
        "rating": "-1",
        "keywords": [
            [
                "robotics",
                "robot"
            ]
        ],
        "abstract": "The complex ways in which humans utilize their bodies in sports and martial arts are remarkable, and human motion analysis is one of the most effective tools for robot body design and control. On the other hand, motion analysis is not easy, and it is difficult to measure complex body motions in detail due to the influence of numerous muscles and soft tissues, mainly ligaments. In response, various musculoskeletal simulators have been developed and applied to motion analysis and robotics. However, none of them reproduce the ligaments but only the muscles, nor do they focus on the shoulder complex, including the clavicle and scapula, which is one of the most complex parts of the body. Therefore, in this study, a detailed simulation model of the shoulder complex including ligaments is constructed. The model will mimic not only the skeletal structure and muscle arrangement but also the ligament arrangement and maximum muscle strength. Through model predictive control based on the constructed simulation, we confirmed that the ligaments contribute to joint stabilization in the first movement and that the proper distribution of maximum muscle force contributes to the equalization of the load on each muscle, demonstrating the effectiveness of this simulation.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "accepted at IROS2024, websites - https://sahara-yuta.github.io/projects/shoulder-complex-simulation"
    },
    {
        "paper id": "2410.05933",
        "abstract url": "https://arxiv.org/abs/2410.05933",
        "title": "CubiX: Portable Wire-Driven Parallel Robot Connecting to and Utilizing the Environment",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "A wire-driven parallel robot is a type of robotic system where multiple wires are used to control the movement of a end-effector. The wires are attached to the end-effector and anchored to fixed points on external structures. This configuration allows for the separation of actuators and end-effectors, enabling lightweight and simplified movable parts in the robot. However, its range of motion remains confined within the space formed by the wires, limiting the wire-driven capability to only within the pre-designed operational range. Here, in this study, we develop a wire-driven robot, CubiX, capable of connecting to and utilizing the environment. CubiX connects itself to the environment using up to 8 wires and drives itself by winding these wires. By integrating actuators for winding the wires into CubiX, a portable wire-driven parallel robot is realized without limitations on its workspace. Consequently, the robot can form parallel wire-driven structures by connecting wires to the environment at any operational location.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted at IROS2024, website - https://shin0805.github.io/cubix-hardware/ , YouTube - https://youtu.be/R5ZrzMPEFZs"
    },
    {
        "paper id": "2410.05935",
        "abstract url": "https://arxiv.org/abs/2410.05935",
        "title": "Learning Gaussian Data Augmentation in Feature Space for One-shot Object Detection in Manga",
        "rating": "-1",
        "keywords": [
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We tackle one-shot object detection in Japanese Manga. The rising global popularity of Japanese manga has made the object detection of character faces increasingly important, with potential applications such as automatic colorization. However, obtaining sufficient data for training conventional object detectors is challenging due to copyright restrictions. Additionally, new characters appear every time a new volume of manga is released, making it impractical to re-train object detectors each time to detect these new characters. Therefore, one-shot object detection, where only a single query (reference) image is required to detect a new character, is an essential task in the manga industry. One challenge with one-shot object detection in manga is the large variation in the poses and facial expressions of characters in target images, despite having only one query image as a reference. Another challenge is that the frequency of character appearances follows a long-tail distribution. To overcome these challenges, we propose a data augmentation method in feature space to increase the variation of the query. The proposed method augments the feature from the query by adding Gaussian noise, with the noise variance at each channel learned during training. The experimental results show that the proposed method improves the performance for both seen and unseen classes, surpassing data augmentation methods in image space.",
        "subjects": [
            "cs.CV",
            "cs.MM"
        ],
        "comment": "Accepted to ACM Multimedia Asia 2024"
    },
    {
        "paper id": "2410.05954",
        "abstract url": "https://arxiv.org/abs/2410.05954",
        "title": "Pyramidal Flow Matching for Efficient Video Generative Modeling",
        "rating": "-1",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "trajectory"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution. Despite reducing computational demands, the separate optimization of each sub-stage hinders knowledge sharing and sacrifices flexibility. This work introduces a unified pyramidal flow matching algorithm. It reinterprets the original denoising trajectory as a series of pyramid stages, where only the final stage operates at the full resolution, thereby enabling more efficient video generative modeling. Through our sophisticated design, the flows of different pyramid stages can be interlinked to maintain continuity. Moreover, we craft autoregressive video generation with a temporal pyramid to compress the full-resolution history. The entire framework can be optimized in an end-to-end manner and with a single unified Diffusion Transformer (DiT). Extensive experiments demonstrate that our method supports generating high-quality 5-second (up to 10-second) videos at 768p resolution and 24 FPS within 20.7k A100 GPU training hours. All code and models will be open-sourced at https://pyramid-flow.github.io.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06008",
        "abstract url": "https://arxiv.org/abs/2410.06008",
        "title": "Sitting, Standing and Walking Control of the Series-Parallel Hybrid Recupera-Reha Exoskeleton",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "This paper presents advancements in the functionalities of the Recupera-Reha lower extremity exoskeleton robot. The exoskeleton features a series-parallel hybrid design characterized by multiple kinematic loops resulting in 148 degrees of freedom in its spanning tree and 102 independent loop closure constraints, which poses significant challenges for modeling and control. To address these challenges, we applied an optimal control approach to generate feasible trajectories such as sitting, standing, and static walking, and tested these trajectories on the exoskeleton robot. Our method efficiently solves the optimal control problem using a serial abstraction of the model to generate trajectories. It then utilizes the full series-parallel hybrid model, which takes all the kinematic loop constraints into account to generate the final actuator commands. The experimental results demonstrate the effectiveness of our approach in generating the desired motions for the exoskeleton.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 16 figures, IEEE-RAS International Conference on Humanoid Robots 2024"
    },
    {
        "paper id": "2410.06014",
        "abstract url": "https://arxiv.org/abs/2410.06014",
        "title": "SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting",
        "rating": "-1",
        "keywords": [
            [
                "Gaussian Splatting"
            ],
            [
                "Trajectory"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Many recent developments for robots to represent environments have focused on photorealistic reconstructions. This paper particularly focuses on generating sequences of images from the photorealistic Gaussian Splatting models, that match instructions that are given by user-inputted language. We contribute a novel framework, SplaTraj, which formulates the generation of images within photorealistic environment representations as a continuous-time trajectory optimization problem. Costs are designed so that a camera following the trajectory poses will smoothly traverse through the environment and render the specified spatial information in a photogenic manner. This is achieved by querying a photorealistic representation with language embedding to isolate regions that correspond to the user-specified inputs. These regions are then projected to the camera's view as it moves over time and a cost is constructed. We can then apply gradient-based optimization and differentiate through the rendering to optimize the trajectory for the defined cost. The resulting trajectory moves to photogenically view each of the specified objects. We empirically evaluate our approach on a suite of environments and instructions, and demonstrate the quality of generated image sequences.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06024",
        "abstract url": "https://arxiv.org/abs/2410.06024",
        "title": "Jet Expansions of Residual Computation",
        "rating": "-1",
        "keywords": [
            [
                "depth"
            ],
            [
                "graphs"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We introduce a framework for expanding residual computational graphs using jets, operators that generalize truncated Taylor series. Our method provides a systematic approach to disentangle contributions of different computational paths to model predictions. In contrast to existing techniques such as distillation, probing, or early decoding, our expansions rely solely on the model itself and requires no data, training, or sampling from the model. We demonstrate how our framework grounds and subsumes logit lens, reveals a (super-)exponential path structure in the recursive residual depth and opens up several applications. These include sketching a transformer large language model with $n$-gram statistics extracted from its computations, and indexing the models' levels of toxicity knowledge. Our approach enables data-free analysis of residual computation for model interpretability, development, and evaluation.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.SC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06025",
        "abstract url": "https://arxiv.org/abs/2410.06025",
        "title": "Sparse Repellency for Shielded Generation in Text-to-image Diffusion Models",
        "rating": "-1",
        "keywords": [
            [
                "Diffusion",
                "Text-to-image"
            ],
            [
                "trajectory"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "The increased adoption of diffusion models in text-to-image generation has triggered concerns on their reliability. Such models are now closely scrutinized under the lens of various metrics, notably calibration, fairness, or compute efficiency. We focus in this work on two issues that arise when deploying these models: a lack of diversity when prompting images, and a tendency to recreate images from the training set. To solve both problems, we propose a method that coaxes the sampled trajectories of pretrained diffusion models to land on images that fall outside of a reference set. We achieve this by adding repellency terms to the diffusion SDE throughout the generation trajectory, which are triggered whenever the path is expected to land too closely to an image in the shielded reference set. Our method is sparse in the sense that these repellency terms are zero and inactive most of the time, and even more so towards the end of the generation trajectory. Our method, named SPELL for sparse repellency, can be used either with a static reference set that contains protected images, or dynamically, by updating the set at each timestep with the expected images concurrently generated within a batch. We show that adding SPELL to popular diffusion models improves their diversity while impacting their FID only marginally, and performs comparatively better than other recent training-free diversity methods. We also demonstrate how SPELL can ensure a shielded generation away from a very large set of protected images by considering all 1.2M images from ImageNet as the protected set.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06033",
        "abstract url": "https://arxiv.org/abs/2410.06033",
        "title": "Nationally Scalable Hydrogen Fueling Infrastructure Deployment: A Megaregion Analysis and Optimization Approach",
        "rating": "-1",
        "keywords": [
            [
                "vehicle"
            ]
        ],
        "abstract": "Decarbonizing regional and long-haul freight faces challenges due to the limitations of battery-electric vehicles and infrastructure. Hydrogen fuel cell medium- and heavy-duty vehicles (MHDVs) present a promising alternative, aligning with the Department of Energy's decarbonization goals. Historically, alternative fuels like compressed natural gas and propane gas have seen slow adoption due to infrastructure barriers. To prevent similar setbacks, planning for zero-emission hydrogen fueling infrastructure is critical. This research develops plans for affordable and accessible hydrogen refueling stations, supporting the decarbonized freight system and benefiting underserved and rural communities by improving air quality, reducing noise pollution, and enhancing energy resilience.It provides a blueprint for replacing diesel in Class 8 trucks with hydrogen fueling solutions, focusing on the Texas Triangle Megaregion (I-45, I-35, I-10), the I-10 corridor between San Antonio, TX, and Los Angeles, CA, and the I-5/CA-99 corridors between Los Angeles and San Francisco. This area accounts for ~8.5% of U.S. heavy-duty freight volume. Using the OR-AGENT (Optimal Regional Architecture Generation for Electrified National Transport) framework, the study analyzes vehicles, freight networks, and energy systems. The framework integrates data on freight mobility, traffic, weather, and energy pathways to deliver optimized powertrain architectures and hydrogen fueling infrastructure deployment. It assesses all vehicle origin-destination pairs and feasible fueling station locations, using a genetic algorithm to identify the minimum number and optimal locations of hydrogen stations. It also determines fuel schedules and quantities, ensuring no vehicle is stranded. A deployment roadmap outlines strategic hydrogen refueling infrastructure rollout across multiple adoption scenarios.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06052",
        "abstract url": "https://arxiv.org/abs/2410.06052",
        "title": "Concurrent-Learning Based Relative Localization in Shape Formation of Robot Swarms",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "In this paper, we address the shape formation problem for massive robot swarms in environments where external localization systems are unavailable. Achieving this task effectively with solely onboard measurements is still scarcely explored and faces some practical challenges. To solve this challenging problem, we propose the following novel results. Firstly, to estimate the relative positions among neighboring robots, a concurrent-learning based estimator is proposed. It relaxes the persistent excitation condition required in the classical ones such as least-square estimator. Secondly, we introduce a finite-time agreement protocol to determine the shape location. This is achieved by estimating the relative position between each robot and a randomly assigned seed robot. The initial position of the seed one marks the shape location. Thirdly, based on the theoretical results of the relative localization, a novel behavior-based control strategy is devised. This strategy not only enables adaptive shape formation of large group of robots but also enhances the observability of inter-robot relative localization. Numerical simulation results are provided to verify the performance of our proposed strategy compared to the state-of-the-art ones. Additionally, outdoor experiments on real robots further demonstrate the practical effectiveness and robustness of our methods.",
        "subjects": [
            "cs.RO",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06067",
        "abstract url": "https://arxiv.org/abs/2410.06067",
        "title": "Contrastive Learning to Fine-Tune Feature Extraction Models for the Visual Cortex",
        "rating": "-1",
        "keywords": [
            [
                "fMRI"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Predicting the neural response to natural images in the visual cortex requires extracting relevant features from the images and relating those feature to the observed responses. In this work, we optimize the feature extraction in order to maximize the information shared between the image features and the neural response across voxels in a given region of interest (ROI) extracted from the BOLD signal measured by fMRI. We adapt contrastive learning (CL) to fine-tune a convolutional neural network, which was pretrained for image classification, such that a mapping of a given image's features are more similar to the corresponding fMRI response than to the responses to other images. We exploit the recently released Natural Scenes Dataset (Allen et al., 2022) as organized for the Algonauts Project (Gifford et al., 2023), which contains the high-resolution fMRI responses of eight subjects to tens of thousands of naturalistic images. We show that CL fine-tuning creates feature extraction models that enable higher encoding accuracy in early visual ROIs as compared to both the pretrained network and a baseline approach that uses a regression loss at the output of the network to tune it for fMRI response encoding. We investigate inter-subject transfer of the CL fine-tuned models, including subjects from another, lower-resolution dataset (Gong et al., 2023). We also pool subjects for fine-tuning to further improve the encoding performance. Finally, we examine the performance of the fine-tuned models on common image classification tasks, explore the landscape of ROI-specific models by applying dimensionality reduction on the Bhattacharya dissimilarity matrix created using the predictions on those tasks (Mao et al., 2024), and investigate lateralization of the processing for early visual ROIs using salience maps of the classifiers built on the CL-tuned models.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06068",
        "abstract url": "https://arxiv.org/abs/2410.06068",
        "title": "Resolution limit of the eye: how many pixels can we see?",
        "rating": "-1",
        "keywords": [
            [
                "retinal"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "As large engineering efforts go towards improving the resolution of mobile, AR and VR displays, it is important to know the maximum resolution at which further improvements bring no noticeable benefit. This limit is often referred to as the \"retinal resolution\", although the limiting factor may not necessarily be attributed to the retina. To determine the ultimate resolution at which an image appears sharp to our eyes with no perceivable blur, we created an experimental setup with a sliding display, which allows for continuous control of the resolution. The lack of such control was the main limitation of the previous studies. We measure achromatic (black-white) and chromatic (red-green and yellow-violet) resolution limits for foveal vision, and at two eccentricities (10 and 20 deg). Our results demonstrate that the resolution limit is higher than what was previously believed, reaching 94 pixels-per-degree (ppd) for foveal achromatic vision, 89 ppd for red-green patterns, and 53 ppd for yellow-violet patterns. We also observe a much larger drop in the resolution limit for chromatic patterns (red-green and yellow-violet) than for achromatic. Our results set the north star for display development, with implications for future imaging, rendering and video coding technologies.",
        "subjects": [
            "cs.HC",
            "cs.GR",
            "cs.MM",
            "eess.IV"
        ],
        "comment": "Main document: 12 pages, 4 figures, 1 table. Supplementary: 14 pages, 12 figures, 4 tables"
    },
    {
        "paper id": "2410.06069",
        "abstract url": "https://arxiv.org/abs/2410.06069",
        "title": "Provable Methods for Searching with an Imperfect Sensor",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Assume that a target is known to be present at an unknown point among a finite set of locations in the plane. We search for it using a mobile robot that has imperfect sensing capabilities. It takes time for the robot to move between locations and search a location; we have a total time budget within which to conduct the search. We study the problem of computing a search path/strategy for the robot that maximizes the probability of detection of the target. Considering non-uniform travel times between points (e.g., based on the distance between them) is crucial for search and rescue applications; such problems have been investigated to a limited extent due to their inherent complexity. In this paper, we describe fast algorithms with performance guarantees for this search problem and some variants, complement them with complexity results, and perform experiments to observe their performance.",
        "subjects": [
            "cs.RO",
            "cs.CG"
        ],
        "comment": "10 pages, 6 figures, 3 algorithms"
    },
    {
        "paper id": "2410.06095",
        "abstract url": "https://arxiv.org/abs/2410.06095",
        "title": "Smoothed analysis for graph isomorphism",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "There is no known polynomial-time algorithm for graph isomorphism testing, but elementary combinatorial \"refinement\" algorithms seem to be very efficient in practice. Some philosophical justification is provided by a classical theorem of Babai, Erd\u0151s and Selkow: an extremely simple polynomial-time combinatorial algorithm (variously known as \"na\u00efve refinement\", \"na\u00efve vertex classification\", \"colour refinement\" or the \"1-dimensional Weisfeiler-Leman algorithm\") yields a so-called canonical labelling scheme for \"almost all graphs\". More precisely, for a typical outcome of a random graph $G(n,1/2)$, this simple combinatorial algorithm assigns labels to vertices in a way that easily permits isomorphism-testing against any other graph. We improve the Babai-Erd\u0151s-Selkow theorem in two directions. First, we consider randomly perturbed graphs, in accordance with the smoothed analysis philosophy of Spielman and Teng: for any graph $G$, na\u00efve refinement becomes effective after a tiny random perturbation to $G$ (specifically, the addition and removal of $O(n\\log n)$ random edges). Actually, with a twist on na\u00efve refinement, we show that $O(n)$ random additions and removals suffice. These results significantly improve on previous work of Gaudio-R\u00e1cz-Sridhar, and are in certain senses best-possible. Second, we complete a long line of research on canonical labelling of random graphs: for any $p$ (possibly depending on $n$), we prove that a random graph $G(n,p)$ can typically be canonically labelled in polynomial time. This is most interesting in the extremely sparse regime where $p$ has order of magnitude $c/n$; denser regimes were previously handled by Bollob\u00e1s, Czajka-Pandurangan, and Linial-Mosheiff. Our proof also provides a description of the automorphism group of a typical outcome of $G(n,p_n)$ (slightly correcting a prediction of Linial-Mosheiff).",
        "subjects": [
            "math.CO",
            "cs.CC",
            "cs.DS"
        ],
        "comment": "this version just shortens the abstract so the full abstract is short enough for the arxiv"
    },
    {
        "paper id": "2410.06113",
        "abstract url": "https://arxiv.org/abs/2410.06113",
        "title": "RealityCraft: An In-Situ CAD+CAM Interface for Novices via Scene-Aware Augmented Reality",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "depth"
            ]
        ],
        "abstract": "Despite the growing accessibility of augmented reality (AR) for visualization, existing computer-aided design systems remain largely confined to traditional screens and are often inaccessible to novice users due to their complexity. We present RealityCraft, an open-sourced AR interface that enables in-situ computer-aided design and manufacturing (CAD+CAM) for novices. Unlike traditional CAD systems confined to computer screens, RealityCraft allows users to design directly within their physical environments, with primitive geometries. RealityCraft recognizes and utilizes physical constraints such as furniture and walls, enhancing user interaction through spatial awareness and depth occlusion. Furthermore, RealityCraft features an integrated AR-based 3D printing workflow, where users can drag and drop designs onto their 3D printer's virtual twin in their immediate space. Through a user study, we demonstrate that RealityCraft enhances engagement and ease of use for novices. By bridging the gap between digital creation and physical output, RealityCraft aims to transform everyday spaces into creative studios.",
        "subjects": [
            "cs.HC",
            "cs.ET",
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06132",
        "abstract url": "https://arxiv.org/abs/2410.06132",
        "title": "Spread blow-up lemma with an application to perturbed random graphs",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "Combining ideas of Pham, Sah, Sawhney, and Simkin on spread perfect matchings in super-regular bipartite graphs with an algorithmic blow-up lemma, we prove a spread version of the blow-up lemma. Intuitively, this means that there exists a probability measure over copies of a desired spanning graph $H$ in a given system of super-regular pairs which does not heavily pin down any subset of vertices. This allows one to complement the use of the blow-up lemma with the recently resolved Kahn-Kalai conjecture. As an application, we prove an approximate version of a conjecture of B\u00f6ttcher, Parczyk, Sgueglia, and Skokan on the threshold for appearance of powers of Hamilton cycles in perturbed random graphs.",
        "subjects": [
            "math.CO",
            "cs.DM",
            "math.PR"
        ],
        "comment": "12 pages"
    },
    {
        "paper id": "2410.06139",
        "abstract url": "https://arxiv.org/abs/2410.06139",
        "title": "Flips in Odd Matchings",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Let $\\mathcal{P}$ be a set of $n=2m+1$ points in the plane in general position. We define the graph $GM_\\mathcal{P}$ whose vertex set is the set of all plane matchings on $\\mathcal{P}$ with exactly $m$ edges. Two vertices in $GM_\\mathcal{P}$ are connected if the two corresponding matchings have $m-1$ edges in common. In this work we show that $GM_\\mathcal{P}$ is connected and give an upper bound of $O(n^2)$ on its diameter. Moreover, we present a tight bound of $\u0398(n)$ for the diameter of the flip graph of points in convex position.",
        "subjects": [
            "cs.CG",
            "math.CO"
        ],
        "comment": "Appeared in CCCG2024"
    },
    {
        "paper id": "2410.06157",
        "abstract url": "https://arxiv.org/abs/2410.06157",
        "title": "Detecting Android Malware by Visualizing App Behaviors from Multiple Complementary Views",
        "rating": "-1",
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "Deep learning has emerged as a promising technology for achieving Android malware detection. To further unleash its detection potentials, software visualization can be integrated for analyzing the details of app behaviors clearly. However, facing increasingly sophisticated malware, existing visualization-based methods, analyzing from one or randomly-selected few views, can only detect limited attack types. We propose and implement LensDroid, a novel technique that detects Android malware by visualizing app behaviors from multiple complementary views. Our goal is to harness the power of combining deep learning and software visualization to automatically capture and aggregate high-level features that are not inherently linked, thereby revealing hidden maliciousness of Android app behaviors. To thoroughly comprehend the details of apps, we visualize app behaviors from three related but distinct views of behavioral sensitivities, operational contexts and supported environments. We then extract high-order semantics based on the views accordingly. To exploit semantic complementarity of the views, we design a deep neural network based model for fusing the visualized features from local to global based on their contributions to downstream tasks. A comprehensive comparison with five baseline techniques is performed on datasets of more than 51K apps in three real-world typical scenarios, including overall threats, app evolution and zero-day malware. The experimental results show that the overall performance of LensDroid is better than the baseline techniques. We also validate the complementarity of the views and demonstrate that the multi-view fusion in LensDroid enhances Android malware detection.",
        "subjects": [
            "cs.CR",
            "cs.SE"
        ],
        "comment": "Submitted to TIFS"
    },
    {
        "paper id": "2410.06161",
        "abstract url": "https://arxiv.org/abs/2410.06161",
        "title": "Automated quality assessment using appearance-based simulations and hippocampus segmentation on low-field paediatric brain MR images",
        "rating": "-1",
        "keywords": [
            [
                "quality assessment"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Understanding the structural growth of paediatric brains is a key step in the identification of various neuro-developmental disorders. However, our knowledge is limited by many factors, including the lack of automated image analysis tools, especially in Low and Middle Income Countries from the lack of high field MR images available. Low-field systems are being increasingly explored in these countries, and, therefore, there is a need to develop automated image analysis tools for these images. In this work, as a preliminary step, we consider two tasks: 1) automated quality assurance and 2) hippocampal segmentation, where we compare multiple approaches. For the automated quality assurance task a DenseNet combined with appearance-based transformations for synthesising artefacts produced the best performance, with a weighted accuracy of 82.3%. For the segmentation task, registration of an average atlas performed the best, with a final Dice score of 0.61. Our results show that although the images can provide understanding of large scale pathologies and gross scale anatomical development, there still remain barriers for their use for more granular analyses.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "MICCAI 2024 Low field pediatric brain magnetic resonance Image Segmentation and quality Assurance (LISA) Challenge"
    },
    {
        "paper id": "2410.06165",
        "abstract url": "https://arxiv.org/abs/2410.06165",
        "title": "GSLoc: Visual Localization with 3D Gaussian Splatting",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ]
        ],
        "abstract": "We present GSLoc: a new visual localization method that performs dense camera alignment using 3D Gaussian Splatting as a map representation of the scene. GSLoc backpropagates pose gradients over the rendering pipeline to align the rendered and target images, while it adopts a coarse-to-fine strategy by utilizing blurring kernels to mitigate the non-convexity of the problem and improve the convergence. The results show that our approach succeeds at visual localization in challenging conditions of relatively small overlap between initial and target frames inside textureless environments when state-of-the-art neural sparse methods provide inferior results. Using the byproduct of realistic rendering from the 3DGS map representation, we show how to enhance localization results by mixing a set of observed and virtual reference keyframes when solving the image retrieval problem. We evaluate our method both on synthetic and real-world data, discussing its advantages and application potential.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06190",
        "abstract url": "https://arxiv.org/abs/2410.06190",
        "title": "Neural-Bayesian Program Learning for Few-shot Dialogue Intent Parsing",
        "rating": "-1",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "With the growing importance of customer service in contemporary business, recognizing the intents behind service dialogues has become essential for the strategic success of enterprises. However, the nature of dialogue data varies significantly across different scenarios, and implementing an intent parser for a specific domain often involves tedious feature engineering and a heavy workload of data labeling. In this paper, we propose a novel Neural-Bayesian Program Learning model named Dialogue-Intent Parser (DI-Parser), which specializes in intent parsing under data-hungry settings and offers promising performance improvements. DI-Parser effectively utilizes data from multiple sources in a \"Learning to Learn\" manner and harnesses the \"wisdom of the crowd\" through few-shot learning capabilities on human-annotated datasets. Experimental results demonstrate that DI-Parser outperforms state-of-the-art deep learning models and offers practical advantages for industrial-scale applications.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06192",
        "abstract url": "https://arxiv.org/abs/2410.06192",
        "title": "Hibikino-Musashi@Home 2024 Team Description Paper",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "This paper provides an overview of the techniques employed by Hibikino-Musashi@Home, which intends to participate in the domestic standard platform league. The team has developed a dataset generator for training a robot vision system and an open-source development environment running on a Human Support Robot simulator. The large language model powered task planner selects appropriate primitive skills to perform the task requested by users. The team aims to design a home service robot that can assist humans in their homes and continuously attends competitions to evaluate and improve the developed system.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06194",
        "abstract url": "https://arxiv.org/abs/2410.06194",
        "title": "Prompting DirectSAM for Semantic Contour Extraction in Remote Sensing Images",
        "rating": "-1",
        "keywords": [
            [
                "Remote Sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The Direct Segment Anything Model (DirectSAM) excels in class-agnostic contour extraction. In this paper, we explore its use by applying it to optical remote sensing imagery, where semantic contour extraction-such as identifying buildings, road networks, and coastlines-holds significant practical value. Those applications are currently handled via training specialized small models separately on small datasets in each domain. We introduce a foundation model derived from DirectSAM, termed DirectSAM-RS, which not only inherits the strong segmentation capability acquired from natural images, but also benefits from a large-scale dataset we created for remote sensing semantic contour extraction. This dataset comprises over 34k image-text-contour triplets, making it at least 30 times larger than individual dataset. DirectSAM-RS integrates a prompter module: a text encoder and cross-attention layers attached to the DirectSAM architecture, which allows flexible conditioning on target class labels or referring expressions. We evaluate the DirectSAM-RS in both zero-shot and fine-tuning setting, and demonstrate that it achieves state-of-the-art performance across several downstream benchmarks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06221",
        "abstract url": "https://arxiv.org/abs/2410.06221",
        "title": "POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings",
        "rating": "-1",
        "keywords": [
            [
                "music"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "When dealing with multimedia data, source attribution is a key challenge from a forensic perspective. This task aims to determine how a given content was captured, providing valuable insights for various applications, including legal proceedings and integrity investigations. The source attribution problem has been addressed in different domains, from identifying the camera model used to capture specific photographs to detecting the synthetic speech generator or microphone model used to create or record given audio tracks. Recent advancements in this area rely heavily on machine learning and data-driven techniques, which often outperform traditional signal processing-based methods. However, a drawback of these systems is their need for large volumes of training data, which must reflect the latest technological trends to produce accurate and reliable predictions. This presents a significant challenge, as the rapid pace of technological progress makes it difficult to maintain datasets that are up-to-date with real-world conditions. For instance, in the task of smartphone model identification from audio recordings, the available datasets are often outdated or acquired inconsistently, making it difficult to develop solutions that are valid beyond a research environment. In this paper we present POLIPHONE, a dataset for smartphone model identification from audio recordings. It includes data from 20 recent smartphones recorded in a controlled environment to ensure reproducibility and scalability for future research. The released tracks contain audio data from various domains (i.e., speech, music, environmental sounds), making the corpus versatile and applicable to a wide range of use cases. We also present numerous experiments to benchmark the proposed dataset using a state-of-the-art classifier for smartphone model identification from audio recordings.",
        "subjects": [
            "cs.SD",
            "cs.MM",
            "eess.AS"
        ],
        "comment": "Submitted to IEEE Access"
    },
    {
        "paper id": "2410.06231",
        "abstract url": "https://arxiv.org/abs/2410.06231",
        "title": "RelitLRM: Generative Relightable Radiance for Large Reconstruction Models",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian splatting"
            ],
            [
                "diffusion"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We propose RelitLRM, a Large Reconstruction Model (LRM) for generating high-quality Gaussian splatting representations of 3D objects under novel illuminations from sparse (4-8) posed images captured under unknown static lighting. Unlike prior inverse rendering methods requiring dense captures and slow optimization, often causing artifacts like incorrect highlights or shadow baking, RelitLRM adopts a feed-forward transformer-based model with a novel combination of a geometry reconstructor and a relightable appearance generator based on diffusion. The model is trained end-to-end on synthetic multi-view renderings of objects under varying known illuminations. This architecture design enables to effectively decompose geometry and appearance, resolve the ambiguity between material and lighting, and capture the multi-modal distribution of shadows and specularity in the relit appearance. We show our sparse-view feed-forward RelitLRM offers competitive relighting results to state-of-the-art dense-view optimization-based baselines while being significantly faster. Our project page is available at: https://relit-lrm.github.io/.",
        "subjects": [
            "cs.CV",
            "cs.GR",
            "cs.LG"
        ],
        "comment": "webpage: https://relit-lrm.github.io/"
    },
    {
        "paper id": "2410.06238",
        "abstract url": "https://arxiv.org/abs/2410.06238",
        "title": "EVOLvE: Evaluating and Optimizing LLMs For Exploration",
        "rating": "-1",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Despite their success in many domains, large language models (LLMs) remain under-studied in scenarios requiring optimal decision-making under uncertainty. This is crucial as many real-world applications, ranging from personalized recommendations to healthcare interventions, demand that LLMs not only predict but also actively learn to make optimal decisions through exploration. In this work, we measure LLMs' (in)ability to make optimal decisions in bandits, a state-less reinforcement learning setting relevant to many applications. We develop a comprehensive suite of environments, including both context-free and contextual bandits with varying task difficulties, to benchmark LLMs' performance. Motivated by the existence of optimal exploration algorithms, we propose efficient ways to integrate this algorithmic knowledge into LLMs: by providing explicit algorithm-guided support during inference; and through algorithm distillation via in-context demonstrations and fine-tuning, using synthetic data generated from these algorithms. Impressively, these techniques allow us to achieve superior exploration performance with smaller models, surpassing larger models on various tasks. We conducted an extensive ablation study to shed light on various factors, such as task difficulty and data representation, that influence the efficiency of LLM exploration. Additionally, we conduct a rigorous analysis of the LLM's exploration efficiency using the concept of regret, linking its ability to explore to the model size and underlying algorithm.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "28 pages"
    },
    {
        "paper id": "2410.06239",
        "abstract url": "https://arxiv.org/abs/2410.06239",
        "title": "OrionNav: Online Planning for Robot Autonomy with Context-Aware LLM and Open-Vocabulary Semantic Scene Graphs",
        "rating": "-1",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "Robot",
                "navigation"
            ],
            [
                "Graphs"
            ]
        ],
        "abstract": "Enabling robots to autonomously navigate unknown, complex, dynamic environments and perform diverse tasks remains a fundamental challenge in developing robust autonomous physical agents. They must effectively perceive their surroundings while leveraging world knowledge for decision-making. While recent approaches utilize vision-language and large language models for scene understanding and planning, they often rely on offline processing, external computing, or restrictive environmental assumptions. We present a novel framework for efficient and scalable real-time, onboard autonomous navigation that integrates multi-level abstraction in both perception and planning in unknown large-scale environments that change over time. Our system fuses data from multiple onboard sensors for localization and mapping and integrates it with open-vocabulary semantics to generate hierarchical scene graphs. An LLM-based planner leverages these graphs to generate high-level task execution strategies, which guide low-level controllers in safely accomplishing goals. Our framework's real-time operation enables continuous updates to scene graphs and plans, allowing swift responses to environmental changes and on-the-fly error correction. This is a key advantage over static or rule-based planning systems. We demonstrate our system's efficacy on a quadruped robot navigating large-scale, dynamic environments, showcasing its adaptability and robustness in diverse scenarios.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06243",
        "abstract url": "https://arxiv.org/abs/2410.06243",
        "title": "Unsupervised Model Diagnosis",
        "rating": "-1",
        "keywords": [
            [
                "Diagnosis"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Ensuring model explainability and robustness is essential for reliable deployment of deep vision systems. Current methods for evaluating robustness rely on collecting and annotating extensive test sets. While this is common practice, the process is labor-intensive and expensive with no guarantee of sufficient coverage across attributes of interest. Recently, model diagnosis frameworks have emerged leveraging user inputs (e.g., text) to assess the vulnerability of the model. However, such dependence on human can introduce bias and limitation given the domain knowledge of particular users. This paper proposes Unsupervised Model Diagnosis (UMO), that leverages generative models to produce semantic counterfactual explanations without any user guidance. Given a differentiable computer vision model (i.e., the target model), UMO optimizes for the most counterfactual directions in a generative latent space. Our approach identifies and visualizes changes in semantics, and then matches these changes to attributes from wide-ranging text sources, such as dictionaries or language models. We validate the framework on multiple vision tasks (e.g., classification, segmentation, keypoint detection). Extensive experiments show that our unsupervised discovery of semantic directions can correctly highlight spurious correlations and visualize the failure mode of target models without any human intervention.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": "9 pages, 9 figures, 3 tables"
    },
    {
        "paper id": "2410.06285",
        "abstract url": "https://arxiv.org/abs/2410.06285",
        "title": "Monocular Visual Place Recognition in LiDAR Maps via Cross-Modal State Space Model and Multi-View Matching",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "LiDAR",
                "SLAM"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Achieving monocular camera localization within pre-built LiDAR maps can bypass the simultaneous mapping process of visual SLAM systems, potentially reducing the computational overhead of autonomous localization. To this end, one of the key challenges is cross-modal place recognition, which involves retrieving 3D scenes (point clouds) from a LiDAR map according to online RGB images. In this paper, we introduce an efficient framework to learn descriptors for both RGB images and point clouds. It takes visual state space model (VMamba) as the backbone and employs a pixel-view-scene joint training strategy for cross-modal contrastive learning. To address the field-of-view differences, independent descriptors are generated from multiple evenly distributed viewpoints for point clouds. A visible 3D points overlap strategy is then designed to quantify the similarity between point cloud views and RGB images for multi-view supervision. Additionally, when generating descriptors from pixel-level features using NetVLAD, we compensate for the loss of geometric information, and introduce an efficient scheme for multi-view generation. Experimental results on the KITTI and KITTI-360 datasets demonstrate the effectiveness and generalization of our method. The code will be released upon acceptance.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06295",
        "abstract url": "https://arxiv.org/abs/2410.06295",
        "title": "A General Formulation for Path Constrained Time-Optimized Trajectory Planning with Environmental and Object Contacts",
        "rating": "-1",
        "keywords": [
            [
                "Trajectory"
            ]
        ],
        "abstract": "A typical manipulation task consists of a manipulator equipped with a gripper to grasp and move an object with constraints on the motion of the hand-held object, which may be due to the nature of the task itself or from object-environment contacts. In this paper, we study the problem of computing joint torques and grasping forces for time-optimal motion of an object, while ensuring that the grasp is not lost and any constraints on the motion of the object, either due to dynamics, environment contact, or no-slip requirements, are also satisfied. We present a second-order cone program (SOCP) formulation of the time-optimal trajectory planning problem that considers nonlinear friction cone constraints at the hand-object and object-environment contacts. Since SOCPs are convex optimization problems that can be solved optimally in polynomial time using interior point methods, we can solve the trajectory optimization problem efficiently. We present simulation results on three examples, including a non-prehensile manipulation task, which shows the generality and effectiveness of our approach.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06315",
        "abstract url": "https://arxiv.org/abs/2410.06315",
        "title": "Incremental Learning for Robot Shared Autonomy",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "Shared autonomy holds promise for improving the usability and accessibility of assistive robotic arms, but current methods often rely on costly expert demonstrations and lack the ability to adapt post-deployment. This paper introduces ILSA, an Incrementally Learned Shared Autonomy framework that continually improves its assistive control policy through repeated user interactions. ILSA leverages synthetic kinematic trajectories for initial pretraining, reducing the need for expert demonstrations, and then incrementally finetunes its policy after each manipulation interaction, with mechanisms to balance new knowledge acquisition with existing knowledge retention during incremental learning. We validate ILSA for complex long-horizon tasks through a comprehensive ablation study and a user study with 20 participants, demonstrating its effectiveness and robustness in both quantitative performance and user-reported qualitative metrics. Code and videos are available at https://ilsa-robo.github.io/.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06321",
        "abstract url": "https://arxiv.org/abs/2410.06321",
        "title": "An Algorithm for Distributed Computation of Reachable Sets for Multi-Agent Systems",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "In this paper, we consider the problem of distributed reachable set computation for multi-agent systems (MASs) interacting over an undirected, stationary graph. A full state-feedback control input for such MASs depends no only on the current agent's state, but also of its neighbors. However, in most MAS applications, the dynamics are obscured by individual agents. This makes reachable set computation, in a fully distributed manner, a challenging problem. We utilize the ideas of polytopic reachable set approximation and generalize it to a MAS setup. We formulate the resulting sub-problems in a fully distributed manner and provide convergence guarantees for the associated computations. The proposed algorithm's convergence is proved for two cases: static MAS graphs, and time-varying graphs under certain restrictions.",
        "subjects": [
            "eess.SY",
            "cs.RO"
        ],
        "comment": "10 pages, 4 figures, 1 algorithm float. Preprint submitted to ACC 2025 for review"
    },
    {
        "paper id": "2410.06337",
        "abstract url": "https://arxiv.org/abs/2410.06337",
        "title": "Faster Algorithms for Graph Monopolarity",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "A graph $G = (V,E)$ is said to be monopolar if its vertex set admits a partition $V = (C \\uplus{} I)$ where $G[C]$ is a cluster graph and $I$ is an independent set in $G$. Monopolar graphs generalize both bipartite graphs and split graphs, and they have been extensively studied from both graph-theoretic and algorithmic points of view. In this work we focus on the problem MONOPOLAR RECOGNITION (MR) of deciding whether a given graph is monopolar. MR is known to be solvable in polynomial time in certain classes of graphs such as cographs and claw-free graphs, and to be NP-Hard in various restricted classes such as subcubic planar graphs. We initiate the study of exact exponential-time algorithms for MR and allied problems. We design an algorithm that solves MR in $\\OhStar(1.3734^{n})$ time on input graphs with $n$ vertices. In fact we solve the more general problems MONOPOLAR EXTENSION (ME) and LIST MONOPOLAR PARTITION (LMP), which were introduced in the literature as part of the study of graph monopolarity, in $\\OhStar(1.3734^{n})$ time. We also design fast parameterized algorithms for MR using two notions of distance from triviality as the parameters. Our FPT algorithms solve MR in $\\OhStar(3.076^{k_{v}})$ and $\\OhStar(2.253^{k_{e}})$ time, where $k_{v}$ and $k_{e}$ are, respectively, the sizes of the smallest claw-free vertex and edge deletion sets of the input graph. These results are a significant addition to the small number of FPT algorithms currently known for MR. Le and Nevries have shown that if a graph $G$ is chair-free, then an instance $(G,C')$ of ME can be solved in polynomial time for any subset $C'$ of its vertices. We significantly generalize this result; we show that we can solve instances $(G,C')$ of ME in polynomial time for arbitrary graphs $G$ and any chair-free vertex deletion set $C'$ of $G$. We believe this result could be of independent interest.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06343",
        "abstract url": "https://arxiv.org/abs/2410.06343",
        "title": "Losing Treewidth In The Presence Of Weights",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "In the Weighted Treewidth-$\u03b7$ Deletion problem we are given a node-weighted graph $G$ and we look for a vertex subset $X$ of minimum weight such that the treewidth of $G-X$ is at most $\u03b7$. We show that Weighted Treewidth-$\u03b7$ Deletion admits a randomized polynomial-time constant-factor approximation algorithm for every fixed $\u03b7$. Our algorithm also works for the more general Weighted Planar $F$-M-Deletion problem. This work extends the results for unweighted graphs by [Fomin, Lokshtanov, Misra, Saurabh; FOCS '12] and answers a question posed by [Agrawal, Lokshtanov, Misra, Saurabh, Zehavi; APPROX/RANDOM '18] and [Kim, Lee, Thilikos; APPROX/RANDOM '21]. The presented algorithm is based on a novel technique of random sampling of so-called protrusions.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "To appear at SODA 2025"
    },
    {
        "paper id": "2410.06345",
        "abstract url": "https://arxiv.org/abs/2410.06345",
        "title": "Work-in-Progress: Traded Control Transfer for Managing Real-Time Sensor Uncertainties in Autonomous Vehicle",
        "rating": "-1",
        "keywords": [
            [
                "autonomous driving",
                "Vehicle"
            ]
        ],
        "abstract": "At Levels 2 and 3 of autonomous driving defined by the Society of Auto-motive Engineers, drivers must take on certain driving responsibilities, and automated driving must sometimes yield to human control. This situation can occur in real time due to uncertainties in sensor measurements caused by environmental factors like fog or smoke. To address this challenge, we propose a method to manage real-time sensor uncertainties in autonomous vehicles by monitoring sensor conflicts and dynamically adjusting control authority to maintain safe operation. However, to achieve this, we have introduced a novel metric called the Degree of Conflicts (DoC), which quantifies the conflict between real-time sensor data by measuring the differences between data from multiple sensors. Our approach aims to demonstrate the importance of selecting an appropriate DoC threshold for transferring control between the automation agent and the human driver. The results have shown that choosing the correct DoC threshold can enhance safety by promptly handing over the driving control from the automation system to the human driver in challenging conditions.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Peer-reviewed and accepted by the 2024 IEEE Real-Time Systems Symposium (RTSS)"
    },
    {
        "paper id": "2410.06353",
        "abstract url": "https://arxiv.org/abs/2410.06353",
        "title": "Language-Assisted Human Part Motion Learning for Skeleton-Based Temporal Action Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "Skeleton"
            ],
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Skeleton-based Temporal Action Segmentation involves the dense action classification of variable-length skeleton sequences. Current approaches primarily apply graph-based networks to extract framewise, whole-body-level motion representations, and use one-hot encoded labels for model optimization. However, whole-body motion representations do not capture fine-grained part-level motion representations and the one-hot encoded labels neglect the intrinsic semantic relationships within the language-based action definitions. To address these limitations, we propose a novel method named Language-assisted Human Part Motion Representation Learning (LPL), which contains a Disentangled Part Motion Encoder (DPE) to extract dual-level (i.e., part and whole-body) motion representations and a Language-assisted Distribution Alignment (LDA) strategy for optimizing spatial relations within representations. Specifically, after part-aware skeleton encoding via DPE, LDA generates dual-level action descriptions to construct a textual embedding space with the help of a large-scale language model. Then, LDA motivates the alignment of the embedding space between text descriptions and motions. This alignment allows LDA not only to enhance intra-class compactness but also to transfer the language-encoded semantic correlations among actions to skeleton-based motion learning. Moreover, we propose a simple yet efficient Semantic Offset Adapter to smooth the cross-domain misalignment. Our experiments indicate that LPL achieves state-of-the-art performance across various datasets (e.g., +4.4\\% Accuracy, +5.6\\% F1 on the PKU-MMD dataset). Moreover, LDA is compatible with existing methods and improves their performance (e.g., +4.8\\% Accuracy, +4.3\\% F1 on the LARa dataset) without additional inference costs.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2410.06365",
        "abstract url": "https://arxiv.org/abs/2410.06365",
        "title": "Network-level ISAC: Performance Analysis and Optimal Antenna-to-BS Allocation",
        "rating": "-1",
        "keywords": [
            [
                "flight"
            ]
        ],
        "abstract": "A cooperative architecture is proposed for integrated sensing and communication (ISAC) networks, incorporating coordinated multi-point (CoMP) transmission along with multi-static sensing. We investigate how the allocation of antennas-to-base stations (BSs) affects cooperative sensing and cooperative communication performance. More explicitly, we balance the benefits of geographically concentrated antennas, which enhance beamforming and coherent processing, against those of geographically distributed antennas, which improve diversity and reduce service distances. Regarding sensing performance, we investigate three localization methods: angle-of-arrival (AOA)-based, time-of-flight (TOF)-based, and a hybrid approach combining both AOA and TOF measurements, for critically appraising their effects on ISAC network performance. Our analysis shows that in networks having N ISAC nodes following a Poisson point process, the localization accuracy of TOF-based methods follow a \\ln^2 N scaling law (explicitly, the Cram\u00e9r-Rao lower bound (CRLB) reduces with \\ln^2 N). The AOA-based methods follow a \\ln N scaling law, while the hybrid methods scale as a\\ln^2 N + b\\ln N, where a and b represent parameters related to TOF and AOA measurements, respectively. The difference between these scaling laws arises from the distinct ways in which measurement results are converted into the target location. In terms of communication performance, we derive a tractable expression for the communication data rate, considering various cooperative region sizes and antenna-to-BS allocation strategy. It is proved that higher path loss exponents favor distributed antenna allocation to reduce access distances, while lower exponents favor centralized antenna allocation to maximize beamforming gain.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "13 pages, 12 figures, submitted to IEEE for possible publication"
    },
    {
        "paper id": "2410.06366",
        "abstract url": "https://arxiv.org/abs/2410.06366",
        "title": "Physics-Informed Regularization for Domain-Agnostic Dynamical System Modeling",
        "rating": "-1",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Learning complex physical dynamics purely from data is challenging due to the intrinsic properties of systems to be satisfied. Incorporating physics-informed priors, such as in Hamiltonian Neural Networks (HNNs), achieves high-precision modeling for energy-conservative systems. However, real-world systems often deviate from strict energy conservation and follow different physical priors. To address this, we present a framework that achieves high-precision modeling for a wide range of dynamical systems from the numerical aspect, by enforcing Time-Reversal Symmetry (TRS) via a novel regularization term. It helps preserve energies for conservative systems while serving as a strong inductive bias for non-conservative, reversible systems. While TRS is a domain-specific physical prior, we present the first theoretical proof that TRS loss can universally improve modeling accuracy by minimizing higher-order Taylor terms in ODE integration, which is numerically beneficial to various systems regardless of their properties, even for irreversible systems. By integrating the TRS loss within neural ordinary differential equation models, the proposed model TREAT demonstrates superior performance on diverse physical systems. It achieves a significant 11.5% MSE improvement in a challenging chaotic triple-pendulum scenario, underscoring TREAT's broad applicability and effectiveness.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Accepted to The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS 2024)"
    },
    {
        "paper id": "2410.06370",
        "abstract url": "https://arxiv.org/abs/2410.06370",
        "title": "HumVI: A Multilingual Dataset for Detecting Violent Incidents Impacting Humanitarian Aid",
        "rating": "-1",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.SI",
                "cs.CL"
            ]
        ],
        "abstract": "Humanitarian organizations can enhance their effectiveness by analyzing data to discover trends, gather aggregated insights, manage their security risks, support decision-making, and inform advocacy and funding proposals. However, data about violent incidents with direct impact and relevance for humanitarian aid operations is not readily available. An automatic data collection and NLP-backed classification framework aligned with humanitarian perspectives can help bridge this gap. In this paper, we present HumVI - a dataset comprising news articles in three languages (English, French, Arabic) containing instances of different types of violent incidents categorized by the humanitarian sector they impact, e.g., aid security, education, food security, health, and protection. Reliable labels were obtained for the dataset by partnering with a data-backed humanitarian organization, Insecurity Insight. We provide multiple benchmarks for the dataset, employing various deep learning architectures and techniques, including data augmentation and mask loss, to address different task-related challenges, e.g., domain expansion. The dataset is publicly available at https://github.com/dataminr-ai/humvi-dataset.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06384",
        "abstract url": "https://arxiv.org/abs/2410.06384",
        "title": "Validation of the Scientific Literature via Chemputation Augmented by Large Language Models",
        "rating": "-1",
        "keywords": [
            [
                "chemistry",
                "chemical"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Chemputation is the process of programming chemical robots to do experiments using a universal symbolic language, but the literature can be error prone and hard to read due to ambiguities. Large Language Models (LLMs) have demonstrated remarkable capabilities in various domains, including natural language processing, robotic control, and more recently, chemistry. Despite significant advancements in standardizing the reporting and collection of synthetic chemistry data, the automatic reproduction of reported syntheses remains a labour-intensive task. In this work, we introduce an LLM-based chemical research agent workflow designed for the automatic validation of synthetic literature procedures. Our workflow can autonomously extract synthetic procedures and analytical data from extensive documents, translate these procedures into universal XDL code, simulate the execution of the procedure in a hardware-specific setup, and ultimately execute the procedure on an XDL-controlled robotic system for synthetic chemistry. This demonstrates the potential of LLM-based workflows for autonomous chemical synthesis with Chemputers. Due to the abstraction of XDL this approach is safe, secure, and scalable since hallucinations will not be chemputable and the XDL can be both verified and encrypted. Unlike previous efforts, which either addressed only a limited portion of the workflow, relied on inflexible hard-coded rules, or lacked validation in physical systems, our approach provides four realistic examples of syntheses directly executed from synthetic literature. We anticipate that our workflow will significantly enhance automation in robotically driven synthetic chemistry research, streamline data extraction, improve the reproducibility, scalability, and safety of synthetic and experimental chemistry.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.IR"
        ],
        "comment": "22 pages, 7 figures, 34 references"
    },
    {
        "paper id": "2410.06385",
        "abstract url": "https://arxiv.org/abs/2410.06385",
        "title": "Skin Cancer Machine Learning Model Tone Bias",
        "rating": "-1",
        "keywords": [
            [
                "health",
                "diagnosis",
                "Cancer",
                "clinical"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Background: Many open-source skin cancer image datasets are the result of clinical trials conducted in countries with lighter skin tones. Due to this tone imbalance, machine learning models derived from these datasets can perform well at detecting skin cancer for lighter skin tones. Any tone bias in these models could introduce fairness concerns and reduce public trust in the artificial intelligence health field. Methods: We examine a subset of images from the International Skin Imaging Collaboration (ISIC) archive that provide tone information. The subset has a significant tone imbalance. These imbalances could explain a model's tone bias. To address this, we train models using the imbalanced dataset and a balanced dataset to compare against. The datasets are used to train a deep convolutional neural network model to classify the images as malignant or benign. We then evaluate the models' disparate impact, based on selection rate, relative to dark or light skin tone. Results: Using the imbalanced dataset, we found that the model is significantly better at detecting malignant images in lighter tone resulting in a disparate impact of 0.577. Using the balanced dataset, we found that the model is also significantly better at detecting malignant images in lighter versus darker tones with a disparate impact of 0.684. Using the imbalanced or balanced dataset to train the model still results in a disparate impact well below the standard threshold of 0.80 which suggests the model is biased with respect to skin tone. Conclusion: The results show that typical skin cancer machine learning models can be tone biased. These results provide evidence that diagnosis or tone imbalance is not the cause of the bias. Other techniques will be necessary to identify and address the bias in these models, an area of future investigation.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06389",
        "abstract url": "https://arxiv.org/abs/2410.06389",
        "title": "Generative Artificial Intelligence (GAI) for Mobile Communications: A Diffusion Model Perspective",
        "rating": "-1",
        "keywords": [
            [
                "Diffusion"
            ]
        ],
        "abstract": "This article targets at unlocking the potentials of a class of prominent generative artificial intelligence (GAI) method, namely diffusion model (DM), for mobile communications. First, a DM-driven communication architecture is proposed, which introduces two key paradigms, i.e., conditional DM and DMdriven deep reinforcement learning (DRL), for wireless data generation and communication management, respectively. Then, we discuss the key advantages of DM-driven communication paradigms. To elaborate further, we explore DM-driven channel generation mechanisms for channel estimation, extrapolation, and feedback in multiple-input multiple-output (MIMO) systems. We showcase the numerical performance of conditional DM using the accurate DeepMIMO channel datasets, revealing its superiority in generating high-fidelity channels and mitigating unforeseen distribution shifts in sophisticated scenes. Furthermore, several DM-driven communication management designs are conceived, which is promising to deal with imperfect channels and taskoriented communications. To inspire future research developments, we highlight the potential applications and open research challenges of DM-driven communications. Code is available at https://github.com/xiaoxiaxusummer/GAI_COMM/",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Accepted by IEEE Communications Magzine"
    },
    {
        "paper id": "2410.06401",
        "abstract url": "https://arxiv.org/abs/2410.06401",
        "title": "Trajectory Improvement and Reward Learning from Comparative Language Feedback",
        "rating": "-1",
        "keywords": [
            [
                "time-efficient"
            ],
            [
                "Trajectory"
            ],
            [
                "robotics",
                "robot"
            ]
        ],
        "abstract": "Learning from human feedback has gained traction in fields like robotics and natural language processing in recent years. While prior works mostly rely on human feedback in the form of comparisons, language is a preferable modality that provides more informative insights into user preferences. In this work, we aim to incorporate comparative language feedback to iteratively improve robot trajectories and to learn reward functions that encode human preferences. To achieve this goal, we learn a shared latent space that integrates trajectory data and language feedback, and subsequently leverage the learned latent space to improve trajectories and learn human preferences. To the best of our knowledge, we are the first to incorporate comparative language feedback into reward learning. Our simulation experiments demonstrate the effectiveness of the learned latent space and the success of our learning algorithms. We also conduct human subject studies that show our reward learning algorithm achieves a 23.9% higher subjective score on average and is 11.3% more time-efficient compared to preference-based reward learning, underscoring the superior performance of our method. Our website is at https://liralab.usc.edu/comparative-language-feedback/",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8th Annual Conference of Robot Learning (2024)"
    },
    {
        "paper id": "2410.06483",
        "abstract url": "https://arxiv.org/abs/2410.06483",
        "title": "Deep Learning Ensemble for Predicting Diabetic Macular Edema Onset Using Ultra-Wide Field Color Fundus Image",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "disease",
                "retina"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Diabetic macular edema (DME) is a severe complication of diabetes, characterized by thickening of the central portion of the retina due to accumulation of fluid. DME is a significant and common cause of visual impairment in diabetic patients. Center-involved DME (ci-DME) is the highest risk form of disease as fluid extends close to the fovea which is responsible for sharp central vision. Earlier diagnosis or prediction of ci-DME may improve treatment outcomes. Here, we propose an ensemble method to predict ci-DME onset within a year using ultra-wide-field color fundus photography (UWF-CFP) images provided by the DIAMOND Challenge. We adopted a variety of baseline state-of-the-art classification networks including ResNet, DenseNet, EfficientNet, and VGG with the aim of enhancing model robustness. The best performing models were Densenet 121, Resnet 152 and EfficientNet b7, and these were assembled into a definitive predictive model. The final ensemble model demonstrates a strong performance with an Area Under Curve (AUC) of 0.7017, an F1 score of 0.6512, and an Expected Calibration Error (ECE) of 0.2057 when deployed on a synthetic dataset. The performance of this ensemble model is comparable to previous studies despite training and testing in a more realistic setting, indicating the potential of UWF-CFP combined with a deep learning classification system to facilitate earlier diagnosis, better treatment decisions, and improved prognostication in ci-DME.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06521",
        "abstract url": "https://arxiv.org/abs/2410.06521",
        "title": "Real-to-Sim Grasp: Rethinking the Gap between Simulation and Real World in Grasp Detection",
        "rating": "-1",
        "keywords": [
            [
                "6-DoF",
                "RGB-D",
                "depth"
            ]
        ],
        "abstract": "For 6-DoF grasp detection, simulated data is expandable to train more powerful model, but it faces the challenge of the large gap between simulation and real world. Previous works bridge this gap with a sim-to-real way. However, this way explicitly or implicitly forces the simulated data to adapt to the noisy real data when training grasp detectors, where the positional drift and structural distortion within the camera noise will harm the grasp learning. In this work, we propose a Real-to-Sim framework for 6-DoF Grasp detection, named R2SGrasp, with the key insight of bridging this gap in a real-to-sim way, which directly bypasses the camera noise in grasp detector training through an inference-time real-to-sim adaption. To achieve this real-to-sim adaptation, our R2SGrasp designs the Real-to-Sim Data Repairer (R2SRepairer) to mitigate the camera noise of real depth maps in data-level, and the Real-to-Sim Feature Enhancer (R2SEnhancer) to enhance real features with precise simulated geometric primitives in feature-level. To endow our framework with the generalization ability, we construct a large-scale simulated dataset cost-efficiently to train our grasp detector, which includes 64,000 RGB-D images with 14.4 million grasp annotations. Sufficient experiments show that R2SGrasp is powerful and our real-to-sim perspective is effective. The real-world experiments further show great generalization ability of R2SGrasp. Project page is available on https://isee-laboratory.github.io/R2SGrasp.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06522",
        "abstract url": "https://arxiv.org/abs/2410.06522",
        "title": "On the Security of Bitstream-level JPEG Encryption with Restart Markers",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "This paper aims to evaluate the security of a bitstream-level JPEG encryption method using restart (RST) markers, where encrypted image can keep the JPEG file format with the same file size as non-encrypted image. Data encrypted using this method can be decoded without altering header information by employing a standard JPEG decoder. Moreover, the use of RST markers enables the definition of extended blocks divided by the markers, so spatially partial encryption and block-permutation-based encryption can be carried out. However, the security of the method was evaluated only with respect to the key space analysis for brute-force attacks and other limited attacks. Accordingly, in this paper, we evaluated the security of the method with respect to robustness against ciphertext-only attacks including state-of-the-art attacks. In experiments, the method is compared with conventional encryption methods, and it is confirmed to be robust against ciphertext-only attacks if parameters used for image encryption are carefully chosen.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "6 pages, 11 figures"
    },
    {
        "paper id": "2410.05697",
        "abstract url": "https://arxiv.org/abs/2410.05697",
        "title": "Diffusing to the Top: Boost Graph Neural Networks with Minimal Hyperparameter Tuning",
        "rating": "-1.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph Neural Networks (GNNs) are proficient in graph representation learning and achieve promising performance on versatile tasks such as node classification and link prediction. Usually, a comprehensive hyperparameter tuning is essential for fully unlocking GNN's top performance, especially for complicated tasks such as node classification on large graphs and long-range graphs. This is usually associated with high computational and time costs and careful design of appropriate search spaces. This work introduces a graph-conditioned latent diffusion framework (GNN-Diff) to generate high-performing GNNs based on the model checkpoints of sub-optimal hyperparameters selected by a light-tuning coarse search. We validate our method through 166 experiments across four graph tasks: node classification on small, large, and long-range graphs, as well as link prediction. Our experiments involve 10 classic and state-of-the-art target models and 20 publicly available datasets. The results consistently demonstrate that GNN-Diff: (1) boosts the performance of GNNs with efficient hyperparameter tuning; and (2) presents high stability and generalizability on unseen data across multiple generation runs. The code is available at https://github.com/lequanlin/GNN-Diff.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05726",
        "abstract url": "https://arxiv.org/abs/2410.05726",
        "title": "Less is more: Embracing sparsity and interpolation with Esiformer for time series forecasting",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Time series forecasting has played a significant role in many practical fields. But time series data generated from real-world applications always exhibits high variance and lots of noise, which makes it difficult to capture the inherent periodic patterns of the data, hurting the prediction accuracy significantly. To address this issue, we propose the Esiformer, which apply interpolation on the original data, decreasing the overall variance of the data and alleviating the influence of noise. What's more, we enhanced the vanilla transformer with a robust Sparse FFN. It can enhance the representation ability of the model effectively, and maintain the excellent robustness, avoiding the risk of overfitting compared with the vanilla implementation. Through evaluations on challenging real-world datasets, our method outperforms leading model PatchTST, reducing MSE by 6.5% and MAE by 5.8% in multivariate time series forecasting. Code is available at: https://github.com/yyg1282142265/Esiformer/tree/main.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05756",
        "abstract url": "https://arxiv.org/abs/2410.05756",
        "title": "Learning the Generalizable Manipulation Skills on Soft-body Tasks via Guided Self-attention Behavior Cloning Policy",
        "rating": "-1.5",
        "keywords": [
            [
                "point cloud"
            ],
            [
                "robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Embodied AI represents a paradigm in AI research where artificial agents are situated within and interact with physical or virtual environments. Despite the recent progress in Embodied AI, it is still very challenging to learn the generalizable manipulation skills that can handle large deformation and topological changes on soft-body objects, such as clay, water, and soil. In this work, we proposed an effective policy, namely GP2E behavior cloning policy, which can guide the agent to learn the generalizable manipulation skills from soft-body tasks, including pouring, filling, hanging, excavating, pinching, and writing. Concretely, we build our policy from three insights:(1) Extracting intricate semantic features from point cloud data and seamlessly integrating them into the robot's end-effector frame; (2) Capturing long-distance interactions in long-horizon tasks through the incorporation of our guided self-attention module; (3) Mitigating overfitting concerns and facilitating model convergence to higher accuracy levels via the introduction of our two-stage fine-tuning strategy. Through extensive experiments, we demonstrate the effectiveness of our approach by achieving the 1st prize in the soft-body track of the ManiSkill2 Challenge at the CVPR 2023 4th Embodied AI workshop. Our findings highlight the potential of our method to improve the generalization abilities of Embodied AI models and pave the way for their practical applications in real-world scenarios.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05806",
        "abstract url": "https://arxiv.org/abs/2410.05806",
        "title": "A Parameter Update Balancing Algorithm for Multi-task Ranking Models in Recommendation Systems",
        "rating": "-1.5",
        "keywords": [
            [
                "industrial",
                "Recommendation"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Multi-task ranking models have become essential for modern real-world recommendation systems. While most recommendation researches focus on designing sophisticated models for specific scenarios, achieving performance improvement for multi-task ranking models across various scenarios still remains a significant challenge. Training all tasks naively can result in inconsistent learning, highlighting the need for the development of multi-task optimization (MTO) methods to tackle this challenge. Conventional methods assume that the optimal joint gradient on shared parameters leads to optimal parameter updates. However, the actual update on model parameters may deviates significantly from gradients when using momentum based optimizers such as Adam, and we design and execute statistical experiments to support the observation. In this paper, we propose a novel Parameter Update Balancing algorithm for multi-task optimization, denoted as PUB. In contrast to traditional MTO method which are based on gradient level tasks fusion or loss level tasks fusion, PUB is the first work to optimize multiple tasks through parameter update balancing. Comprehensive experiments on benchmark multi-task ranking datasets demonstrate that PUB consistently improves several multi-task backbones and achieves state-of-the-art performance. Additionally, experiments on benchmark computer vision datasets show the great potential of PUB in various multi-task learning scenarios. Furthermore, we deployed our method for an industrial evaluation on the real-world commercial platform, HUAWEI AppGallery, where PUB significantly enhances the online multi-task ranking model, efficiently managing the primary traffic of a crucial channel.",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted by ICDM'24"
    },
    {
        "paper id": "2410.05819",
        "abstract url": "https://arxiv.org/abs/2410.05819",
        "title": "CAP: Detecting Unauthorized Data Usage in Generative Models via Prompt Generation",
        "rating": "-1.5",
        "keywords": [
            [
                "IoT"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "To achieve accurate and unbiased predictions, Machine Learning (ML) models rely on large, heterogeneous, and high-quality datasets. However, this could raise ethical and legal concerns regarding copyright and authorization aspects, especially when information is gathered from the Internet. With the rise of generative models, being able to track data has become of particular importance, especially since they may (un)intentionally replicate copyrighted contents. Therefore, this work proposes Copyright Audit via Prompts generation (CAP), a framework for automatically testing whether an ML model has been trained with unauthorized data. Specifically, we devise an approach to generate suitable keys inducing the model to reveal copyrighted contents. To prove its effectiveness, we conducted an extensive evaluation campaign on measurements collected in four IoT scenarios. The obtained results showcase the effectiveness of CAP, when used against both realistic and synthetic datasets.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05877",
        "abstract url": "https://arxiv.org/abs/2410.05877",
        "title": "MDAP: A Multi-view Disentangled and Adaptive Preference Learning Framework for Cross-Domain Recommendation",
        "rating": "-1.5",
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Cross-domain Recommendation systems leverage multi-domain user interactions to improve performance, especially in sparse data or new user scenarios. However, CDR faces challenges such as effectively capturing user preferences and avoiding negative transfer. To address these issues, we propose the Multi-view Disentangled and Adaptive Preference Learning (MDAP) framework. Our MDAP framework uses a multiview encoder to capture diverse user preferences. The framework includes a gated decoder that adaptively combines embeddings from different views to generate a comprehensive user representation. By disentangling representations and allowing adaptive feature selection, our model enhances adaptability and effectiveness. Extensive experiments on benchmark datasets demonstrate that our method significantly outperforms state-of-the-art CDR and single-domain models, providing more accurate recommendations and deeper insights into user behavior across different domains.",
        "subjects": [
            "cs.IR",
            "cs.LG"
        ],
        "comment": "The International Web Information Systems Engineering conference"
    },
    {
        "paper id": "2410.05889",
        "abstract url": "https://arxiv.org/abs/2410.05889",
        "title": "Deep learning-based fault identification in condition monitoring",
        "rating": "-1.5",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Vibration-based condition monitoring techniques are commonly used to identify faults in rolling element bearings. Accuracy and speed of fault detection procedures are critical performance measures in condition monitoring. Delay is especially important in remote condition monitoring and time-sensitive industrial applications. While most existing methods focus on accuracy, little attention has been given to the inference time in the fault identification process. In this paper, we address this gap by presenting a Convolutional Neural Network (CNN) based approach for real-time fault identification in rolling element bearings. We encode raw vibration signals into two-dimensional images using various encoding methods and use these with a CNN to classify several categories of bearing fault types and sizes. We analyse the interplay between fault identification accuracy and processing time. For training and evaluation we use a bearing failure CWRU dataset.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05892",
        "abstract url": "https://arxiv.org/abs/2410.05892",
        "title": "Towards an Autonomous Surface Vehicle Prototype for Artificial Intelligence Applications of Water Quality Monitoring",
        "rating": "-1.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "Vehicle"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The use of Autonomous Surface Vehicles, equipped with water quality sensors and artificial vision systems, allows for a smart and adaptive deployment in water resources environmental monitoring. This paper presents a real implementation of a vehicle prototype that to address the use of Artificial Intelligence algorithms and enhanced sensing techniques for water quality monitoring. The vehicle is fully equipped with high-quality sensors to measure water quality parameters and water depth. Furthermore, by means of a stereo-camera, it also can detect and locate macro-plastics in real environments by means of deep visual models, such as YOLOv5. In this paper, experimental results, carried out in Lago Mayor (Sevilla), has been presented as proof of the capabilities of the proposed architecture. The overall system, and the early results obtained, are expected to provide a solid example of a real platform useful for the water resource monitoring task, and to serve as a real case scenario for deploying Artificial Intelligence algorithms, such as path planning, artificial vision, etc.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05894",
        "abstract url": "https://arxiv.org/abs/2410.05894",
        "title": "DimOL: Dimensional Awareness as A New 'Dimension' in Operator Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the realm of computational physics, an enduring topic is the numerical solutions to partial differential equations (PDEs). Recently, the attention of researchers has shifted towards Neural Operator methods, renowned for their capability to approximate ``operators'' -- mappings from functions to functions. Despite the universal approximation theorem within neural operators, ensuring error bounds often requires employing numerous Fourier layers. However, what about lightweight models? In response to this question, we introduce DimOL (Dimension-aware Operator Learning), drawing insights from dimensional analysis. To implement DimOL, we propose the ProdLayer, which can be seamlessly integrated into FNO-based and Transformer-based PDE solvers, enhancing their ability to handle sum-of-products structures inherent in many physical systems. Empirically, DimOL models achieve up to 48% performance gain within the PDE datasets. Furthermore, by analyzing Fourier components' weights, we can symbolically discern the physical significance of each term. This sheds light on the opaque nature of neural networks, unveiling underlying physical principles.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05899",
        "abstract url": "https://arxiv.org/abs/2410.05899",
        "title": "Brain-inspired continual pre-trained learner via silent synaptic consolidation",
        "rating": "-1.5",
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Pre-trained models have demonstrated impressive generalization capabilities, yet they remain vulnerable to catastrophic forgetting when incrementally trained on new tasks. Existing architecture-based strategies encounter two primary challenges: 1) Integrating a pre-trained network with a trainable sub-network complicates the delicate balance between learning plasticity and memory stability across evolving tasks during learning. 2) The absence of robust interconnections between pre-trained networks and various sub-networks limits the effective retrieval of pertinent information during inference. In this study, we introduce the Artsy, inspired by the activation mechanisms of silent synapses via spike-timing-dependent plasticity observed in mature brains, to enhance the continual learning capabilities of pre-trained models. The Artsy integrates two key components: During training, the Artsy mimics mature brain dynamics by maintaining memory stability for previously learned knowledge within the pre-trained network while simultaneously promoting learning plasticity in task-specific sub-networks. During inference, artificial silent and functional synapses are utilized to establish precise connections between the pre-synaptic neurons in the pre-trained network and the post-synaptic neurons in the sub-networks, facilitated through synaptic consolidation, thereby enabling effective extraction of relevant information from test samples. Comprehensive experimental evaluations reveal that our model significantly outperforms conventional methods on class-incremental learning tasks, while also providing enhanced biological interpretability for architecture-based approaches. Moreover, we propose that the Artsy offers a promising avenue for simulating biological synaptic mechanisms, potentially advancing our understanding of neural plasticity in both artificial and biological systems.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05982",
        "abstract url": "https://arxiv.org/abs/2410.05982",
        "title": "DeMo: Decoupling Motion Forecasting into Directional Intentions and Dynamic States",
        "rating": "-1.5",
        "keywords": [
            [
                "autonomous driving",
                "trajectory"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Accurate motion forecasting for traffic agents is crucial for ensuring the safety and efficiency of autonomous driving systems in dynamically changing environments. Mainstream methods adopt a one-query-one-trajectory paradigm, where each query corresponds to a unique trajectory for predicting multi-modal trajectories. While straightforward and effective, the absence of detailed representation of future trajectories may yield suboptimal outcomes, given that the agent states dynamically evolve over time. To address this problem, we introduce DeMo, a framework that decouples multi-modal trajectory queries into two types: mode queries capturing distinct directional intentions and state queries tracking the agent's dynamic states over time. By leveraging this format, we separately optimize the multi-modality and dynamic evolutionary properties of trajectories. Subsequently, the mode and state queries are integrated to obtain a comprehensive and detailed representation of the trajectories. To achieve these operations, we additionally introduce combined Attention and Mamba techniques for global information aggregation and state sequence modeling, leveraging their respective strengths. Extensive experiments on both the Argoverse 2 and nuScenes benchmarks demonstrate that our DeMo achieves state-of-the-art performance in motion forecasting.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "NeurIPS 2024"
    },
    {
        "paper id": "2410.06007",
        "abstract url": "https://arxiv.org/abs/2410.06007",
        "title": "Motion Forecasting in Continuous Driving",
        "rating": "-1.5",
        "keywords": [
            [
                "autonomous driving",
                "trajectory"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Motion forecasting for agents in autonomous driving is highly challenging due to the numerous possibilities for each agent's next action and their complex interactions in space and time. In real applications, motion forecasting takes place repeatedly and continuously as the self-driving car moves. However, existing forecasting methods typically process each driving scene within a certain range independently, totally ignoring the situational and contextual relationships between successive driving scenes. This significantly simplifies the forecasting task, making the solutions suboptimal and inefficient to use in practice. To address this fundamental limitation, we propose a novel motion forecasting framework for continuous driving, named RealMotion. It comprises two integral streams both at the scene level: (1) The scene context stream progressively accumulates historical scene information until the present moment, capturing temporal interactive relationships among scene elements. (2) The agent trajectory stream optimizes current forecasting by sequentially relaying past predictions. Besides, a data reorganization strategy is introduced to narrow the gap between existing benchmarks and real-world applications, consistent with our network. These approaches enable exploiting more broadly the situational and progressive insights of dynamic motion across space and time. Extensive experiments on Argoverse series with different settings demonstrate that our RealMotion achieves state-of-the-art performance, along with the advantage of efficient real-world inference. The source code will be available at https://github.com/fudan-zvg/RealMotion.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at NeurIPS 2024 Spotlight"
    },
    {
        "paper id": "2410.06031",
        "abstract url": "https://arxiv.org/abs/2410.06031",
        "title": "Patient flow networks absorb healthcare stress during pandemic crises",
        "rating": "-1.5",
        "keywords": [
            [
                "medical",
                "healthcare"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Disasters, such as the recent COVID-19 pandemic, impose recurrent and heterogeneous stress on healthcare systems, necessitating the redistribution of stress to enhance healthcare resilience. However, existing studies have been hindered by limited datasets and approaches for assessing its absorptive capacity - defined as the system's ability to absorb stress by redistributing patient flows. This study addresses this gap by analyzing patient flow networks constructed from billions of electronic medical records and introducing an approach to quantify network absorptivity under crisis conditions. Our analysis of U.S. healthcare systems reveals that during the COVID-19 pandemic, cross-regional patient flows increased by 3.89%, a 0.90% rise from pre-pandemic levels. The networks exhibited an average absorptivity of 0.21, representing a 10% increase over pre-pandemic conditions. Flow networks with higher connectivity and heterogeneity showed a greater capacity to alleviate system burdens. These empirical and analytical insights underscore the critical role of proactive patient flow management in strengthening healthcare resilience during crises.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "36 pages, 7 figures"
    },
    {
        "paper id": "2410.06060",
        "abstract url": "https://arxiv.org/abs/2410.06060",
        "title": "Hierarchical Matrix Completion for the Prediction of Properties of Binary Mixtures",
        "rating": "-1.5",
        "keywords": [
            [
                "chemical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Predicting the thermodynamic properties of mixtures is crucial for process design and optimization in chemical engineering. Machine learning (ML) methods are gaining increasing attention in this field, but experimental data for training are often scarce, which hampers their application. In this work, we introduce a novel generic approach for improving data-driven models: inspired by the ancient rule \"similia similibus solvuntur\", we lump components that behave similarly into chemical classes and model them jointly in the first step of a hierarchical approach. While the information on class affiliations can stem in principle from any source, we demonstrate how classes can reproducibly be defined based on mixture data alone by agglomerative clustering. The information from this clustering step is then used as an informed prior for fitting the individual data. We demonstrate the benefits of this approach by applying it in connection with a matrix completion method (MCM) for predicting isothermal activity coefficients at infinite dilution in binary mixtures. Using clustering leads to significantly improved predictions compared to an MCM without clustering. Furthermore, the chemical classes learned from the clustering give exciting insights into what matters on the molecular level for modeling given mixture properties.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06065",
        "abstract url": "https://arxiv.org/abs/2410.06065",
        "title": "Posets and Bounded Probabilities for Discovering Order-inducing Features in Event Knowledge Graphs",
        "rating": "-1.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "Graphs"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Event knowledge graphs (EKG) extend the classical notion of a trace to capture multiple, interacting views of a process execution. In this paper, we tackle the open problem of automating EKG discovery from uncurated data through a principled, probabilistic framing based on the outcome space resulting from featured-derived partial orders on events. From this, we derive an EKG discovery algorithm based upon statistical inference rather than an ad-hoc or heuristic-based strategy, or relying on manual analysis from domain experts. This approach comes at the computational cost of exploring a large, non-convex hypothesis space. In particular, solving the maximum likelihood term involves counting the number of linear extensions of posets, which in general is #P-complete. Fortunately, bound estimates suffice for model comparison, and admit incorporation into a bespoke branch-and-bound algorithm. We show that the posterior probability as defined is antitonic w.r.t. search depth for branching rules that are monotonic w.r.t. model inclusion. This allows pruning of large portions of the search space, which we show experimentally leads to rapid convergence toward optimal solutions that are consistent with manually built EKGs.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06070",
        "abstract url": "https://arxiv.org/abs/2410.06070",
        "title": "Enforcing Interpretability in Time Series Transformers: A Concept Bottleneck Framework",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "There has been a recent push of research on Transformer-based models for long-term time series forecasting, even though they are inherently difficult to interpret and explain. While there is a large body of work on interpretability methods for various domains and architectures, the interpretability of Transformer-based forecasting models remains largely unexplored. To address this gap, we develop a framework based on Concept Bottleneck Models to enforce interpretability of time series Transformers. We modify the training objective to encourage a model to develop representations similar to predefined interpretable concepts. In our experiments, we enforce similarity using Centered Kernel Alignment, and the predefined concepts include time features and an interpretable, autoregressive surrogate model (AR). We apply the framework to the Autoformer model, and present an in-depth analysis for a variety of benchmark tasks. We find that the model performance remains mostly unaffected, while the model shows much improved interpretability. Additionally, interpretable concepts become local, which makes the trained model easily intervenable. As a proof of concept, we demonstrate a successful intervention in the scenario of a time shift in the data, which eliminates the need to retrain.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06084",
        "abstract url": "https://arxiv.org/abs/2410.06084",
        "title": "Diversity-Rewarded CFG Distillation",
        "rating": "-1.5",
        "keywords": [
            [
                "music",
                "text-to-music"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Generative models are transforming creative domains such as music generation, with inference-time strategies like Classifier-Free Guidance (CFG) playing a crucial role. However, CFG doubles inference cost while limiting originality and diversity across generated contents. In this paper, we introduce diversity-rewarded CFG distillation, a novel finetuning procedure that distills the strengths of CFG while addressing its limitations. Our approach optimises two training objectives: (1) a distillation objective, encouraging the model alone (without CFG) to imitate the CFG-augmented predictions, and (2) an RL objective with a diversity reward, promoting the generation of diverse outputs for a given prompt. By finetuning, we learn model weights with the ability to generate high-quality and diverse outputs, without any inference overhead. This also unlocks the potential of weight-based model merging strategies: by interpolating between the weights of two models (the first focusing on quality, the second on diversity), we can control the quality-diversity trade-off at deployment time, and even further boost performance. We conduct extensive experiments on the MusicLM (Agostinelli et al., 2023) text-to-music generative model, where our approach surpasses CFG in terms of quality-diversity Pareto optimality. According to human evaluators, our finetuned-then-merged model generates samples with higher quality-diversity than the base model augmented with CFG. Explore our generations at https://google-research.github.io/seanet/musiclm/diverse_music/.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06119",
        "abstract url": "https://arxiv.org/abs/2410.06119",
        "title": "E3STO: Orbital Inspired SE(3)-Equivariant Molecular Representation for Electron Density Prediction",
        "rating": "-1.5",
        "keywords": [
            [
                "quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Electron density prediction stands as a cornerstone challenge in molecular systems, pivotal for various applications such as understanding molecular interactions and conducting precise quantum mechanical calculations. However, the scaling of density functional theory (DFT) calculations is prohibitively expensive. Machine learning methods provide an alternative, offering efficiency and accuracy. We introduce a novel SE(3)-equivariant architecture, drawing inspiration from Slater-Type Orbitals (STO), to learn representations of molecular electronic structures. Our approach offers an alternative functional form for learned orbital-like molecular representation. We showcase the effectiveness of our method by achieving SOTA prediction accuracy of molecular electron density with 30-70\\% improvement over other work on Molecular Dynamics data.",
        "subjects": [
            "physics.chem-ph",
            "cs.LG",
            "q-bio.BM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06170",
        "abstract url": "https://arxiv.org/abs/2410.06170",
        "title": "QGym: Scalable Simulation and Benchmarking of Queuing Network Controllers",
        "rating": "-1.5",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Queuing network control determines the allocation of scarce resources to manage congestion, a fundamental problem in manufacturing, communications, and healthcare. Compared to standard RL problems, queueing problems are distinguished by unique challenges: i) a system operating in continuous time, ii) high stochasticity, and iii) long horizons over which the system can become unstable (exploding delays). To spur methodological progress tackling these challenges, we present an open-sourced queueing simulation framework, QGym, that benchmark queueing policies across realistic problem instances. Our modular framework allows the researchers to build on our initial instances, which provide a wide range of environments including parallel servers, criss-cross, tandem, and re-entrant networks, as well as a realistically calibrated hospital queuing system. QGym makes it easy to compare multiple policies, including both model-free RL methods and classical queuing policies. Our testbed complements the traditional focus on evaluating algorithms based on mathematical guarantees in idealized settings, and significantly expands the scope of empirical benchmarking in prior work. QGym code is open-sourced at https://github.com/namkoong-lab/QGym.",
        "subjects": [
            "cs.LG",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06188",
        "abstract url": "https://arxiv.org/abs/2410.06188",
        "title": "Beyond the Alphabet: Deep Signal Embedding for Enhanced DNA Clustering",
        "rating": "-1.5",
        "keywords": [
            [
                "DNA"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The emerging field of DNA storage employs strands of DNA bases (A/T/C/G) as a storage medium for digital information to enable massive density and durability. The DNA storage pipeline includes: (1) encoding the raw data into sequences of DNA bases; (2) synthesizing the sequences as DNA \\textit{strands} that are stored over time as an unordered set; (3) sequencing the DNA strands to generate DNA \\textit{reads}; and (4) deducing the original data. The DNA synthesis and sequencing stages each generate several independent error-prone duplicates of each strand which are then utilized in the final stage to reconstruct the best estimate for the original strand. Specifically, the reads are first \\textit{clustered} into groups likely originating from the same strand (based on their similarity to each other), and then each group approximates the strand that led to the reads of that group. This work improves the DNA clustering stage by embedding it as part of the DNA sequencing. Traditional DNA storage solutions begin after the DNA sequencing process generates discrete DNA reads (A/T/C/G), yet we identify that there is untapped potential in using the raw signals generated by the Nanopore DNA sequencing machine before they are discretized into bases, a process known as \\textit{basecalling}, which is done using a deep neural network. We propose a deep neural network that clusters these signals directly, demonstrating superior accuracy, and reduced computation times compared to current approaches that cluster after basecalling.",
        "subjects": [
            "q-bio.GN",
            "cs.CE",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06211",
        "abstract url": "https://arxiv.org/abs/2410.06211",
        "title": "A mechanistically interpretable neural network for regulatory genomics",
        "rating": "-1.5",
        "keywords": [
            [
                "DNA"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep neural networks excel in mapping genomic DNA sequences to associated readouts (e.g., protein-DNA binding). Beyond prediction, the goal of these networks is to reveal to scientists the underlying motifs (and their syntax) which drive genome regulation. Traditional methods that extract motifs from convolutional filters suffer from the uninterpretable dispersion of information across filters and layers. Other methods which rely on importance scores can be unstable and unreliable. Instead, we designed a novel mechanistically interpretable architecture for regulatory genomics, where motifs and their syntax are directly encoded and readable from the learned weights and activations. We provide theoretical and empirical evidence of our architecture's full expressivity, while still being highly interpretable. Through several experiments, we show that our architecture excels in de novo motif discovery and motif instance calling, is robust to variable sequence contexts, and enables fully interpretable generation of novel functional sequences.",
        "subjects": [
            "q-bio.GN",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06232",
        "abstract url": "https://arxiv.org/abs/2410.06232",
        "title": "Don't Cut Corners: Exact Conditions for Modularity in Biologically Inspired Representations",
        "rating": "-1.5",
        "keywords": [
            [
                "Biologically"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Why do biological and artificial neurons sometimes modularise, each encoding a single meaningful variable, and sometimes entangle their representation of many variables? In this work, we develop a theory of when biologically inspired representations -- those that are nonnegative and energy efficient -- modularise with respect to source variables (sources). We derive necessary and sufficient conditions on a sample of sources that determine whether the neurons in an optimal biologically-inspired linear autoencoder modularise. Our theory applies to any dataset, extending far beyond the case of statistical independence studied in previous work. Rather, we show that sources modularise if their support is \"sufficiently spread\". From this theory, we extract and validate predictions in a variety of empirical studies on how data distribution affects modularisation in nonlinear feedforward and recurrent neural networks trained on supervised and unsupervised tasks. Furthermore, we apply these ideas to neuroscience data. First, we explain why two studies that recorded prefrontal activity in working memory tasks conflict on whether memories are encoded in orthogonal subspaces: the support of the sources differed due to a critical discrepancy in experimental protocol. Second, we use similar arguments to understand why preparatory and potent subspaces in RNN models of motor cortex are only sometimes orthogonal. Third, we study spatial and reward information mixing in entorhinal recordings, and show our theory matches data better than previous work. And fourth, we suggest a suite of surprising settings in which neurons can be (or appear) mixed selective, without requiring complex nonlinear readouts as in traditional theories. In sum, our theory prescribes precise conditions on when neural activities modularise, providing tools for inducing and elucidating modular representations in brains and machines.",
        "subjects": [
            "q-bio.NC",
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ],
        "comment": "47 pages, 23 figures. WD and KH contributed equally; LH and JHL contributed equally"
    },
    {
        "paper id": "2410.06240",
        "abstract url": "https://arxiv.org/abs/2410.06240",
        "title": "Using Crank-Nikolson Scheme to Solve the Korteweg-de Vries (KdV) Equation",
        "rating": "-1.5",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The Korteweg-de Vries (KdV) equation is a fundamental partial differential equation that models wave propagation in shallow water and other dispersive media. Accurately solving the KdV equation is essential for understanding wave dynamics in physics and engineering applications. This project focuses on implementing the Crank-Nicolson scheme, a finite difference method known for its stability and accuracy, to solve the KdV equation. The Crank-Nicolson scheme's implicit nature allows for a more stable numerical solution, especially in handling the dispersive and nonlinear terms of the KdV equation. We investigate the performance of the scheme through various test cases, analyzing its convergence and error behavior. The results demonstrate that the Crank-Nicolson method provides a robust approach for solving the KdV equation, with improved accuracy over traditional explicit methods. Code is available at the end of the paper.",
        "subjects": [
            "math.NA",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06294",
        "abstract url": "https://arxiv.org/abs/2410.06294",
        "title": "A New Architecture for Neural Enhanced Multiobject Tracking",
        "rating": "-1.5",
        "keywords": [
            [
                "autonomous driving",
                "LiDAR"
            ],
            [
                "robotics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Multiobject tracking (MOT) is an important task in robotics, autonomous driving, and maritime surveillance. Traditional work on MOT is model-based and aims to establish algorithms in the framework of sequential Bayesian estimation. More recent methods are fully data-driven and rely on the training of neural networks. The two approaches have demonstrated advantages in certain scenarios. In particular, in problems where plenty of labeled data for the training of neural networks is available, data-driven MOT tends to have advantages compared to traditional methods. A natural thought is whether a general and efficient framework can integrate the two approaches. This paper advances a recently introduced hybrid model-based and data-driven method called neural-enhanced belief propagation (NEBP). Compared to existing work on NEBP for MOT, it introduces a novel neural architecture that can improve data association and new object initialization, two critical aspects of MOT. The proposed tracking method is leading the nuScenes LiDAR-only tracking challenge at the time of submission of this paper.",
        "subjects": [
            "eess.SP",
            "cs.LG",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06329",
        "abstract url": "https://arxiv.org/abs/2410.06329",
        "title": "Bayesian Estimation and Tuning-Free Rank Detection for Probability Mass Function Tensors",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Obtaining a reliable estimate of the joint probability mass function (PMF) of a set of random variables from observed data is a significant objective in statistical signal processing and machine learning. Modelling the joint PMF as a tensor that admits a low-rank canonical polyadic decomposition (CPD) has enabled the development of efficient PMF estimation algorithms. However, these algorithms require the rank (model order) of the tensor to be specified beforehand. In real-world applications, the true rank is unknown. Therefore, an appropriate rank is usually selected from a candidate set either by observing validation errors or by computing various likelihood-based information criteria, a procedure which is computationally expensive for large datasets. This paper presents a novel Bayesian framework for estimating the joint PMF and automatically inferring its rank from observed data. We specify a Bayesian PMF estimation model and employ appropriate prior distributions for the model parameters, allowing for tuning-free rank inference via a single training run. We then derive a deterministic solution based on variational inference (VI) to approximate the posterior distributions of various model parameters. Additionally, we develop a scalable version of the VI-based approach by leveraging stochastic variational inference (SVI) to arrive at an efficient algorithm whose complexity scales sublinearly with the size of the dataset. Numerical experiments involving both synthetic data and real movie recommendation data illustrate the advantages of our VI and SVI-based methods in terms of estimation accuracy, automatic rank detection, and computational efficiency.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06333",
        "abstract url": "https://arxiv.org/abs/2410.06333",
        "title": "Batched Bayesian optimization with correlated candidate uncertainties",
        "rating": "-1.5",
        "keywords": [
            [
                "chemical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Batched Bayesian optimization (BO) can accelerate molecular design by efficiently identifying top-performing compounds from a large chemical library. Existing acquisition strategies for batch design in BO aim to balance exploration and exploitation. This often involves optimizing non-additive batch acquisition functions, necessitating approximation via myopic construction and/or diversity heuristics. In this work, we propose an acquisition strategy for discrete optimization that is motivated by pure exploitation, qPO (multipoint Probability of Optimality). qPO maximizes the probability that the batch includes the true optimum, which is expressible as the sum over individual acquisition scores and thereby circumvents the combinatorial challenge of optimizing a batch acquisition function. We differentiate the proposed strategy from parallel Thompson sampling and discuss how it implicitly captures diversity. Finally, we apply our method to the model-guided exploration of large chemical libraries and provide empirical evidence that it performs better than or on par with state-of-the-art methods in batched Bayesian optimization.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06412",
        "abstract url": "https://arxiv.org/abs/2410.06412",
        "title": "Stochastic Sparse Sampling: A Framework for Variable-Length Medical Time Series Classification",
        "rating": "-1.5",
        "keywords": [
            [
                "Medical",
                "healthcare"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "While the majority of time series classification research has focused on modeling fixed-length sequences, variable-length time series classification (VTSC) remains critical in healthcare, where sequence length may vary among patients and events. To address this challenge, we propose $\\textbf{S}$tochastic $\\textbf{S}$parse $\\textbf{S}$ampling (SSS), a novel VTSC framework developed for medical time series. SSS manages variable-length sequences by sparsely sampling fixed windows to compute local predictions, which are then aggregated and calibrated to form a global prediction. We apply SSS to the task of seizure onset zone (SOZ) localization, a critical VTSC problem requiring identification of seizure-inducing brain regions from variable-length electrophysiological time series. We evaluate our method on the Epilepsy iEEG Multicenter Dataset, a heterogeneous collection of intracranial electroencephalography (iEEG) recordings obtained from four independent medical centers. SSS demonstrates superior performance compared to state-of-the-art (SOTA) baselines across most medical centers, and superior performance on all out-of-distribution (OOD) unseen medical centers. Additionally, SSS naturally provides post-hoc insights into local signal characteristics related to the SOZ, by visualizing temporally averaged local predictions throughout the signal.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "20 pages, 8 figures, 2 tables"
    },
    {
        "paper id": "2410.06452",
        "abstract url": "https://arxiv.org/abs/2410.06452",
        "title": "Modeling chaotic Lorenz ODE System using Scientific Machine Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In climate science, models for global warming and weather prediction face significant challenges due to the limited availability of high-quality data and the difficulty in obtaining it, making data efficiency crucial. In the past few years, Scientific Machine Learning (SciML) models have gained tremendous traction as they can be trained in a data-efficient manner, making them highly suitable for real-world climate applications. Despite this, very little attention has been paid to chaotic climate system modeling utilizing SciML methods. In this paper, we have integrated SciML methods into foundational weather models, where we have enhanced large-scale climate predictions with a physics-informed approach that achieves high accuracy with reduced data. We successfully demonstrate that by combining the interpretability of physical climate models with the computational power of neural networks, SciML models can prove to be a reliable tool for modeling climate. This indicates a shift from the traditional black box-based machine learning modeling of climate systems to physics-informed decision-making, leading to effective climate policy implementation.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "13 pages, 8 figures, 3 tables"
    },
    {
        "paper id": "2410.06476",
        "abstract url": "https://arxiv.org/abs/2410.06476",
        "title": "Quantitative Theory of Meaning. Application to Financial Markets. EUR/USD case study",
        "rating": "-1.5",
        "keywords": [
            [
                "forecast"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "The paper focuses on the link between information, investors' expectations and market price movement. EUR/USD market is examined from communication-theoretical perspective on the dynamics of information and meaning. We build upon the quantitative theory of meaning as a complement to the quantitative theory of information. Different groups of investors entertain different criteria to process information, so that the same information can be supplied with different meanings. Meanings shape investors' expectations which are revealed in market asset price movement. This dynamics can be captured by non-linear evolutionary equation. We use a computationally efficient technique of logistic Continuous Wavelet Transformation (CWT) to analyze EUR/USD market. The results reveal the latent EUR/USD trend structure which coincides with the model predicted time series indicating that proposed model can adequately describe some patterns of investors' behavior. Proposed methodology can be used to better understand and forecast future market assets' price movement.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06480",
        "abstract url": "https://arxiv.org/abs/2410.06480",
        "title": "TCGU: Data-centric Graph Unlearning based on Transferable Condensation",
        "rating": "-1.5",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "GNN",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "With growing demands for data privacy and model robustness, graph unlearning (GU), which erases the influence of specific data on trained GNN models, has gained significant attention. However, existing exact unlearning methods suffer from either low efficiency or poor model performance. While being more utility-preserving and efficient, current approximate unlearning methods are not applicable in the zero-glance privacy setting, where the deleted samples cannot be accessed during unlearning due to immediate deletion requested by regulations. Besides, these approximate methods, which try to directly perturb model parameters still involve high privacy concerns in practice. To fill the gap, we propose Transferable Condensation Graph Unlearning (TCGU), a data-centric solution to zero-glance graph unlearning. Specifically, we first design a two-level alignment strategy to pre-condense the original graph into a small yet utility-preserving dataset. Upon receiving an unlearning request, we fine-tune the pre-condensed data with a low-rank plugin, to directly align its distribution with the remaining graph, thus efficiently revoking the information of deleted data without accessing them. A novel similarity distribution matching approach and a discrimination regularizer are proposed to effectively transfer condensed data and preserve its utility in GNN training, respectively. Finally, we retrain the GNN on the transferred condensed data. Extensive experiments on 6 benchmark datasets demonstrate that TCGU can achieve superior performance in terms of model utility, unlearning efficiency, and unlearning efficacy than existing GU methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "14 pages, 18 figures"
    },
    {
        "paper id": "2410.06497",
        "abstract url": "https://arxiv.org/abs/2410.06497",
        "title": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta's Ads System",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The increasing complexity of deep learning models used for calculating user representations presents significant challenges, particularly with limited computational resources and strict service-level agreements (SLAs). Previous research efforts have focused on optimizing model inference but have overlooked a critical question: is it necessary to perform user model inference for every ad request in large-scale social networks? To address this question and these challenges, we first analyze user access patterns at Meta and find that most user model inferences occur within a short timeframe. T his observation reveals a triangular relationship among model complexity, embedding freshness, and service SLAs. Building on this insight, we designed, implemented, and evaluated ERCache, an efficient and robust caching framework for large-scale user representations in ads recommendation systems on social networks. ERCache categorizes cache into direct and failover types and applies customized settings and eviction policies for each model, effectively balancing model complexity, embedding freshness, and service SLAs, even considering the staleness introduced by caching. ERCache has been deployed at Meta for over six months, supporting more than 30 ranking models while efficiently conserving computational resources and complying with service SLA requirements.",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06509",
        "abstract url": "https://arxiv.org/abs/2410.06509",
        "title": "PFAttack: Stealthy Attack Bypassing Group Fairness in Federated Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Attack"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning (FL), integrating group fairness mechanisms, allows multiple clients to collaboratively train a global model that makes unbiased decisions for different populations grouped by sensitive attributes (e.g., gender and race). Due to its distributed nature, previous studies have demonstrated that FL systems are vulnerable to model poisoning attacks. However, these studies primarily focus on perturbing accuracy, leaving a critical question unexplored: Can an attacker bypass the group fairness mechanisms in FL and manipulate the global model to be biased? The motivations for such an attack vary; an attacker might seek higher accuracy, yet fairness considerations typically limit the accuracy of the global model or aim to cause ethical disruption. To address this question, we design a novel form of attack in FL, termed Profit-driven Fairness Attack (PFATTACK), which aims not to degrade global model accuracy but to bypass fairness mechanisms. Our fundamental insight is that group fairness seeks to weaken the dependence of outputs on input attributes related to sensitive information. In the proposed PFATTACK, an attacker can recover this dependence through local fine-tuning across various sensitive groups, thereby creating a biased yet accuracy-preserving malicious model and injecting it into FL through model replacement. Compared to attacks targeting accuracy, PFATTACK is more stealthy. The malicious model in PFATTACK exhibits subtle parameter variations relative to the original global model, making it robust against detection and filtering by Byzantine-resilient aggregations. Extensive experiments on benchmark datasets are conducted for four fair FL frameworks and three Byzantine-resilient aggregations against model poisoning, demonstrating the effectiveness and stealth of PFATTACK in bypassing group fairness mechanisms in FL.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06523",
        "abstract url": "https://arxiv.org/abs/2410.06523",
        "title": "Phase Diagram from Nonlinear Interaction between Superconducting Order and Density: Toward Data-Based Holographic Superconductor",
        "rating": "-1.5",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "We address an inverse problem in modeling holographic superconductors. We focus our research on the critical temperature behavior depicted by experiments. We use a physics-informed neural network method to find a mass function $M(F^2)$, which is necessary to understand phase transition behavior. This mass function describes a nonlinear interaction between superconducting order and charge carrier density. We introduce positional embedding layers to improve the learning process in our algorithm, and the Adam optimization is used to predict the critical temperature data via holographic calculation with appropriate accuracy. Consideration of the positional embedding layers is motivated by the transformer model of natural-language processing in the artificial intelligence (AI) field. We obtain holographic models that reproduce borderlines of the normal and superconducting phases provided by actual data. Our work is the first holographic attempt to match phase transition data quantitatively obtained from experiments. Also, the present work offers a new methodology for data-based holographic models.",
        "subjects": [
            "hep-th",
            "cond-mat.dis-nn",
            "cond-mat.supr-con",
            "cs.AI"
        ],
        "comment": "22 pages, 20 figures"
    },
    {
        "paper id": "2410.06534",
        "abstract url": "https://arxiv.org/abs/2410.06534",
        "title": "EEG-estimated functional connectivity, and not behavior, differentiates Parkinson's patients from health controls during the Simon conflict task",
        "rating": "-1.5",
        "keywords": [
            [
                "biomarkers",
                "health",
                "EEG",
                "disease"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Neural biomarkers that can classify or predict disease are of broad interest to the neurological and psychiatric communities. Such biomarkers can be informative of disease state or treatment efficacy, even before there are changes in symptoms and/or behavior. This work investigates EEG-estimated functional connectivity (FC) as a Parkinson's Disease (PD) biomarker. Specifically, we investigate FC mediated via neural oscillations and consider such activity during the Simons conflict task. This task yields sensory-motor conflict, and one might expect differences in behavior between PD patients and healthy controls (HCs). In addition to considering spatially focused approaches, such as FC, as a biomarker, we also consider temporal biomarkers, which are more sensitive to ongoing changes in neural activity. We find that FC, estimated from delta (1-4Hz) and theta (4-7Hz) oscillations, yields spatial FC patterns significantly better at distinguishing PD from HC than temporal features or behavior. This study reinforces that FC in spectral bands is informative of differences in brain-wide processes and can serve as a biomarker distinguishing normal brain function from that seen in disease.",
        "subjects": [
            "q-bio.NC",
            "cs.LG"
        ],
        "comment": "This work is accepted at IEEE EMBC 2024. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications standards/publications/rights/index.html for more information"
    },
    {
        "paper id": "2410.07260",
        "abstract url": "https://arxiv.org/abs/2410.07260",
        "title": "Precision Cancer Classification and Biomarker Identification from mRNA Gene Expression via Dimensionality Reduction and Explainable AI",
        "rating": "-1.5",
        "keywords": [
            [
                "Biomarker",
                "Cancer"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Gene expression analysis is a critical method for cancer classification, enabling precise diagnoses through the identification of unique molecular signatures associated with various tumors. Identifying cancer-specific genes from gene expression values enables a more tailored and personalized treatment approach. However, the high dimensionality of mRNA gene expression data poses challenges for analysis and data extraction. This research presents a comprehensive pipeline designed to accurately identify 33 distinct cancer types and their corresponding gene sets. It incorporates a combination of normalization and feature selection techniques to reduce dataset dimensionality effectively while ensuring high performance. Notably, our pipeline successfully identifies a substantial number of cancer-specific genes using a reduced feature set of just 500, in contrast to using the full dataset comprising 19,238 features. By employing an ensemble approach that combines three top-performing classifiers, a classification accuracy of 96.61% was achieved. Furthermore, we leverage Explainable AI to elucidate the biological significance of the identified cancer-specific genes, employing Differential Gene Expression (DGE) analysis.",
        "subjects": [
            "q-bio.QM",
            "cs.LG"
        ],
        "comment": "37 pages, 2 figures, 8 tables, Submitted to Journal of Computational Science"
    },
    {
        "paper id": "2410.05686",
        "abstract url": "https://arxiv.org/abs/2410.05686",
        "title": "Deep Learning and Machine Learning with GPGPU and CUDA: Unlocking the Power of Parallel Computing",
        "rating": "-2",
        "keywords": [
            [
                "FPGA"
            ]
        ],
        "abstract": "This book presents a comprehensive exploration of GPGPU (General Purpose Graphics Processing Unit) and its applications in deep learning and machine learning. It focuses on how parallel computing, particularly through the use of CUDA (Compute Unified Device Architecture), can unlock unprecedented computational power for complex tasks. The book provides detailed discussions on CPU and GPU architectures, data flow in deep learning, and advanced GPU features like streams, concurrency, and dynamic parallelism. Furthermore, it delves into practical applications of GPGPU in various domains such as scientific computing, machine learning acceleration, real-time rendering, and cryptocurrency mining. The authors also emphasize the importance of selecting the right parallel architecture (e.g., GPU, FPGA, TPU, ASIC) based on specific tasks, offering insights into optimizing algorithms for these platforms. The book also provides practical examples with popular machine learning frameworks like PyTorch, TensorFlow, and XGBoost, demonstrating how to efficiently leverage GPU resources in both training and inference. This resource is valuable for both beginners and advanced readers who are looking to deepen their understanding of GPU-based parallel computing and its significant role in modern machine learning and AI applications.",
        "subjects": [
            "cs.DC",
            "cs.AR"
        ],
        "comment": "106 pages"
    },
    {
        "paper id": "2410.05711",
        "abstract url": "https://arxiv.org/abs/2410.05711",
        "title": "Diffusion Auto-regressive Transformer for Effective Self-supervised Time Series Forecasting",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Self-supervised learning has become a popular and effective approach for enhancing time series forecasting, enabling models to learn universal representations from unlabeled data. However, effectively capturing both the global sequence dependence and local detail features within time series data remains challenging. To address this, we propose a novel generative self-supervised method called TimeDART, denoting Diffusion Auto-regressive Transformer for Time series forecasting. In TimeDART, we treat time series patches as basic modeling units. Specifically, we employ an self-attention based Transformer encoder to model the dependencies of inter-patches. Additionally, we introduce diffusion and denoising mechanisms to capture the detail locality features of intra-patch. Notably, we design a cross-attention-based denoising decoder that allows for adjustable optimization difficulty in the self-supervised task, facilitating more effective self-supervised pre-training. Furthermore, the entire model is optimized in an auto-regressive manner to obtain transferable representations. Extensive experiments demonstrate that TimeDART achieves state-of-the-art fine-tuning performance compared to the most advanced competitive methods in forecasting tasks. Our code is publicly available at https://github.com/Melmaphother/TimeDART.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "19 pages, 3 figures, ICLR 2025"
    },
    {
        "paper id": "2410.05717",
        "abstract url": "https://arxiv.org/abs/2410.05717",
        "title": "Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery",
        "rating": "-2",
        "keywords": [
            [
                "navigation"
            ],
            [
                "remote sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This research addresses the need for high-definition (HD) maps for autonomous vehicles (AVs), focusing on road lane information derived from aerial imagery. While Earth observation data offers valuable resources for map creation, specialized models for road lane extraction are still underdeveloped in remote sensing. In this study, we perform an extensive comparison of twelve foundational deep learning-based semantic segmentation models for road lane marking extraction from high-definition remote sensing images, assessing their performance under transfer learning with partially labeled datasets. These models were fine-tuned on the partially labeled Waterloo Urban Scene dataset, and pre-trained on the SkyScapes dataset, simulating a likely scenario of real-life model deployment under partial labeling. We observed and assessed the fine-tuning performance and overall performance. Models showed significant performance improvements after fine-tuning, with mean IoU scores ranging from 33.56% to 76.11%, and recall ranging from 66.0% to 98.96%. Transformer-based models outperformed convolutional neural networks, emphasizing the importance of model pre-training and fine-tuning in enhancing HD map development for AV navigation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05725",
        "abstract url": "https://arxiv.org/abs/2410.05725",
        "title": "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server",
        "rating": "-2",
        "keywords": [
            [
                "federated learning"
            ],
            [
                "medical"
            ],
            [
                "cs.AI"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "The success of large language models (LLMs) facilitate many parties to fine-tune LLMs on their own private data. However, this practice raises privacy concerns due to the memorization of LLMs. Existing solutions, such as utilizing synthetic data for substitution, struggle to simultaneously improve performance and preserve privacy. They either rely on a local model for generation, resulting in a performance decline, or take advantage of APIs, directly exposing the data to API servers. To address this issue, we propose KnowledgeSG, a novel client-server framework which enhances synthetic data quality and improves model performance while ensuring privacy. We achieve this by learning local knowledge from the private data with differential privacy (DP) and distilling professional knowledge from the server. Additionally, inspired by federated learning, we transmit models rather than data between the client and server to prevent privacy leakage. Extensive experiments in medical and financial domains demonstrate the effectiveness of KnowledgeSG. Our code is now publicly available at https://github.com/wwh0411/KnowledgeSG.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": "EMNLP 2024 Main"
    },
    {
        "paper id": "2410.05737",
        "abstract url": "https://arxiv.org/abs/2410.05737",
        "title": "Thrust Microstepping via Acceleration Feedback in Quadrotor Control for Aerial Grasping of Dynamic Payload",
        "rating": "-2",
        "keywords": [
            [
                "UAV"
            ]
        ],
        "abstract": "In this work, we propose an end-to-end Thrust Microstepping and Decoupled Control (TMDC) of quadrotors. TMDC focuses on precise off-centered aerial grasping of payloads dynamically, which are attached rigidly to the UAV body via a gripper contrary to the swinging payload. The dynamic payload grasping quickly changes UAV's mass, inertia etc, causing instability while performing a grasping operation in-air. We identify that to handle unknown payload grasping, the role of thrust controller is crucial. Hence, we focus on thrust control without involving system parameters such as mass etc. TMDC is based on our novel Thrust Microstepping via Acceleration Feedback (TMAF) thrust controller and Decoupled Motion Control (DMC). TMAF precisely estimates the desired thrust even at smaller loop rates while DMC decouples the horizontal and vertical motion to counteract disturbances in the case of dynamic payloads. We prove the controller's efficacy via exhaustive experiments in practically interesting and adverse real-world cases, such as fully onboard state estimation without any positioning sensor, narrow and indoor flying workspaces with intense wind turbulence, heavy payloads, non-uniform loop rates, etc. Our TMDC outperforms recent direct acceleration feedback thrust controller (DA) and geometric tracking control (GT) in flying stably for aerial grasping and achieves RMSE below 0.04m in contrast to 0.15m of DA and 0.16m of GT.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05763",
        "abstract url": "https://arxiv.org/abs/2410.05763",
        "title": "Information Discovery in e-Commerce",
        "rating": "-2",
        "keywords": [
            [
                "graphs"
            ],
            [
                "recommendation"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Electronic commerce, or e-commerce, is the buying and selling of goods and services, or the transmitting of funds or data online. E-commerce platforms come in many kinds, with global players such as Amazon, Airbnb, Alibaba, Booking.com, eBay, JD.com and platforms targeting specific geographic regions such as Bol.com and Flipkart.com.Information retrieval has a natural role to play in e-commerce, especially in connecting people to goods and services. Information discovery in e-commerce concerns different types of search (e.g., exploratory search vs. lookup tasks), recommender systems, and natural language processing in e-commerce portals. The rise in popularity of e-commerce sites has made research on information discovery in e-commerce an increasingly active research area. This is witnessed by an increase in publications and dedicated workshops in this space. Methods for information discovery in e-commerce largely focus on improving the effectiveness of e-commerce search and recommender systems, on enriching and using knowledge graphs to support e-commerce, and on developing innovative question answering and bot-based solutions that help to connect people to goods and services. In this survey, an overview is given of the fundamental infrastructure, algorithms, and technical solutions for information discovery in e-commerce. The topics covered include user behavior and profiling, search, recommendation, and language technology in e-commerce.",
        "subjects": [
            "cs.IR",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05798",
        "abstract url": "https://arxiv.org/abs/2410.05798",
        "title": "Integrating Online Learning and Connectivity Maintenance for Communication-Aware Multi-Robot Coordination",
        "rating": "-2",
        "keywords": [
            [
                "Robot"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "This paper proposes a novel data-driven control strategy for maintaining connectivity in networked multi-robot systems. Existing approaches often rely on a pre-determined communication model specifying whether pairwise robots can communicate given their relative distance to guide the connectivity-aware control design, which may not capture real-world communication conditions. To relax that assumption, we present the concept of Data-driven Connectivity Barrier Certificates, which utilize Control Barrier Functions (CBF) and Gaussian Processes (GP) to characterize the admissible control space for pairwise robots based on communication performance observed online. This allows robots to maintain a satisfying level of pairwise communication quality (measured by the received signal strength) while in motion. Then we propose a Data-driven Connectivity Maintenance (DCM) algorithm that combines (1) online learning of the communication signal strength and (2) a bi-level optimization-based control framework for the robot team to enforce global connectivity of the realistic multi-robot communication graph and minimally deviate from their task-related motions. We provide theoretical proofs to justify the properties of our algorithm and demonstrate its effectiveness through simulations with up to 20 robots.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, accepted to IROS 2024"
    },
    {
        "paper id": "2410.05808",
        "abstract url": "https://arxiv.org/abs/2410.05808",
        "title": "Vision Transformer based Random Walk for Group Re-Identification",
        "rating": "-2",
        "keywords": [
            [
                "depth"
            ],
            [
                "Re-Identification"
            ],
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Group re-identification (re-ID) aims to match groups with the same people under different cameras, mainly involves the challenges of group members and layout changes well. Most existing methods usually use the k-nearest neighbor algorithm to update node features to consider changes in group membership, but these methods cannot solve the problem of group layout changes. To this end, we propose a novel vision transformer based random walk framework for group re-ID. Specifically, we design a vision transformer based on a monocular depth estimation algorithm to construct a graph through the average depth value of pedestrian features to fully consider the impact of camera distance on group members relationships. In addition, we propose a random walk module to reconstruct the graph by calculating affinity scores between target and gallery images to remove pedestrians who do not belong to the current group. Experimental results show that our framework is superior to most methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "6 pages"
    },
    {
        "paper id": "2410.05866",
        "abstract url": "https://arxiv.org/abs/2410.05866",
        "title": "Long-Range Reading of Multiple Chipless Sensors from the Isoline Processing of 3D Radar Images",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Radar"
            ]
        ],
        "abstract": "In this paper, we report the long-range and wireless interrogation of multiple chipless sensors from the isoline processing of three-dimensional polarimetric radar images. A Frequency-Modulated Continuous-Wave Radar operating at 24 GHz is used for the indoor interrogation of four sensors in the basement of a Laboratory. In such cluttered environment, the proposed radar image processing based on isolines computation allows the wireless measurement range of sensors up to 5.8m.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05900",
        "abstract url": "https://arxiv.org/abs/2410.05900",
        "title": "MTFL: Multi-Timescale Feature Learning for Weakly-Supervised Anomaly Detection in Surveillance Videos",
        "rating": "-2",
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "Crime"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Detection of anomaly events is relevant for public safety and requires a combination of fine-grained motion information and contextual events at variable time-scales. To this end, we propose a Multi-Timescale Feature Learning (MTFL) method to enhance the representation of anomaly features. Short, medium, and long temporal tubelets are employed to extract spatio-temporal video features using a Video Swin Transformer. Experimental results demonstrate that MTFL outperforms state-of-the-art methods on the UCF-Crime dataset, achieving an anomaly detection performance 89.78% AUC. Moreover, it performs complementary to SotA with 95.32% AUC on the ShanghaiTech and 84.57% AP on the XD-Violence dataset. Furthermore, we generate an extended dataset of the UCF-Crime for development and evaluation on a wider range of anomalies, namely Video Anomaly Detection Dataset (VADD), involving 2,591 videos in 18 classes with extensive coverage of realistic anomalies.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05905",
        "abstract url": "https://arxiv.org/abs/2410.05905",
        "title": "MedUniSeg: 2D and 3D Medical Image Segmentation via a Prompt-driven Universal Model",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Universal segmentation models offer significant potential in addressing a wide range of tasks by effectively leveraging discrete annotations. As the scope of tasks and modalities expands, it becomes increasingly important to generate and strategically position task- and modal-specific priors within the universal model. However, existing universal models often overlook the correlations between different priors, and the optimal placement and frequency of these priors remain underexplored. In this paper, we introduce MedUniSeg, a prompt-driven universal segmentation model designed for 2D and 3D multi-task segmentation across diverse modalities and domains. MedUniSeg employs multiple modal-specific prompts alongside a universal task prompt to accurately characterize the modalities and tasks. To generate the related priors, we propose the modal map (MMap) and the fusion and selection (FUSE) modules, which transform modal and task prompts into corresponding priors. These modal and task priors are systematically introduced at the start and end of the encoding process. We evaluate MedUniSeg on a comprehensive multi-modal upstream dataset consisting of 17 sub-datasets. The results demonstrate that MedUniSeg achieves superior multi-task segmentation performance, attaining a 1.2% improvement in the mean Dice score across the 17 upstream tasks compared to nnUNet baselines, while using less than 1/10 of the parameters. For tasks that underperform during the initial multi-task joint training, we freeze MedUniSeg and introduce new modules to re-learn these tasks. This approach yields an enhanced version, MedUniSeg*, which consistently outperforms MedUniSeg across all tasks. Moreover, MedUniSeg surpasses advanced self-supervised and supervised pre-trained models on six downstream tasks, establishing itself as a high-quality, highly generalizable pre-trained segmentation model.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05939",
        "abstract url": "https://arxiv.org/abs/2410.05939",
        "title": "RLRF4Rec: Reinforcement Learning from Recsys Feedback for Enhanced Recommendation Reranking",
        "rating": "-2",
        "keywords": [
            [
                "Recommendation"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains, prompting researchers to explore their potential for use in recommendation systems. Initial attempts have leveraged the exceptional capabilities of LLMs, such as rich knowledge and strong generalization through In-context Learning, which involves phrasing the recommendation task as prompts. Nevertheless, the performance of LLMs in recommendation tasks remains suboptimal due to a substantial disparity between the training tasks for LLMs and recommendation tasks and inadequate recommendation data during pre-training. This paper introduces RLRF4Rec, a novel framework integrating Reinforcement Learning from Recsys Feedback for Enhanced Recommendation Reranking(RLRF4Rec) with LLMs to address these challenges. Specifically, We first have the LLM generate inferred user preferences based on user interaction history, which is then used to augment traditional ID-based sequence recommendation models. Subsequently, we trained a reward model based on knowledge augmentation recommendation models to evaluate the quality of the reasoning knowledge from LLM. We then select the best and worst responses from the N samples to construct a dataset for LLM tuning. Finally, we design a structure alignment strategy with Direct Preference Optimization(DPO). We validate the effectiveness of RLRF4Rec through extensive experiments, demonstrating significant improvements in recommendation re-ranking metrics compared to baselines. This demonstrates that our approach significantly improves the capability of LLMs to respond to instructions within recommender systems.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05992",
        "abstract url": "https://arxiv.org/abs/2410.05992",
        "title": "Linking Code and Documentation Churn: Preliminary Analysis",
        "rating": "-2",
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "Code churn refers to the measure of the amount of code added, modified, or deleted in a project and is often used to assess codebase stability and maintainability. Program comprehension or how understandable the changes are, is equally important for maintainability. Documentation is crucial for knowledge transfer, especially when new maintainers take over abandoned code. We emphasize the need for corresponding documentation updates, as this reflects project health and trustworthiness as a third-party library. Therefore, we argue that every code change should prompt a documentation update (defined as documentation churn). Linking code churn changes with documentation updates is important for project sustainability, as it facilitates knowledge transfer and reduces the effort required for program comprehension. This study investigates the synchrony between code churn and documentation updates in three GitHub open-source projects. We will use qualitative analysis and repository mining to examine the alignment and correlation of code churn and documentation updates over time. We want to identify which code changes are likely synchronized with documentation and to what extent documentation can be auto-generated. Preliminary results indicate varying degrees of synchrony across projects, highlighting the importance of integrated concurrent documentation practices and providing insights into how recent technologies like AI, in the form of Large Language Models (i.e., LLMs), could be leveraged to keep code and documentation churn in sync. The novelty of this study lies in demonstrating how synchronizing code changes with documentation updates can improve the development lifecycle by enhancing diversity and efficiency.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06004",
        "abstract url": "https://arxiv.org/abs/2410.06004",
        "title": "Corrections to \"Computer Vision Aided mmWave Beam Alignment in V2X Communications\"",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "vehicle"
            ]
        ],
        "abstract": "In this document, we revise the results of [1] based on more reasonable assumptions regarding data shuffling and parameter setup of deep neural networks (DNNs). Thus, the simulation results can now more reasonably demonstrate the performance of both the proposed and compared beam alignment methods. We revise the simulation steps and make moderate modifications to the design of the vehicle distribution feature (VDF) for the proposed vision based beam alignment when the MS location is available (VBALA). Specifically, we replace the 2D grids of the VDF with 3D grids and utilize the vehicle locations to expand the dimensions of the VDF. Then, we revise the simulation results of Fig. 11, Fig. 12, Fig. 13, Fig. 14, and Fig. 15 in [1] to reaffirm the validity of the conclusions.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "3 pages, 6 figures"
    },
    {
        "paper id": "2410.06011",
        "abstract url": "https://arxiv.org/abs/2410.06011",
        "title": "Large Language Model Enhanced Text-to-SQL Generation: A Survey",
        "rating": "-2",
        "keywords": [
            [
                "SQL"
            ]
        ],
        "abstract": "Text-to-SQL translates natural language queries into Structured Query Language (SQL) commands, enabling users to interact with databases using natural language. Essentially, the text-to-SQL task is a text generation task, and its development is primarily dependent on changes in language models. Especially with the rapid development of Large Language Models (LLMs), the pattern of text-to-SQL has undergone significant changes. Existing survey work mainly focuses on rule-based and neural-based approaches, but it still lacks a survey of Text-to-SQL with LLMs. In this paper, we survey the large language model enhanced text-to-SQL generations, classifying them into prompt engineering, fine-tuning, pre-trained, and Agent groups according to training strategies. We also summarize datasets and evaluation metrics comprehensively. This survey could help people better understand the pattern, research status, and challenges of LLM-based text-to-SQL generations.",
        "subjects": [
            "cs.DB"
        ],
        "comment": "14 pages, 2 figures"
    },
    {
        "paper id": "2410.06026",
        "abstract url": "https://arxiv.org/abs/2410.06026",
        "title": "Content-based Wake-up for Energy-efficient and Timely Top-k IoT Sensing Data Retrieval",
        "rating": "-2",
        "keywords": [
            [
                "Industrial",
                "IoT"
            ]
        ],
        "abstract": "Energy efficiency and information freshness are key requirements for sensor nodes serving Industrial Internet of Things (IIoT) applications, where a sink node collects informative and fresh data before a deadline, e.g., to control an external actuator. Content-based wake-up (CoWu) activates a subset of nodes that hold data relevant for the sink's goal, thereby offering an energy-efficient way to attain objectives related to information freshness. This paper focuses on a scenario where the sink collects fresh information on top-k values, defined as data from the nodes observing the k highest readings at the deadline. We introduce a new metric called top-k Query Age of Information (k-QAoI), which allows us to characterize the performance of CoWu by considering the characteristics of the physical process. Further, we show how to select the CoWu parameters, such as its timing and threshold, to attain both information freshness and energy efficiency. The numerical results reveal the effectiveness of the CoWu approach, which is able to collect top-k data with higher energy efficiency while reducing k-QAoI when compared to round-robin scheduling, especially when the number of nodes is large and the required size of k is small.",
        "subjects": [
            "cs.NI",
            "eess.SP"
        ],
        "comment": "Submitted for IEEE Transactions on Communications"
    },
    {
        "paper id": "2410.06029",
        "abstract url": "https://arxiv.org/abs/2410.06029",
        "title": "Unclonable Functional Encryption",
        "rating": "-2",
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "In a functional encryption (FE) scheme, a user that holds a ciphertext and a function-key can learn the result of applying the function to the plaintext message. Security requires that the user does not learn anything beyond the function evaluation. On the other hand, unclonable encryption (UE) is a uniquely quantum primitive, which ensures that an adversary cannot duplicate a ciphertext to decrypt the same message multiple times. In this work we introduce unclonable quantum functional encryption (UFE), which both extends the notion of FE to the quantum setting and also possesses the unclonable security of UE. We give a construction for UFE that supports arbitrary quantum messages and polynomialy-sized circuit, and achieves unclonable-indistinguishable security for independently sampled function keys. In particular, our UFE guarantees that two parties cannot simultaneously recover the correct function outputs using two independently sampled function keys. Our construction combines quantum garbled circuits [BY22], and quantum-key unclonable encryption [AKY24], and leverages techniques from the plaintext expansion arguments in [Hir+23]. As an application we give the first construction for public-key UE with variable decryption keys. Lastly, we establish a connection between quantum indistinguishability obfuscation (qiO) and quantum functional encryption (QFE); Showing that any multi-input indistinguishability-secure quantum functional encryption scheme unconditionally implies the existence of qiO.",
        "subjects": [
            "quant-ph",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06041",
        "abstract url": "https://arxiv.org/abs/2410.06041",
        "title": "Block Induced Signature Generative Adversarial Network (BISGAN): Signature Spoofing Using GANs and Their Evaluation",
        "rating": "-2",
        "keywords": [
            [
                "GAN"
            ],
            [
                "biometrics"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Deep learning is actively being used in biometrics to develop efficient identification and verification systems. Handwritten signatures are a common subset of biometric data for authentication purposes. Generative adversarial networks (GANs) learn from original and forged signatures to generate forged signatures. While most GAN techniques create a strong signature verifier, which is the discriminator, there is a need to focus more on the quality of forgeries generated by the generator model. This work focuses on creating a generator that produces forged samples that achieve a benchmark in spoofing signature verification systems. We use CycleGANs infused with Inception model-like blocks with attention heads as the generator and a variation of the SigCNN model as the base Discriminator. We train our model with a new technique that results in 80% to 100% success in signature spoofing. Additionally, we create a custom evaluation technique to act as a goodness measure of the generated forgeries. Our work advocates generator-focused GAN architectures for spoofing data quality that aid in a better understanding of biometric data generation and evaluation.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06080",
        "abstract url": "https://arxiv.org/abs/2410.06080",
        "title": "Packing a Knapsack with Items Owned by Strategic Agents",
        "rating": "-2",
        "keywords": [
            [
                "MRI"
            ]
        ],
        "abstract": "This paper considers a scenario within the field of mechanism design without money where a mechanism designer is interested in selecting items with maximum total value under a knapsack constraint. The items, however, are controlled by strategic agents who aim to maximize the total value of their items in the knapsack. This is a natural setting, e.g., when agencies select projects for funding, companies select products for sale in their shops, or hospitals schedule MRI scans for the day. A mechanism governing the packing of the knapsack is strategyproof if no agent can benefit from hiding items controlled by them to the mechanism. We are interested in mechanisms that are strategyproof and $\u03b1$-approximate in the sense that they always approximate the maximum value of the knapsack by a factor of $\u03b1\\in [0,1]$. First, we give a deterministic mechanism that is $\\frac{1}{3}$-approximate. For the special case where all items have unit density, we design a $\\frac{1}\u03c6$-approximate mechanism where $1/\u03c6\\approx 0.618$ is the inverse of the golden ratio. This result is tight as we show that no deterministic strategyproof mechanism with a better approximation exists. We further give randomized mechanisms with approximation guarantees of $1/2$ for the general case and $2/3$ for the case of unit densities. For both cases, no strategyproof mechanism can achieve an approximation guarantee better than $1/(5\u03c6-7)\\approx 0.917$.",
        "subjects": [
            "cs.GT",
            "econ.TH",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06094",
        "abstract url": "https://arxiv.org/abs/2410.06094",
        "title": "Listen to the Patient: Enhancing Medical Dialogue Generation with Patient Hallucination Detection and Mitigation",
        "rating": "-2",
        "keywords": [
            [
                "graph"
            ],
            [
                "Medical",
                "health"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Medical dialogue systems aim to provide medical services through patient-agent conversations. Previous methods typically regard patients as ideal users, focusing mainly on common challenges in dialogue systems, while neglecting the potential biases or misconceptions that might be introduced by real patients, who are typically non-experts. This study investigates the discrepancy between patients' expressions during medical consultations and their actual health conditions, defined as patient hallucination. Such phenomena often arise from patients' lack of knowledge and comprehension, concerns, and anxieties, resulting in the transmission of inaccurate or wrong information during consultations. To address this issue, we propose MedPH, a Medical dialogue generation method for mitigating the problem of Patient Hallucinations designed to detect and cope with hallucinations. MedPH incorporates a detection method that utilizes one-dimensional structural entropy over a temporal dialogue entity graph, and a mitigation strategy based on hallucination-related information to guide patients in expressing their actual conditions. Experimental results indicate the high effectiveness of MedPH when compared to existing approaches in both medical entity prediction and response generation tasks, while also demonstrating its effectiveness in mitigating hallucinations within interactive scenarios.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06114",
        "abstract url": "https://arxiv.org/abs/2410.06114",
        "title": "UnSeGArmaNet: Unsupervised Image Segmentation using Graph Neural Networks with Convolutional ARMA Filters",
        "rating": "-2",
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The data-hungry approach of supervised classification drives the interest of the researchers toward unsupervised approaches, especially for problems such as medical image segmentation, where labeled data are difficult to get. Motivated by the recent success of Vision transformers (ViT) in various computer vision tasks, we propose an unsupervised segmentation framework with a pre-trained ViT. Moreover, by harnessing the graph structure inherent within the image, the proposed method achieves a notable performance in segmentation, especially in medical images. We further introduce a modularity-based loss function coupled with an Auto-Regressive Moving Average (ARMA) filter to capture the inherent graph topology within the image. Finally, we observe that employing Scaled Exponential Linear Unit (SELU) and SILU (Swish) activation functions within the proposed Graph Neural Network (GNN) architecture enhances the performance of segmentation. The proposed method provides state-of-the-art performance (even comparable to supervised methods) on benchmark image segmentation datasets such as ECSSD, DUTS, and CUB, as well as challenging medical image segmentation datasets such as KVASIR, CVC-ClinicDB, ISIC-2018. The github repository of the code is available on \\url{https://github.com/ksgr5566/UnSeGArmaNet}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at BMVC-2024. arXiv admin note: text overlap with arXiv:2405.06057"
    },
    {
        "paper id": "2410.06115",
        "abstract url": "https://arxiv.org/abs/2410.06115",
        "title": "A physics-based perspective for understanding and utilizing spatial resources of wireless channels",
        "rating": "-2",
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "To satisfy the increasing demands for transmission rates of wireless communications, it is necessary to use spatial resources of electromagnetic (EM) waves. In this context, EM information theory (EIT) has become a hot topic by integrating the theoretical framework of deterministic mathematics and stochastic statistics to explore the transmission mechanisms of continuous EM waves. However, the previous studies were primarily focused on frame analysis, with limited exploration of practical applications and a comprehensive understanding of its essential physical characteristics. In this paper, we present a three-dimensional (3-D) line-of-sight channel capacity formula that captures the vector EM physics and accommodates both near- and far-field scenes. Based on the rigorous mathematical equation and the physical mechanism of fast multipole expansion, a channel model is established, and the finite angular spectral bandwidth feature of scattered waves is revealed. To adapt to the feature of the channel, an optimization problem is formulated for determining the mode currents on the transmitter, aiming to obtain the optimal design of the precoder and combiner. We make comprehensive analyses to investigate the relationship among the spatial degree of freedom, noise, and transmitted power, thereby establishing a rigorous upper bound of channel capacity. A series of simulations are conducted to validate the theoretical model and numerical method. This work offers a novel perspective and methodology for understanding and leveraging EIT, and provides a theoretical foundation for the design and optimization of future wireless communications.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "31pages, 8 figures"
    },
    {
        "paper id": "2410.06149",
        "abstract url": "https://arxiv.org/abs/2410.06149",
        "title": "Toward Scalable Image Feature Compression: A Content-Adaptive and Diffusion-Based Approach",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "facial"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Traditional image codecs emphasize signal fidelity and human perception, often at the expense of machine vision tasks. Deep learning methods have demonstrated promising coding performance by utilizing rich semantic embeddings optimized for both human and machine vision. However, these compact embeddings struggle to capture fine details such as contours and textures, resulting in imperfect reconstructions. Furthermore, existing learning-based codecs lack scalability. To address these limitations, this paper introduces a content-adaptive diffusion model for scalable image compression. The proposed method encodes fine textures through a diffusion process, enhancing perceptual quality while preserving essential features for machine vision tasks. The approach employs a Markov palette diffusion model combined with widely used feature extractors and image generators, enabling efficient data compression. By leveraging collaborative texture-semantic feature extraction and pseudo-label generation, the method accurately captures texture information. A content-adaptive Markov palette diffusion model is then applied to represent both low-level textures and high-level semantic content in a scalable manner. This framework offers flexible control over compression ratios by selecting intermediate diffusion states, eliminating the need for retraining deep learning models at different operating points. Extensive experiments demonstrate the effectiveness of the proposed framework in both image reconstruction and downstream machine vision tasks such as object detection, segmentation, and facial landmark detection, achieving superior perceptual quality compared to state-of-the-art methods.",
        "subjects": [
            "cs.CV",
            "cs.MM",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06224",
        "abstract url": "https://arxiv.org/abs/2410.06224",
        "title": "The Fast M\u00f6bius Transform: An algebraic approach to information decomposition",
        "rating": "-2",
        "keywords": [
            [
                "music"
            ]
        ],
        "abstract": "The partial information decomposition (PID) and its extension integrated information decomposition ($\u03a6$ID) are promising frameworks to investigate information phenomena involving multiple variables. An important limitation of these approaches is the high computational cost involved in their calculation. Here we leverage fundamental algebraic properties of these decompositions to enable a computationally-efficient method to estimate them, which we call the fast M\u00f6bius transform. Our approach is based on a novel formula for estimating the M\u00f6bius function that circumvents important computational bottlenecks. We showcase the capabilities of this approach by presenting two analyses that would be unfeasible without this method: decomposing the information that neural activity at different frequency bands yield about the brain's macroscopic functional organisation, and identifying distinctive dynamical properties of the interactions between multiple voices in baroque music. Overall, our proposed approach illuminates the value of algebraic facets of information decomposition and opens the way to a wide range of future analyses.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "15 pages, 6 figures, 2 tables"
    },
    {
        "paper id": "2410.06263",
        "abstract url": "https://arxiv.org/abs/2410.06263",
        "title": "BoxMap: Efficient Structural Mapping and Navigation",
        "rating": "-2",
        "keywords": [
            [
                "robot",
                "Navigation"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "While humans can successfully navigate using abstractions, ignoring details that are irrelevant to the task at hand, most existing robotic applications require the maintenance of a detailed environment representation which consumes a significant amount of sensing, computing, and storage. These issues are particularly important in a resource-constrained setting with limited power budget. Deep learning methods can learn from prior experience to abstract knowledge of unknown environments, and use it to execute tasks (e.g., frontier exploration, object search, or scene understanding) more efficiently. We propose BoxMap, a Detection-Transformer-based architecture that takes advantage of the structure of the sensed partial environment to update a topological graph of the environment as a set of semantic entities (e.g. rooms and doors) and their relations (e.g. connectivity). These predictions from low-level measurements can then be leveraged to achieve high-level goals with lower computational costs than methods based on detailed representations. As an example application, we consider a robot equipped with a 2-D laser scanner tasked with exploring a residential building. Our BoxMap representation scales quadratically with the number of rooms (with a small constant), resulting in significant savings over a full geometric map. Moreover, our high-level topological representation results in 30.9% shorter trajectories in the exploration task with respect to a standard method.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "This manuscript has been submitted to IEEE ICRA 2025"
    },
    {
        "paper id": "2410.06336",
        "abstract url": "https://arxiv.org/abs/2410.06336",
        "title": "Exploring Large Language Models Through a Neurodivergent Lens: Use, Challenges, Community-Driven Workarounds, and Concerns",
        "rating": "-2",
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "Despite the increasing use of large language models (LLMs) in everyday life among neurodivergent individuals, our knowledge of how they engage with, and perceive LLMs remains limited. In this study, we investigate how neurodivergent individuals interact with LLMs by qualitatively analyzing topically related discussions from 61 neurodivergent communities on Reddit. Our findings reveal 20 specific LLM use cases across five core thematic areas of use among neurodivergent users: emotional well-being, mental health support, interpersonal communication, learning, and professional development and productivity. We also identified key challenges, including overly neurotypical LLM responses and the limitations of text-based interactions. In response to such challenges, some users actively seek advice by sharing input prompts and corresponding LLM responses. Others develop workarounds by experimenting and modifying prompts to be more neurodivergent-friendly. Despite these efforts, users have significant concerns around LLM use, including potential overreliance and fear of replacing human connections. Our analysis highlights the need to make LLMs more inclusive for neurodivergent users and implications around how LLM technologies can reinforce unintended consequences and behaviors.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "29 pages with 1 figure"
    },
    {
        "paper id": "2410.06429",
        "abstract url": "https://arxiv.org/abs/2410.06429",
        "title": "A QUBO Formulation for the Generalized LinkedIn Queens Game",
        "rating": "-2",
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "In this paper, we present a QUBO formulation designed to solve a series of generalisations of the LinkedIn queens game, a version of the N-queens problem. We adapt this formulation for several particular cases of the problem by trying to optimise the number of variables and interactions, improving the possibility of applying it on quantum hardware by means of Quantum Annealing or the Quantum Approximated Optimization Algorithm (QAOA). We also present two new types of problems, the Coloured Chess Piece Problem and the Max Chess Pieces Problem, with their corresponding QUBO formulations.",
        "subjects": [
            "quant-ph",
            "cs.ET",
            "physics.pop-ph"
        ],
        "comment": "9 pages, 4 figures"
    },
    {
        "paper id": "2410.06469",
        "abstract url": "https://arxiv.org/abs/2410.06469",
        "title": "Hybrid Fusion for Battery Degradation Diagnostics Using Minimal Real-World Data: Bridging Laboratory and Practical Applications",
        "rating": "-2",
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "Unpredictability of battery lifetime has been a key stumbling block to technology advancement of safety-critical systems such as electric vehicles and stationary energy storage systems. In this work, we present a novel hybrid fusion strategy that combines physics-based and data-driven approaches to accurately predict battery capacity. This strategy achieves an average estimation error of only 0.63% over the entire battery lifespan, utilizing merely 45 real-world data segments along with over 1.7 million simulated data segments derived from random partial charging cycles. By leveraging a thoroughly validated physics-based battery model, we extract typical aging patterns from laboratory aging data and extend them into a more comprehensive parameter space, encompassing diverse battery aging states in potential real-world applications while accounting for practical cell-to-cell variations. By bridging the gap between controlled laboratory experiments and real-world usage scenarios, this method highlights the significant potential of transferring underlying knowledge from high-fidelity physics-based models to data-driven models for predicting the behavior of complex dynamical systems.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06478",
        "abstract url": "https://arxiv.org/abs/2410.06478",
        "title": "MaskBlur: Spatial and Angular Data Augmentation for Light Field Image Super-Resolution",
        "rating": "-2",
        "keywords": [
            [
                "Super-Resolution"
            ],
            [
                "low-light enhancement"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Data augmentation (DA) is an effective approach for enhancing model performance with limited data, such as light field (LF) image super-resolution (SR). LF images inherently possess rich spatial and angular information. Nonetheless, there is a scarcity of DA methodologies explicitly tailored for LF images, and existing works tend to concentrate solely on either the spatial or angular domain. This paper proposes a novel spatial and angular DA strategy named MaskBlur for LF image SR by concurrently addressing spatial and angular aspects. MaskBlur consists of spatial blur and angular dropout two components. Spatial blur is governed by a spatial mask, which controls where pixels are blurred, i.e., pasting pixels between the low-resolution and high-resolution domains. The angular mask is responsible for angular dropout, i.e., selecting which views to perform the spatial blur operation. By doing so, MaskBlur enables the model to treat pixels differently in the spatial and angular domains when super-resolving LF images rather than blindly treating all pixels equally. Extensive experiments demonstrate the efficacy of MaskBlur in significantly enhancing the performance of existing SR methods. We further extend MaskBlur to other LF image tasks such as denoising, deblurring, low-light enhancement, and real-world SR. Code is publicly available at \\url{https://github.com/chaowentao/MaskBlur}.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "accepted by IEEE Transactions on Multimedia"
    },
    {
        "paper id": "2410.06492",
        "abstract url": "https://arxiv.org/abs/2410.06492",
        "title": "Overcoming Autoware-Ubuntu Incompatibility in Autonomous Driving Systems-Equipped Vehicles: Lessons Learned",
        "rating": "-2",
        "keywords": [
            [
                "Autonomous Driving",
                "vehicle"
            ],
            [
                "Robot"
            ]
        ],
        "abstract": "Autonomous vehicles have been rapidly developed as demand that provides safety and efficiency in transportation systems. As autonomous vehicles are designed based on open-source operating and computing systems, there are numerous resources aimed at building an operating platform composed of Ubuntu, Autoware, and Robot Operating System (ROS). However, no explicit guidelines exist to help scholars perform trouble-shooting due to incompatibility between the Autoware platform and Ubuntu operating systems installed in autonomous driving systems-equipped vehicles (i.e., Chrysler Pacifica). The paper presents an overview of integrating the Autoware platform into the autonomous vehicle's interface based on lessons learned from trouble-shooting processes for resolving incompatible issues. The trouble-shooting processes are presented based on resolving the incompatibility and integration issues of Ubuntu 20.04, Autoware.AI, and ROS Noetic software installed in an autonomous driving systems-equipped vehicle. Specifically, the paper focused on common incompatibility issues and code-solving protocols involving Python compatibility, Compute Unified Device Architecture (CUDA) installation, Autoware installation, and simulation in Autoware.AI. The objective of the paper is to provide an explicit and detail-oriented presentation to showcase how to address incompatibility issues among an autonomous vehicle's operating interference. The lessons and experience presented in the paper will be useful for researchers who encountered similar issues and could follow up by performing trouble-shooting activities and implementing ADS-related projects in the Ubuntu, Autoware, and ROS operating systems.",
        "subjects": [
            "cs.RO",
            "cs.OS",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06504",
        "abstract url": "https://arxiv.org/abs/2410.06504",
        "title": "Transformer-assisted Parametric CSI Feedback for mmWave Massive MIMO Systems",
        "rating": "-2",
        "keywords": [
            [
                "5G",
                "6G"
            ]
        ],
        "abstract": "As a key technology to meet the ever-increasing data rate demand in beyond 5G and 6G communications, millimeter-wave (mmWave) massive multiple-input multiple-output (MIMO) systems have gained much attention recently.To make the most of mmWave massive MIMO systems, acquisition of accurate channel state information (CSI) at the base station (BS) is crucial. However, this task is by no means easy due to the CSI feedback overhead induced by the large number of antennas. In this paper, we propose a parametric CSI feedback technique for mmWave massive MIMO systems. Key idea of the proposed technique is to compress the mmWave MIMO channel matrix into a few geometric channel parameters (e.g., angles, delays, and path gains). Due to the limited scattering of mmWave signal, the number of channel parameters is much smaller than the number of antennas, thereby reducing the CSI feedback overhead significantly. Moreover, by exploiting the deep learning (DL) technique for the channel parameter extraction and the MIMO channel reconstruction, we can effectively suppress the channel quantization error. From the numerical results, we demonstrate that the proposed technique outperforms the conventional CSI feedback techniques in terms of normalized mean square error (NMSE) and bit error rate (BER).",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "14 pages, 13 figures, accepted to IEEE Transactions on Wireless Communications"
    },
    {
        "paper id": "2410.06533",
        "abstract url": "https://arxiv.org/abs/2410.06533",
        "title": "OpenEarable ExG: Open-Source Hardware for Ear-Based Biopotential Sensing Applications",
        "rating": "-2",
        "keywords": [
            [
                "Biopotential",
                "EEG",
                "facial"
            ]
        ],
        "abstract": "While traditional earphones primarily offer private audio spaces, so-called \"earables\" emerged to offer a variety of sensing capabilities. Pioneering platforms like OpenEarable have introduced novel sensing platforms targeted at the ears, incorporating various sensors. The proximity of the ears to the eyes, brain, and facial muscles has also sparked investigation into sensing biopotentials. However, currently there is no platform available that is targeted at the ears to sense biopotentials. To address this gap, we introduce OpenEarable ExG - an open-source hardware platform designed to measure biopotentials in and around the ears. OpenEarable ExG can be freely configured and has up to 7 sensing channels. We initially validate OpenEarable ExG in a study with a left-right in-ear dual-electrode montage setup with 3 participants. Our results demonstrate the successful detection of smooth pursuit eye movements via Electrooculography (EOG), alpha brain activity via Electroencephalography (EEG), and jaw clenching via Electromyography (EMG). OpenEarable ExG is part of the OpenEarable initiative and is fully open-source under MIT license.",
        "subjects": [
            "cs.AR"
        ],
        "comment": "5 pages, 3 figures"
    },
    {
        "paper id": "2410.06536",
        "abstract url": "https://arxiv.org/abs/2410.06536",
        "title": "Learning Recommender Systems with Soft Target: A Decoupled Perspective",
        "rating": "-2",
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "Learning recommender systems with multi-class optimization objective is a prevalent setting in recommendation. However, as observed user feedback often accounts for a tiny fraction of the entire item pool, the standard Softmax loss tends to ignore the difference between potential positive feedback and truly negative feedback. To address this challenge, we propose a novel decoupled soft label optimization framework to consider the objectives as two aspects by leveraging soft labels, including target confidence and the latent interest distribution of non-target items. Futhermore, based on our carefully theoretical analysis, we design a decoupled loss function to flexibly adjust the importance of these two aspects. To maximize the performance of the proposed method, we additionally present a sensible soft-label generation algorithm that models a label propagation algorithm to explore users' latent interests in unobserved feedback via neighbors. We conduct extensive experiments on various recommendation system models and public datasets, the results demonstrate the effectiveness and generality of the proposed method.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "Accepted by DASFAA 2024"
    },
    {
        "paper id": "2410.06542",
        "abstract url": "https://arxiv.org/abs/2410.06542",
        "title": "MedImageInsight: An Open-Source Embedding Model for General Domain Medical Imaging",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Medical",
                "MRI",
                "CT",
                "X-Ray",
                "disease",
                "clinical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "In this work, we present MedImageInsight, an open-source medical imaging embedding model. MedImageInsight is trained on medical images with associated text and labels across a diverse collection of domains, including X-Ray, CT, MRI, dermoscopy, OCT, fundus photography, ultrasound, histopathology, and mammography. Rigorous evaluations demonstrate MedImageInsight's ability to achieve state-of-the-art (SOTA) or human expert level performance across classification, image-image search, and fine-tuning tasks. Specifically, on public datasets, MedImageInsight achieves SOTA in CT 3D medical image retrieval, as well as SOTA in disease classification and search for chest X-ray, dermatology, and OCT imaging. Furthermore, MedImageInsight achieves human expert performance in bone age estimation (on both public and partner data), as well as AUC above 0.9 in most other domains. When paired with a text decoder, MedImageInsight achieves near SOTA level single image report findings generation with less than 10\\% the parameters of other models. Compared to fine-tuning GPT-4o with only MIMIC-CXR data for the same task, MedImageInsight outperforms in clinical metrics, but underperforms on lexical metrics where GPT-4o sets a new SOTA. Importantly for regulatory purposes, MedImageInsight can generate ROC curves, adjust sensitivity and specificity based on clinical need, and provide evidence-based decision support through image-image search (which can also enable retrieval augmented generation). In an independent clinical evaluation of image-image search in chest X-ray, MedImageInsight outperformed every other publicly available foundation model evaluated by large margins (over 6 points AUC), and significantly outperformed other models in terms of AI fairness (across age and gender). We hope releasing MedImageInsight will help enhance collective progress in medical imaging AI research and development.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06543",
        "abstract url": "https://arxiv.org/abs/2410.06543",
        "title": "Gumbel Rao Monte Carlo based Bi-Modal Neural Architecture Search for Audio-Visual Deepfake Detection",
        "rating": "-2",
        "keywords": [
            [
                "Audio-Visual"
            ],
            [
                "Architecture Search"
            ],
            [
                "Deepfake"
            ],
            [
                "biometric"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Deepfakes pose a critical threat to biometric authentication systems by generating highly realistic synthetic media. Existing multimodal deepfake detectors often struggle to adapt to diverse data and rely on simple fusion methods. To address these challenges, we propose Gumbel-Rao Monte Carlo Bi-modal Neural Architecture Search (GRMC-BMNAS), a novel architecture search framework that employs Gumbel-Rao Monte Carlo sampling to optimize multimodal fusion. It refines the Straight through Gumbel Softmax (STGS) method by reducing variance with Rao-Blackwellization, stabilizing network training. Using a two-level search approach, the framework optimizes the network architecture, parameters, and performance. Crucial features are efficiently identified from backbone networks, while within the cell structure, a weighted fusion operation integrates information from various sources. By varying parameters such as temperature and number of Monte carlo samples yields an architecture that maximizes classification performance and better generalisation capability. Experimental results on the FakeAVCeleb and SWAN-DF datasets demonstrate an impressive AUC percentage of 95.4\\%, achieved with minimal model parameters.",
        "subjects": [
            "cs.CR",
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06544",
        "abstract url": "https://arxiv.org/abs/2410.06544",
        "title": "SRC-gAudio: Sampling-Rate-Controlled Audio Generation",
        "rating": "-2",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "text-to-audio"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "We introduce SRC-gAudio, a novel audio generation model designed to facilitate text-to-audio generation across a wide range of sampling rates within a single model architecture. SRC-gAudio incorporates the sampling rate as part of the generation condition to guide the diffusion-based audio generation process. Our model enables the generation of audio at multiple sampling rates with a single unified model. Furthermore, we explore the potential benefits of large-scale, low-sampling-rate data in enhancing the generation quality of high-sampling-rate audio. Through extensive experiments, we demonstrate that SRC-gAudio effectively generates audio under controlled sampling rates. Additionally, our results indicate that pre-training on low-sampling-rate data can lead to significant improvements in audio quality across various metrics.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": "Accepted by APSIPA2024"
    },
    {
        "paper id": "2410.07264",
        "abstract url": "https://arxiv.org/abs/2410.07264",
        "title": "First experimental study of multiple orientation muon tomography, with image optimization in sparse data environments",
        "rating": "-2",
        "keywords": [
            [
                "depth"
            ],
            [
                "CT"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Due to the high penetrating power of cosmic ray muons, they can be used to probe very thick and dense objects. As charged particles, they can be tracked by ionization detectors, determining the position and direction of the muons. With detectors on either side of an object, particle direction changes can be used to extract scattering information within an object. This can be used to produce a scattering intensity image within the object related to density and atomic number. Such imaging is typically performed with a single detector-object orientation, taking advantage of the more intense downward flux of muons, producing planar imaging with some depth-of-field information in the third dimension. Several simulation studies have been published with multi-orientation tomography, which can form a three-dimensional representation faster than a single orientation view. In this work we present the first experimental multiple orientation muon tomography study. Experimental muon-scatter based tomography was performed using a concrete filled steel drum with several different metal wedges inside, between detector planes. Data was collected from different detector-object orientations by rotating the steel drum. The data collected from each orientation were then combined using two different tomographic methods. Results showed that using a combination of multiple depth-of-field reconstructions, rather than a traditional inverse Radon transform approach used for CT, resulted in more useful images for sparser data. As cosmic ray muon flux imaging is rate limited, the imaging techniques were compared for sparse data. Using the combined depth-of-field reconstruction technique, fewer detector-object orientations were needed to reconstruct images that could be used to differentiate the metal wedge compositions.",
        "subjects": [
            "eess.IV",
            "nucl-ex",
            "physics.app-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07268",
        "abstract url": "https://arxiv.org/abs/2410.07268",
        "title": "Learning Content-Aware Multi-Modal Joint Input Pruning via Bird's-Eye-View Representation",
        "rating": "-2",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "BEV"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "In the landscape of autonomous driving, Bird's-Eye-View (BEV) representation has recently garnered substantial academic attention, serving as a transformative framework for the fusion of multi-modal sensor inputs. This BEV paradigm effectively shifts the sensor fusion challenge from a rule-based methodology to a data-centric approach, thereby facilitating more nuanced feature extraction from an array of heterogeneous sensors. Notwithstanding its evident merits, the computational overhead associated with BEV-based techniques often mandates high-capacity hardware infrastructures, thus posing challenges for practical, real-world implementations. To mitigate this limitation, we introduce a novel content-aware multi-modal joint input pruning technique. Our method leverages BEV as a shared anchor to algorithmically identify and eliminate non-essential sensor regions prior to their introduction into the perception model's backbone. We validatethe efficacy of our approach through extensive experiments on the NuScenes dataset, demonstrating substantial computational efficiency without sacrificing perception accuracy. To the best of our knowledge, this work represents the first attempt to alleviate the computational burden from the input pruning point.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07269",
        "abstract url": "https://arxiv.org/abs/2410.07269",
        "title": "Deep Learning for Surgical Instrument Recognition and Segmentation in Robotic-Assisted Surgeries: A Systematic Review",
        "rating": "-2",
        "keywords": [
            [
                "robot"
            ],
            [
                "Surgical",
                "clinical"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Applying deep learning (DL) for annotating surgical instruments in robot-assisted minimally invasive surgeries (MIS) represents a significant advancement in surgical technology. This systematic review examines 48 studies that and advanced DL methods and architectures. These sophisticated DL models have shown notable improvements in the precision and efficiency of detecting and segmenting surgical tools. The enhanced capabilities of these models support various clinical applications, including real-time intraoperative guidance, comprehensive postoperative evaluations, and objective assessments of surgical skills. By accurately identifying and segmenting surgical instruments in video data, DL models provide detailed feedback to surgeons, thereby improving surgical outcomes and reducing complication risks. Furthermore, the application of DL in surgical education is transformative. The review underscores the significant impact of DL on improving the accuracy of skill assessments and the overall quality of surgical training programs. However, implementing DL in surgical tool detection and segmentation faces challenges, such as the need for large, accurately annotated datasets to train these models effectively. The manual annotation process is labor-intensive and time-consuming, posing a significant bottleneck. Future research should focus on automating the detection and segmentation process and enhancing the robustness of DL models against environmental variations. Expanding the application of DL models across various surgical specialties will be essential to fully realize this technology's potential. Integrating DL with other emerging technologies, such as augmented reality (AR), also offers promising opportunities to further enhance the precision and efficacy of surgical procedures.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "57 pages, 9 figures, Accepted for publication in Artificial Intelligence Reviews journal <https://link.springer.com/journal/10462>"
    },
    {
        "paper id": "2410.05687",
        "abstract url": "https://arxiv.org/abs/2410.05687",
        "title": "Extreme Value Modelling of Feature Residuals for Anomaly Detection in Dynamic Graphs",
        "rating": "-2.5",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Detecting anomalies in a temporal sequence of graphs can be applied is areas such as the detection of accidents in transport networks and cyber attacks in computer networks. Existing methods for detecting abnormal graphs can suffer from multiple limitations, such as high false positive rates as well as difficulties with handling variable-sized graphs and non-trivial temporal dynamics. To address this, we propose a technique where temporal dependencies are explicitly modelled via time series analysis of a large set of pertinent graph features, followed by using residuals to remove the dependencies. Extreme Value Theory is then used to robustly model and classify any remaining extremes, aiming to produce low false positives rates. Comparative evaluations on a multitude of graph instances show that the proposed approach obtains considerably better accuracy than TensorSplat and Laplacian Anomaly Detection.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "extended and revised version of arXiv:2210.07407"
    },
    {
        "paper id": "2410.05777",
        "abstract url": "https://arxiv.org/abs/2410.05777",
        "title": "Integrated Encoding and Quantization to Enhance Quanvolutional Neural Networks",
        "rating": "-2.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "quantum"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Image processing is one of the most promising applications for quantum machine learning (QML). Quanvolutional Neural Networks with non-trainable parameters are the preferred solution to run on current and near future quantum devices. The typical input preprocessing pipeline for quanvolutional layers comprises of four steps: optional input binary quantization, encoding classical data into quantum states, processing the data to obtain the final quantum states, decoding quantum states back to classical outputs. In this paper we propose two ways to enhance the efficiency of quanvolutional models. First, we propose a flexible data quantization approach with memoization, applicable to any encoding method. This allows us to increase the number of quantization levels to retain more information or lower them to reduce the amount of circuit executions. Second, we introduce a new integrated encoding strategy, which combines the encoding and processing steps in a single circuit. This method allows great flexibility on several architectural parameters (e.g., number of qubits, filter size, and circuit depth) making them adjustable to quantum hardware requirements. We compare our proposed integrated model with a classical convolutional neural network and the well-known rotational encoding method, on two different classification tasks. The results demonstrate that our proposed model encoding exhibits a comparable or superior performance to the other models while requiring fewer quantum resources.",
        "subjects": [
            "quant-ph",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05898",
        "abstract url": "https://arxiv.org/abs/2410.05898",
        "title": "Manifolds, Random Matrices and Spectral Gaps: The geometric phases of generative diffusion",
        "rating": "-2.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we investigate the latent geometry of generative diffusion models under the manifold hypothesis. To this purpose, we analyze the spectrum of eigenvalues (and singular values) of the Jacobian of the score function, whose discontinuities (gaps) reveal the presence and dimensionality of distinct sub-manifolds. Using a statistical physics approach, we derive the spectral distributions and formulas for the spectral gaps under several distributional assumptions and we compare these theoretical predictions with the spectra estimated from trained networks. Our analysis reveals the existence of three distinct qualitative phases during the generative process: a trivial phase; a manifold coverage phase where the diffusion process fits the distribution internal to the manifold; a consolidation phase where the score becomes orthogonal to the manifold and all particles are projected on the support of the data. This `division of labor' between different timescales provides an elegant explanation on why generative diffusion models are not affected by the manifold overfitting phenomenon that plagues likelihood-based models, since the internal distribution and the manifold geometry are produced at different time points during generation.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06010",
        "abstract url": "https://arxiv.org/abs/2410.06010",
        "title": "A large collection of bioinformatics question-query pairs over federated knowledge graphs: methodology and applications",
        "rating": "-2.5",
        "keywords": [
            [
                "graphs"
            ],
            [
                "bioinformatics"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Background. In the last decades, several life science resources have structured data using the same framework and made these accessible using the same query language to facilitate interoperability. Knowledge graphs have seen increased adoption in bioinformatics due to their advantages for representing data in a generic graph format. For example, yummydata.org catalogs more than 60 knowledge graphs accessible through SPARQL, a technical query language. Although SPARQL allows powerful, expressive queries, even across physically distributed knowledge graphs, formulating such queries is a challenge for most users. Therefore, to guide users in retrieving the relevant data, many of these resources provide representative examples. These examples can also be an important source of information for machine learning, if a sufficiently large number of examples are provided and published in a common, machine-readable and standardized format across different resources. Findings. We introduce a large collection of human-written natural language questions and their corresponding SPARQL queries over federated bioinformatics knowledge graphs (KGs) collected for several years across different research groups at the SIB Swiss Institute of Bioinformatics. The collection comprises more than 1000 example questions and queries, including 65 federated queries. We propose a methodology to uniformly represent the examples with minimal metadata, based on existing standards. Furthermore, we introduce an extensive set of open-source applications, including query graph visualizations and smart query editors, easily reusable by KG maintainers who adopt the proposed methodology. Conclusions. We encourage the community to adopt and extend the proposed methodology, towards richer KG metadata and improved Semantic Web services.",
        "subjects": [
            "cs.DB",
            "cs.AI",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06062",
        "abstract url": "https://arxiv.org/abs/2410.06062",
        "title": "LLM-based SPARQL Query Generation from Natural Language over Federated Knowledge Graphs",
        "rating": "-2.5",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "bioinformatics"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "We introduce a Retrieval-Augmented Generation (RAG) system for translating user questions into accurate federated SPARQL queries over bioinformatics knowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance accuracy and reduce hallucinations in query generation, our system utilises metadata from the KGs, including query examples and schema information, and incorporates a validation step to correct generated queries. The system is available online at chat.expasy.org.",
        "subjects": [
            "cs.DB",
            "cs.AI",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06127",
        "abstract url": "https://arxiv.org/abs/2410.06127",
        "title": "De-VertiFL: A Solution for Decentralized Vertical Federated Learning",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "tabular"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL), introduced in 2016, was designed to enhance data privacy in collaborative model training environments. Among the FL paradigm, horizontal FL, where clients share the same set of features but different data samples, has been extensively studied in both centralized and decentralized settings. In contrast, Vertical Federated Learning (VFL), which is crucial in real-world decentralized scenarios where clients possess different, yet sensitive, data about the same entity, remains underexplored. Thus, this work introduces De-VertiFL, a novel solution for training models in a decentralized VFL setting. De-VertiFL contributes by introducing a new network architecture distribution, an innovative knowledge exchange scheme, and a distributed federated training process. Specifically, De-VertiFL enables the sharing of hidden layer outputs among federation clients, allowing participants to benefit from intermediate computations, thereby improving learning efficiency. De-VertiFL has been evaluated using a variety of well-known datasets, including both image and tabular data, across binary and multiclass classification tasks. The results demonstrate that De-VertiFL generally surpasses state-of-the-art methods in F1-score performance, while maintaining a decentralized and privacy-preserving framework.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06311",
        "abstract url": "https://arxiv.org/abs/2410.06311",
        "title": "A Comparative Study of Hybrid Models in Health Misinformation Text Classification",
        "rating": "-2.5",
        "keywords": [
            [
                "SVM"
            ],
            [
                "Health"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "This study evaluates the effectiveness of machine learning (ML) and deep learning (DL) models in detecting COVID-19-related misinformation on online social networks (OSNs), aiming to develop more effective tools for countering the spread of health misinformation during the pan-demic. The study trained and tested various ML classifiers (Naive Bayes, SVM, Random Forest, etc.), DL models (CNN, LSTM, hybrid CNN+LSTM), and pretrained language models (DistilBERT, RoBERTa) on the \"COVID19-FNIR DATASET\". These models were evaluated for accuracy, F1 score, recall, precision, and ROC, and used preprocessing techniques like stemming and lemmatization. The results showed SVM performed well, achieving a 94.41% F1-score. DL models with Word2Vec embeddings exceeded 98% in all performance metrics (accuracy, F1 score, recall, precision & ROC). The CNN+LSTM hybrid models also exceeded 98% across performance metrics, outperforming pretrained models like DistilBERT and RoBERTa. Our study concludes that DL and hybrid DL models are more effective than conventional ML algorithms for detecting COVID-19 misinformation on OSNs. The findings highlight the importance of advanced neural network approaches and large-scale pretraining in misinformation detection. Future research should optimize these models for various misinformation types and adapt to changing OSNs, aiding in combating health misinformation.",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "8 pages, 4 tables presented at the OASIS workshop of the ACM Hypertext and Social Media Conference 2024"
    },
    {
        "paper id": "2410.06395",
        "abstract url": "https://arxiv.org/abs/2410.06395",
        "title": "Multimodal Representation Learning using Adaptive Graph Construction",
        "rating": "-2.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "medical",
                "disease"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Multimodal contrastive learning train neural networks by levergaing data from heterogeneous sources such as images and text. Yet, many current multimodal learning architectures cannot generalize to an arbitrary number of modalities and need to be hand-constructed. We propose AutoBIND, a novel contrastive learning framework that can learn representations from an arbitrary number of modalites through graph optimization. We evaluate AutoBIND on Alzhiemer's disease detection because it has real-world medical applicability and it contains a broad range of data modalities. We show that AutoBIND outperforms previous methods on this task, highlighting the generalizablility of the approach.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06408",
        "abstract url": "https://arxiv.org/abs/2410.06408",
        "title": "Automating Data Science Pipelines with Tensor Completion",
        "rating": "-2.5",
        "keywords": [
            [
                "architecture search"
            ],
            [
                "SQL"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Hyperparameter optimization is an essential component in many data science pipelines and typically entails exhaustive time and resource-consuming computations in order to explore the combinatorial search space. Similar to this problem, other key operations in data science pipelines exhibit the exact same properties. Important examples are: neural architecture search, where the goal is to identify the best design choices for a neural network, and query cardinality estimation, where given different predicate values for a SQL query the goal is to estimate the size of the output. In this paper, we abstract away those essential components of data science pipelines and we model them as instances of tensor completion, where each variable of the search space corresponds to one mode of the tensor, and the goal is to identify all missing entries of the tensor, corresponding to all combinations of variable values, starting from a very small sample of observed entries. In order to do so, we first conduct a thorough experimental evaluation of existing state-of-the-art tensor completion techniques and introduce domain-inspired adaptations (such as smoothness across the discretized variable space) and an ensemble technique which is able to achieve state-of-the-art performance. We extensively evaluate existing and proposed methods in a number of datasets generated corresponding to (a) hyperparameter optimization for non-neural network models, (b) neural architecture search, and (c) variants of query cardinality estimation, demonstrating the effectiveness of tensor completion as a tool for automating data science pipelines. Furthermore, we release our generated datasets and code in order to provide benchmarks for future work on this topic.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06442",
        "abstract url": "https://arxiv.org/abs/2410.06442",
        "title": "MaD-Scientist: AI-based Scientist solving Convection-Diffusion-Reaction Equations Using Massive PINN-Based Prior Data",
        "rating": "-2.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "physics"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Large language models (LLMs), like ChatGPT, have shown that even trained with noisy prior data, they can generalize effectively to new tasks through in-context learning (ICL) and pre-training techniques. Motivated by this, we explore whether a similar approach can be applied to scientific foundation models (SFMs). Our methodology is structured as follows: (i) we collect low-cost physics-informed neural network (PINN)-based approximated prior data in the form of solutions to partial differential equations (PDEs) constructed through an arbitrary linear combination of mathematical dictionaries; (ii) we utilize Transformer architectures with self and cross-attention mechanisms to predict PDE solutions without knowledge of the governing equations in a zero-shot setting; (iii) we provide experimental evidence on the one-dimensional convection-diffusion-reaction equation, which demonstrate that pre-training remains robust even with approximated prior data, with only marginal impacts on test accuracy. Notably, this finding opens the path to pre-training SFMs with realistic, low-cost data instead of (or in conjunction with) numerical high-cost data. These results support the conjecture that SFMs can improve in a manner similar to LLMs, where fully cleaning the vast set of sentences crawled from the Internet is nearly impossible.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06482",
        "abstract url": "https://arxiv.org/abs/2410.06482",
        "title": "OledFL: Unleashing the Potential of Decentralized Federated Learning via Opposite Lookahead Enhancement",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Pathological"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Decentralized Federated Learning (DFL) surpasses Centralized Federated Learning (CFL) in terms of faster training, privacy preservation, and light communication, making it a promising alternative in the field of federated learning. However, DFL still exhibits significant disparities with CFL in terms of generalization ability such as rarely theoretical understanding and degraded empirical performance due to severe inconsistency. In this paper, we enhance the consistency of DFL by developing an opposite lookahead enhancement technique (Ole), yielding OledFL to optimize the initialization of each client in each communication round, thus significantly improving both the generalization and convergence speed. Moreover, we rigorously establish its convergence rate in non-convex setting and characterize its generalization bound through uniform stability, which provides concrete reasons why OledFL can achieve both the fast convergence speed and high generalization ability. Extensive experiments conducted on the CIFAR10 and CIFAR100 datasets with Dirichlet and Pathological distributions illustrate that our OledFL can achieve up to 5\\% performance improvement and 8$\\times$ speedup, compared to the most popular DFedAvg optimizer in DFL.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06493",
        "abstract url": "https://arxiv.org/abs/2410.06493",
        "title": "BiC-MPPI: Goal-Pursuing, Sampling-Based Bidirectional Rollout Clustering Path Integral for Trajectory Optimization",
        "rating": "-2.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "Trajectory"
            ],
            [
                "navigation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper introduces the Bidirectional Clustered MPPI (BiC-MPPI) algorithm, a novel trajectory optimization method aimed at enhancing goal-directed guidance within the Model Predictive Path Integral (MPPI) framework. BiC-MPPI incorporates bidirectional dynamics approximations and a new guide cost mechanism, improving both trajectory planning and goal-reaching performance. By leveraging forward and backward rollouts, the bidirectional approach ensures effective trajectory connections between initial and terminal states, while the guide cost helps discover dynamically feasible paths. Experimental results demonstrate that BiC-MPPI outperforms existing MPPI variants in both 2D and 3D environments, achieving higher success rates and competitive computation times across 900 simulations on a modified BARN dataset for autonomous navigation. GitHub: https://github.com/i-ASL/BiC-MPPI",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "eess.SY",
            "math.OC"
        ],
        "comment": "7 pages, 1 figures"
    },
    {
        "paper id": "2410.06530",
        "abstract url": "https://arxiv.org/abs/2410.06530",
        "title": "TopoTune : A Framework for Generalized Combinatorial Complex Neural Networks",
        "rating": "-2.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "biological"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Graph Neural Networks (GNNs) excel in learning from relational datasets, processing node and edge features in a way that preserves the symmetries of the graph domain. However, many complex systems--such as biological or social networks--involve multiway complex interactions that are more naturally represented by higher-order topological spaces. The emerging field of Topological Deep Learning (TDL) aims to accommodate and leverage these higher-order structures. Combinatorial Complex Neural Networks (CCNNs), fairly general TDL models, have been shown to be more expressive and better performing than GNNs. However, differently from the graph deep learning ecosystem, TDL lacks a principled and standardized framework for easily defining new architectures, restricting its accessibility and applicability. To address this issue, we introduce Generalized CCNNs (GCCNs), a novel simple yet powerful family of TDL models that can be used to systematically transform any (graph) neural network into its TDL counterpart. We prove that GCCNs generalize and subsume CCNNs, while extensive experiments on a diverse class of GCCNs show that these architectures consistently match or outperform CCNNs, often with less model complexity. In an effort to accelerate and democratize TDL, we introduce TopoTune, a lightweight software that allows practitioners to define, build, and train GCCNs with unprecedented flexibility and ease.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07250",
        "abstract url": "https://arxiv.org/abs/2410.07250",
        "title": "Reconstruction of Particle Flow Energy Distribution Using Deep Learning Algorithms",
        "rating": "-2.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "physics"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In high-energy particle physics, extracting information from complex detector signals is crucial for energy reconstruction. Recent advancements involve using deep learning to process calorimeter images from various sub-detectors in experiments like the Large Hadron Collider (LHC) for energy map reconstruction. This paper compares classical algorithms\\-MLP, CNN, U-Net, and RNN\\-with variants that include self-attention and 3D convolution modules to evaluate their effectiveness in reconstructing the initial energy distribution. Additionally, a test dataset of jet events is utilized to analyze and compare models' performance in handling anomalous high-energy events. The analysis highlights the effectiveness of deep learning techniques for energy image reconstruction and explores their potential in this area.",
        "subjects": [
            "physics.ins-det",
            "cs.AI"
        ],
        "comment": "11 pages, 1 tables, 9 figures Code available at https://github.com/Image-processing-Particle-flow/Project1"
    },
    {
        "paper id": "2410.05738",
        "abstract url": "https://arxiv.org/abs/2410.05738",
        "title": "Design, Localization, Perception, and Control for GPS-Denied Autonomous Aerial Grasping and Harvesting",
        "rating": "-3",
        "keywords": [
            [
                "flight"
            ],
            [
                "UAV",
                "Drone",
                "agricultural"
            ]
        ],
        "abstract": "In this paper, we present a comprehensive UAV system design to perform the highly complex task of off-centered aerial grasping. This task has several interdisciplinary research challenges which need to be addressed at once. The main design challenges are GPS-denied functionality, solely onboard computing, and avoiding off-the-shelf costly positioning systems. While in terms of algorithms, visual perception, localization, control, and grasping are the leading research problems. Hence in this paper, we make interdisciplinary contributions: (i) A detailed description of the fundamental challenges in indoor aerial grasping, (ii) a novel lightweight gripper design, (iii) a complete aerial platform design and in-lab fabrication, and (iv) localization, perception, control, grasping systems, and an end-to-end flight autonomy state-machine. Finally, we demonstrate the resulting aerial grasping system Drone-Bee achieving a high grasping rate for a highly challenging agricultural task of apple-like fruit harvesting, indoors in a vertical farming setting (Fig. 1). To our knowledge, such a system has not been previously discussed in the literature, and with its capabilities, this system pushes aerial manipulation towards 4th generation.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05747",
        "abstract url": "https://arxiv.org/abs/2410.05747",
        "title": "Closed-Loop phase selection in EEG-TMS using Bayesian Optimization",
        "rating": "-3",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "EEG"
            ]
        ],
        "abstract": "Research on transcranial magnetic stimulation (TMS) combined with encephalography feedback (EEG-TMS) has shown that the phase of the sensorimotor mu rhythm is predictive of corticospinal excitability. Thus, if the subject-specific optimal phase is known, stimulation can be timed to be more efficient. In this paper, we present a closed-loop algorithm to determine the optimal phase linked to the highest excitability with few trials. We used Bayesian optimization as an automated, online search tool in an EEG-TMS simulation experiment. From a sample of 38 participants, we selected all participants with a significant single-subject phase effect (N = 5) for simulation. We then simulated 1000 experimental sessions per participant where we used Bayesian optimization to find the optimal phase. We tested two objective functions: Fitting a sinusoid in Bayesian linear regression or Gaussian Process (GP) regression. We additionally tested adaptive sampling using a knowledge gradient as the acquisition function compared with random sampling. We evaluated the algorithm's performance in a fast optimization (100 trials) and a long-term optimization (1000 trials). For fast optimization, the Bayesian linear regression in combination with adaptive sampling gives the best results with a mean phase location accuracy of 79 % after 100 trials. With either sampling approach, Bayesian linear regression performs better than GP regression in the fast optimization. In the long-term optimization, Bayesian regression with random sampling shows the best trajectory, with a rather steep improvement and good final performance of 87 % mean phase location accuracy. We show the suitability of closed-loop Bayesian optimization for phase selection. We could increase the speed and accuracy by using prior knowledge about the expected function shape compared with traditional Bayesian optimization with GP regression.",
        "subjects": [
            "q-bio.NC",
            "eess.SP"
        ],
        "comment": "6 pages, 4 figures. This work was accepted by the International Conference on Systems, Man, and Cybernetics, Kuching, Malaysia, 2024. \u00a92024 IEEE"
    },
    {
        "paper id": "2410.05882",
        "abstract url": "https://arxiv.org/abs/2410.05882",
        "title": "Future frame prediction in chest cine MR imaging using the PCA respiratory motion model and dynamically trained recurrent neural networks",
        "rating": "-3",
        "keywords": [
            [
                "MRI",
                "tumor"
            ],
            [
                "forecast"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Lung radiotherapy treatment systems are subject to a latency that leads to uncertainty in the estimated tumor location and high irradiation of healthy tissue. This work addresses future frame prediction in chest dynamic MRI sequences to compensate for that delay using RNNs trained with online learning algorithms. The latter enable networks to mitigate irregular movements, as they update synaptic weights with each new training example. Experiments were conducted using four publicly available 2D thoracic cine-MRI sequences. PCA decomposes the time-varying deformation vector field (DVF), computed with the Lucas-Kanade optical flow algorithm, into static deformation fields and low-dimensional time-dependent weights. We compare various algorithms to forecast the latter: linear regression, least mean squares (LMS), and RNNs trained with real-time recurrent learning (RTRL), unbiased online recurrent optimization, decoupled neural interfaces and sparse 1-step approximation (SnAp-1). That enables estimating the future DVFs and, in turn, the next frames by warping the initial image. Linear regression led to the lowest mean DVF error at a horizon h = 0.32s (the time interval in advance for which the prediction is made), equal to 1.30mm, followed by SnAp-1 and RTRL, whose error increased from 1.37mm to 1.44mm as h increased from 0.62s to 2.20s. Similarly, the structural similarity index measure (SSIM) of LMS decreased from 0.904 to 0.898 as h increased from 0.31s to 1.57s and was the highest among the algorithms compared for the latter horizons. SnAp-1 attained the highest SSIM for h $\\geq$ 1.88s, with values of less than 0.898. The predicted images look similar to the original ones, and the highest errors occurred at challenging areas such as the diaphragm boundary at the end-of-inhale phase, where motion variability is more prominent, and regions where out-of-plane motion was more prevalent.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG",
            "cs.NE"
        ],
        "comment": "28 pages, 16 figures"
    },
    {
        "paper id": "2410.05884",
        "abstract url": "https://arxiv.org/abs/2410.05884",
        "title": "A Robust Quadruped Robot with Twisting Waist for Flexible Motions",
        "rating": "-3",
        "keywords": [
            [
                "Robot"
            ],
            [
                "biological"
            ]
        ],
        "abstract": "The waist plays a crucial role in the agile movement of many animals in nature. It provides the torso with additional degrees of freedom and flexibility, inspiring researchers to incorporate this biological feature into robotic structures to enhance robot locomotion. This paper presents a cost-effective and low-complexity waist mechanism integrated into the structure of the open-source robot solo8, adding a new degree of freedom (DOF) to its torso. We refer to this novel robot as solo9. Additionally, we propose a full-body control method for the waist-equipped quadruped robot based on generative adversarial imitation learning (GAIL). During training, the discriminator is used as input for iterative optimization of the policy and dataset, enabling solo9 to achieve flexible steering maneuvers across various gaits. Extensive tests of solo9's steering capabilities, terrain adaptability, and robustness are conducted in both simulation and real-world scenarios, with detailed comparisons to solo8 and solo12, demonstrating the effectiveness of the control algorithm and the advantages of the waist mechanism.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06129",
        "abstract url": "https://arxiv.org/abs/2410.06129",
        "title": "Pinv-Recon: Generalized MR Image Reconstruction via Pseudoinversion of the Encoding Matrix",
        "rating": "-3",
        "keywords": [
            [
                "clinical"
            ],
            [
                "chemical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Purpose: To present a novel generalized MR image reconstruction based on pseudoinversion of the encoding matrix (Pinv-Recon) as a simple yet powerful method, and demonstrate its computational feasibility for diverse MR imaging applications. Methods: MR image encoding constitutes a linear mapping of the unknown image to the measured k-space data mediated via an encoding matrix ($ data = Encode \\times image$). Pinv-Recon addresses MR image reconstruction as a linear inverse problem ($image = Encode^{-1} \\times data$), explicitly calculating the Moore-Penrose pseudoinverse of the encoding matrix using truncated singular value decomposition (tSVD). Using a discretized, algebraic notation, we demonstrate constructing a generalized encoding matrix by stacking relevant encoding mechanisms (e.g., gradient encoding, coil sensitivity encoding, chemical shift inversion) and encoding distortions (e.g., off-center positioning, B$_0$ inhomogeneity, spatiotemporal gradient imperfections, transient relaxation effects). Iterative reconstructions using the explicit generalized encoding matrix, and the computation of the spatial-response-function (SRF) and noise amplification, were demonstrated. Results: We evaluated the computation times and memory requirements (time ~ (size of the encoding matrix)$^{1.4}$). Using the Shepp-Logan phantom, we demonstrated the versatility of the method for various intertwined MR image encoding and distortion mechanisms, achieving better MSE, PSNR and SSIM metrics than conventional methods. A diversity of datasets, including the ISMRM CG-SENSE challenge, were used to validate Pinv-Recon. Conclusion: Although pseudo-inversion of large encoding matrices was once deemed computationally intractable, recent advances make Pinv-Recon feasible. It has great promise for both research and clinical applications, and for educational use.",
        "subjects": [
            "physics.med-ph",
            "eess.IV"
        ],
        "comment": "26 pages, 8 figures, 2 tables (+ Supplementary Material). Submitted to Magnetic Resonance in Medicine"
    },
    {
        "paper id": "2410.06410",
        "abstract url": "https://arxiv.org/abs/2410.06410",
        "title": "BEVLoc: Cross-View Localization and Matching via Birds-Eye-View Synthesis",
        "rating": "-3",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "robotics"
            ],
            [
                "BEV"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Ground to aerial matching is a crucial and challenging task in outdoor robotics, particularly when GPS is absent or unreliable. Structures like buildings or large dense forests create interference, requiring GNSS replacements for global positioning estimates. The true difficulty lies in reconciling the perspective difference between the ground and air images for acceptable localization. Taking inspiration from the autonomous driving community, we propose a novel framework for synthesizing a birds-eye-view (BEV) scene representation to match and localize against an aerial map in off-road environments. We leverage contrastive learning with domain specific hard negative mining to train a network to learn similar representations between the synthesized BEV and the aerial map. During inference, BEVLoc guides the identification of the most probable locations within the aerial map through a coarse-to-fine matching strategy. Our results demonstrate promising initial outcomes in extremely difficult forest environments with limited semantic diversity. We analyze our model's performance for coarse and fine matching, assessing both the raw matching capability of our model and its performance as a GNSS replacement. Our work delves into off-road map localization while establishing a foundational baseline for future developments in localization. Our code is available at: https://github.com/rpl-cmu/bevloc",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": "8 pages, 6 figures, Conference: IROS 2024"
    },
    {
        "paper id": "2410.06467",
        "abstract url": "https://arxiv.org/abs/2410.06467",
        "title": "WAPITI: A Watermark for Finetuned Open-Source LLMs",
        "rating": "-3",
        "keywords": [
            [
                "attacks"
            ],
            [
                "Watermark"
            ]
        ],
        "abstract": "Watermarking of large language models (LLMs) generation embeds an imperceptible statistical pattern within texts, making it algorithmically detectable. Watermarking is a promising method for addressing potential harm and biases from LLMs, as it enables traceability, accountability, and detection of manipulated content, helping to mitigate unintended consequences. However, for open-source models, watermarking faces two major challenges: (i) incompatibility with fine-tuned models, and (ii) vulnerability to fine-tuning attacks. In this work, we propose WAPITI, a new method that transfers watermarking from base models to fine-tuned models through parameter integration. To the best of our knowledge, we propose the first watermark for fine-tuned open-source LLMs that preserves their fine-tuned capabilities. Furthermore, our approach offers an effective defense against fine-tuning attacks. We test our method on various model architectures and watermarking strategies. Results demonstrate that our method can successfully inject watermarks and is highly compatible with fine-tuned models. Additionally, we offer an in-depth analysis of how parameter editing influences the watermark strength and overall capabilities of the resulting models.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05916",
        "abstract url": "https://arxiv.org/abs/2410.05916",
        "title": "TIMBA: Time series Imputation with Bi-directional Mamba Blocks and Diffusion models",
        "rating": "-3.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Graph"
            ],
            [
                "healthcare",
                "clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The problem of imputing multivariate time series spans a wide range of fields, from clinical healthcare to multi-sensor systems. Initially, Recurrent Neural Networks (RNNs) were employed for this task; however, their error accumulation issues led to the adoption of Transformers, leveraging attention mechanisms to mitigate these problems. Concurrently, the promising results of diffusion models in capturing original distributions have positioned them at the forefront of current research, often in conjunction with Transformers. In this paper, we propose replacing time-oriented Transformers with State-Space Models (SSM), which are better suited for temporal data modeling. Specifically, we utilize the latest SSM variant, S6, which incorporates attention-like mechanisms. By embedding S6 within Mamba blocks, we develop a model that integrates SSM, Graph Neural Networks, and node-oriented Transformers to achieve enhanced spatiotemporal representations. Implementing these architectural modifications, previously unexplored in this field, we present Time series Imputation with Bi-directional mamba blocks and diffusion models (TIMBA). TIMBA achieves superior performance in almost all benchmark scenarios and performs comparably in others across a diverse range of missing value situations and three real-world datasets. We also evaluate how the performance of our model varies with different amounts of missing values and analyse its performance on downstream tasks. In addition, we provide the original code to replicate the results.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "14 pages, 7 tables and 2 figures"
    },
    {
        "paper id": "2410.06422",
        "abstract url": "https://arxiv.org/abs/2410.06422",
        "title": "Predicting Battery Capacity Fade Using Probabilistic Machine Learning Models With and Without Pre-Trained Priors",
        "rating": "-3.5",
        "keywords": [
            [
                "health"
            ],
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Lithium-ion batteries are a key energy storage technology driving revolutions in mobile electronics, electric vehicles and renewable energy storage. Capacity retention is a vital performance measure that is frequently utilized to assess whether these batteries have approached their end-of-life. Machine learning (ML) offers a powerful tool for predicting capacity degradation based on past data, and, potentially, prior physical knowledge, but the degree to which an ML prediction can be trusted is of significant practical importance in situations where consequential decisions must be made based on battery state of health. This study explores the efficacy of fully Bayesian machine learning in forecasting battery health with the quantification of uncertainty in its predictions. Specifically, we implemented three probabilistic ML approaches and evaluated the accuracy of their predictions and uncertainty estimates: a standard Gaussian process (GP), a structured Gaussian process (sGP), and a fully Bayesian neural network (BNN). In typical applications of GP and sGP, their hyperparameters are learned from a single sample while, in contrast, BNNs are typically pre-trained on an existing dataset to learn the weight distributions before being used for inference. This difference in methodology gives the BNN an advantage in learning global trends in a dataset and makes BNNs a good choice when training data is available. However, we show that pre-training can also be leveraged for GP and sGP approaches to learn the prior distributions of the hyperparameters and that in the case of the pre-trained sGP, similar accuracy and improved uncertainty estimation compared to the BNN can be achieved. This approach offers a framework for a broad range of probabilistic machine learning scenarios where past data is available and can be used to learn priors for (hyper)parameters of probabilistic ML models.",
        "subjects": [
            "cs.LG",
            "cond-mat.mtrl-sci"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06516",
        "abstract url": "https://arxiv.org/abs/2410.06516",
        "title": "QuadBEV: An Efficient Quadruple-Task Perception Framework via Bird's-Eye-View Representation",
        "rating": "-3.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "autonomous driving"
            ],
            [
                "BEV"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Bird's-Eye-View (BEV) perception has become a vital component of autonomous driving systems due to its ability to integrate multiple sensor inputs into a unified representation, enhancing performance in various downstream tasks. However, the computational demands of BEV models pose challenges for real-world deployment in vehicles with limited resources. To address these limitations, we propose QuadBEV, an efficient multitask perception framework that leverages the shared spatial and contextual information across four key tasks: 3D object detection, lane detection, map segmentation, and occupancy prediction. QuadBEV not only streamlines the integration of these tasks using a shared backbone and task-specific heads but also addresses common multitask learning challenges such as learning rate sensitivity and conflicting task objectives. Our framework reduces redundant computations, thereby enhancing system efficiency, making it particularly suited for embedded systems. We present comprehensive experiments that validate the effectiveness and robustness of QuadBEV, demonstrating its suitability for real-world applications.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06545",
        "abstract url": "https://arxiv.org/abs/2410.06545",
        "title": "Signal Watermark on Large Language Models",
        "rating": "-3.5",
        "keywords": [
            [
                "Watermark"
            ],
            [
                "grammatical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "As Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources. In this paper, we proposed a watermarking method embedding a specific watermark into the text during its generation by LLMs, based on a pre-defined signal pattern. This technique not only ensures the watermark's invisibility to humans but also maintains the quality and grammatical integrity of model-generated text. We utilize LLMs and Fast Fourier Transform (FFT) for token probability computation and detection of the signal watermark. The unique application of signal processing principles within the realm of text generation by LLMs allows for subtle yet effective embedding of watermarks, which do not compromise the quality or coherence of the generated text. Our method has been empirically validated across multiple LLMs, consistently maintaining high detection accuracy, even with variations in temperature settings during text generation. In the experiment of distinguishing between human-written and watermarked text, our method achieved an AUROC score of 0.97, significantly outperforming existing methods like GPTZero, which scored 0.64. The watermark's resilience to various attacking scenarios further confirms its robustness, addressing significant challenges in model-generated text authentication.",
        "subjects": [
            "cs.CR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07267",
        "abstract url": "https://arxiv.org/abs/2410.07267",
        "title": "Efficient representation learning of scintillation signal characteristics with spectrum-inspired temporal neural networks",
        "rating": "-3.5",
        "keywords": [
            [
                "industrial"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Nuclear radiation detectors based on scintillators are widely used in particle and high energy physics experiments, nuclear medicine imaging, industrial and environmental detection, etc. Precisely extracting scintillation signal characteristics at the event level is important for these applications, not only in respect of understanding the scintillator itself, but also kinds and physical property of incident particles. Recent researches demonstrate data-driven neural networks are superior to traditional statistical methods, especially when the analytical form of signals is hard to obtain, or noise is significant. However, most densely connected or convolution-based networks fail to fully exploit the spectral and temporal structure of scintillation signals, leaving large space for performance improvement. In this paper, we propose a network architecture specially tailored for scintillation signal characterization based on previous works on time series analysis. By directly applying Fast Fourier Transform on original signals without data embedding, including the zero-frequency component, adjusting convolution scheme for low-frequency components, and unbiasedly re-weighting features from different frequencies, the proposed network architecture can serve as a lightweight and enhanced representation learning backbone. We prove our idea on simulation data generated with the setting of the LUX dark matter detector, and on experimental electrical signals with fast electronics to emulate scintillation variations. The proposed model achieves significantly better results than the reference model in literature and densely connected models without representation learning.",
        "subjects": [
            "physics.ins-det",
            "cs.LG",
            "physics.data-an"
        ],
        "comment": "24 pages, 13 figures"
    },
    {
        "paper id": "2410.05973",
        "abstract url": "https://arxiv.org/abs/2410.05973",
        "title": "Komet: A Serverless Platform for Low-Earth Orbit Edge Services",
        "rating": "-4",
        "keywords": [
            [
                "industrial",
                "IoT"
            ],
            [
                "satellite"
            ]
        ],
        "abstract": "Low-Earth orbit satellite networks can provide global broadband Internet access using constellations of thousands of satellites. Integrating edge computing resources in such networks can enable global low-latency access to compute services, supporting end users in rural areas, remote industrial applications, or the IoT. To achieve this, resources must be carefully allocated to various services from multiple tenants. Moreover, applications must navigate the dynamic nature of satellite networks, where orbital mechanics necessitate frequent client hand-offs. Therefore, managing applications on the low-Earth orbit edge will require the right platform abstractions. We introduce Komet, a serverless platform for low-Earth orbit edge computing. Komet integrates Function-as-a-Service compute with data replication, enabling on-demand elastic edge resource allocation and frequent service migration against satellite orbital trajectories to keep services deployed in the same geographic region. We implement Komet as a proof-of-concept prototype and demonstrate how its abstractions can be used to build low-Earth orbit edge applications with high availability despite constant mobility. Further, we propose simple heuristics for service migration scheduling in different application scenarios and evaluate them in simulation based on our experiment traces, showing the trade-off between selecting an optimal satellite server at every instance and minimizing service migration frequency.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "15th ACM Symposium on Cloud Computing"
    },
    {
        "paper id": "2410.06371",
        "abstract url": "https://arxiv.org/abs/2410.06371",
        "title": "Improved Estimation of Ranks for Learning ItemRecommenders with Negative Sampling",
        "rating": "-4",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "music"
            ]
        ],
        "abstract": "In recommendation systems, there has been a growth in the num-ber of recommendable items (# of movies, music, products). Whenthe set of recommendable items is large, training and evaluationof item recommendation models becomes computationally expen-sive. To lower this cost, it has become common to sample negativeitems. However, the recommendation quality can suffer from biasesintroduced by traditional negative sampling mechanisms.In this work, we demonstrate the benefits from correcting thebias introduced by sampling of negatives. We first provide sampledbatch version of the well-studied WARP and LambdaRank methods.Then, we present how these methods can benefit from improvedranking estimates. Finally, we evaluate the recommendation qualityas a result of correcting rank estimates and demonstrate that WARPand LambdaRank can be learned efficiently with negative samplingand our proposed correction technique.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06471",
        "abstract url": "https://arxiv.org/abs/2410.06471",
        "title": "Cost-Effective Cyber-Physical System Prototype for Precision Agriculture with a Focus on Crop Growth",
        "rating": "-4",
        "keywords": [
            [
                "biomass",
                "health"
            ],
            [
                "agricultural"
            ]
        ],
        "abstract": "In precision agriculture, integrating advanced technologies is crucial for optimizing plant growth and health monitoring. Cyber-physical system (CPS) platforms tailored to specific agricultural environments have emerged, but the diversity of these environments poses challenges in developing adaptive CPS platforms. This paper explores rapid prototyping methods to address these challenges, focusing on non-destructive techniques for estimating plant growth. We present a CPS prototype that combines sensors, microcontrollers, digital image processing, and predictive modeling to measure leaf area and biomass accumulation in hydroponic environments. Our results show that the prototype effectively monitors and predicts plant growth, highlighting the potential of rapid CPS prototyping in promoting sustainability and improving crop yields at a moderate cost of hardware.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "To appear in Proceedings of the 35th IEEE International Workshop on Rapid System Prototyping (RSP 2024)"
    },
    {
        "paper id": "2410.05780",
        "abstract url": "https://arxiv.org/abs/2410.05780",
        "title": "GlucoBench: Curated List of Continuous Glucose Monitoring Datasets with Prediction Benchmarks",
        "rating": "-4.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "medical",
                "clinical"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The rising rates of diabetes necessitate innovative methods for its management. Continuous glucose monitors (CGM) are small medical devices that measure blood glucose levels at regular intervals providing insights into daily patterns of glucose variation. Forecasting of glucose trajectories based on CGM data holds the potential to substantially improve diabetes management, by both refining artificial pancreas systems and enabling individuals to make adjustments based on predictions to maintain optimal glycemic range.Despite numerous methods proposed for CGM-based glucose trajectory prediction, these methods are typically evaluated on small, private datasets, impeding reproducibility, further research, and practical adoption. The absence of standardized prediction tasks and systematic comparisons between methods has led to uncoordinated research efforts, obstructing the identification of optimal tools for tackling specific challenges. As a result, only a limited number of prediction methods have been implemented in clinical practice. To address these challenges, we present a comprehensive resource that provides (1) a consolidated repository of curated publicly available CGM datasets to foster reproducibility and accessibility; (2) a standardized task list to unify research objectives and facilitate coordinated efforts; (3) a set of benchmark models with established baseline performance, enabling the research community to objectively gauge new methods' efficacy; and (4) a detailed analysis of performance-influencing factors for model development. We anticipate these resources to propel collaborative research endeavors in the critical domain of CGM-based glucose predictions. {Our code is available online at github.com/IrinaStatsLab/GlucoBench.",
        "subjects": [
            "q-bio.QM",
            "cs.LG",
            "stat.AP"
        ],
        "comment": "The Twelfth International Conference on Learning Representations. 2024"
    },
    {
        "paper id": "2410.06502",
        "abstract url": "https://arxiv.org/abs/2410.06502",
        "title": "Chemistry-Inspired Diffusion with Non-Differentiable Guidance",
        "rating": "-4.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Chemistry"
            ],
            [
                "quantum"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Recent advances in diffusion models have shown remarkable potential in the conditional generation of novel molecules. These models can be guided in two ways: (i) explicitly, through additional features representing the condition, or (ii) implicitly, using a property predictor. However, training property predictors or conditional diffusion models requires an abundance of labeled data and is inherently challenging in real-world applications. We propose a novel approach that attenuates the limitations of acquiring large labeled datasets by leveraging domain knowledge from quantum chemistry as a non-differentiable oracle to guide an unconditional diffusion model. Instead of relying on neural networks, the oracle provides accurate guidance in the form of estimated gradients, allowing the diffusion process to sample from a conditional distribution specified by quantum chemistry. We show that this results in more precise conditional generation of novel and stable molecular structures. Our experiments demonstrate that our method: (1) significantly reduces atomic forces, enhancing the validity of generated molecules when used for stability optimization; (2) is compatible with both explicit and implicit guidance in diffusion models, enabling joint optimization of molecular properties and stability; and (3) generalizes effectively to molecular optimization tasks beyond stability optimization.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "preprint"
    },
    {
        "paper id": "2410.05996",
        "abstract url": "https://arxiv.org/abs/2410.05996",
        "title": "AIVIO: Closed-loop, Object-relative Navigation of UAVs with AI-aided Visual Inertial Odometry",
        "rating": "-5",
        "keywords": [
            [
                "6-DoF"
            ],
            [
                "vehicle",
                "flight"
            ],
            [
                "robot",
                "Navigation"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "Object-relative mobile robot navigation is essential for a variety of tasks, e.g. autonomous critical infrastructure inspection, but requires the capability to extract semantic information about the objects of interest from raw sensory data. While deep learning-based (DL) methods excel at inferring semantic object information from images, such as class and relative 6 degree of freedom (6-DoF) pose, they are computationally demanding and thus often not suitable for payload constrained mobile robots. In this letter we present a real-time capable unmanned aerial vehicle (UAV) system for object-relative, closed-loop navigation with a minimal sensor configuration consisting of an inertial measurement unit (IMU) and RGB camera. Utilizing a DL-based object pose estimator, solely trained on synthetic data and optimized for companion board deployment, the object-relative pose measurements are fused with the IMU data to perform object-relative localization. We conduct multiple real-world experiments to validate the performance of our system for the challenging use case of power pole inspection. An example closed-loop flight is presented in the supplementary video.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted for publication in the IEEE Robotics and Automation Letters (RA-L), 2024"
    },
    {
        "paper id": "2410.05759",
        "abstract url": "https://arxiv.org/abs/2410.05759",
        "title": "3D UAV Trajectory Planning for IoT Data Collection via Matrix-Based Evolutionary Computation",
        "rating": "-6",
        "keywords": [
            [
                "3D"
            ],
            [
                "Trajectory"
            ],
            [
                "IoT"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "UAVs are increasingly becoming vital tools in various wireless communication applications including internet of things (IoT) and sensor networks, thanks to their rapid and agile non-terrestrial mobility. Despite recent research, planning three-dimensional (3D) UAV trajectories over a continuous temporal-spatial domain remains challenging due to the need to solve computationally intensive optimization problems. In this paper, we study UAV-assisted IoT data collection aimed at minimizing total energy consumption while accounting for the UAV's physical capabilities, the heterogeneous data demands of IoT nodes, and 3D terrain. We propose a matrix-based differential evolution with constraint handling (MDE-CH), a computation-efficient evolutionary algorithm designed to address non-convex constrained optimization problems with several different types of constraints. Numerical evaluations demonstrate that the proposed MDE-CH algorithm provides a continuous 3D temporal-spatial UAV trajectory capable of efficiently minimizing energy consumption under various practical constraints and outperforms the conventional fly-hover-fly model for both two-dimensional (2D) and 3D trajectory planning.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05791",
        "abstract url": "https://arxiv.org/abs/2410.05791",
        "title": "F\u00fcrElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance",
        "rating": "-8",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "trajectory"
            ],
            [
                "biomechanics"
            ],
            [
                "music"
            ],
            [
                "physics"
            ],
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Piano playing requires agile, precise, and coordinated hand control that stretches the limits of dexterity. Hand motion models with the sophistication to accurately recreate piano playing have a wide range of applications in character animation, embodied AI, biomechanics, and VR/AR. In this paper, we construct a first-of-its-kind large-scale dataset that contains approximately 10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153 pieces of classical music. To capture natural performances, we designed a markerless setup in which motions are reconstructed from multi-view videos using state-of-the-art pose estimation models. The motion data is further refined via inverse kinematics using the high-resolution MIDI key-pressing data obtained from sensors in a specialized Yamaha Disklavier piano. Leveraging the collected dataset, we developed a pipeline that can synthesize physically-plausible hand motions for musical scores outside of the dataset. Our approach employs a combination of imitation learning and reinforcement learning to obtain policies for physics-based bimanual control involving the interaction between hands and piano keys. To solve the sampling efficiency problem with the large motion dataset, we use a diffusion model to generate natural reference motions, which provide high-level trajectory and fingering (finger order and placement) information. However, the generated reference motion alone does not provide sufficient accuracy for piano performance modeling. We then further augmented the data by using musical similarity to retrieve similar motions from the captured dataset to boost the precision of the RL policy. With the proposed method, our model generates natural, dexterous motions that generalize to music from outside the training dataset.",
        "subjects": [
            "cs.GR",
            "cs.AI",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "SIGGRAPH Asia 2024. Project page: https://for-elise.github.io/"
    },
    {
        "paper id": "2410.05688",
        "abstract url": "https://arxiv.org/abs/2410.05688",
        "title": "Fishery resources management",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider management of the fish species Plecoglossus altivelis altivelis, a major inland fishery resource in Japan playing important roles from economic, cultural, and recreational viewpoints. We firstly summarize the collected body weight data of the fish in the Hii River, Japan since 2016. The two kinds of data are available in each year with few exceptions: the historical data during summer and autumn collected with the help of an angular and the annual distribution data at the Toami (casting net) competition where we could obtain the data from many anglers during two hours in one day. We fit deterministic and uncertain logistic growth models to the data in each year and discuss their performance. The fitted uncertain logistic growth model is applied to an optimal harvesting problem of the fish subject to a sustainability concern and model distortion. Several numerical schemes for solving the problem are examined and compared both theoretically and numerically.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "This manuscript is a preprint version of a chapter manuscript for a book"
    },
    {
        "paper id": "2410.05701",
        "abstract url": "https://arxiv.org/abs/2410.05701",
        "title": "Balancing Pareto Front exploration of Non-dominated Tournament Genetic Algorithm (B-NTGA) in solving multi-objective NP-hard problems with constraints",
        "rating": "-10",
        "keywords": [],
        "abstract": "The paper presents a new balanced selection operator applied to the proposed Balanced Non-dominated Tournament Genetic Algorithm (B-NTGA) that actively uses archive to solve multi- and many-objective NP-hard combinatorial optimization problems with constraints. The primary motivation is to make B-NTGA more efficient in exploring Pareto Front Approximation (PFa), focusing on 'gaps' and reducing some PFa regions' sampling too frequently. Such a balancing mechanism allows B-NTGA to be more adaptive and focus on less explored PFa regions. The proposed B-NTGA is investigated on two benchmark multi- and many-objective optimization real-world problems, like Thief Traveling Problem and Multi-Skill Resource-Constrained Project Scheduling Problem. The results of experiments show that B-NTGA has a higher efficiency and better performance than state-of-the-art methods.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "PREPRINT"
    },
    {
        "paper id": "2410.05702",
        "abstract url": "https://arxiv.org/abs/2410.05702",
        "title": "Data Informativity for Quadratic Stabilization under Data Perturbation",
        "rating": "-10",
        "keywords": [],
        "abstract": "Assessing data informativity, determining whether the measured data contains sufficient information for a specific control objective, is a fundamental challenge in data-driven control. In noisy scenarios, existing studies deal with system noise and measurement noise separately, using quadratic matrix inequalities. Moreover, the analysis of measurement noise requires restrictive assumptions on noise properties. To provide a unified framework without any restrictions, this study introduces data perturbation, a novel notion that encompasses both existing noise models. It is observed that the admissible system set with data perturbation does not meet preconditions necessary for applying the key lemma in the matrix S-procedure. Our analysis overcomes this limitation by developing an extended version of this lemma, making it applicable to data perturbation. Our results unify the existing analyses while eliminating the need for restrictive assumptions made in the measurement noise scenario.",
        "subjects": [
            "math.OC",
            "eess.SY"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2410.05722",
        "abstract url": "https://arxiv.org/abs/2410.05722",
        "title": "A Novel Low-Complexity Peak-Power-Assisted Data-Aided Channel Estimation Scheme for MIMO-OFDM Wireless Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper, for the first time, presents a low-complexity peak-power-assisted data-aided channel estimation (DACE) scheme for both single-input single-output (SISO) and multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) wireless systems. In OFDM, high peak-power levels occur when the subcarriers align in phase and constructively interfere with each other. The research proposes a peak-power-assisted channel estimation scheme that accurately selects peak-power carriers at the transmitter of an OFDM system and uses them as reliable carriers for the DACE scheme. By incorporating these reliable carriers with known pilot symbols as additional pilot signals, channel estimation accuracy significantly improves in MIMO-OFDM systems. This eliminates the need to determine reliable data symbols at the receiver, thereby significantly reducing the computational complexity of the system. However, high peak-powers are considered a major drawback in OFDM. In this work, we incorporate a companding technique to mitigate this issue and provide sufficient margin for the DACE scheme. The performance of the proposed DACE scheme is evaluated using both least square (LS) and linear minimum mean square error (LMMSE) channel estimators. In this regard, the proposed technique not only improves channel estimation accuracy but also enhances the spectral efficiency of the wireless system. It outperforms traditional channel estimators in terms of system mean square error (MSE) and bit-error-rate (BER) performance. It also reduces the pilot overhead by 50$\\%$ compared to traditional channel estimators and provides bandwidth optimization for MIMO-OFDM systems. This makes it a promising solution for improving the performance and efficiency of wireless communication systems.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "This paper is submitted to arXiv as a preprint because it has not yet been published in a journal"
    },
    {
        "paper id": "2410.05766",
        "abstract url": "https://arxiv.org/abs/2410.05766",
        "title": "StagedVulBERT: Multi-Granular Vulnerability Detection with a Novel Pre-trained Code Model",
        "rating": "-10",
        "keywords": [],
        "abstract": "The emergence of pre-trained model-based vulnerability detection methods has significantly advanced the field of automated vulnerability detection. However, these methods still face several challenges, such as difficulty in learning effective feature representations of statements for fine-grained predictions and struggling to process overly long code sequences. To address these issues, this study introduces StagedVulBERT, a novel vulnerability detection framework that leverages a pre-trained code language model and employs a coarse-to-fine strategy. The key innovation and contribution of our research lies in the development of the CodeBERT-HLS component within our framework, specialized in hierarchical, layered, and semantic encoding. This component is designed to capture semantics at both the token and statement levels simultaneously, which is crucial for achieving more accurate multi-granular vulnerability detection. Additionally, CodeBERT-HLS efficiently processes longer code token sequences, making it more suited to real-world vulnerability detection. Comprehensive experiments demonstrate that our method enhances the performance of vulnerability detection at both coarse- and fine-grained levels. Specifically, in coarse-grained vulnerability detection, StagedVulBERT achieves an F1 score of 92.26%, marking a 6.58% improvement over the best-performing methods. At the fine-grained level, our method achieves a Top-5% accuracy of 65.69%, which outperforms the state-of-the-art methods by up to 75.17%.",
        "subjects": [
            "cs.CR",
            "cs.SE"
        ],
        "comment": "18 pages,13 figures"
    },
    {
        "paper id": "2410.05776",
        "abstract url": "https://arxiv.org/abs/2410.05776",
        "title": "Viscoelasticity Estimation of Sports Prosthesis by Energy-minimizing Inverse Kinematics and Its Validation by Forward Dynamics",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this study, we present a method for estimating the viscoelasticity of a leaf-spring sports prosthesis using advanced energy minimizing inverse kinematics based on the Piece-wise Constant Strain (PCS) model to reconstruct the three-dimensional dynamic behavior. Dynamic motion analysis of the athlete and prosthesis is important to clarify the effect of prosthesis characteristics on foot function. However, three-dimensional deformation calculations of the prosthesis and viscoelasticity have rarely been investigated. In this letter, we apply the PCS model to a prosthesis deformation, which can calculate flexible deformation with low computational cost and handle kinematics and dynamics. In addition, we propose an inverse kinematics calculation method that is consistent with the material properties of the prosthesis by considering the minimization of elastic energy. Furthermore, we propose a method to estimate the viscoelasticity by solving a quadratic programming based on the measured motion capture data. The calculated strains are more reasonable than the results obtained by conventional inverse kinematics calculation. From the result of the viscoelasticity estimation, we simulate the prosthetic motion by forward dynamics calculation and confirm that this result corresponds to the measured motion. These results indicate that our approach adequately models the dynamic phenomena, including the viscoelasticity of the prosthesis.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05787",
        "abstract url": "https://arxiv.org/abs/2410.05787",
        "title": "An Adaptive Dual-Domain Prediction Strategy based on Second-order Derivatives for Dynamic Multi-Objective Optimization",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper addresses the problem of dynamic multi-objective optimization problems (DMOPs), by demonstrating new approaches to change prediction strategies within an evolutionary algorithm paradigm. Because the objectives of such problems change over time, the Pareto optimal set (PS) and Pareto optimal front (PF) are also dynamic. To accurately track the changing PS and PF in the decision and objective spaces, we propose a novel adaptive prediction strategy, which utilizes the concept of second-order derivatives adaptively in different domains. %to deal with DMOPs. Firstly, the changes in both the PS and the PF are considered in this paper, which makes the proposed a dual-domain based method. Firstly, we propose a dual-domain method, which takes into account changes in both the PS and the PF simultaneously. An adaptive strategy is adopted to self-adjust the proportion of the search space. Secondly, a second-order derivative prediction strategy is proposed to predicatively re-initialize the population. We compare the performance of the proposed algorithm against four other state-of-the-art algorithms from the literature, using DMOPs benchmark problems. Experimental results show that the proposed method outperforms the other algorithms on most of the test problems.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "Dynamic Multi-objective Optimization Problems, Second-order Derivative, Adaptive Dual-Domain Prediction"
    },
    {
        "paper id": "2410.05789",
        "abstract url": "https://arxiv.org/abs/2410.05789",
        "title": "Hybrid Gripper with Passive Pneumatic Soft Joints for Grasping Deformable Thin Objects",
        "rating": "-10",
        "keywords": [],
        "abstract": "Grasping a variety of objects remains a key challenge in the development of versatile robotic systems. The human hand is remarkably dexterous, capable of grasping and manipulating objects with diverse shapes, mechanical properties, and textures. Inspired by how humans use two fingers to pick up thin and large objects such as fabric or sheets of paper, we aim to develop a gripper optimized for grasping such deformable objects. Observing how the soft and flexible fingertip joints of the hand approach and grasp thin materials, a hybrid gripper design that incorporates both soft and rigid components was proposed. The gripper utilizes a soft pneumatic ring wrapped around a rigid revolute joint to create a flexible two-fingered gripper. Experiments were conducted to characterize and evaluate the gripper performance in handling sheets of paper and other objects. Compared to rigid grippers, the proposed design improves grasping efficiency and reduces the gripping distance by up to eightfold.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05796",
        "abstract url": "https://arxiv.org/abs/2410.05796",
        "title": "Software analytics for software engineering: A tertiary review",
        "rating": "-10",
        "keywords": [],
        "abstract": "Software analytics (SA) is frequently proposed as a tool to support practitioners in software engineering (SE) tasks. We have observed that several secondary studies on SA have been published. Some of these studies have overlapping aims and some have even been published in the same calendar year. This presents an opportunity to analyze the congruence or divergence of the conclusions in these studies. Such an analysis can help identify broader generalizations beyond any of the individual secondary studies. We identified five secondary studies on the use of SA for SE. These secondary studies cover primary research from 2000 to 2021. Despite the overlapping objectives and search time frames of these secondary studies, there is negligible overlap of primary studies between these secondary studies. Thus, each of them provides an isolated view, and together, they provide a fragmented view, i.e., there is no ``common picture'' of the area. Thus, we conclude that an overview of the literature identified by these secondary studies would be useful in providing a more comprehensive overview of the topic.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05811",
        "abstract url": "https://arxiv.org/abs/2410.05811",
        "title": "lintsampler: Easy random sampling via linear interpolation",
        "rating": "-10",
        "keywords": [],
        "abstract": "'lintsampler' provides a Python implementation of a technique we term 'linear interpolant sampling': an algorithm to efficiently draw pseudo-random samples from an arbitrary probability density function (PDF). First, the PDF is evaluated on a grid-like structure. Then, it is assumed that the PDF can be approximated between grid vertices by the (multidimensional) linear interpolant. With this assumption, random samples can be efficiently drawn via inverse transform sampling. lintsampler is primarily written with 'numpy', drawing some additional functionality from 'scipy'. Under the most basic usage of lintsampler, the user provides a Python function defining the target PDF and some parameters describing a grid-like structure to the 'LintSampler' class, and is then able to draw samples via the 'sample' method. Additionally, there is functionality for the user to set the random seed, employ quasi-Monte Carlo sampling, or sample within a premade grid ('DensityGrid') or tree ('DensityTree') structure.",
        "subjects": [
            "stat.CO",
            "astro-ph.IM",
            "cs.MS",
            "math.PR"
        ],
        "comment": "Accepted by Journal of Open Source Software. Describes code repository at https://github.com/aneeshnaik/lintsampler"
    },
    {
        "paper id": "2410.05842",
        "abstract url": "https://arxiv.org/abs/2410.05842",
        "title": "Privacy-aware Fully Model-Free Event-triggered Cloud-based HVAC Control",
        "rating": "-10",
        "keywords": [],
        "abstract": "Privacy is a major concern when computing-as-a-service (CaaS) platforms, e.g., cloud-computing platforms, are utilized for building automation, as CaaS platforms can infer sensitive information, such as occupancy, using the sensor measurements of a building. Although the existing encrypted model-based control algorithms can ensure the security and privacy of sensor measurements, they are highly complex to implement and require high computational resources, which result in a high cost of using CaaS platforms. To address these issues, in this paper, we propose an encrypted fully model-free event-triggered cloud-based HVAC control framework that ensures the privacy of occupancy information and minimizes the communication and computation overhead associated with encrypted HVAC control. To this end, we first develop a model-free controller for regulating indoor temperature and CO2 levels. We then design a model-free event-triggering unit which reduces the communication and computation costs of encrypted HVAC control using an optimal triggering policy. Finally, we evaluate the performance of the proposed encrypted fully model-free event-triggered cloud-based HVAC control framework using the TRNSYS simulator, comparing it to an encrypted model-based event-triggered control framework, which uses model predictive control to regulate the indoor climate. Our numerical results demonstrate that, compared to the encrypted model-based method, the proposed fully model-free framework improves the control performance while reducing the communication and computation costs. More specifically, it reduces the communication between the system and the CaaS platform by 64% amount, and its computation time is 75% less than that of the model-based control.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05844",
        "abstract url": "https://arxiv.org/abs/2410.05844",
        "title": "Spectrally Efficient LDPC Codes For IRIG-106 Waveforms via Random Puncturing",
        "rating": "-10",
        "keywords": [],
        "abstract": "Low-density parity-check (LDPC) codes form part of the IRIG-106 standard and have been successfully deployed for the Telemetry Group version of shaped-offset quadrature phase shift keying (SOQPSK-TG) modulation. Recently, LDPC code solutions have been proposed and optimized for continuous phase modulations (CPMs), including the pulse code modulation/frequency modulation (PCM/FM) and the multi-h CPM developed by the Advanced Range TeleMetry program (ARTM CPM). These codes were shown to perform around one dB from the respective channel capacities of these modulations. In this paper, we consider the effect of random puncturing of these LDPC codes to further improve spectrum efficiency. We present numerical simulation results that affirm the robust decoding performance promised by LDPC codes designed for ARTM CPM.",
        "subjects": [
            "eess.SY",
            "cs.IT",
            "eess.SP"
        ],
        "comment": "Accepted for inclusion in the 2024 International Telemetry Conference"
    },
    {
        "paper id": "2410.05852",
        "abstract url": "https://arxiv.org/abs/2410.05852",
        "title": "A$^3$L-FEC: Age-Aware Application Layer Forward Error Correction Flow Control",
        "rating": "-10",
        "keywords": [],
        "abstract": "Age of Information (AoI) is a metric and KPI that has been developed for measuring and controlling data freshness. Optimization of AoI in a real-life network requires adapting the rate and timing of transmissions to varying network conditions. The vast majority of previous research on the control of AoI has been theoretical, using idealized models which ignored certain implementation aspects. As such, there is still a gap between the research on AoI and real-world protocols. In this paper we present an effort toward closing this gap by introducing an age-aware flow control algorithm. The algorithm, Age-Aware Application Layer Forward Error Correction (A$^3$L-FEC), is a packet generation mechanism operating on top of the user datagram protocol (UDP). The purpose is to control peak Age of the end-to-end packet flow, specifically, to reduce the rate of what we call \"Age Violations\", that is, the events where the peak age exceeds a given threshold. Evaluations in Mininet-WiFi and MATLAB indicate that A$^3$L-FEC reduces age violations compared to two related protocols in the literature, namely TCP-BBR and ACP+.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "16 pages"
    },
    {
        "paper id": "2410.05854",
        "abstract url": "https://arxiv.org/abs/2410.05854",
        "title": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in Blockchain Networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "The perpetual growth of data stored on popular blockchains such as Ethereum leads to significant scalability challenges and substantial storage costs for operators of full nodes. Increasing costs may lead to fewer independently operated nodes in the network, which poses risks to decentralization (and hence network security), but also pushes decentralized app developers towards centrally hosted API services. This paper introduces a new protocol that allows validator nodes to participate in a blockchain network without the need to store the full state of the network on each node. The key idea is to use the blockchain network as both a replicated state machine and as a distributed storage system. By distributing states across nodes and enabling efficient data retrieval through a Kademlia-inspired routing protocol, we reduce storage costs for validators. Cryptographic proofs (such as Merkle proofs) are used to allow nodes to verify data stored by other nodes without having to trust those nodes directly. While the protocol trades off data storage for increased network bandwidth, we show how gossiping and caching can minimize the increased bandwidth needs. To validate our state sharing protocol, we conduct an extensive quantitative analysis of Ethereum's data storage and data access patterns. Our findings indicate that while our protocol significantly lowers storage needs, it comes with an increased bandwidth usage ranging from 1.5 MB to 5 MB per block, translating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite this, the size remains small enough such that it can be passed to all nodes and validated within Ethereum's 12-second block validation window. Further analysis shows that Merkle proofs are the most significant contributor to the additional bandwidth. To address this concern, we also analyze the impact of switching to the more space-efficient Verkle Proofs.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05863",
        "abstract url": "https://arxiv.org/abs/2410.05863",
        "title": "Enhancing Playback Performance in Video Recommender Systems with an On-Device Gating and Ranking Framework",
        "rating": "-10",
        "keywords": [],
        "abstract": "Video recommender systems (RSs) have gained increasing attention in recent years. Existing mainstream RSs focus on optimizing the matching function between users and items. However, we noticed that users frequently encounter playback issues such as slow loading or stuttering while browsing the videos, especially in weak network conditions, which will lead to a subpar browsing experience, and may cause users to leave, even when the video content and recommendations are superior. It is quite a serious issue, yet easily overlooked. To tackle this issue, we propose an on-device Gating and Ranking Framework (GRF) that cooperates with server-side RS. Specifically, we utilize a gate model to identify videos that may have playback issues in real-time, and then we employ a ranking model to select the optimal result from a locally-cached pool to replace the stuttering videos. Our solution has been fully deployed on Kwai, a large-scale short video platform with hundreds of millions of users globally. Moreover, it significantly enhances video playback performance and improves overall user experience and retention rates.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "CIKM 2024 applied research track, 7 pages"
    },
    {
        "paper id": "2410.05912",
        "abstract url": "https://arxiv.org/abs/2410.05912",
        "title": "Two-Timescale Design for Movable Antennas Enabled-Multiuser MIMO Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Movable antennas (MAs), which can be swiftly repositioned within a defined region, offer a promising solution to the limitations of fixed-position antennas (FPAs) in adapting to spatial variations in wireless channels, thereby improving channel conditions and communication between transceivers. However, frequent MA position adjustments based on instantaneous channel state information (CSI) incur high operational complexity, making real-time CSI acquisition impractical, especially in fast-fading channels. To address these challenges, we propose a two-timescale transmission framework for MA-enabled multiuser multiple-input-multiple-output (MU-MIMO) systems. In the large timescale, statistical CSI is exploited to optimize MA positions for long-term ergodic performance, whereas, in the small timescale, beamforming vectors are designed using instantaneous CSI to handle short-term channel fluctuations. Within this new framework, we analyze the ergodic sum rate and develop efficient MA position optimization algorithms for both maximum-ratio-transmission (MRT) and zero-forcing (ZF) beamforming schemes. These algorithms employ alternating optimization (AO), successive convex approximation (SCA), and majorization-minimization (MM) techniques, iteratively optimizing antenna positions and refining surrogate functions that approximate the ergodic sum rate. Numerical results show significant ergodic sum rate gains with the proposed two-timescale MA design over conventional FPA systems, particularly under moderate to strong line-of-sight (LoS) conditions. Notably, MA with ZF beamforming consistently outperforms MA with MRT, highlighting the synergy between beamforming and MAs for superior interference management in environments with moderate Rician factors and high user density, while MA with MRT can offer a simplified alternative to complex beamforming designs in strong LoS conditions.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "13 pages, 12 figures, submitted to an IEEE journal for possible publication"
    },
    {
        "paper id": "2410.05934",
        "abstract url": "https://arxiv.org/abs/2410.05934",
        "title": "Chameleon: An Efficient FHE Scheme Switching Acceleration on GPUs",
        "rating": "-10",
        "keywords": [],
        "abstract": "Fully homomorphic encryption (FHE) enables direct computation on encrypted data, making it a crucial technology for privacy protection. However, FHE suffers from significant performance bottlenecks. In this context, GPU acceleration offers a promising solution to bridge the performance gap. Existing efforts primarily focus on single-class FHE schemes, which fail to meet the diverse requirements of data types and functions, prompting the development of hybrid multi-class FHE schemes. However, studies have yet to thoroughly investigate specific GPU optimizations for hybrid FHE schemes. In this paper, we present an efficient GPU-based FHE scheme switching acceleration named Chameleon. First, we propose a scalable NTT acceleration design that adapts to larger CKKS polynomials and smaller TFHE polynomials. Specifically, Chameleon tackles synchronization issues by fusing stages to reduce synchronization, employing polynomial coefficient shuffling to minimize synchronization scale, and utilizing an SM-aware combination strategy to identify the optimal switching point. Second, Chameleon is the first to comprehensively analyze and optimize critical switching operations. It introduces CMux-level parallelization to accelerate LUT evaluation and a homomorphic rotation-free matrix-vector multiplication to improve repacking efficiency. Finally, Chameleon outperforms the state-of-the-art GPU implementations by 1.23x in CKKS HMUL and 1.15x in bootstrapping. It also achieves up to 4.87x and 1.51x speedups for TFHE gate bootstrapping compared to CPU and GPU versions, respectively, and delivers a 67.3x average speedup for scheme switching over CPU-based implementation.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "15 pages, 14 figures"
    },
    {
        "paper id": "2410.05947",
        "abstract url": "https://arxiv.org/abs/2410.05947",
        "title": "Maximal Length Cellular Automata : A Survey",
        "rating": "-10",
        "keywords": [],
        "abstract": "This article surveys some theoretical aspects of Cellular Automata (CAs) research. In particular, we discuss on maximal length CA. An n-cell CA is a maximal length CA, if all the configurations except one form a single cycle. There is a bonding between maximal length CA and primitive polynomial. So, primitive polynomials occupy a good amount of space in this survey. The main goal of this survey is to provide a tutorial on maximal length CA theory to researchers with classical and new results on maximality. We also give a compact collection of known results with references to their proofs, and to suggest some open problems. Additionally, some new theorems and corollaries are added to bridge the gaps among several known results.",
        "subjects": [
            "cs.FL",
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05953",
        "abstract url": "https://arxiv.org/abs/2410.05953",
        "title": "The Cyber Alliance Game: How Alliances Influence Cyber-Warfare",
        "rating": "-10",
        "keywords": [],
        "abstract": "Cyber-warfare has become the norm in current ongoing military conflicts. Over the past decade, numerous examples have shown the extent to which nation-states become vulnerable if they do not focus on building their cyber capacities. Adding to the inherent complexity of cyberwar scenarios, a state is usually a member of one or more alliances. Alliance policies and internal struggles could shape the individual actions of member states; intuitively, this also holds for the cyber domain. In this paper, we define and study a simple Cyber Alliance Game with the objective of understanding the fundamental influence of alliances on cyber conflicts between nation-states. Specifically, we focus on the decision of whether to exploit a newly found vulnerability individually or share it with the alliance. First, we characterize the impact of vulnerability-sharing rewards on the resulting equilibrium. Second, we study the implications of the internal power structure of alliances on cyberwar outcomes and infer the expected behavior of Dictator, Veto, and Dummy players. Finally, we investigate how alliances can nudge their members via rewards and punishments to adhere to their defensive or offensive cyber policy. We believe that our results contribute to the fundamental understanding of real-world cyber-conflicts by characterizing the impact of alliances.",
        "subjects": [
            "cs.GT",
            "cs.CR"
        ],
        "comment": "accepted at NordSec'24"
    },
    {
        "paper id": "2410.05956",
        "abstract url": "https://arxiv.org/abs/2410.05956",
        "title": "Waveguide-multiplexed photonic matrix-vector multiplication processor using multiport photodetectors",
        "rating": "-10",
        "keywords": [],
        "abstract": "The slowing down of Moore's law has driven the development of application-specific processors for deep learning. Analog photonic processors offer a promising solution for accelerating matrix-vector multiplications (MVMs) in deep learning by leveraging parallel computations in the optical domain. Intensity-based photonic MVM processors, which do not utilize the phase information of light, are appealing due to their simplified operations. However, existing intensity-based schemes for such processors often employ wavelength multiplexing or mode multiplexing, both of which have limited scalability due to high insertion loss or wavelength crosstalk. In this work, we present a scalable intensity-based photonic MVM processor based on the concept of waveguide multiplexing. This scheme employs multiport photodetectors (PDs) to sum the intensities of multiple optical signals, eliminating the need for multiple wavelengths or modes. A 16-port Ge PD with a 3 dB bandwidth of 11.8 GHz at a bias voltage of -3 V is demonstrated, and it can be further scaled up to handle 250 ports while maintaining a 6.1 GHz operation bandwidth. A 4 $\\times$ 4 circuit fabricated on a Si-on-insulator (SOI) platform is used to perform MVMs in a 3-layer neural network designed for classifying Iris flowers, achieving a classification accuracy of 93.3%. Furthermore, the performance of large-scale circuits in a convolutional neural network (CNN) for Fashion-MNIST is simulated, resulting in a classification accuracy of 90.53%. This work provides a simplified and scalable approach to photonic MVM, laying a foundation for large-scale and multi-dimensional photonic matrix-matrix multiplication in optical neural networks.",
        "subjects": [
            "physics.optics",
            "cs.ET"
        ],
        "comment": null
    },
    {
        "paper id": "2410.05961",
        "abstract url": "https://arxiv.org/abs/2410.05961",
        "title": "Active and Passive Beamforming Designs for SER Minimization in RIS-Assisted MIMO Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "This research exploits the applications of reconfigurable intelligent surface (RIS)-assisted multiple input multiple output (MIMO) systems, specifically addressing the enhancement of communication reliability with modulated signals. Specifically, we first derive the analytical downlink symbol error rate (SER) of each user as a multivariate function of both the phase-shift and beamforming vectors. The analytical SER enables us to obtain insights into the synergistic dynamics between the RIS and MIMO communication. We then introduce a novel average SER minimization problem subject to the practical constraints of the transmitted power budget and phase shift coefficients, which is NP-hard. By incorporating the differential evolution (DE) algorithm as a pivotal tool for optimizing the intricate active and passive beamforming variables in RIS-assisted communication systems, the non-convexity of the considered SER optimization problem can be effectively handled. Furthermore, an efficient local search is incorporated into the DE algorithm to overcome the local optimum, and hence offer low SER and high communication reliability. Monte Carlo simulations validate the analytical results and the proposed optimization framework, indicating that the joint active and passive beamforming design is superior to the other benchmarks.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "16 pages, 13 figures, and 1 table . Accepted by TWC"
    },
    {
        "paper id": "2410.06013",
        "abstract url": "https://arxiv.org/abs/2410.06013",
        "title": "Characterization of input-to-output stability for infinite dimensional systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "We prove a superposition theorem for input-to-output stability (IOS) of a broad class of nonlinear infinite-dimensional systems with outputs including both continuous-time and discrete-time systems. It contains, as a special case, the superposition theorem for input-to-state stability (ISS) of infinite-dimensional systems from [1] and the IOS superposition theorem for systems of ordinary differential equations from [2]. To achieve this result, we introduce and examine several novel stability and attractivity concepts for infinite dimensional systems with outputs: We prove criteria for the uniform limit property for systems with outputs, several of which are new already for systems with full-state output, we provide superposition theorems for systems which satisfy both the output-Lagrange stability property (OL) and IOS, give a sufficient condition for OL and characterize ISS in terms of IOS and input/output-to-state stability. Finally, by means of counterexamples, we illustrate the challenges appearing on the way of extension of the superposition theorems from [1] and [2] to infinite-dimensional systems with outputs.",
        "subjects": [
            "math.OC",
            "eess.SY",
            "math.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06043",
        "abstract url": "https://arxiv.org/abs/2410.06043",
        "title": "KwicKwocKwac, a tool for rapidly generating concordances and marking up a literary text",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper introduces KwicKwocKwac 1.0 (KwicKK), a web application designed to enhance the annotation and enrichment of digital texts in the humanities. KwicKK provides a user-friendly interface that enables scholars and researchers to perform semi-automatic markup of textual documents, facilitating the identification of relevant entities such as people, organizations, and locations. Key functionalities include the visualization of annotated texts using KeyWord in Context (KWIC), KeyWord Out Of Context (KWOC), and KeyWord After Context (KWAC) methodologies, alongside automatic disambiguation of generic references and integration with Wikidata for Linked Open Data connections. The application supports metadata input and offers multiple download formats, promoting accessibility and ease of use. Developed primarily for the National Edition of Aldo Moro's works, KwicKK aims to lower the technical barriers for users while fostering deeper engagement with digital scholarly resources. The architecture leverages contemporary web technologies, ensuring scalability and reliability. Future developments will explore user experience enhancements, collaborative features, and integration of additional data sources.",
        "subjects": [
            "cs.DL",
            "cs.IR"
        ],
        "comment": "10 pages, 5 figures"
    },
    {
        "paper id": "2410.06049",
        "abstract url": "https://arxiv.org/abs/2410.06049",
        "title": "\"Diversity is Having the Diversity\": Unpacking and Designing for Diversity in Applicant Selection",
        "rating": "-10",
        "keywords": [],
        "abstract": "When selecting applicants for scholarships, universities, or jobs, practitioners often aim for a diverse cohort of qualified recipients. However, differing articulations, constructs, and notions of diversity prevents decision-makers from operationalising and progressing towards the diversity they all agree is needed. To understand this challenge of translation from values, to requirements, to decision support tools (DSTs), we conducted participatory design studies exploring professionals' varied perceptions of diversity and how to build for them. Our results suggest three definitions of diversity: bringing together different perspectives; ensuring representativeness of a base population; and contextualising applications, which we use to create the Diversity Triangle. We experience-prototyped DSTs reflecting each angle of the Diversity Triangle to enhance decision-making around diversity. We find that notions of diversity are highly diverse; efforts to design DSTs for diversity should start by working with organisations to distil 'diversity' into definitions and design requirements.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "32 pages, 11 figures"
    },
    {
        "paper id": "2410.06059",
        "abstract url": "https://arxiv.org/abs/2410.06059",
        "title": "Maximum Achievable Rate of Resistive Random-Access Memory Channels by Mutual Information Spectrum Analysis",
        "rating": "-10",
        "keywords": [],
        "abstract": "The maximum achievable rate is derived for resistive random-access memory (ReRAM) channel with sneak path interference. Based on the mutual information spectrum analysis, the maximum achievable rate of ReRAM channel with independent and identically distributed (i.i.d.) binary inputs is derived as an explicit function of channel parameters such as the distribution of cell selector failures and channel noise level. Due to the randomness of cell selector failures, the ReRAM channel demonstrates multi-status characteristic. For each status, it is shown that as the array size is large, the fraction of cells affected by sneak paths approaches a constant value. Therefore, the mutual information spectrum of the ReRAM channel is formulated as a mixture of multiple stationary channels. Maximum achievable rates of the ReRAM channel with different settings, such as single- and across-array codings, with and without data shaping, and optimal and treating-interference-as-noise (TIN) decodings, are compared. These results provide valuable insights on the code design for ReRAM.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06061",
        "abstract url": "https://arxiv.org/abs/2410.06061",
        "title": "Optimizing Energy Efficiency with RSMA: Balancing Low and High QoS Requirements",
        "rating": "-10",
        "keywords": [],
        "abstract": "Future wireless systems are expected to deliver significantly higher quality-of-service (QoS) albeit with fewer energy resources for diverse, already existing and also novel wireless applications. The optimal resource allocation for a system in this regard could be investigated by reducing the overall power available at the expense of reduced QoS for the inefficient users. In other words, we maximize the system energy efficiency by achieving power saving through a minimal back-off in terms of QoS. In this paper, we investigate the energy efficiency vs. delivered QoS trade-off for the rate-splitting multiple access (RSMA) assisted downlink system. We first determine the user grouping with a normalised channel similarity metric so as to allow a large number of users with non-zero achievable private message rates. Through the private message removal (PMR) of these users, we aim to investigate the QoS vs. energy efficiency trade-off. Numerical results indicate a peak of ~$10\\%$ increase in the network energy efficiency for the proposed normalised channel similarity metric based user grouping with scheduled PMR.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "5 pages, 4 figures, IEEE SPAWC 2024 Conference version"
    },
    {
        "paper id": "2410.06066",
        "abstract url": "https://arxiv.org/abs/2410.06066",
        "title": "An Analysis of QUIC Connection Migration in the Wild",
        "rating": "-10",
        "keywords": [],
        "abstract": "As QUIC gains attention, more applications that leverage its capabilities are emerging. These include defenses against on-path IP tracking and traffic analysis. However, the deployment of the underlying required support for connection migration remains largely unexplored. This paper provides a comprehensive examination of the support of the QUIC connection migration mechanism over the Internet. We perform Internet-wide scans revealing that despite a rapid evolution in the deployment of QUIC on web servers, some of the most popular destinations do not support connection migration yet.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "7 pages, 4 figures"
    },
    {
        "paper id": "2410.06083",
        "abstract url": "https://arxiv.org/abs/2410.06083",
        "title": "Classification of simulation relations for symbolic control",
        "rating": "-10",
        "keywords": [],
        "abstract": "Abstraction-based control design is a promising approach for ensuring safety-critical control of complex cyber-physical systems. A key aspect of this methodology is the relation between the original and abstract systems, which ensures that the abstract controller can be transformed into a valid controller for the original system through a concretization procedure. In this paper, we provide a comprehensive and systematic framework that characterizes various simulation relations, through their associated concretization procedures. We introduce the concept of augmented system, which universally enables a feedback refinement relation with the abstract system. This augmented system encapsulates the specific characteristics of each simulation relation within an interface, enabling a plug-and-play control architecture. Our results demonstrate that the existence of a particular simulation relation between the concrete and abstract systems is equivalent to the implementability of a specific control architecture, which depends on the considered simulation relation. This allows us to introduce new types of relations, and to establish the advantages and drawbacks of different relations, which we exhibit through detailed examples.",
        "subjects": [
            "eess.SY",
            "math.DS"
        ],
        "comment": "14 pages, 6 figures"
    },
    {
        "paper id": "2410.06106",
        "abstract url": "https://arxiv.org/abs/2410.06106",
        "title": "Distributed Tomographic Reconstruction with Quantization",
        "rating": "-10",
        "keywords": [],
        "abstract": "Conventional tomographic reconstruction typically depends on centralized servers for both data storage and computation, leading to concerns about memory limitations and data privacy. Distributed reconstruction algorithms mitigate these issues by partitioning data across multiple nodes, reducing server load and enhancing privacy. However, these algorithms often encounter challenges related to memory constraints and communication overhead between nodes. In this paper, we introduce a decentralized Alternating Directions Method of Multipliers (ADMM) with configurable quantization. By distributing local objectives across nodes, our approach is highly scalable and can efficiently reconstruct images while adapting to available resources. To overcome communication bottlenecks, we propose two quantization techniques based on K-means clustering and JPEG compression. Numerical experiments with benchmark images illustrate the tradeoffs between communication efficiency, memory use, and reconstruction accuracy.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "26 pages, 8 figures"
    },
    {
        "paper id": "2410.06143",
        "abstract url": "https://arxiv.org/abs/2410.06143",
        "title": "blockLAW: Blockchain Technology for Legal Automation and Workflow -- Cyber Ethics and Cybersecurity Platforms",
        "rating": "-10",
        "keywords": [],
        "abstract": "In the current legal environment, it is essential to prioritize the protection and reliability of data to promote trust and effectiveness. This study examines how blockchain technology in the form of blockLAW can be applicable to investigate its effects on legal automation, cybersecurity, and ethical concerns. The decentralized ledger and unchangeable characteristics of Blockchain provide opportunities to simplify legal procedures, automate contract execution with smart contracts, and improve transparency in legal transactions. Blockchain is seen as a crucial instrument for updating legal processes while maintaining ethical standards, tackling issues like scalability, regulatory adherence, and ethical dilemmas such as privacy and fairness. The study examines recent developments and evaluates blockchain impact on legal structures, offering perspectives on its potential to enhance legal procedures and guarantee transparency in legal systems. It further emphasizes blockchain ability to redefine how legal professionals handle and protect sensitive information, leading to stronger, more effective, and reliable legal procedures. We have also discussed the technological considerations when it comes to blockchain integration into legal systems like integration planning, implementation strategies, innovations, advancements, trends with Blockchain Integration Framework for legal systems.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "14 pages, 2 figures"
    },
    {
        "paper id": "2410.06145",
        "abstract url": "https://arxiv.org/abs/2410.06145",
        "title": "Serverless Cold Starts and Where to Find Them",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper releases and analyzes a month-long trace of 85 billion user requests and 11.9 million cold starts from Huawei's serverless cloud platform. Our analysis spans workloads from five data centers. We focus on cold starts and provide a comprehensive examination of the underlying factors influencing the number and duration of cold starts. These factors include trigger types, request synchronicity, runtime languages, and function resource allocations. We investigate components of cold starts, including pod allocation time, code and dependency deployment time, and scheduling delays, and examine their relationships with runtime languages, trigger types, and resource allocation. We introduce pod utility ratio to measure the pod's useful lifetime relative to its cold start time, giving a more complete picture of cold starts, and see that some pods with long cold start times have longer useful lifetimes. Our findings reveal the complexity and multifaceted origins of the number, duration, and characteristics of cold starts, driven by differences in trigger types, runtime languages, and function resource allocations. For example, cold starts in Region 1 take up to 7 seconds, dominated by dependency deployment time and scheduling. In Region 2, cold starts take up to 3 seconds and are dominated by pod allocation time. Based on this, we identify opportunities to reduce the number and duration of cold starts using strategies for multi-region scheduling. Finally, we suggest directions for future research to address these challenges and enhance the performance of serverless cloud platforms. Our datasets and code are available here https://github.com/sir-lab/data-release",
        "subjects": [
            "cs.DC",
            "cs.OS",
            "cs.PF"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06208",
        "abstract url": "https://arxiv.org/abs/2410.06208",
        "title": "A Physical Layer Security Framework for Integrated Sensing and Semantic Communication Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we address a physical layer security (PLS) framework for the integrated sensing and semantic communication (ISASC) system, where a multi-antenna dual-function semantic base station serves multiple single-antenna semantic communication users (SCUs) and monitors a malicious sensing target (MST), in the presence of a single-antenna eavesdropper (EVE), with both the MST and EVE aiming to wiretap information from the SCUs' signals. To enhance PLS, we employ joint artificial noise (AN) and dedicated sensing signal (DSS) in addition to wiretap coding. To evaluate the sensing accuracy, we derive the Cramer-Rao bound (CRB) as a function of the communication, sensing, and AN beamforming (BF) vectors. Subsequently, to assess the PLS level of the ISASC system, we determine a closed-form expression for the semantic secrecy rate (SSR). To achieve an optimal trade-off region between these two competing objectives, we formulate a multi-objective optimization problem for the joint design of the BF vectors. We apply semi-definite programming, Gaussian randomization method, and golden-section search techniques to address this problem. Simulation results demonstrate that the proposed scheme outperforms baseline schemes, achieving a superior trade-off between SSR and CRB.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "4 pages"
    },
    {
        "paper id": "2410.06233",
        "abstract url": "https://arxiv.org/abs/2410.06233",
        "title": "A Generalized Metriplectic System via Free Energy and System~Identification via Bilevel Convex Optimization",
        "rating": "-10",
        "keywords": [],
        "abstract": "This work generalizes the classical metriplectic formalism to model Hamiltonian systems with nonconservative dissipation. Classical metriplectic representations allow for the description of energy conservation and production of entropy via a suitable selection of an entropy function and a bilinear symmetric metric. By relaxing the Casimir invariance requirement of the entropy function, this paper shows that the generalized formalism induces the free energy analogous to thermodynamics. The monotonic change of free energy can serve as a more precise criterion than mechanical energy or entropy alone. This paper provides examples of the generalized metriplectic system in a 2-dimensional Hamiltonian system and $\\mathrm{SO}(3)$. This paper also provides a bilevel convex optimization approach for the identification of the metriplectic system given measurements of the system.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06266",
        "abstract url": "https://arxiv.org/abs/2410.06266",
        "title": "Near Exact Privacy Amplification for Matrix Mechanisms",
        "rating": "-10",
        "keywords": [],
        "abstract": "We study the problem of computing the privacy parameters for DP machine learning when using privacy amplification via random batching and noise correlated across rounds via a correlation matrix $\\textbf{C}$ (i.e., the matrix mechanism). Past work on this problem either only applied to banded $\\textbf{C}$, or gave loose privacy parameters. In this work, we give a framework for computing near-exact privacy parameters for any lower-triangular, non-negative $\\textbf{C}$. Our framework allows us to optimize the correlation matrix $\\textbf{C}$ while accounting for amplification, whereas past work could not. Empirically, we show this lets us achieve smaller RMSE on prefix sums than the previous state-of-the-art (SOTA). We also show that we can improve on the SOTA performance on deep learning tasks. Our two main technical tools are (i) using Monte Carlo accounting to bypass composition, which was the main technical challenge for past work, and (ii) a \"balls-in-bins\" batching scheme that enables easy privacy analysis and is closer to practical random batching than Poisson sampling.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06325",
        "abstract url": "https://arxiv.org/abs/2410.06325",
        "title": "Meta-Learning Augmented MPC for Disturbance-Aware Motion Planning and Control of Quadrotors",
        "rating": "-10",
        "keywords": [],
        "abstract": "A major challenge in autonomous flights is unknown disturbances, which can jeopardize safety and lead to collisions, especially in obstacle-rich environments. This paper presents a disturbance-aware motion planning and control framework designed for autonomous aerial flights. The framework is composed of two key components: a disturbance-aware motion planner and a tracking controller. The disturbance-aware motion planner consists of a predictive control scheme and a learned model of disturbances that is adapted online. The tracking controller is designed using contraction control methods to provide safety bounds on the quadrotor behaviour in the vicinity of the obstacles with respect to the disturbance-aware motion plan. Finally, the algorithm is tested in simulation scenarios with a quadrotor facing strong crosswind and ground-induced disturbances.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06330",
        "abstract url": "https://arxiv.org/abs/2410.06330",
        "title": "Local Surface Parameterizations via Geodesic Splines",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present a general method for computing local parameterizations rooted at a point on a surface, where the surface is described only through a signed implicit function and a corresponding projection function. Using a two-stage process, we compute several points radially emanating from the map origin, and interpolate between them with a spline surface. The narrow interface of our method allows it to support several kinds of geometry such as signed distance functions, general analytic implicit functions, triangle meshes, neural implicits, and point clouds. We demonstrate the high quality of our generated parameterizations on a variety of examples, and show applications in local texturing and surface curve drawing.",
        "subjects": [
            "cs.GR"
        ],
        "comment": "12 pages, 14 figures"
    },
    {
        "paper id": "2410.06351",
        "abstract url": "https://arxiv.org/abs/2410.06351",
        "title": "Moving Faster and Reducing Risk: Using LLMs in Release Deployment",
        "rating": "-10",
        "keywords": [],
        "abstract": "Release engineering has traditionally focused on continuously delivering features and bug fixes to users, but at a certain scale, it becomes impossible for a release engineering team to determine what should be released. At Meta's scale, the responsibility appropriately and necessarily falls back on the engineer writing and reviewing the code. To address this challenge, we developed models of diff risk scores (DRS) to determine how likely a diff is to cause a SEV, i.e., a severe fault that impacts end-users. Assuming that SEVs are only caused by diffs, a naive model could randomly gate X% of diffs from landing, which would automatically catch X% of SEVs on average. However, we aimed to build a model that can capture Y% of SEVs by gating X% of diffs, where Y >> X. By training the model on historical data on diffs that have caused SEVs in the past, we can predict the riskiness of an outgoing diff to cause a SEV. Diffs that are beyond a particular threshold of risk can then be gated. We have four types of gating: no gating (green), weekend gating (weekend), medium impact on end-users (yellow), and high impact on end-users (red). The input parameter for our models is the level of gating, and the outcome measure is the number of captured SEVs. Our research approaches include a logistic regression model, a BERT-based model, and generative LLMs. Our baseline regression model captures 18.7%, 27.9%, and 84.6% of SEVs while respectively gating the top 5% (weekend), 10% (yellow), and 50% (red) of risky diffs. The BERT-based model, StarBERT, only captures 0.61x, 0.85x, and 0.81x as many SEVs as the logistic regression for the weekend, yellow, and red gating zones, respectively. The generative LLMs, iCodeLlama-34B and iDiffLlama-13B, when risk-aligned, capture more SEVs than the logistic regression model in production: 1.40x, 1.52x, 1.05x, respectively.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06361",
        "abstract url": "https://arxiv.org/abs/2410.06361",
        "title": "Hierarchy of chaotic dynamics in random modular networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "We introduce a model of randomly connected neural populations and study its dynamics by means of the dynamical mean-field theory and simulations. Our analysis uncovers a rich phase diagram, featuring high- and low-dimensional chaotic phases, separated by a crossover region characterized by low values of the maximal Lyapunov exponent and participation ratio dimension, but with high and rapidly changing values of the Lyapunov dimension. Counterintuitively, chaos can be attenuated by either adding noise to strongly modular connectivity or by introducing modularity into random connectivity. Extending the model to include a multilevel, hierarchical connectivity reveals that a loose balance between activities across levels drives the system towards the edge of chaos.",
        "subjects": [
            "physics.bio-ph",
            "cond-mat.dis-nn",
            "cs.NE",
            "nlin.CD",
            "q-bio.NC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06386",
        "abstract url": "https://arxiv.org/abs/2410.06386",
        "title": "A novel, finite-element-based framework for sparse data solution reconstruction and multiple choices",
        "rating": "-10",
        "keywords": [],
        "abstract": "Digital twinning offers a capability of effective real-time monitoring and control, which are vital for cost-intensive experimental facilities, particularly the ones where extreme conditions exist. Sparse experimental measurements collected by various diagnostic sensors are usually the only source of information available during the course of a physical experiment. Consequently, in order to enable monitoring and control of the experiment (digital twinning), the ability to perform inverse analysis, facilitating the full field solution reconstruction from the sparse experimental data in real time, is crucial. This paper shows for the first time that it is possible to directly solve inverse problems, such as solution reconstruction, where some or all boundary conditions (BCs) are unknown, by purely using a finite-element (FE) approach, without needing to employ any traditional inverse analysis techniques or any machine learning models, as is normally done in the field. This novel and efficient FE-based inverse analysis framework employs a conventional FE discretisation, splits the loading vector into two parts corresponding to the known and unknown BCs, and then defines a loss function based on that split. In spite of the loading vector split, the loss function preserves the element connectivity. This function is minimised using a gradient-based optimisation. Furthermore, this paper presents a novel modification of this approach, which allows it to generate a range of different solutions satisfying given requirements in a controlled manner. Controlled multiple solution generation in the context of inverse problems and their intrinsic ill-posedness is a novel notion, which has not been explored before. This is done in order to potentially introduce the capability of semi-autonomous system control with intermittent human intervention to the workflow.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06416",
        "abstract url": "https://arxiv.org/abs/2410.06416",
        "title": "Evaluating the Dependency Between Cyclomatic Complexity and Response For Class",
        "rating": "-10",
        "keywords": [],
        "abstract": "In object-oriented programming, it is reasonable to hypothesize that smaller classes with fewer methods are less complex. Should this hypothesis hold true, it would be advisable for programmers to design classes with fewer methods, as complexity significantly contributes to poor maintainability. To test this assumption, we analyzed 862,517 Java classes from 1,000 open GitHub repositories. Our findings indicate a strong Pearson correlation of 0.79 between the cumulative McCabe's Cyclomatic Complexity (CC) of all class methods and the number of methods, a metric known as Response for Class (RFC).",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06425",
        "abstract url": "https://arxiv.org/abs/2410.06425",
        "title": "Embedded State Estimation for Optimization of Cislunar Space Domain Awareness Constellation Design",
        "rating": "-10",
        "keywords": [],
        "abstract": "The traffic in cislunar space is expected to increase over the coming years, leading to a higher likelihood of conjunction events among active satellites, orbital debris, and non-cooperative satellites. This increase necessitates enhanced space domain awareness (SDA) capabilities that include state estimation for targets of interest. Both Earth surface-based and space-based observation platforms in geosynchronous orbit or below face challenges such as range, exclusion, and occlusion that hinder observation. Motivated by the need to place space-based observers in the cislunar space regime to overcome these challenges, this paper proposes a cislunar SDA constellation design and analysis framework that integrates state estimation into an optimization problem for determining the placement of observers for optimal state estimation performance on a set of targets. The proposed multi-observer placement optimization problem samples from a range of possible target orbits. Upon convergence, the optimized constellation is validated against a broader set of targets to assess its effectiveness. Two comparative analyses are presented to evaluate the effects of changes in the sensor tasking procedure and sensor fidelity on the optimized constellation, comparing these to a single observer baseline case. The results demonstrate that the optimized constellations can provide accurate state estimation for various orbit families.",
        "subjects": [
            "math.OC",
            "eess.SY",
            "physics.space-ph"
        ],
        "comment": "36 pages, 14 figures, Journal of Spacecraft and Rockets (accepted)"
    },
    {
        "paper id": "2410.06438",
        "abstract url": "https://arxiv.org/abs/2410.06438",
        "title": "Leroy: Library Learning for Imperative Programming Languages",
        "rating": "-10",
        "keywords": [],
        "abstract": "Library learning is the process of building a library of common functionalities from a given set of programs. Typically, this process is applied in the context of aiding program synthesis: concise functions can help the synthesizer produce modularized code that is smaller in size. Previous work has focused on functional Lisp-like languages, as their regularity makes them more amenable to extracting repetitive structures. Our work introduces Leroy, which extends existing library learning techniques to imperative higher-level programming languages, with the goal of facilitating reusability and ease of maintenance. Leroy wraps the existing Stitch framework for library learning and converts imperative programs into a Lisp-like format using the AST. Our solution uses Stitch to do a top-down, corpus-guided extraction of repetitive expressions. Further, we prune abstractions that cannot be implemented in the programming language and convert the best abstractions back to the original language. We implement our technique in a tool for a subset of the Python programming language and evaluate it on a large corpus of programs. Leroy achieves a compression ratio of 1.04x of the original code base, with a slight expansion when the library is included. Additionally, we show that our technique prunes invalid abstractions.",
        "subjects": [
            "cs.PL"
        ],
        "comment": "Presented at the 5th Intl. Wkshp. on Human Aspects of Types and Reasoning Assistants (HATRA). Pasadena, CA, USA. 2024"
    },
    {
        "paper id": "2410.06440",
        "abstract url": "https://arxiv.org/abs/2410.06440",
        "title": "Checker Bug Detection and Repair in Deep Learning Libraries",
        "rating": "-10",
        "keywords": [],
        "abstract": "Checker bugs in Deep Learning (DL) libraries are critical yet not well-explored. These bugs are often concealed in the input validation and error-checking code of DL libraries and can lead to silent failures, incorrect results, or unexpected program behavior in DL applications. Despite their potential to significantly impact the reliability and performance of DL-enabled systems built with these libraries, checker bugs have received limited attention. We present the first comprehensive study of DL checker bugs in two widely-used DL libraries, i.e., TensorFlow and PyTorch. Initially, we automatically collected a dataset of 2,418 commits from TensorFlow and PyTorch repositories on GitHub from Sept. 2016 to Dec. 2023 using specific keywords related to checker bugs. Through manual inspection, we identified 527 DL checker bugs. Subsequently, we analyzed these bugs from three perspectives, i.e., root causes, symptoms, and fixing patterns. Using the knowledge gained via root cause analysis of checker bugs, we further propose TensorGuard, a proof-of-concept RAG-based LLM-based tool to detect and fix checker bugs in DL libraries via prompt engineering a series of ChatGPT prompts. We evaluated TensorGuard's performance on a test dataset that includes 92 buggy and 135 clean checker-related changes in TensorFlow and PyTorch from January 2024 to July 2024. Our results demonstrate that TensorGuard has high average recall (94.51\\%) using Chain of Thought prompting, a balanced performance between precision and recall using Zero-Shot prompting and Few-Shot prompting strategies. In terms of patch generation, TensorGuard achieves an accuracy of 11.1\\%, which outperforms the state-of-the-art bug repair baseline by 2\\%. We have also applied TensorGuard on the latest six months' checker-related changes (493 changes) of the JAX library from Google, which resulted in the detection of 64 new checker bugs.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06443",
        "abstract url": "https://arxiv.org/abs/2410.06443",
        "title": "Categorizing Social Media Screenshots for Identifying Author Misattribution",
        "rating": "-10",
        "keywords": [],
        "abstract": "Mis/disinformation is a common and dangerous occurrence on social media. Misattribution is a form of mis/disinformation that deals with a false claim of authorship, which means a user is claiming someone said (posted) something they never did. We discuss the difference between misinformation and disinformation and how screenshots are used to spread author misattribution on social media platforms. It is important to be able to find the original post of a screenshot to determine if the screenshot is being correctly attributed. To do this we have built several tools to aid in automating this search process. The first is a Python script that aims to categorize Twitter posts based on their structure, extract the metadata from a screenshot, and use this data to group all the posts within a screenshot together. We tested this process on 75 Twitter posts containing screenshots collected by hand to determine how well the script extracted metadata and grouped the individual posts, F1 = 0.80. The second is a series of scrapers being used to collect a dataset that can train and test a model to differentiate between various social media platforms. We collected 16,620 screenshots have been collected from Facebook, Instagram, Truth Social, and Twitter. Screenshots were taken by the scrapers of the web version and mobile version of each platform in both light and dark mode.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06444",
        "abstract url": "https://arxiv.org/abs/2410.06444",
        "title": "Multi-label Classification for Android Malware Based on Active Learning",
        "rating": "-10",
        "keywords": [],
        "abstract": "The existing malware classification approaches (i.e., binary and family classification) can barely benefit subsequent analysis with their outputs. Even the family classification approaches suffer from lacking a formal naming standard and an incomplete definition of malicious behaviors. More importantly, the existing approaches are powerless for one malware with multiple malicious behaviors, while this is a very common phenomenon for Android malware in the wild. So, neither of them can provide researchers with a direct and comprehensive enough understanding of malware. In this paper, we propose MLCDroid, an ML-based multi-label classification approach that can directly indicate the existence of pre-defined malicious behaviors. With an in-depth analysis, we summarize six basic malicious behaviors from real-world malware with security reports and construct a labeled dataset. We compare the results of 70 algorithm combinations to evaluate the effectiveness (best at 73.3%). Faced with the challenge of the expensive cost of data annotation, we further propose an active learning approach based on data augmentation, which can improve the overall accuracy to 86.7% with a data augmentation of 5,000+ high-quality samples from an unlabeled malware dataset. This is the first multi-label Android malware classification approach intending to provide more information on fine-grained malicious behaviors.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "18 pages, in IEEE Transactions on Dependable and Secure Computing, 2022"
    },
    {
        "paper id": "2410.06453",
        "abstract url": "https://arxiv.org/abs/2410.06453",
        "title": "Challenges of the QWERTY Keyboard for Quechua Speakers in the Puno Region in Per\u00fa",
        "rating": "-10",
        "keywords": [],
        "abstract": "The widespread adoption of the QWERTY keyboard layout, designed primarily for English, presents significant challenges for speakers of indigenous languages such as Quechua, particularly in the Puno region of Peru. This research examines the extent to which the QWERTY layout affects the writing and digital communication of Quechua speakers. Through an analysis of the Quechua languages unique alphabet and character frequency, combined with insights from local speakers, we identify the limitations imposed by the QWERTY system on the efficient digital transcription of Quechua. The study further proposes alternative keyboard layouts, including optimizations of QWERTY and DVORAK, designed to enhance typing efficiency and reduce the digital divide for Quechua speakers. Our findings underscore the need for localized technological solutions to preserve linguistic diversity while improving digital literacy for indigenous communities. The proposed modifications offer a pathway toward more inclusive digital tools that respect and accommodate linguistic diversity.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06454",
        "abstract url": "https://arxiv.org/abs/2410.06454",
        "title": "Efficient Coordination for Distributed Discrete-Event Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Timing control while preserving determinism is often a key requirement for ensuring the safety and correctness of distributed cyber-physical systems (CPS). Discrete-event (DE) systems provide a suitable model of computation (MoC) for time-sensitive distributed CPS. The high-level architecture (HLA) is a useful tool for the distributed simulation of DE systems, but its techniques can be adapted for implementing distributed CPS. However, HLA incurs considerable overhead in network messages conveying timing information between the distributed nodes and the centralized run-time infrastructure (RTI). This paper gives a novel approach and implementation that reduces such network messages while preserving DE semantics. An evaluation of our runtime demonstrates that our approach significantly reduces the volume of messages for timing information in HLA.",
        "subjects": [
            "cs.DC",
            "eess.SY"
        ],
        "comment": "To appear in Proceedings of the 22nd ACM-IEEE International Conference on Formal Methods and Models for System Design (MEMOCODE'24)"
    },
    {
        "paper id": "2410.06485",
        "abstract url": "https://arxiv.org/abs/2410.06485",
        "title": "A Decomposition Approach to the Weighted $k$-server Problem",
        "rating": "-10",
        "keywords": [],
        "abstract": "A natural variant of the classical online $k$-server problem is the Weighted $k$-server problem, where the cost of moving a server is its weight times the distance through which it moves. Despite its apparent simplicity, the weighted $k$-server problem is extremely poorly understood. Specifically, even on uniform metric spaces, finding the optimum competitive ratio of randomized algorithms remains an open problem -- the best upper bound known is $2^{2^{k+O(1)}}$ due to a deterministic algorithm (Bansal et al., 2018), and the best lower bound known is $\u03a9(2^k)$ (Ayyadevara and Chiplunkar, 2021). With the aim of closing this exponential gap between the upper and lower bounds, we propose a decomposition approach for designing a randomized algorithm for weighted $k$-server on uniform metrics. Our first contribution includes two relaxed versions of the problem and a technique to obtain an algorithm for weighted $k$-server from algorithms for the two relaxed versions. Specifically, we prove that if there exists an $\u03b1_1$-competitive algorithm for one version (which we call Weighted $k$-Server - Service Pattern Construction (W$k$S-SPC) and there exists an $\u03b1_2$-competitive algorithm for the other version (which we call Weighted $k$-server - Revealed Service Pattern (W$k$S-RSP)), then there exists an $(\u03b1_1\u03b1_2)$-competitive algorithm for weighted $k$-server on uniform metric spaces. Our second contribution is a $2^{O(k^2)}$-competitive randomized algorithm for W$k$S-RSP. As a consequence, the task of designing a $2^{poly(k)}$-competitive randomized algorithm for weighted $k$-server on uniform metrics reduces to designing a $2^{poly(k)}$-competitive randomized algorithm for W$k$S-SPC. Finally, we also prove that the $\u03a9(2^k)$ lower bound for weighted $k$-server, in fact, holds for W$k$S-RSP.",
        "subjects": [
            "cs.DS",
            "cs.CC",
            "cs.DM"
        ],
        "comment": "In proceedings at the 44th IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science (FSTTCS) 2024. 17 pages, 1 figure"
    },
    {
        "paper id": "2410.06506",
        "abstract url": "https://arxiv.org/abs/2410.06506",
        "title": "Cooperative Multi-Target Positioning for Cell-Free Massive MIMO with Multi-Agent Reinforcement Learning",
        "rating": "-10",
        "keywords": [],
        "abstract": "Cell-free massive multiple-input multiple-output (mMIMO) is a promising technology to empower next-generation mobile communication networks. In this paper, to address the computational complexity associated with conventional fingerprint positioning, we consider a novel cooperative positioning architecture that involves certain relevant access points (APs) to establish positioning similarity coefficients. Then, we propose an innovative joint positioning and correction framework employing multi-agent reinforcement learning (MARL) to tackle the challenges of high-dimensional sophisticated signal processing, which mainly leverages on the received signal strength information for preliminary positioning, supplemented by the angle of arrival information to refine the initial position estimation. Moreover, to mitigate the bias effects originating from remote APs, we design a cooperative weighted K-nearest neighbor (Co-WKNN)-based estimation scheme to select APs with a high correlation to participate in user positioning. In the numerical results, we present comparisons of various user positioning schemes, which reveal that the proposed MARL-based positioning scheme with Co-WKNN can effectively improve positioning performance. It is important to note that the cooperative positioning architecture is a critical element in striking a balance between positioning performance and computational complexity.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06512",
        "abstract url": "https://arxiv.org/abs/2410.06512",
        "title": "In-Band Full-Duplex MIMO Systems for Simultaneous Communications and Sensing: Challenges, Methods, and Future Perspectives",
        "rating": "-10",
        "keywords": [],
        "abstract": "In-band Full-Duplex (FD) Multiple-Input Multiple-Output (MIMO) systems offer a significant opportunity for Integrated Sensing and Communications (ISAC) due to their capability to realize simultaneous signal transmissions and receptions. This feature has been recently exploited to devise spectrum-efficient simultaneous information transmission and monostatic sensing operations, a line of research typically referred to as MIMO FD-ISAC. In this article, capitalizing on a recent FD MIMO architecture with reduced complexity analog cancellation, we present an FD-enabled framework for simultaneous communications and sensing using data signals. In contrast to communications applications, the framework's goal is not to mitigate self interference, since it includes reflections of the downlink data transmissions from targets in the FD node's vicinity, but to optimize the system parameters for the intended dual functionality. The unique characteristics and challenges of a generic MIMO FD-ISAC system are discussed along with a broad overview of state-of-the-art special cases, including numerical investigations. Several directions for future work on FD-enabled ISAC relevant to signal processing communities are also provided.",
        "subjects": [
            "cs.IT",
            "cs.ET",
            "eess.SP"
        ],
        "comment": "12 pages, 5 figures, White Paper to appear at IEEE SPM"
    },
    {
        "paper id": "2410.06514",
        "abstract url": "https://arxiv.org/abs/2410.06514",
        "title": "MORSE: An Efficient Homomorphic Secret Sharing Scheme Enabling Non-Linear Operation",
        "rating": "-10",
        "keywords": [],
        "abstract": "Homomorphic secret sharing (HSS) enables two servers to locally perform functions on encrypted data directly and obtain the results in the form of shares. A Paillier-based HSS solution seamlessly achieves multiplicative homomorphism and consumes less communication costs. Unfortunately, existing Paillier-based HSS schemes suffer from a large private key size, potential calculation error, expensive computation and storage overhead, and only valid on linear operations (e.g., addition and multiplication). To this end, inspired by the Paillier cryptosystem with fast encryption and decryption, we propose MORSE, an efficient homomorphic secret sharing scheme enabling non-linear operation, which enjoys a small key size, no calculation error and low overhead. In terms of functions, MORSE supports addition, subtraction, multiplication, scalar-multiplication, and comparison. Particularly, we carefully design two conversion protocols achieving the mutual conversion between one Paillier ciphertext and two secret shares, which allows MORSE to continuously perform the above operations. Rigorous analyses demonstrate that MORSE securely outputs correct results. Experimental results show that MORSE makes a runtime improvement of up to 9.3 times in terms of secure multiplication, and a communication costs reduction of up to 16.6% in secure comparison, compared to the state-of-the-art.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06515",
        "abstract url": "https://arxiv.org/abs/2410.06515",
        "title": "Studying Practitioners' Expectations on Clear Code Review Comments",
        "rating": "-10",
        "keywords": [],
        "abstract": "The code review comment (CRC) is pivotal in the process of modern code review. It provides reviewers with the opportunity to identify potential bugs, offer constructive feedback, and suggest improvements. Clear and concise code review comments (CRCs) facilitate the communication between developers and is crucial to the correct understanding of the issues identified and proposed solutions. Despite the importance of CRCs' clarity, there is still a lack of guidelines on what constitutes a good clarity and how to evaluate it. In this paper, we conduct a comprehensive study on understanding and evaluating the clarity of CRCs. We first derive a set of attributes related to the clarity of CRCs, namely RIE attributes (i.e., Relevance, Informativeness, and Expression), as well as their corresponding evaluation criteria based on our literature review and survey with practitioners. We then investigate the clarity of CRCs in open-source projects written in nine programming languages and find that a large portion (i.e., 28.8%) of the CRCs lack the clarity in at least one of the attributes. Finally, we propose ClearCRC, an automated framework that evaluates the clarity of CRCs. Experimental results show that ClearCRC can effectively evaluate the clarity of CRCs and outperform the baselines.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06526",
        "abstract url": "https://arxiv.org/abs/2410.06526",
        "title": "KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we introduce Knowledge-Orthogonal Reasoning (KOR), which minimizes the impact of domain-specific knowledge for a more accurate evaluation of models' reasoning abilities in out-of-distribution scenarios. Based on this concept, we propose the Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench), encompassing five task categories: Operation, Logic, Cipher, Puzzle, and Counterfactual. KOR-Bench emphasizes the effectiveness of models in applying new rule descriptions to solve novel rule-driven questions, revealing that top-performing models like Claude-3.5-Sonnet and GPT-4o only achieve 58.96% and 58.00% accuracy, respectively. We conduct thorough analyses to identify bottlenecks in the Cipher task using Stepwise Prompting, discovering that two rounds of Self-Correction yield optimal results. Complex Task Processing evaluates model performance across three integrated tasks, while we also explore the impact of Tricks on the Puzzle task and visualize rule-focused attention to enhance our understanding of model behavior. We aim for KOR-Bench to be a valuable resource for enhancing models' reasoning capabilities and fostering further research in this field.",
        "subjects": [
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06528",
        "abstract url": "https://arxiv.org/abs/2410.06528",
        "title": "Deciding subspace reachability problems with application to Skolem's Problem",
        "rating": "-10",
        "keywords": [],
        "abstract": "The higher-dimensional version of Kannan and Lipton's Orbit Problem asks whether it is decidable if a target vector space can be reached from a starting point under repeated application of a linear transformation. This problem has remained open since its formulation, and in fact generalizes Skolem's Problem -- a long-standing open problem concerning the existence of zeros in linear recurrence sequences. Both problems have traditionally been studied using algebraic and number theoretic machinery. In contrast, this paper reduces the Orbit Problem to an equivalent version in real projective space, introducing a basic geometric reference for examining and deciding problem instances. We find this geometric toolkit enables basic proofs of sweeping assertions concerning the decidability of certain problem classes, including results where the only other known proofs rely on sophisticated number-theoretic arguments.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "23 pages, 0 figures, feedback welcome"
    },
    {
        "paper id": "2410.07258",
        "abstract url": "https://arxiv.org/abs/2410.07258",
        "title": "BlockMEDC: Blockchain Smart Contracts for Securing Moroccan Higher Education Digital Certificates",
        "rating": "-10",
        "keywords": [],
        "abstract": "Morocco's Vision 2030, known as Maroc Digital 2030, aims to position the country as a regional leader in digital technology by boosting digital infrastructure, fostering innovation, and advancing digital skills. Complementing this initiative, the Pacte ESRI 2030 strategy, launched in 2023, seeks to transform the higher education, research, and innovation sectors by integrating state-of-the-art digital technologies. In alignment with these national strategies, this paper introduces BlockMEDC, a blockchain-based system for securing and managing Moroccan educational digital certificates. Leveraging Ethereum smart contracts and the InterPlanetary File System, BlockMEDC automates the issuance, management, and verification of academic credentials across Moroccan universities. The proposed system addresses key issues such as document authenticity, manual verification, and lack of interoperability, delivering a secure, transparent, and cost-effective solution that aligns with Morocco's digital transformation goals for the education sector.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    }
]