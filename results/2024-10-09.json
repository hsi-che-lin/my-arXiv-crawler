[
    {
        "paper id": "2410.06645",
        "abstract url": "https://arxiv.org/abs/2410.06645",
        "title": "Continual Learning in the Frequency Domain",
        "rating": "2.5",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Continual learning (CL) is designed to learn new tasks while preserving existing knowledge. Replaying samples from earlier tasks has proven to be an effective method to mitigate the forgetting of previously acquired knowledge. However, the current research on the training efficiency of rehearsal-based methods is insufficient, which limits the practical application of CL systems in resource-limited scenarios. The human visual system (HVS) exhibits varying sensitivities to different frequency components, enabling the efficient elimination of visually redundant information. Inspired by HVS, we propose a novel framework called Continual Learning in the Frequency Domain (CLFD). To our knowledge, this is the first study to utilize frequency domain features to enhance the performance and efficiency of CL training on edge devices. For the input features of the feature extractor, CLFD employs wavelet transform to map the original input image into the frequency domain, thereby effectively reducing the size of input feature maps. Regarding the output features of the feature extractor, CLFD selectively utilizes output features for distinct classes for classification, thereby balancing the reusability and interference of output features based on the frequency domain similarity of the classes across various tasks. Optimizing only the input and output features of the feature extractor allows for seamless integration of CLFD with various rehearsal-based methods. Extensive experiments conducted in both cloud and edge environments demonstrate that CLFD consistently improves the performance of state-of-the-art (SOTA) methods in both precision and training efficiency. Specifically, CLFD can increase the accuracy of the SOTA CL method by up to 6.83% and reduce the training time by 2.6$\\times$.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by NeurIPS 2024"
    },
    {
        "paper id": "2410.07112",
        "abstract url": "https://arxiv.org/abs/2410.07112",
        "title": "VHELM: A Holistic Evaluation of Vision Language Models",
        "rating": "2.5",
        "keywords": [
            [
                "Vision Language",
                "VLMs"
            ],
            [
                "cs.AI",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiency-focused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website (https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "NeurIPS 2024. First three authors contributed equally"
    },
    {
        "paper id": "2410.06682",
        "abstract url": "https://arxiv.org/abs/2410.06682",
        "title": "Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization",
        "rating": "2",
        "keywords": [
            [
                "audio-visual"
            ],
            [
                "cs.CV",
                "eess.IV",
                "cs.CL"
            ]
        ],
        "abstract": "Videos contain a wealth of information, and generating detailed and accurate descriptions in natural language is a key aspect of video understanding. In this paper, we present video-SALMONN 2, an advanced audio-visual large language model (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with paired audio) captioning through directed preference optimization (DPO). We propose new metrics to evaluate the completeness and accuracy of video descriptions, which are optimized using DPO. To further improve training, we introduce a novel multi-round DPO (mrDPO) approach, which involves periodically updating the DPO reference model, merging and re-initializing the LoRA module as a proxy for parameter updates after each training round (1,000 steps), and incorporating guidance from ground-truth video captions to stabilize the process. To address potential catastrophic forgetting of non-captioning abilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO LLM by using the captions generated by the mrDPO-trained model as supervised labels. Experiments show that mrDPO significantly enhances video-SALMONN 2's captioning accuracy, reducing global and local error rates by 40\\% and 20\\%, respectively, while decreasing the repetition rate by 35\\%. The final video-SALMONN 2 model, with just 7 billion parameters, surpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining competitive performance to the state-of-the-art on widely used video question-answering benchmark among models of similar size. Upon acceptance, we will release the code, model checkpoints, and training and test data. Demos are available at \\href{https://video-salmonn-2.github.io}{https://video-salmonn-2.github.io}.",
        "subjects": [
            "cs.CV",
            "cs.CL",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06795",
        "abstract url": "https://arxiv.org/abs/2410.06795",
        "title": "From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Hallucinations in large vision-language models (LVLMs) are a significant challenge, i.e., generating objects that are not presented in the visual input, which impairs their reliability. Recent studies often attribute hallucinations to a lack of understanding of visual input, yet ignore a more fundamental issue: the model's inability to effectively extract or decouple visual features. In this paper, we revisit the hallucinations in LVLMs from an architectural perspective, investigating whether the primary cause lies in the visual encoder (feature extraction) or the modal alignment module (feature decoupling). Motivated by our findings on the preliminary investigation, we propose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs. This plug-and-play method can be integrated into various LVLMs, utilizing adaptive virtual tokens to extract object features from bounding boxes, thereby addressing hallucinations caused by insufficient decoupling of visual features. PATCH achieves state-of-the-art performance on multiple multi-modal hallucination datasets. We hope this approach provides researchers with deeper insights into the underlying causes of hallucinations in LVLMs, fostering further advancements and innovation in this field.",
        "subjects": [
            "cs.CL",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06912",
        "abstract url": "https://arxiv.org/abs/2410.06912",
        "title": "Compositional Entailment Learning for Hyperbolic Vision-Language Models",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Image-text representation learning forms a cornerstone in vision-language models, where pairs of images and textual descriptions are contrastively aligned in a shared embedding space. Since visual and textual concepts are naturally hierarchical, recent work has shown that hyperbolic space can serve as a high-potential manifold to learn vision-language representation with strong downstream performance. In this work, for the first time we show how to fully leverage the innate hierarchical nature of hyperbolic embeddings by looking beyond individual image-text pairs. We propose Compositional Entailment Learning for hyperbolic vision-language models. The idea is that an image is not only described by a sentence but is itself a composition of multiple object boxes, each with their own textual description. Such information can be obtained freely by extracting nouns from sentences and using openly available localized grounding models. We show how to hierarchically organize images, image boxes, and their textual descriptions through contrastive and entailment-based objectives. Empirical evaluation on a hyperbolic vision-language model trained with millions of image-text pairs shows that the proposed compositional learning approach outperforms conventional Euclidean CLIP learning, as well as recent hyperbolic alternatives, with better zero-shot and retrieval generalization and clearly stronger hierarchical performance.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "23 pages, 12 figures, 8 tables"
    },
    {
        "paper id": "2410.06967",
        "abstract url": "https://arxiv.org/abs/2410.06967",
        "title": "$\\texttt{ModSCAN}$: Measuring Stereotypical Bias in Large Vision-Language Models from Vision and Language Modalities",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CY"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Large vision-language models (LVLMs) have been rapidly developed and widely used in various fields, but the (potential) stereotypical bias in the model is largely unexplored. In this study, we present a pioneering measurement framework, $\\texttt{ModSCAN}$, to $\\underline{SCAN}$ the stereotypical bias within LVLMs from both vision and language $\\underline{Mod}$alities. $\\texttt{ModSCAN}$ examines stereotypical biases with respect to two typical stereotypical attributes (gender and race) across three kinds of scenarios: occupations, descriptors, and persona traits. Our findings suggest that 1) the currently popular LVLMs show significant stereotype biases, with CogVLM emerging as the most biased model; 2) these stereotypical biases may stem from the inherent biases in the training dataset and pre-trained models; 3) the utilization of specific prompt prefixes (from both vision and language modalities) performs well in reducing stereotypical biases. We believe our work can serve as the foundation for understanding and addressing stereotypical bias in LVLMs.",
        "subjects": [
            "cs.CR",
            "cs.CY"
        ],
        "comment": "Accepted in EMNLP 2024. 29 pages, 22 figures"
    },
    {
        "paper id": "2410.07030",
        "abstract url": "https://arxiv.org/abs/2410.07030",
        "title": "Clean Evaluations on Contaminated Visual Language Models",
        "rating": "2",
        "keywords": [
            [
                "Visual Language",
                "VLMs"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "How to evaluate large language models (LLMs) cleanly has been established as an important research era to genuinely report the performance of possibly contaminated LLMs. Yet, how to cleanly evaluate the visual language models (VLMs) is an under-studied problem. We propose a novel approach to achieve such goals through data augmentation methods on the visual input information. We then craft a new visual clean evaluation benchmark with thousands of data instances. Through extensive experiments, we found that the traditional visual data augmentation methods are useful, but they are at risk of being used as a part of the training data as a workaround. We further propose using BGR augmentation to switch the colour channel of the visual information. We found that it is a simple yet effective method for reducing the effect of data contamination and fortunately, it is also harmful to be used as a data augmentation method during training. It means that it is hard to integrate such data augmentation into training by malicious trainers and it could be a promising technique to cleanly evaluate visual LLMs. Our code, data, and model weights will be released upon publication.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07073",
        "abstract url": "https://arxiv.org/abs/2410.07073",
        "title": "Pixtral 12B",
        "rating": "2",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07149",
        "abstract url": "https://arxiv.org/abs/2410.07149",
        "title": "Towards Interpreting Visual Information Processing in Vision-Language Models",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Vision-Language Models (VLMs) are powerful tools for processing and understanding text and images. We study the processing of visual tokens in the language model component of LLaVA, a prominent VLM. Our approach focuses on analyzing the localization of object information, the evolution of visual token representations across layers, and the mechanism of integrating visual information for predictions. Through ablation studies, we demonstrated that object identification accuracy drops by over 70\\% when object-specific tokens are removed. We observed that visual token representations become increasingly interpretable in the vocabulary space across layers, suggesting an alignment with textual tokens corresponding to image content. Finally, we found that the model extracts object information from these refined representations at the last token position for prediction, mirroring the process in text-only language models for factual association tasks. These findings provide crucial insights into how VLMs process and integrate visual information, bridging the gap between our understanding of language and vision models, and paving the way for more interpretable and controllable multimodal systems.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07167",
        "abstract url": "https://arxiv.org/abs/2410.07167",
        "title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "We present the Modality Integration Rate (MIR), an effective, robust, and generalized metric to indicate the multi-modal pre-training quality of Large Vision Language Models (LVLMs). Large-scale pre-training plays a critical role in building capable LVLMs, while evaluating its training quality without the costly supervised fine-tuning stage is under-explored. Loss, perplexity, and in-context evaluation results are commonly used pre-training metrics for Large Language Models (LLMs), while we observed that these metrics are less indicative when aligning a well-trained LLM with a new modality. Due to the lack of proper metrics, the research of LVLMs in the critical pre-training stage is hindered greatly, including the training data choice, efficient module design, etc. In this paper, we propose evaluating the pre-training quality from the inter-modal distribution distance perspective and present MIR, the Modality Integration Rate, which is 1) \\textbf{Effective} to represent the pre-training quality and show a positive relation with the benchmark performance after supervised fine-tuning. 2) \\textbf{Robust} toward different training/evaluation data. 3) \\textbf{Generalize} across training configurations and architecture choices. We conduct a series of pre-training experiments to explore the effectiveness of MIR and observe satisfactory results that MIR is indicative about training data selection, training strategy schedule, and model architecture design to get better pre-training results. We hope MIR could be a helpful metric for building capable LVLMs and inspire the following research about modality alignment in different areas. Our code is at: https://github.com/shikiw/Modality-Integration-Rate.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": "Project page: https://github.com/shikiw/Modality-Integration-Rate"
    },
    {
        "paper id": "2410.07383",
        "abstract url": "https://arxiv.org/abs/2410.07383",
        "title": "SparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers",
        "rating": "2",
        "keywords": [
            [
                "parameter-efficient",
                "PEFT",
                "Efficient Fine-tuning"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The performance of Transformer models has been enhanced by increasing the number of parameters and the length of the processed text. Consequently, fine-tuning the entire model becomes a memory-intensive process. High-performance methods for parameter-efficient fine-tuning (PEFT) typically work with Attention blocks and often overlook MLP blocks, which contain about half of the model parameters. We propose a new selective PEFT method, namely SparseGrad, that performs well on MLP blocks. We transfer layer gradients to a space where only about 1\\% of the layer's elements remain significant. By converting gradients into a sparse structure, we reduce the number of updated parameters. We apply SparseGrad to fine-tune BERT and RoBERTa for the NLU task and LLaMa-2 for the Question-Answering task. In these experiments, with identical memory requirements, our method outperforms LoRA and MeProp, robust popular state-of-the-art PEFT approaches.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07571",
        "abstract url": "https://arxiv.org/abs/2410.07571",
        "title": "How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Vision-Language adaptation (VL adaptation) transforms Large Language Models (LLMs) into Large Vision-Language Models (LVLMs) for multimodal tasks, but this process often compromises the inherent safety capabilities embedded in the original LLMs. Despite potential harmfulness due to weakened safety measures, in-depth analysis on the effects of VL adaptation on safety remains under-explored. This study examines how VL adaptation influences safety and evaluates the impact of safety fine-tuning methods. Our analysis reveals that safety degradation occurs during VL adaptation, even when the training data is safe. While safety tuning techniques like supervised fine-tuning with safety datasets or reinforcement learning from human feedback mitigate some risks, they still lead to safety degradation and a reduction in helpfulness due to over-rejection issues. Further analysis of internal model weights suggests that VL adaptation may impact certain safety-related layers, potentially lowering overall safety levels. Additionally, our findings demonstrate that the objectives of VL adaptation and safety tuning are divergent, which often results in their simultaneous application being suboptimal. To address this, we suggest the weight merging approach as an optimal solution effectively reducing safety degradation while maintaining helpfulness. These insights help guide the development of more reliable and secure LVLMs for real-world applications.",
        "subjects": [
            "cs.CL",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06554",
        "abstract url": "https://arxiv.org/abs/2410.06554",
        "title": "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Reinforcement Learning from Human Feedback significantly enhances Natural Language Processing by aligning language models with human expectations. A critical factor in this alignment is the strength of reward models used during training. This study explores whether stronger reward models invariably lead to better language models. In this paper, through experiments on relevance, factuality, and completeness tasks using the QA-FEEDBACK dataset and reward models based on Longformer, we uncover a surprising paradox: language models trained with moderately accurate reward models outperform those guided by highly accurate ones. This challenges the widely held belief that stronger reward models always lead to better language models, and opens up new avenues for future research into the key factors driving model performance and how to choose the most suitable reward models. Code and additional details are available at [https://github.com/EIT-NLP/AccuracyParadox-RLHF](https://github.com/EIT-NLP/AccuracyParadox-RLHF).",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "10 pages, 27 figures (including 18 in the appendix), submitted to EMNLP 2024"
    },
    {
        "paper id": "2410.06558",
        "abstract url": "https://arxiv.org/abs/2410.06558",
        "title": "Deep Correlated Prompting for Visual Recognition with Missing Modalities",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Large-scale multimodal models have shown excellent performance over a series of tasks powered by the large corpus of paired multimodal training data. Generally, they are always assumed to receive modality-complete inputs. However, this simple assumption may not always hold in the real world due to privacy constraints or collection difficulty, where models pretrained on modality-complete data easily demonstrate degraded performance on missing-modality cases. To handle this issue, we refer to prompt learning to adapt large pretrained multimodal models to handle missing-modality scenarios by regarding different missing cases as different types of input. Instead of only prepending independent prompts to the intermediate layers, we present to leverage the correlations between prompts and input features and excavate the relationships between different layers of prompts to carefully design the instructions. We also incorporate the complementary semantics of different modalities to guide the prompting design for each modality. Extensive experiments on three commonly-used datasets consistently demonstrate the superiority of our method compared to the previous approaches upon different missing scenarios. Plentiful ablations are further given to show the generalizability and reliability of our method upon different modality-missing ratios and types.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "NeurIPS 2024, Update the checklist"
    },
    {
        "paper id": "2410.06733",
        "abstract url": "https://arxiv.org/abs/2410.06733",
        "title": "Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "While advancements in NLP have significantly improved the performance of Large Language Models (LLMs) on tasks requiring vertical thinking, their lateral thinking capabilities remain under-explored and challenging to measure due to the complexity of assessing creative thought processes and the scarcity of relevant data. To address these challenges, we introduce SPLAT, a benchmark leveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs. This benchmark, containing 975 graded situation puzzles across three difficulty levels, employs a new multi-turn player-judge framework instead of the traditional model-based evaluation, which often necessitates a stronger evaluation model. This framework simulates an interactive game where the model (player) asks the evaluation model (judge) questions about an incomplete story to infer the full scenario. The judge answers based on a detailed reference scenario or evaluates if the player's predictions align with the reference one. This approach lessens dependence on more robust evaluation models, enabling the assessment of state-of-the-art LLMs. The experiments demonstrate that a robust evaluation model, such as WizardLM-2, closely matches human judgements in both intermediate question-answering and final scenario accuracy, achieving over 80% agreement-similar to the agreement levels among humans. Furthermore, applying data and reasoning processes from our benchmark to other lateral thinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to performance enhancements. This suggests that our benchmark effectively evaluates and elicits the lateral thinking abilities of LLMs. Code is available at: https://github.com/chenqi008/LateralThinking.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "Accepted by NeurIPS 2024"
    },
    {
        "paper id": "2410.06741",
        "abstract url": "https://arxiv.org/abs/2410.06741",
        "title": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Multi-task learning (MTL) benefits the fine-tuning of large language models (LLMs) by providing a single model with improved performance and generalization ability across tasks, presenting a resource-efficient alternative to developing separate models for each task. Yet, existing MTL strategies for LLMs often fall short by either being computationally intensive or failing to ensure simultaneous task convergence. This paper presents CoBa, a new MTL approach designed to effectively manage task convergence balance with minimal computational overhead. Utilizing Relative Convergence Scores (RCS), Absolute Convergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically adjusts task weights during the training process, ensuring that the validation loss of all tasks progress towards convergence at an even pace while mitigating the issue of individual task divergence. The results of our experiments involving three disparate datasets underscore that this approach not only fosters equilibrium in task improvement but enhances the LLMs' performance by up to 13% relative to the second-best baselines. Code is open-sourced at https://github.com/codefuse-ai/MFTCoder.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "15 pages, main conference of EMNLP 2024"
    },
    {
        "paper id": "2410.06765",
        "abstract url": "https://arxiv.org/abs/2410.06765",
        "title": "To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "In recent years, multimodal large language models (MLLMs) have garnered significant attention from both industry and academia. However, there is still considerable debate on constructing MLLM architectures, particularly regarding the selection of appropriate connectors for perception tasks of varying granularities. This paper systematically investigates the impact of connectors on MLLM performance. Specifically, we classify connectors into feature-preserving and feature-compressing types. Utilizing a unified classification standard, we categorize sub-tasks from three comprehensive benchmarks, MMBench, MME, and SEED-Bench, into three task types: coarse-grained perception, fine-grained perception, and reasoning, and evaluate the performance. Our findings reveal that feature-preserving connectors excel in \\emph{fine-grained perception} tasks due to their ability to retain detailed visual information. In contrast, feature-compressing connectors, while less effective in fine-grained perception tasks, offer significant speed advantages and perform comparably in \\emph{coarse-grained perception} and \\emph{reasoning} tasks. These insights are crucial for guiding MLLM architecture design and advancing the optimization of MLLM architectures.",
        "subjects": [
            "cs.CL",
            "cs.CV"
        ],
        "comment": "Accepted to EMNLP 2024 Main Conference"
    },
    {
        "paper id": "2410.06802",
        "abstract url": "https://arxiv.org/abs/2410.06802",
        "title": "Seg2Act: Global Context-aware Action Generation for Document Logical Structuring",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Document logical structuring aims to extract the underlying hierarchical structure of documents, which is crucial for document intelligence. Traditional approaches often fall short in handling the complexity and the variability of lengthy documents. To address these issues, we introduce Seg2Act, an end-to-end, generation-based method for document logical structuring, revisiting logical structure extraction as an action generation task. Specifically, given the text segments of a document, Seg2Act iteratively generates the action sequence via a global context-aware generative model, and simultaneously updates its global context and current logical structure based on the generated actions. Experiments on ChCatExt and HierDoc datasets demonstrate the superior performance of Seg2Act in both supervised and transfer learning settings.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by EMNLP 2024 Main Conference"
    },
    {
        "paper id": "2410.06949",
        "abstract url": "https://arxiv.org/abs/2410.06949",
        "title": "Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks, leading to fragile code. This problem is particularly evident in open source projects and impacts the overall quality of the software ecosystem. To address this challenge, we explore the use of large language models (LLMs) to improve exception handling in code. Through extensive analysis, we identify three key issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception Types, and Distorted Handling Solutions. These problems are widespread across real world repositories, suggesting that robust exception handling practices are often overlooked or mishandled. In response, we propose Seeker, a multi agent framework inspired by expert developer strategies for exception handling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist LLMs in detecting, capturing, and resolving exceptions more effectively. Our work is the first systematic study on leveraging LLMs to enhance exception handling practices, providing valuable insights for future improvements in code reliability.",
        "subjects": [
            "cs.SE",
            "cs.CL"
        ],
        "comment": "26 pages, 7 figures. Submitted ICLR 2025"
    },
    {
        "paper id": "2410.07166",
        "abstract url": "https://arxiv.org/abs/2410.07166",
        "title": "Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We aim to evaluate Large Language Models (LLMs) for embodied decision making. While a significant body of work has been leveraging LLMs for decision making in embodied environments, we still lack a systematic understanding of their performance because they are usually applied in different domains, for different purposes, and built based on different inputs and outputs. Furthermore, existing evaluations tend to rely solely on a final success rate, making it difficult to pinpoint what ability is missing in LLMs and where the problem lies, which in turn blocks embodied agents from leveraging LLMs effectively and selectively. To address these limitations, we propose a generalized interface (Embodied Agent Interface) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules. Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics which break down evaluation into various types of errors, such as hallucination errors, affordance errors, various types of planning errors, etc. Overall, our benchmark offers a comprehensive assessment of LLMs' performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems, and providing insights for effective and selective use of LLMs in embodied decision making.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "comment": "Accepted for oral presentation at NeurIPS 2024 in the Datasets and Benchmarks track"
    },
    {
        "paper id": "2410.07331",
        "abstract url": "https://arxiv.org/abs/2410.07331",
        "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously design the evaluation suite to ensure the accuracy and robustness of the evaluation. We develop the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at [https://da-code-bench.github.io](https://da-code-bench.github.io).",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "EMNLP 2024"
    },
    {
        "paper id": "2410.07391",
        "abstract url": "https://arxiv.org/abs/2410.07391",
        "title": "The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks",
        "rating": "1.5",
        "keywords": [
            [
                "vision language"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "There is increasing interest in tracking the capabilities of general intelligence foundation models. This study benchmarks leading large language models and vision language models against human performance on the Wechsler Adult Intelligence Scale (WAIS-IV), a comprehensive, population-normed assessment of underlying human cognition and intellectual abilities, with a focus on the domains of VerbalComprehension (VCI), Working Memory (WMI), and Perceptual Reasoning (PRI). Most models demonstrated exceptional capabilities in the storage, retrieval, and manipulation of tokens such as arbitrary sequences of letters and numbers, with performance on the Working Memory Index (WMI) greater or equal to the 99.5th percentile when compared to human population normative ability. Performance on the Verbal Comprehension Index (VCI) which measures retrieval of acquired information, and linguistic understanding about the meaning of words and their relationships to each other, also demonstrated consistent performance at or above the 98th percentile. Despite these broad strengths, we observed consistently poor performance on the Perceptual Reasoning Index (PRI; range 0.1-10th percentile) from multimodal models indicating profound inability to interpret and reason on visual information. Smaller and older model versions consistently performed worse, indicating that training data, parameter count and advances in tuning are resulting in significant advances in cognitive ability.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07461",
        "abstract url": "https://arxiv.org/abs/2410.07461",
        "title": "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Network pruning has emerged as a potential solution to make LLMs cheaper to deploy. However, existing LLM pruning approaches universally rely on the C4 dataset as the calibration data for calculating pruning scores, leaving its optimality unexplored. In this study, we evaluate the choice of calibration data on LLM pruning, across a wide range of datasets that are most commonly used in LLM training and evaluation, including four pertaining datasets as well as three categories of downstream tasks encompassing nine datasets. Each downstream dataset is prompted with In-Context Learning (ICL) and Chain-of-Thought (CoT), respectively. Besides the already intriguing observation that the choice of calibration data significantly impacts the performance of pruned LLMs, our results also uncover several subtle and often unexpected findings, summarized as follows: (1) C4 is not the optimal choice for LLM pruning, even among commonly used pre-training datasets; (2) arithmetic datasets, when used as calibration data, performs on par or even better than pre-training datasets; (3) pruning with downstream datasets does not necessarily help the corresponding downstream task, compared to pre-training data; (4) ICL is widely beneficial to all data categories, whereas CoT is only useful on certain tasks. Our findings shed light on the importance of carefully selecting calibration data for LLM pruning and pave the way for more efficient deployment of these powerful models in real-world applications. We release our code at: https://github.com/abx393/llm-pruning-calibration-data.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "EMNLP 2024"
    },
    {
        "paper id": "2410.07520",
        "abstract url": "https://arxiv.org/abs/2410.07520",
        "title": "News Reporter: A Multi-lingual LLM Framework for Broadcast T.V News",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have fast become an essential tools to many conversational chatbots due to their ability to provide coherent answers for varied queries. Datasets used to train these LLMs are often a mix of generic and synthetic samples, thus lacking the verification needed to provide correct and verifiable answers for T.V. News. We collect and share a large collection of QA pairs extracted from transcripts of news recordings from various news-channels across the United States. Resultant QA pairs are then used to fine-tune an off-the-shelf LLM model. Our model surpasses base models of similar size on several open LLM benchmarks. We further integrate and propose a RAG method to improve contextualization of our answers and also point it to a verifiable news recording.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "5 pages, under review at ICASSP 2025"
    },
    {
        "paper id": "2410.07547",
        "abstract url": "https://arxiv.org/abs/2410.07547",
        "title": "Comprehensive Online Training and Deployment for Spiking Neural Networks",
        "rating": "1.5",
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Spiking Neural Networks (SNNs) are considered to have enormous potential in the future development of Artificial Intelligence (AI) due to their brain-inspired and energy-efficient properties. In the current supervised learning domain of SNNs, compared to vanilla Spatial-Temporal Back-propagation (STBP) training, online training can effectively overcome the risk of GPU memory explosion and has received widespread academic attention. However, the current proposed online training methods cannot tackle the inseparability problem of temporal dependent gradients and merely aim to optimize the training memory, resulting in no performance advantages compared to the STBP training models in the inference phase. To address the aforementioned challenges, we propose Efficient Multi-Precision Firing (EM-PF) model, which is a family of advanced spiking models based on floating-point spikes and binary synaptic weights. We point out that EM-PF model can effectively separate temporal gradients and achieve full-stage optimization towards computation speed and memory footprint. Experimental results have demonstrated that EM-PF model can be flexibly combined with various techniques including random back-propagation, parallel computation and channel attention mechanism, to achieve state-of-the-art performance with extremely low computational overhead in the field of online learning.",
        "subjects": [
            "cs.NE",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07549",
        "abstract url": "https://arxiv.org/abs/2410.07549",
        "title": "OneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Entity Linking (EL) is the process of associating ambiguous textual mentions to specific entities in a knowledge base. Traditional EL methods heavily rely on large datasets to enhance their performance, a dependency that becomes problematic in the context of few-shot entity linking, where only a limited number of examples are available for training. To address this challenge, we present OneNet, an innovative framework that utilizes the few-shot learning capabilities of Large Language Models (LLMs) without the need for fine-tuning. To the best of our knowledge, this marks a pioneering approach to applying LLMs to few-shot entity linking tasks. OneNet is structured around three key components prompted by LLMs: (1) an entity reduction processor that simplifies inputs by summarizing and filtering out irrelevant entities, (2) a dual-perspective entity linker that combines contextual cues and prior knowledge for precise entity linking, and (3) an entity consensus judger that employs a unique consistency algorithm to alleviate the hallucination in the entity linking reasoning. Comprehensive evaluations across seven benchmark datasets reveal that OneNet outperforms current state-of-the-art entity linking methods.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Accepted by EMNLP 2024 Main"
    },
    {
        "paper id": "2410.07579",
        "abstract url": "https://arxiv.org/abs/2410.07579",
        "title": "Teddy: Efficient Large-Scale Dataset Distillation via Taylor-Approximated Matching",
        "rating": "1.5",
        "keywords": [
            [
                "memory-efficient"
            ],
            [
                "graph"
            ],
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Dataset distillation or condensation refers to compressing a large-scale dataset into a much smaller one, enabling models trained on this synthetic dataset to generalize effectively on real data. Tackling this challenge, as defined, relies on a bi-level optimization algorithm: a novel model is trained in each iteration within a nested loop, with gradients propagated through an unrolled computation graph. However, this approach incurs high memory and time complexity, posing difficulties in scaling up to large datasets such as ImageNet. Addressing these concerns, this paper introduces Teddy, a Taylor-approximated dataset distillation framework designed to handle large-scale dataset and enhance efficiency. On the one hand, backed up by theoretical analysis, we propose a memory-efficient approximation derived from Taylor expansion, which transforms the original form dependent on multi-step gradients to a first-order one. On the other hand, rather than repeatedly training a novel model in each iteration, we unveil that employing a pre-cached pool of weak models, which can be generated from a single base model, enhances both time efficiency and performance concurrently, particularly when dealing with large-scale datasets. Extensive experiments demonstrate that the proposed Teddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet and original-sized ImageNet-1K dataset, notably surpassing prior methods by up to 12.8%, while reducing 46.6% runtime. Our code will be available at https://github.com/Lexie-YU/Teddy.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by ECCV2024"
    },
    {
        "paper id": "2410.07593",
        "abstract url": "https://arxiv.org/abs/2410.07593",
        "title": "A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks",
        "rating": "1.5",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "text-to-image"
            ],
            [
                "cs.AI",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Recent advancements in Vision-Language Models (VLMs) have enabled complex multimodal tasks by processing text and image data simultaneously, significantly enhancing the field of artificial intelligence. However, these models often exhibit biases that can skew outputs towards societal stereotypes, thus necessitating debiasing strategies. Existing debiasing methods focus narrowly on specific modalities or tasks, and require extensive retraining. To address these limitations, this paper introduces Selective Feature Imputation for Debiasing (SFID), a novel methodology that integrates feature pruning and low confidence imputation (LCI) to effectively reduce biases in VLMs. SFID is versatile, maintaining the semantic integrity of outputs and costly effective by eliminating the need for retraining. Our experimental results demonstrate SFID's effectiveness across various VLMs tasks including zero-shot classification, text-to-image retrieval, image captioning, and text-to-image generation, by significantly reducing gender biases without compromising performance. This approach not only enhances the fairness of VLMs applications but also preserves their efficiency and utility across diverse scenarios.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "NeurIPS 2024, the Thirty-Eighth Annual Conference on Neural Information Processing Systems"
    },
    {
        "paper id": "2410.06550",
        "abstract url": "https://arxiv.org/abs/2410.06550",
        "title": "Investigating Cost-Efficiency of LLM-Generated Training Data for Conversational Semantic Frame Analysis",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Recent studies have demonstrated that few-shot learning allows LLMs to generate training data for supervised models at a low cost. However, the quality of LLM-generated data may not entirely match that of human-labeled data. This raises a crucial question: how should one balance the trade-off between the higher quality but more expensive human data and the lower quality yet substantially cheaper LLM-generated data? In this paper, we synthesized training data for conversational semantic frame analysis using GPT-4 and examined how to allocate budgets optimally to achieve the best performance. Our experiments, conducted across various budget levels, reveal that optimal cost-efficiency is achieved by combining both human and LLM-generated data across a wide range of budget levels. Notably, as the budget decreases, a higher proportion of LLM-generated data becomes more preferable.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "12 pages including 4 pages of references and appendix. 7 figures"
    },
    {
        "paper id": "2410.06553",
        "abstract url": "https://arxiv.org/abs/2410.06553",
        "title": "DCP: Learning Accelerator Dataflow for Neural Network via Propagation",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "eess.IV"
            ]
        ],
        "abstract": "Deep neural network (DNN) hardware (HW) accelerators have achieved great success in improving DNNs' performance and efficiency. One key reason is dataflow in executing a DNN layer, including on-chip data partitioning, computation parallelism, and scheduling policy, which have large impacts on latency and energy consumption. Unlike prior works that required considerable efforts from HW engineers to design suitable dataflows for different DNNs, this work proposes an efficient data-centric approach, named Dataflow Code Propagation (DCP), to automatically find the optimal dataflow for DNN layers in seconds without human effort. It has several attractive benefits that prior arts do not have. (i) We translate the HW dataflow configuration into a code representation in a unified dataflow coding space, which can be optimized by backpropagating gradients given a DNN layer or network. (ii) DCP learns a neural predictor to efficiently update the dataflow codes towards the desired gradient directions to minimize various optimization objectives e.g., latency and energy. (iii) It can be easily generalized to unseen HW configurations in a zero-shot or few-shot learning manner. For example, without using additional training data, DCP surpasses the GAMMA method that performs a full search using thousands of samples. Extensive experiments on several representative models such as MobileNet, ResNet, and ViT show that DCP outperforms its counterparts in various settings.",
        "subjects": [
            "cs.LG",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06555",
        "abstract url": "https://arxiv.org/abs/2410.06555",
        "title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "As multimodal large language models (MLLMs) continue to demonstrate increasingly competitive performance across a broad spectrum of tasks, more intricate and comprehensive benchmarks have been developed to assess these cutting-edge models. These benchmarks introduce new challenges to core capabilities such as perception, reasoning, and planning. However, existing multimodal benchmarks fall short in providing a focused evaluation of multi-step planning based on spatial relationships in images. To bridge this gap, we present ING-VP, the first INteractive Game-based Vision Planning benchmark, specifically designed to evaluate the spatial imagination and multi-step reasoning abilities of MLLMs. ING-VP features 6 distinct games, encompassing 300 levels, each with 6 unique configurations. A single model engages in over 60,000 rounds of interaction. The benchmark framework allows for multiple comparison settings, including image-text vs. text-only inputs, single-step vs. multi-step reasoning, and with-history vs. without-history conditions, offering valuable insights into the model's capabilities. We evaluated numerous state-of-the-art MLLMs, with the highest-performing model, Claude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the anticipated standard. This work aims to provide a specialized evaluation framework to drive advancements in MLLMs' capacity for complex spatial reasoning and planning. The code is publicly available at https://github.com/Thisisus7/ING-VP.git.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "49 pages, 12 figures"
    },
    {
        "paper id": "2410.06577",
        "abstract url": "https://arxiv.org/abs/2410.06577",
        "title": "Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in Transformer-based large language models (LLMs) have set new standards in natural language processing. However, the classical softmax attention incurs significant computational costs, leading to a $O(T)$ complexity for per-token generation, where $T$ represents the context length. This work explores reducing LLMs' complexity while maintaining performance by introducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an innovative data-dependent tempered selection (DDTS) mechanism within a linear attention-based, purely recurrent framework, achieving significant accuracy while drastically reducing the memory usage typically associated with recurrent models. This method exemplifies semantic compression by maintaining essential input information with fixed-size hidden states. Building on this, Rodimus$+$ combines Rodimus with the innovative Sliding Window Shared-Key Attention (SW-SKA) in a hybrid approach, effectively leveraging the complementary semantic, token, and head compression techniques. Our experiments demonstrate that Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior downstream performance against models trained on more tokens, including Qwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the accuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints will be available soon.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06593",
        "abstract url": "https://arxiv.org/abs/2410.06593",
        "title": "Towards Natural Image Matting in the Wild via Real-Scenario Prior",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent approaches attempt to adapt powerful interactive segmentation models, such as SAM, to interactive matting and fine-tune the models based on synthetic matting datasets. However, models trained on synthetic data fail to generalize to complex and occlusion scenes. We address this challenge by proposing a new matting dataset based on the COCO dataset, namely COCO-Matting. Specifically, the construction of our COCO-Matting includes accessory fusion and mask-to-matte, which selects real-world complex images from COCO and converts semantic segmentation masks to matting labels. The built COCO-Matting comprises an extensive collection of 38,251 human instance-level alpha mattes in complex natural scenarios. Furthermore, existing SAM-based matting methods extract intermediate features and masks from a frozen SAM and only train a lightweight matting decoder by end-to-end matting losses, which do not fully exploit the potential of the pre-trained SAM. Thus, we propose SEMat which revamps the network architecture and training objectives. For network architecture, the proposed feature-aligned transformer learns to extract fine-grained edge and transparency features. The proposed matte-aligned decoder aims to segment matting-specific objects and convert coarse masks into high-precision mattes. For training objectives, the proposed regularization and trimap loss aim to retain the prior from the pre-trained model and push the matting logits extracted from the mask decoder to contain trimap-based semantic information. Extensive experiments across seven diverse datasets demonstrate the superior performance of our method, proving its efficacy in interactive natural image matting. We open-source our code, models, and dataset at https://github.com/XiaRho/SEMat.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06615",
        "abstract url": "https://arxiv.org/abs/2410.06615",
        "title": "$\u03b2$-calibration of Language Model Confidence Scores for Generative QA",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim to ensure that the confidence score is on average indicative of the likelihood that the answer is correct. We argue, however, that this standard (average-case) notion of calibration is difficult to interpret for decision-making in generative QA. To address this, we generalize the standard notion of average calibration and introduce $\u03b2$-calibration, which ensures calibration holds across different question-and-answer groups. We then propose discretized posthoc calibration schemes for achieving $\u03b2$-calibration.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06617",
        "abstract url": "https://arxiv.org/abs/2410.06617",
        "title": "Learning Evolving Tools for Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Tool learning enables large language models (LLMs) to interact with external tools and APIs, greatly expanding the application scope of LLMs. However, due to the dynamic nature of external environments, these tools and APIs may become outdated over time, preventing LLMs from correctly invoking tools. Existing research primarily focuses on static environments and overlooks this issue, limiting the adaptability of LLMs in real-world applications. In this paper, we propose ToolEVO, a novel framework designed to enhance the adaptive and reflective capabilities of LLMs against tool variability. By leveraging Monte Carlo Tree Search, ToolEVO facilitates active exploration and interaction of LLMs within dynamic environments, allowing for autonomous self-reflection and self-updating of tool usage based on environmental feedback. Additionally, we introduce ToolQA-D, a benchmark specifically designed to evaluate the impact of tool variability. Extensive experiments demonstrate the effectiveness and stability of our approach, highlighting the importance of adaptability to tool variability for effective tool learning.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Ongoning Work"
    },
    {
        "paper id": "2410.06618",
        "abstract url": "https://arxiv.org/abs/2410.06618",
        "title": "Decomposing Relationship from 1-to-N into N 1-to-1 for Text-Video Retrieval",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Text-video retrieval (TVR) has seen substantial advancements in recent years, fueled by the utilization of pre-trained models and large language models (LLMs). Despite these advancements, achieving accurate matching in TVR remains challenging due to inherent disparities between video and textual modalities and irregularities in data representation. In this paper, we propose Text-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the conventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships. By replacing a single text query with a series of text proxies, TV-ProxyNet not only broadens the query scope but also achieves a more precise expansion. Each text proxy is crafted through a refined iterative process, controlled by mechanisms we term as the director and dash, which regulate the proxy's direction and distance relative to the original text query. This setup not only facilitates more precise semantic alignment but also effectively manages the disparities and noise inherent in multimodal data. Our experiments on three representative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet Captions, demonstrate the effectiveness of TV-ProxyNet. The results show an improvement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved state-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0% improvement on DiDeMo compared to existing methods, validating our approach's ability to enhance semantic mapping and reduce error propensity.",
        "subjects": [
            "cs.CV",
            "cs.IR",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06625",
        "abstract url": "https://arxiv.org/abs/2410.06625",
        "title": "ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time",
        "rating": "1",
        "keywords": [
            [
                "Vision Language",
                "VLMs"
            ],
            [
                "attacks"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Vision Language Models (VLMs) have become essential backbones for multimodal intelligence, yet significant safety challenges limit their real-world application. While textual inputs are often effectively safeguarded, adversarial visual inputs can easily bypass VLM defense mechanisms. Existing defense methods are either resource-intensive, requiring substantial data and compute, or fail to simultaneously ensure safety and usefulness in responses. To address these limitations, we propose a novel two-phase inference-time alignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual contents and output responses to establish a robust safety awareness in multimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep levels by conditioning the VLMs' generative distribution with an interference prefix and performing sentence-level best-of-N to search the most harmless and helpful generation paths. Extensive experiments show that ETA outperforms baseline methods in terms of harmlessness, helpfulness, and efficiency, reducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6% win-ties in GPT-4 helpfulness evaluation. The code is publicly available at https://github.com/DripNowhy/ETA.",
        "subjects": [
            "cs.CV",
            "cs.CL",
            "cs.PL"
        ],
        "comment": "27pages"
    },
    {
        "paper id": "2410.06638",
        "abstract url": "https://arxiv.org/abs/2410.06638",
        "title": "Subtle Errors Matter: Preference Learning via Error-injected Self-editing",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have exhibited strong mathematical reasoning and computational prowess, tackling tasks ranging from basic arithmetic to advanced competition-level problems. However, frequently occurring subtle errors, such as miscalculations or incorrect substitutions, limit the models' full mathematical potential. Existing studies to improve mathematical ability typically involve distilling reasoning skills from stronger LLMs or applying preference learning to step-wise response pairs. Although these methods leverage samples of varying granularity to mitigate reasoning errors, they overlook the frequently occurring subtle errors. A major reason is that sampled preference pairs involve differences unrelated to the errors, which may distract the model from focusing on subtle errors. In this work, we propose a novel preference learning framework called eRror-Injected Self-Editing (RISE), which injects predefined subtle errors into partial tokens of correct solutions to construct hard pairs for error mitigation. In detail, RISE uses the model itself to edit a small number of tokens in the solution, injecting designed subtle errors. Then, pairs composed of self-edited solutions and their corresponding correct ones, along with pairs of correct and incorrect solutions obtained through sampling, are used together for subtle error-aware DPO training. Compared with other preference learning methods, RISE further refines the training objective to focus on predefined errors and their tokens, without requiring fine-grained sampling or preference annotation. Extensive experiments validate the effectiveness of RISE, with preference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0% on GSM8K and 7.9% on MATH.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06664",
        "abstract url": "https://arxiv.org/abs/2410.06664",
        "title": "Decouple-Then-Merge: Towards Better Training for Diffusion Models",
        "rating": "1",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion models are trained by learning a sequence of models that reverse each step of noise corruption. Typically, the model parameters are fully shared across multiple timesteps to enhance training efficiency. However, since the denoising tasks differ at each timestep, the gradients computed at different timesteps may conflict, potentially degrading the overall performance of image generation. To solve this issue, this work proposes a Decouple-then-Merge (DeMe) framework, which begins with a pretrained model and finetunes separate models tailored to specific timesteps. We introduce several improved techniques during the finetuning stage to promote effective knowledge sharing while minimizing training interference across timesteps. Finally, after finetuning, these separate models can be merged into a single model in the parameter space, ensuring efficient and practical inference. Experimental results show significant generation quality improvements upon 6 benchmarks including Stable Diffusion on COCO30K, ImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN Bedroom, and CIFAR10.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06667",
        "abstract url": "https://arxiv.org/abs/2410.06667",
        "title": "Large Language Models as Code Executors: An Exploratory Study",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The capabilities of Large Language Models (LLMs) have significantly evolved, extending from natural language processing to complex tasks like code understanding and generation. We expand the scope of LLMs' capabilities to a broader context, using LLMs to execute code snippets to obtain the output. This paper pioneers the exploration of LLMs as code executors, where code snippets are directly fed to the models for execution, and outputs are returned. We are the first to comprehensively examine this feasibility across various LLMs, including OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, the o1 model achieved over 90% accuracy in code execution, while others demonstrated lower accuracy levels. Furthermore, we introduce an Iterative Instruction Prompting (IIP) technique that processes code snippets line by line, enhancing the accuracy of weaker models by an average of 7.22% (with the highest improvement of 18.96%) and an absolute average improvement of 3.86% against CoT prompting (with the highest improvement of 19.46%). Our study not only highlights the transformative potential of LLMs in coding but also lays the groundwork for future advancements in automated programming and the completion of complex tasks.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06670",
        "abstract url": "https://arxiv.org/abs/2410.06670",
        "title": "LS-EEND: Long-Form Streaming End-to-End Neural Diarization with Online Attractor Extraction",
        "rating": "1",
        "keywords": [
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "This work proposes a frame-wise online/streaming end-to-end neural diarization (EEND) method, which detects speaker activities in a frame-in-frame-out fashion. The proposed model mainly consists of a causal embedding encoder and an online attractor decoder. Speakers are modeled in the self-attention-based decoder along both the time and speaker dimensions, and frame-wise speaker attractors are automatically generated and updated for new speakers and existing speakers, respectively. Retention mechanism is employed and especially adapted for long-form diarization with a linear temporal complexity. A multi-step progressive training strategy is proposed for gradually learning from easy tasks to hard tasks in terms of the number of speakers and audio length. Finally, the proposed model (referred to as long-form streaming EEND, LS-EEND) is able to perform streaming diarization for a high (up to 8) and flexible number speakers and very long (say one hour) audio recordings. Experiments on various simulated and real-world datasets show that: 1) when not using oracle speech activity information, the proposed model achieves new state-of-the-art online diarization error rate on all datasets, including CALLHOME (12.11%), DIHARD II (27.58%), DIHARD III (19.61%), and AMI (20.76%); 2) Due to the frame-in-frame-out processing fashion and the linear temporal complexity, the proposed model achieves several times lower real-time-factor than comparison online diarization models.",
        "subjects": [
            "eess.AS",
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06672",
        "abstract url": "https://arxiv.org/abs/2410.06672",
        "title": "Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The hypothesis of Universality in interpretability suggests that different neural networks may converge to implement similar algorithms on similar tasks. In this work, we investigate two mainstream architectures for language modeling, namely Transformers and Mambas, to explore the extent of their mechanistic similarity. We propose to use Sparse Autoencoders (SAEs) to isolate interpretable features from these models and show that most features are similar in these two models. We also validate the correlation between feature similarity and Universality. We then delve into the circuit-level analysis of Mamba models and find that the induction circuits in Mamba are structurally analogous to those in Transformers. We also identify a nuanced difference we call \\emph{Off-by-One motif}: The information of one token is written into the SSM state in its next position. Whilst interaction between tokens in Transformers does not exhibit such trend.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "22 pages, 13 figures"
    },
    {
        "paper id": "2410.06694",
        "abstract url": "https://arxiv.org/abs/2410.06694",
        "title": "OmniPose6D: Towards Short-Term Object Pose Tracking in Dynamic Scenes from Monocular RGB",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "To address the challenge of short-term object pose tracking in dynamic environments with monocular RGB input, we introduce a large-scale synthetic dataset OmniPose6D, crafted to mirror the diversity of real-world conditions. We additionally present a benchmarking framework for a comprehensive comparison of pose tracking algorithms. We propose a pipeline featuring an uncertainty-aware keypoint refinement network, employing probabilistic modeling to refine pose estimation. Comparative evaluations demonstrate that our approach achieves performance superior to existing baselines on real datasets, underscoring the effectiveness of our synthetic dataset and refinement technique in enhancing tracking precision in dynamic contexts. Our contributions set a new precedent for the development and assessment of object pose tracking methodologies in complex scenes.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "13 pages, 9 figures"
    },
    {
        "paper id": "2410.06699",
        "abstract url": "https://arxiv.org/abs/2410.06699",
        "title": "Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models",
        "rating": "1",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "Attacks"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Large vision-language models (LVLMs) integrate visual information into large language models, showcasing remarkable multi-modal conversational capabilities. However, the visual modules introduces new challenges in terms of robustness for LVLMs, as attackers can craft adversarial images that are visually clean but may mislead the model to generate incorrect answers. In general, LVLMs rely on vision encoders to transform images into visual tokens, which are crucial for the language models to perceive image contents effectively. Therefore, we are curious about one question: Can LVLMs still generate correct responses when the encoded visual tokens are attacked and disrupting the visual information? To this end, we propose a non-targeted attack method referred to as VT-Attack (Visual Tokens Attack), which constructs adversarial examples from multiple perspectives, with the goal of comprehensively disrupting feature representations and inherent relationships as well as the semantic properties of visual tokens output by image encoders. Using only access to the image encoder in the proposed attack, the generated adversarial examples exhibit transferability across diverse LVLMs utilizing the same image encoder and generality across different tasks. Extensive experiments validate the superior attack performance of the VT-Attack over baseline methods, demonstrating its effectiveness in attacking LVLMs with image encoders, which in turn can provide guidance on the robustness of LVLMs, particularly in terms of the stability of the visual feature space.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted to ACMMM 2024"
    },
    {
        "paper id": "2410.06707",
        "abstract url": "https://arxiv.org/abs/2410.06707",
        "title": "Calibrating Verbalized Probabilities for Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Calibrating verbalized probabilities presents a novel approach for reliably assessing and leveraging outputs from black-box Large Language Models (LLMs). Recent methods have demonstrated improved calibration by applying techniques like Platt scaling or temperature scaling to the confidence scores generated by LLMs. In this paper, we explore the calibration of verbalized probability distributions for discriminative tasks. First, we investigate the capability of LLMs to generate probability distributions over categorical labels. We theoretically and empirically identify the issue of re-softmax arising from the scaling of verbalized probabilities, and propose using the invert softmax trick to approximate the \"logit\" by inverting verbalized probabilities. Through extensive evaluation on three public datasets, we demonstrate: (1) the robust capability of LLMs in generating class distributions, and (2) the effectiveness of the invert softmax trick in estimating logits, which, in turn, facilitates post-calibration adjustments.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "21 pages"
    },
    {
        "paper id": "2410.06716",
        "abstract url": "https://arxiv.org/abs/2410.06716",
        "title": "Guaranteed Generation from Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "As large language models (LLMs) are increasingly used across various applications, there is a growing need to control text generation to satisfy specific constraints or requirements. This raises a crucial question: Is it possible to guarantee strict constraint satisfaction in generated outputs while preserving the distribution of the original model as much as possible? We first define the ideal distribution - the one closest to the original model, which also always satisfies the expressed constraint - as the ultimate goal of guaranteed generation. We then state a fundamental limitation, namely that it is impossible to reach that goal through autoregressive training alone. This motivates the necessity of combining training-time and inference-time methods to enforce such guarantees. Based on this insight, we propose GUARD, a simple yet effective approach that combines an autoregressive proposal distribution with rejection sampling. Through GUARD's theoretical properties, we show how controlling the KL divergence between a specific proposal and the target ideal distribution simultaneously optimizes inference speed and distributional closeness. To validate these theoretical concepts, we conduct extensive experiments on two text generation settings with hard-to-satisfy constraints: a lexical constraint scenario and a sentiment reversal scenario. These experiments show that GUARD achieves perfect constraint satisfaction while almost preserving the ideal distribution with highly improved inference efficiency. GUARD provides a principled approach to enforcing strict guarantees for LLMs without compromising their generative capabilities.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "22 pages, 11 figures"
    },
    {
        "paper id": "2410.06718",
        "abstract url": "https://arxiv.org/abs/2410.06718",
        "title": "MatMamba: A Matryoshka State Space Model",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "State Space Models (SSMs) like Mamba2 are a promising alternative to Transformers, with faster theoretical training and inference times -- especially for long context lengths. Recent work on Matryoshka Representation Learning -- and its application to Transformer backbones in works like MatFormer -- showed how to introduce nested granularities of smaller submodels in one universal elastic model. In this work, we present MatMamba: a state space model which combines Matryoshka-style learning with Mamba2, by modifying the block to contain nested dimensions to enable joint training and adaptive inference. MatMamba allows for efficient and adaptive deployment across various model sizes. We train a single large MatMamba model and are able to get a number of smaller nested models for free -- while maintaining or improving upon the performance of a baseline smaller model trained from scratch. We train language and image models at a variety of parameter sizes from 35M to 1.4B. Our results on ImageNet and FineWeb show that MatMamba models scale comparably to Transformers, while having more efficient inference characteristics. This makes MatMamba a practically viable option for deploying large-scale models in an elastic way based on the available inference compute. Code and models are open sourced at \\url{https://github.com/ScaledFoundations/MatMamba}",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "cs.CV"
        ],
        "comment": "10 pages, 7 figures"
    },
    {
        "paper id": "2410.06722",
        "abstract url": "https://arxiv.org/abs/2410.06722",
        "title": "Scaling Laws for Mixed quantization in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Post-training quantization of Large Language Models (LLMs) has proven effective in reducing the computational requirements for running inference on these models. In this study, we focus on a straightforward question: When aiming for a specific accuracy or perplexity target for low-precision quantization, how many high-precision numbers or calculations are required to preserve as we scale LLMs to larger sizes? We first introduce a critical metric named the quantization ratio, which compares the number of parameters quantized to low-precision arithmetic against the total parameter count. Through extensive and carefully controlled experiments across different model families, arithmetic types, and quantization granularities (e.g. layer-wise, matmul-wise), we identify two central phenomenons. 1) The larger the models, the better they can preserve performance with an increased quantization ratio, as measured by perplexity in pre-training tasks or accuracy in downstream tasks. 2) The finer the granularity of mixed-precision quantization (e.g., matmul-wise), the more the model can increase the quantization ratio. We believe these observed phenomena offer valuable insights for future AI hardware design and the development of advanced Efficient AI algorithms.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06777",
        "abstract url": "https://arxiv.org/abs/2410.06777",
        "title": "HERM: Benchmarking and Enhancing Multimodal LLMs for Human-Centric Understanding",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The significant advancements in visual understanding and instruction following from Multimodal Large Language Models (MLLMs) have opened up more possibilities for broader applications in diverse and universal human-centric scenarios. However, existing image-text data may not support the precise modality alignment and integration of multi-grained information, which is crucial for human-centric visual understanding. In this paper, we introduce HERM-Bench, a benchmark for evaluating the human-centric understanding capabilities of MLLMs. Our work reveals the limitations of existing MLLMs in understanding complex human-centric scenarios. To address these challenges, we present HERM-100K, a comprehensive dataset with multi-level human-centric annotations, aimed at enhancing MLLMs' training. Furthermore, we develop HERM-7B, a MLLM that leverages enhanced training data from HERM-100K. Evaluations on HERM-Bench demonstrate that HERM-7B significantly outperforms existing MLLMs across various human-centric dimensions, reflecting the current inadequacy of data annotations used in MLLM training for human-centric visual understanding. This research emphasizes the importance of specialized datasets and benchmarks in advancing the MLLMs' capabilities for human-centric understanding.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06781",
        "abstract url": "https://arxiv.org/abs/2410.06781",
        "title": "Transesophageal Echocardiography Generation using Anatomical Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Through automation, deep learning (DL) can enhance the analysis of transesophageal echocardiography (TEE) images. However, DL methods require large amounts of high-quality data to produce accurate results, which is difficult to satisfy. Data augmentation is commonly used to tackle this issue. In this work, we develop a pipeline to generate synthetic TEE images and corresponding semantic labels. The proposed data generation pipeline expands on an existing pipeline that generates synthetic transthoracic echocardiography images by transforming slices from anatomical models into synthetic images. We also demonstrate that such images can improve DL network performance through a left-ventricle semantic segmentation task. For the pipeline's unpaired image-to-image (I2I) translation section, we explore two generative methods: CycleGAN and contrastive unpaired translation. Next, we evaluate the synthetic images quantitatively using the Fr\u00e9chet Inception Distance (FID) Score and qualitatively through a human perception quiz involving expert cardiologists and the average researcher. In this study, we achieve a dice score improvement of up to 10% when we augment datasets with our synthetic images. Furthermore, we compare established methods of assessing unpaired I2I translation and observe a disagreement when evaluating the synthetic images. Finally, we see which metric better predicts the generated data's efficacy when used for data augmentation.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "MICCAI2023; DALI Workshop"
    },
    {
        "paper id": "2410.06806",
        "abstract url": "https://arxiv.org/abs/2410.06806",
        "title": "QuadMamba: Learning Quadtree-based Selective Scan for Visual State Space Model",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in State Space Models, notably Mamba, have demonstrated superior performance over the dominant Transformer models, particularly in reducing the computational complexity from quadratic to linear. Yet, difficulties in adapting Mamba from language to vision tasks arise due to the distinct characteristics of visual data, such as the spatial locality and adjacency within images and large variations in information granularity across visual tokens. Existing vision Mamba approaches either flatten tokens into sequences in a raster scan fashion, which breaks the local adjacency of images, or manually partition tokens into windows, which limits their long-range modeling and generalization capabilities. To address these limitations, we present a new vision Mamba model, coined QuadMamba, that effectively captures local dependencies of varying granularities via quadtree-based image partition and scan. Concretely, our lightweight quadtree-based scan module learns to preserve the 2D locality of spatial regions within learned window quadrants. The module estimates the locality score of each token from their features, before adaptively partitioning tokens into window quadrants. An omnidirectional window shifting scheme is also introduced to capture more intact and informative features across different local regions. To make the discretized quadtree partition end-to-end trainable, we further devise a sequence masking strategy based on Gumbel-Softmax and its straight-through gradient estimator. Extensive experiments demonstrate that QuadMamba achieves state-of-the-art performance in various vision tasks, including image classification, object detection, instance segmentation, and semantic segmentation. The code is in https://github.com/VISION-SJTU/QuadMamba.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by Neurip2024"
    },
    {
        "paper id": "2410.06809",
        "abstract url": "https://arxiv.org/abs/2410.06809",
        "title": "Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have demonstrated immense utility across various industries. However, as LLMs advance, the risk of harmful outputs increases due to incorrect or malicious instruction prompts. While current methods effectively address jailbreak risks, they share common limitations: 1) Judging harmful responses from the prefill-level lacks utilization of the model's decoding outputs, leading to relatively lower effectiveness and robustness. 2) Rejecting potentially harmful responses based on a single evaluation can significantly impair the model's helpfulness.This paper examines the LLMs' capability to recognize harmful outputs, revealing and quantifying their proficiency in assessing the danger of previous tokens. Motivated by pilot experiment results, we design a robust defense mechanism at the decoding level. Our novel decoder-oriented, step-by-step defense architecture corrects harmful queries directly rather than rejecting them outright. We introduce speculative decoding to enhance usability and facilitate deployment to boost secure decoding speed. Extensive experiments demonstrate that our approach improves model security without compromising reasoning speed. Notably, our method leverages the model's ability to discern hazardous information, maintaining its helpfulness compared to existing methods.",
        "subjects": [
            "cs.CL",
            "cs.CR"
        ],
        "comment": "19 pages, 9 figures"
    },
    {
        "paper id": "2410.06842",
        "abstract url": "https://arxiv.org/abs/2410.06842",
        "title": "SurANet: Surrounding-Aware Network for Concealed Object Detection via Highly-Efficient Interactive Contrastive Learning Strategy",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Concealed object detection (COD) in cluttered scenes is significant for various image processing applications. However, due to that concealed objects are always similar to their background, it is extremely hard to distinguish them. Here, the major obstacle is the tiny feature differences between the inside and outside object boundary region, which makes it trouble for existing COD methods to achieve accurate results. In this paper, considering that the surrounding environment information can be well utilized to identify the concealed objects, and thus, we propose a novel deep Surrounding-Aware Network, namely SurANet, for COD tasks, which introduces surrounding information into feature extraction and loss function to improve the discrimination. First, we enhance the semantics of feature maps using differential fusion of surrounding features to highlight concealed objects. Next, a Surrounding-Aware Contrastive Loss is applied to identify the concealed object via learning surrounding feature maps contrastively. Then, SurANet can be trained end-to-end with high efficiency via our proposed Spatial-Compressed Correlation Transmission strategy after our investigation of feature dynamics, and extensive experiments improve that such features can be well reserved respectively. Finally, experimental results demonstrate that the proposed SurANet outperforms state-of-the-art COD methods on multiple real datasets. Our source code will be available at https://github.com/kyh433/SurANet.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06879",
        "abstract url": "https://arxiv.org/abs/2410.06879",
        "title": "Evaluating Model Performance with Hard-Swish Activation Function Adjustments",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In the field of pattern recognition, achieving high accuracy is essential. While training a model to recognize different complex images, it is vital to fine-tune the model to achieve the highest accuracy possible. One strategy for fine-tuning a model involves changing its activation function. Most pre-trained models use ReLU as their default activation function, but switching to a different activation function like Hard-Swish could be beneficial. This study evaluates the performance of models using ReLU, Swish and Hard-Swish activation functions across diverse image datasets. Our results show a 2.06% increase in accuracy for models on the CIFAR-10 dataset and a 0.30% increase in accuracy for models on the ATLAS dataset. Modifying the activation functions in architecture of pre-trained models lead to improved overall accuracy.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "2 pages"
    },
    {
        "paper id": "2410.06886",
        "abstract url": "https://arxiv.org/abs/2410.06886",
        "title": "FltLM: An Intergrated Long-Context Large Language Model for Effective Context Filtering and Understanding",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The development of Long-Context Large Language Models (LLMs) has markedly advanced natural language processing by facilitating the process of textual data across long documents and multiple corpora. However, Long-Context LLMs still face two critical challenges: The lost in the middle phenomenon, where crucial middle-context information is likely to be missed, and the distraction issue that the models lose focus due to overly extended contexts. To address these challenges, we propose the Context Filtering Language Model (FltLM), a novel integrated Long-Context LLM which enhances the ability of the model on multi-document question-answering (QA) tasks. Specifically, FltLM innovatively incorporates a context filter with a soft mask mechanism, identifying and dynamically excluding irrelevant content to concentrate on pertinent information for better comprehension and reasoning. Our approach not only mitigates these two challenges, but also enables the model to operate conveniently in a single forward pass. Experimental results demonstrate that FltLM significantly outperforms supervised fine-tuning and retrieval-based methods in complex QA scenarios, suggesting a promising solution for more accurate and reliable long-context natural language understanding applications.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by the 27th European Conference on Artificial Intelligence (ECAI-2024), this is the full version of the paper including technical appendices. This final version features enhanced formatting and corrections to errors present in other online versions. We regret any inconvenience this may have caused our readers"
    },
    {
        "paper id": "2410.06898",
        "abstract url": "https://arxiv.org/abs/2410.06898",
        "title": "Generative Model for Less-Resourced Language with 1 billion parameters",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are a basic infrastructure for modern natural language processing. Many commercial and open-source LLMs exist for English, e.g., ChatGPT, Llama, Falcon, and Mistral. As these models are trained on mostly English texts, their fluency and knowledge of low-resource languages and societies are superficial. We present the development of large generative language models for a less-resourced language. GaMS 1B - Generative Model for Slovene with 1 billion parameters was created by continuing pretraining of the existing English OPT model. We developed a new tokenizer adapted to Slovene, Croatian, and English languages and used embedding initialization methods FOCUS and WECHSEL to transfer the embeddings from the English OPT model. We evaluate our models on several classification datasets from the Slovene suite of benchmarks and generative sentence simplification task SENTA. We only used a few-shot in-context learning of our models, which are not yet instruction-tuned. For classification tasks, in this mode, the generative models lag behind the existing Slovene BERT-type models fine-tuned for specific tasks. On a sentence simplification task, the GaMS models achieve comparable or better performance than the GPT-3.5-Turbo model.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06913",
        "abstract url": "https://arxiv.org/abs/2410.06913",
        "title": "Utilize the Flow before Stepping into the Same River Twice: Certainty Represented Knowledge Flow for Refusal-Aware Instruction Tuning",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs) to refuse to answer unknown questions. By modifying responses of unknown questions in the training data to refusal responses such as \"I don't know\", RAIT enhances the reliability of LLMs and reduces their hallucination. Generally, RAIT modifies training samples based on the correctness of the initial LLM's response. However, this crude approach can cause LLMs to excessively refuse answering questions they could have correctly answered, the problem we call over-refusal. In this paper, we explore two primary causes of over-refusal: Static conflict emerges when the RAIT data is constructed solely on correctness criteria, causing similar samples in the LLM's feature space to be assigned different labels (original vs. modified \"I don't know\"). Dynamic conflict occurs due to the changes of LLM's knowledge state during fine-tuning, which transforms previous unknown questions into knowns, while the training data, which is constructed based on the initial LLM, remains unchanged. These conflicts cause the trained LLM to misclassify known questions as unknown, resulting in over-refusal. To address this issue, we introduce Certainty Represented Knowledge Flow for Refusal-Aware Instructions Construction (CRaFT). CRaFT centers on two main contributions: First, we additionally incorporate response certainty to selectively filter and modify data, reducing static conflicts. Second, we implement preliminary rehearsal training to characterize changes in the LLM's knowledge state, which helps mitigate dynamic conflicts during the fine-tuning process. We conducted extensive experiments on open-ended question answering and multiple-choice question task. Experiment results show that CRaFT can improve LLM's overall performance during the RAIT process. Source code and training data will be released at Github.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Equal contribution: Runchuan Zhu, Zhipeng Ma, Jiang Wu; Corresponding author: Conghui He"
    },
    {
        "paper id": "2410.06916",
        "abstract url": "https://arxiv.org/abs/2410.06916",
        "title": "SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Speculative decoding (SD) has emerged as a widely used paradigm to accelerate the inference of large language models (LLMs) without compromising generation quality. It works by first employing a compact model to draft multiple tokens efficiently and then using the target LLM to verify them in parallel. While this technique has achieved notable speedups, most existing approaches necessitate either additional parameters or extensive training to construct effective draft models, thereby restricting their applicability across different LLMs and tasks. To address this limitation, we explore a novel plug-and-play SD solution with layer-skipping, which skips intermediate layers of the target LLM as the compact draft model. Our analysis reveals that LLMs exhibit great potential for self-acceleration through layer sparsity and the task-specific nature of this sparsity. Building on these insights, we introduce SWIFT, an on-the-fly self-speculative decoding algorithm that adaptively selects intermediate layers of LLMs to skip during inference. SWIFT does not require auxiliary models or additional training, making it a plug-and-play solution for accelerating LLM inference across diverse input data streams. Our extensive experiments across a wide range of models and downstream tasks demonstrate that SWIFT can achieve over a 1.3x-1.6x speedup while preserving the original distribution of the generated text.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06927",
        "abstract url": "https://arxiv.org/abs/2410.06927",
        "title": "Spectral and Rhythm Features for Audio Classification with Deep Convolutional Neural Networks",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Convolutional neural networks (CNNs) are widely used in computer vision. They can be used not only for conventional digital image material to recognize patterns, but also for feature extraction from digital imagery representing spectral and rhythm features extracted from time-domain digital audio signals for the acoustic classification of sounds. Different spectral and rhythm feature representations like mel-scaled spectrograms, mel-frequency cepstral coefficients (MFCCs), cyclic tempograms, short-time Fourier transform (STFT) chromagrams, constant-Q transform (CQT) chromagrams and chroma energy normalized statistics (CENS) chromagrams are investigated in terms of the audio classification performance using a deep convolutional neural network. It can be clearly shown that the mel-scaled spectrograms and the mel-frequency cepstral coefficients (MFCCs) perform significantly better then the other spectral and rhythm features investigated in this research for audio classification tasks using deep CNNs. The experiments were carried out with the aid of the ESC-50 dataset with 2,000 labeled environmental audio recordings.",
        "subjects": [
            "cs.SD",
            "cs.LG",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06940",
        "abstract url": "https://arxiv.org/abs/2410.06940",
        "title": "Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think",
        "rating": "1",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that one main bottleneck in training large-scale diffusion models for generation lies in effectively learning these representations. Moreover, training can be made easier by incorporating high-quality external visual representations, rather than relying solely on the diffusion models to learn them independently. We study this by introducing a straightforward regularization called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations obtained from external, pretrained visual encoders. The results are striking: our simple strategy yields significant improvements in both training efficiency and generation quality when applied to popular diffusion and flow-based transformers, such as DiTs and SiTs. For instance, our method can speed up SiT training by over 17.5$\\times$, matching the performance (without classifier-free guidance) of a SiT-XL model trained for 7M steps in less than 400K steps. In terms of final generation quality, our approach achieves state-of-the-art results of FID=1.42 using classifier-free guidance with the guidance interval.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Preprint. Project page: https://sihyun.me/REPA"
    },
    {
        "paper id": "2410.06961",
        "abstract url": "https://arxiv.org/abs/2410.06961",
        "title": "Self-Boosting Large Language Models with Synthetic Preference Data",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Through alignment with human preferences, Large Language Models (LLMs) have advanced significantly in generating honest, harmless, and helpful responses. However, collecting high-quality preference data is a resource-intensive and creativity-demanding process, especially for the continual improvement of LLMs. We introduce SynPO, a self-boosting paradigm that leverages synthetic preference data for model alignment. SynPO employs an iterative mechanism wherein a self-prompt generator creates diverse prompts, and a response improver refines model responses progressively. This approach trains LLMs to autonomously learn the generative rewards for their own outputs and eliminates the need for large-scale annotation of prompts and human preferences. After four SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements in instruction-following abilities, achieving over 22.1% win rate improvements on AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general performance of LLMs on various tasks, validated by a 3.2 to 5.0 average score increase on the well-recognized Open LLM leaderboard.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06965",
        "abstract url": "https://arxiv.org/abs/2410.06965",
        "title": "Uncovering Factor Level Preferences to Improve Human-Model Alignment",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Despite advancements in Large Language Model (LLM) alignment, understanding the reasons behind LLM preferences remains crucial for bridging the gap between desired and actual behavior. LLMs often exhibit biases or tendencies that diverge from human preferences, such as favoring certain writing styles or producing overly verbose outputs. However, current methods for evaluating preference alignment often lack explainability, relying on coarse-grained comparisons. To address this, we introduce PROFILE (PRObing Factors of InfLuence for Explainability), a novel framework that uncovers and quantifies the influence of specific factors driving preferences. PROFILE's factor level analysis explains the 'why' behind human-model alignment and misalignment, offering insights into the direction of model improvement. We apply PROFILE to analyze human and LLM preferences across three tasks: summarization, helpful response generation, and document-based question-answering. Our factor level analysis reveals a substantial discrepancy between human and LLM preferences in generation tasks, whereas LLMs show strong alignment with human preferences in evaluation tasks. We demonstrate how leveraging factor level insights, including addressing misaligned factors or exploiting the generation-evaluation gap, can improve alignment with human preferences. This work underscores the importance of explainable preference analysis and highlights PROFILE's potential to provide valuable training signals, driving further improvements in human-model alignment.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06973",
        "abstract url": "https://arxiv.org/abs/2410.06973",
        "title": "Personal Intelligence System UniLM: Hybrid On-Device Small Language Model and Server-Based Large Language Model for Malay Nusantara",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In contexts with limited computational and data resources, high-resource language models often prove inadequate, particularly when addressing the specific needs of Malay languages. This paper introduces a Personal Intelligence System designed to efficiently integrate both on-device and server-based models. The system incorporates SLiM-34M for on-device processing, optimized for low memory and power usage, and MANYAK-1.3B for server-based tasks, allowing for scalable, high-performance language processing. The models achieve significant results across various tasks, such as machine translation, question-answering, and translate IndoMMLU. Particularly noteworthy is SLiM-34M's ability to achieve a high improvement in accuracy compared to other LLMs while using 2 times fewer pre-training tokens. This work challenges the prevailing assumption that large-scale computational resources are necessary to build effective language models, contributing to the development of resource-efficient models for the Malay language with the unique orchestration between SLiM-34M and MANYAK-1.3B.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "20 pages, 5 tables, 4 figures"
    },
    {
        "paper id": "2410.06981",
        "abstract url": "https://arxiv.org/abs/2410.06981",
        "title": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We investigate feature universality in large language models (LLMs), a research field that aims to understand how different models similarly represent concepts in the latent spaces of their intermediate layers. Demonstrating feature universality allows discoveries about latent representations to generalize across several models. However, comparing features across LLMs is challenging due to polysemanticity, in which individual neurons often correspond to multiple features rather than distinct ones. This makes it difficult to disentangle and match features across different models. To address this issue, we employ a method known as dictionary learning by using sparse autoencoders (SAEs) to transform LLM activations into more interpretable spaces spanned by neurons corresponding to individual features. After matching feature neurons across models via activation correlation, we apply representational space similarity metrics like Singular Value Canonical Correlation Analysis to analyze these SAE features across different LLMs. Our experiments reveal significant similarities in SAE feature spaces across various LLMs, providing new evidence for feature universality.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07002",
        "abstract url": "https://arxiv.org/abs/2410.07002",
        "title": "CursorCore: Assist Programming through Aligning Anything",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models have been successfully applied to programming assistance tasks, such as code completion, code insertion, and instructional code editing. However, these applications remain insufficiently automated and struggle to effectively integrate various types of information during the programming process, including coding history, current code, and user instructions. In this work, we propose a new conversational framework that comprehensively integrates these information sources, collect data to train our models and evaluate their performance. Firstly, to thoroughly evaluate how well models align with different types of information and the quality of their outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to comprehensively assess the performance of models in programming assistance tasks. Then, for data collection, we develop a data generation pipeline, Programming-Instruct, which synthesizes training data from diverse sources, such as GitHub and online judge platforms. This pipeline can automatically generate various types of messages throughout the programming process. Finally, using this pipeline, we generate 219K samples, fine-tune multiple models, and develop the CursorCore series. We show that CursorCore outperforms other models of comparable size. This framework unifies applications such as inline chat and automated editing, contributes to the advancement of coding assistants. Code, models and data are freely available at https://github.com/TechxGenus/CursorCore.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07018",
        "abstract url": "https://arxiv.org/abs/2410.07018",
        "title": "Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Out-of-Distribution (OOD) generalization in machine learning is a burgeoning area of study. Its primary goal is to enhance the adaptability and resilience of machine learning models when faced with new, unseen, and potentially adversarial data that significantly diverges from their original training datasets. In this paper, we investigate time series OOD generalization via pre-trained Large Language Models (LLMs). We first propose a novel \\textbf{T}ri-level learning framework for \\textbf{T}ime \\textbf{S}eries \\textbf{O}OD generalization, termed TTSO, which considers both sample-level and group-level uncertainties. This formula offers a fresh theoretic perspective for formulating and analyzing OOD generalization problem. In addition, we provide a theoretical analysis to justify this method is well motivated. We then develop a stratified localization algorithm tailored for this tri-level optimization problem, theoretically demonstrating the guaranteed convergence of the proposed algorithm. Our analysis also reveals that the iteration complexity to obtain an $\u03b5$-stationary point is bounded by O($\\frac{1}{\u03b5^{2}}$). Extensive experiments on real-world datasets have been conducted to elucidate the effectiveness of the proposed method.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Accepted at NeurIPS 2024"
    },
    {
        "paper id": "2410.07035",
        "abstract url": "https://arxiv.org/abs/2410.07035",
        "title": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities across various domains, including role-playing, creative writing, mathematical reasoning, and coding. Despite these advancements, LLMs still encounter challenges with length control, frequently failing to adhere to specific length constraints due to their token-level operations and insufficient training on data with strict length limitations. We identify this issue as stemming from a lack of positional awareness and propose novel approaches--PositionID Prompting and PositionID Fine-Tuning--to address it. These methods enhance the model's ability to continuously monitor and manage text length during generation. Additionally, we introduce PositionID CP Prompting to enable LLMs to perform copy and paste operations accurately. Furthermore, we develop two benchmarks for evaluating length control and copy-paste abilities. Our experiments demonstrate that our methods significantly improve the model's adherence to length constraints and copy-paste accuracy without compromising response quality.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "39 pages. CP-Bench and LenCtrl-Bench are available in https://huggingface.co/datasets/ZenMoore/CP-Bench and https://huggingface.co/datasets/ZenMoore/LenCtrl-Bench"
    },
    {
        "paper id": "2410.07053",
        "abstract url": "https://arxiv.org/abs/2410.07053",
        "title": "Robots in the Middle: Evaluating LLMs in Dispute Resolution",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Mediation is a dispute resolution method featuring a neutral third-party (mediator) who intervenes to help the individuals resolve their dispute. In this paper, we investigate to which extent large language models (LLMs) are able to act as mediators. We investigate whether LLMs are able to analyze dispute conversations, select suitable intervention types, and generate appropriate intervention messages. Using a novel, manually created dataset of 50 dispute scenarios, we conduct a blind evaluation comparing LLMs with human annotators across several key metrics. Overall, the LLMs showed strong performance, even outperforming our human annotators across dimensions. Specifically, in 62% of the cases, the LLMs chose intervention types that were rated as better than or equivalent to those chosen by humans. Moreover, in 84% of the cases, the intervention messages generated by the LLMs were rated as better than or equal to the intervention messages written by humans. LLMs likewise performed favourably on metrics such as impartiality, understanding and contextualization. Our results demonstrate the potential of integrating AI in online dispute resolution (ODR) platforms.",
        "subjects": [
            "cs.HC",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07062",
        "abstract url": "https://arxiv.org/abs/2410.07062",
        "title": "TinyEmo: Scaling down Emotional Reasoning via Metric Projection",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper introduces TinyEmo, a family of small multi-modal language models for emotional reasoning and classification. Our approach features: (1) a synthetic emotional instruct dataset for both pre-training and fine-tuning stages, (2) a Metric Projector that delegates classification from the language model allowing for more efficient training and inference, (3) a multi-modal large language model (MM-LLM) for emotional reasoning, and (4) a semi-automated framework for bias detection. TinyEmo is able to perform emotion classification and emotional reasoning, all while using substantially fewer parameters than comparable models. This efficiency allows us to freely incorporate more diverse emotional datasets, enabling strong performance on classification tasks, with our smallest model (700M parameters) outperforming larger state-of-the-art models based on general-purpose MM-LLMs with over 7B parameters. Additionally, the Metric Projector allows for interpretability and indirect bias detection in large models without additional training, offering an approach to understand and improve AI systems. We release code, models, and dataset at https://github.com/ggcr/TinyEmo",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07064",
        "abstract url": "https://arxiv.org/abs/2410.07064",
        "title": "Data Selection via Optimal Control for Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics. Based on these theoretical results, we introduce PMP-based Data Selection (PDS), a framework that approximates optimal data selection by solving the PMP conditions. In our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on a wide range of downstream tasks across various model sizes. Moreover, the benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws. PDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which mitigates the quick exhaustion of available web-crawled corpora. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/data_selection.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07069",
        "abstract url": "https://arxiv.org/abs/2410.07069",
        "title": "ReIFE: Re-evaluating Instruction-Following Evaluation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The automatic evaluation of instruction following typically involves using large language models (LLMs) to assess response quality. However, there is a lack of comprehensive evaluation of these LLM-based evaluators across two dimensions: the base LLMs and the evaluation protocols. Therefore, we present a thorough meta-evaluation of instruction following, including 25 base LLMs and 15 recently proposed evaluation protocols, on 4 human-annotated datasets, assessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows us to identify the best-performing base LLMs and evaluation protocols with a high degree of robustness. Moreover, our large-scale evaluation reveals: (1) Base LLM performance ranking remains largely consistent across evaluation protocols, with less capable LLMs showing greater improvement from protocol enhancements; (2) Robust evaluation of evaluation protocols requires many base LLMs with varying capability levels, as protocol effectiveness can depend on the base LLM used; (3) Evaluation results on different datasets are not always consistent, so a rigorous evaluation requires multiple datasets with distinctive features. We release our meta-evaluation suite ReIFE, which provides the codebase and evaluation result collection for more than 500 LLM-evaluator configurations, to support future research in instruction-following evaluation.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "GitHub Repo: https://github.com/yale-nlp/ReIFE, Evaluation Result Collection: https://huggingface.co/datasets/yale-nlp/ReIFE"
    },
    {
        "paper id": "2410.07083",
        "abstract url": "https://arxiv.org/abs/2410.07083",
        "title": "Stanceformer: Target-Aware Transformer for Stance Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The task of Stance Detection involves discerning the stance expressed in a text towards a specific subject or target. Prior works have relied on existing transformer models that lack the capability to prioritize targets effectively. Consequently, these models yield similar performance regardless of whether we utilize or disregard target information, undermining the task's significance. To address this challenge, we introduce Stanceformer, a target-aware transformer model that incorporates enhanced attention towards the targets during both training and inference. Specifically, we design a \\textit{Target Awareness} matrix that increases the self-attention scores assigned to the targets. We demonstrate the efficacy of the Stanceformer with various BERT-based models, including state-of-the-art models and Large Language Models (LLMs), and evaluate its performance across three stance detection datasets, alongside a zero-shot dataset. Our approach Stanceformer not only provides superior performance but also generalizes even to other domains, such as Aspect-based Sentiment Analysis. We make the code publicly available.\\footnote{\\scriptsize\\url{https://github.com/kgarg8/Stanceformer}}",
        "subjects": [
            "cs.CL"
        ],
        "comment": "16 pages, 2 figures, 14 tables including Appendix"
    },
    {
        "paper id": "2410.07093",
        "abstract url": "https://arxiv.org/abs/2410.07093",
        "title": "LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Language plays a vital role in the realm of human motion. Existing methods have largely depended on CLIP text embeddings for motion generation, yet they fall short in effectively aligning language and motion due to CLIP's pretraining on static image-text pairs. This work introduces LaMP, a novel Language-Motion Pretraining model, which transitions from a language-vision to a more suitable language-motion latent space. It addresses key limitations by generating motion-informative text embeddings, significantly enhancing the relevance and semantics of generated motion sequences. With LaMP, we advance three key tasks: text-to-motion generation, motion-text retrieval, and motion captioning through aligned language-motion representation learning. For generation, we utilize LaMP to provide the text condition instead of CLIP, and an autoregressive masked prediction is designed to achieve mask modeling without rank collapse in transformers. For retrieval, motion features from LaMP's motion transformer interact with query tokens to retrieve text features from the text transformer, and vice versa. For captioning, we finetune a large language model with the language-informative motion features to develop a strong motion captioning model. In addition, we introduce the LaMP-BertScore metric to assess the alignment of generated motions with textual descriptions. Extensive experimental results on multiple datasets demonstrate substantial improvements over previous methods across all three tasks. The code of our method will be made public.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07094",
        "abstract url": "https://arxiv.org/abs/2410.07094",
        "title": "An Approach for Auto Generation of Labeling Functions for Software Engineering Chatbots",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Software engineering (SE) chatbots are increasingly gaining attention for their role in enhancing development processes. At the core of chatbots are the Natural Language Understanding platforms (NLUs), which enable them to comprehend and respond to user queries. Before deploying NLUs, there is a need to train them with labeled data. However, acquiring such labeled data for SE chatbots is challenging due to the scarcity of high-quality datasets. This challenge arises because training SE chatbots requires specialized vocabulary and phrases not found in typical language datasets. Consequently, chatbot developers often resort to manually annotating user queries to gather the data necessary for training effective chatbots, a process that is both time-consuming and resource-intensive. Previous studies propose approaches to support chatbot practitioners in annotating users' posed queries. However, these approaches require human intervention to generate rules, called labeling functions (LFs), that identify and categorize user queries based on specific patterns in the data. To address this issue, we propose an approach to automatically generate LFs by extracting patterns from labeled user queries. We evaluate the effectiveness of our approach by applying it to the queries of four diverse SE datasets (namely AskGit, MSA, Ask Ubuntu, and Stack Overflow) and measure the performance improvement gained from training the NLU on the queries labeled by the generated LFs. We find that the generated LFs effectively label data with AUC scores of up to 85.3%, and NLU's performance improvement of up to 27.2% across the studied datasets. Furthermore, our results show that the number of LFs used to generate LFs affects the labeling performance. We believe that our approach can save time and resources in labeling users' queries, allowing practitioners to focus on core chatbot functionalities.",
        "subjects": [
            "cs.SE",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Submitted to IEEE Transactions on Software Engineering for review"
    },
    {
        "paper id": "2410.07095",
        "abstract url": "https://arxiv.org/abs/2410.07095",
        "title": "MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup--OpenAI's o1-preview with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code (github.com/openai/mle-bench/) to facilitate future research in understanding the ML engineering capabilities of AI agents.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "10 pages. Plus 17 pages appendix. 8 figures. Equal contribution by first seven authors. Authors randomized. Work by Neil Chowdhury done while at OpenAI"
    },
    {
        "paper id": "2410.07103",
        "abstract url": "https://arxiv.org/abs/2410.07103",
        "title": "Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Multi-hop reasoning, which requires multi-step reasoning based on the supporting documents within a given context, remains challenging for large language models (LLMs). LLMs often struggle to filter out irrelevant documents within the context, and their performance is sensitive to the position of supporting documents within that context. In this paper, we identify an additional challenge: LLMs' performance is also sensitive to the order in which the supporting documents are presented. We refer to this as the misordered context problem. To address this issue, we propose a simple yet effective method called context repetition (CoRe), which involves prompting the model by repeatedly presenting the context to ensure the supporting documents are presented in the optimal order for the model. Using CoRe, we improve the F1 score by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p on a synthetic task. Additionally, CoRe helps mitigate the well-known \"lost-in-the-middle\" problem in LLMs and can be effectively combined with retrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07109",
        "abstract url": "https://arxiv.org/abs/2410.07109",
        "title": "I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "As Large Language Model (LLM)-based agents become increasingly autonomous and will more freely interact with each other, studying interactions between them becomes crucial to anticipate emergent phenomena and potential risks. Drawing inspiration from the widely popular Stanford Prison Experiment, we contribute to this line of research by studying interaction patterns of LLM agents in a context characterized by strict social hierarchy. We do so by specifically studying two types of phenomena: persuasion and anti-social behavior in simulated scenarios involving a guard and a prisoner agent who seeks to achieve a specific goal (i.e., obtaining additional yard time or escape from prison). Leveraging 200 experimental scenarios for a total of 2,000 machine-machine conversations across five different popular LLMs, we provide a set of noteworthy findings. We first document how some models consistently fail in carrying out a conversation in our multi-agent setup where power dynamics are at play. Then, for the models that were able to engage in successful interactions, we empirically show how the goal that an agent is set to achieve impacts primarily its persuasiveness, while having a negligible effect with respect to the agent's anti-social behavior. Third, we highlight how agents' personas, and particularly the guard's personality, drive both the likelihood of successful persuasion from the prisoner and the emergence of anti-social behaviors. Fourth, we show that even without explicitly prompting for specific personalities, anti-social behavior emerges by simply assigning agents' roles. These results bear implications for the development of interactive LLM agents as well as the debate on their societal impact.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CY",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07110",
        "abstract url": "https://arxiv.org/abs/2410.07110",
        "title": "Continual Learning: Less Forgetting, More OOD Generalization via Adaptive Contrastive Replay",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Machine learning models often suffer from catastrophic forgetting of previously learned knowledge when learning new classes. Various methods have been proposed to mitigate this issue. However, rehearsal-based learning, which retains samples from previous classes, typically achieves good performance but tends to memorize specific instances, struggling with Out-of-Distribution (OOD) generalization. This often leads to high forgetting rates and poor generalization. Surprisingly, the OOD generalization capabilities of these methods have been largely unexplored. In this paper, we highlight this issue and propose a simple yet effective strategy inspired by contrastive learning and data-centric principles to address it. We introduce Adaptive Contrastive Replay (ACR), a method that employs dual optimization to simultaneously train both the encoder and the classifier. ACR adaptively populates the replay buffer with misclassified samples while ensuring a balanced representation of classes and tasks. By refining the decision boundary in this way, ACR achieves a balance between stability and plasticity. Our method significantly outperforms previous approaches in terms of OOD generalization, achieving an improvement of 13.41\\% on Split CIFAR-100, 9.91\\% on Split Mini-ImageNet, and 5.98\\% on Split Tiny-ImageNet.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07113",
        "abstract url": "https://arxiv.org/abs/2410.07113",
        "title": "Personalized Visual Instruction Tuning",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in multimodal large language models (MLLMs) have demonstrated significant progress; however, these models exhibit a notable limitation, which we refer to as \"face blindness\". Specifically, they can engage in general conversations but fail to conduct personalized dialogues targeting at specific individuals. This deficiency hinders the application of MLLMs in personalized settings, such as tailored visual assistants on mobile devices, or domestic robots that need to recognize members of the family. In this paper, we introduce Personalized Visual Instruction Tuning (PVIT), a novel data curation and training framework designed to enable MLLMs to identify target individuals within an image and engage in personalized and coherent dialogues. Our approach involves the development of a sophisticated pipeline that autonomously generates training data containing personalized conversations. This pipeline leverages the capabilities of various visual experts, image generation models, and (multi-modal) large language models. To evaluate the personalized potential of MLLMs, we present a benchmark called P-Bench, which encompasses various question types with different levels of difficulty. The experiments demonstrate a substantial personalized performance enhancement after fine-tuning with our curated dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07118",
        "abstract url": "https://arxiv.org/abs/2410.07118",
        "title": "Exploring the Readiness of Prominent Small Language Models for the Democratization of Financial Literacy",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The use of small language models (SLMs), herein defined as models with less than three billion parameters, is increasing across various domains and applications. Due to their ability to run on more accessible hardware and preserve user privacy, SLMs possess the potential to democratize access to language models for individuals of different socioeconomic status and with different privacy preferences. This study assesses several state-of-the-art SLMs (e.g., Apple's OpenELM, Microsoft's Phi, Google's Gemma, and the Tinyllama project) for use in the financial domain to support the development of financial literacy LMs. Democratizing access to quality financial information for those who are financially under educated is greatly needed in society, particularly as new financial markets and products emerge and participation in financial markets increases due to ease of access. We are the first to examine the use of open-source SLMs to democratize access to financial question answering capabilities for individuals and students. To this end, we provide an analysis of the memory usage, inference time, similarity comparisons to ground-truth answers, and output readability of prominent SLMs to determine which models are most accessible and capable of supporting access to financial information. We analyze zero-shot and few-shot learning variants of the models. The results suggest that some off-the-shelf SLMs merit further exploration and fine-tuning to prepare them for individual use, while others may have limits to their democratization.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07133",
        "abstract url": "https://arxiv.org/abs/2410.07133",
        "title": "EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models",
        "rating": "1",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in generation models have showcased remarkable capabilities in generating fantastic content. However, most of them are trained on proprietary high-quality data, and some models withhold their parameters and only provide accessible application programming interfaces (APIs), limiting their benefits for downstream tasks. To explore the feasibility of training a text-to-image generation model comparable to advanced models using publicly available resources, we introduce EvolveDirector. This framework interacts with advanced models through their public APIs to obtain text-image data pairs to train a base model. Our experiments with extensive data indicate that the model trained on generated data of the advanced model can approximate its generation capability. However, it requires large-scale samples of 10 million or more. This incurs significant expenses in time, computational resources, and especially the costs associated with calling fee-based APIs. To address this problem, we leverage pre-trained large vision-language models (VLMs) to guide the evolution of the base model. VLM continuously evaluates the base model during training and dynamically updates and refines the training dataset by the discrimination, expansion, deletion, and mutation operations. Experimental results show that this paradigm significantly reduces the required data volume. Furthermore, when approaching multiple advanced models, EvolveDirector can select the best samples generated by them to learn powerful and balanced abilities. The final trained model Edgen is demonstrated to outperform these advanced models. The code and model weights are available at https://github.com/showlab/EvolveDirector.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07137",
        "abstract url": "https://arxiv.org/abs/2410.07137",
        "title": "Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a \"null model\" that always outputs a constant response (irrelevant to input instructions) can cheat automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench. Moreover, the crafted cheating outputs are transferable because we assume that the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are private and cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks. The code is available at https://github.com/sail-sg/Cheating-LLM-Benchmarks.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07145",
        "abstract url": "https://arxiv.org/abs/2410.07145",
        "title": "Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "One essential advantage of recurrent neural networks (RNNs) over transformer-based language models is their linear computational complexity concerning the sequence length, which makes them much faster in handling long sequences during inference. However, most publicly available RNNs (e.g., Mamba and RWKV) are trained on sequences with less than 10K tokens, and their effectiveness in longer contexts remains largely unsatisfying so far. In this paper, we study the cause of the inability to process long context for RNNs and suggest critical mitigations. We examine two practical concerns when applying state-of-the-art RNNs to long contexts: (1) the inability to extrapolate to inputs longer than the training length and (2) the upper bound of memory capacity. Addressing the first concern, we first investigate *state collapse* (SC), a phenomenon that causes severe performance degradation on sequence lengths not encountered during training. With controlled experiments, we attribute this to overfitting due to the recurrent state being overparameterized for the training length. For the second concern, we train a series of Mamba-2 models on long documents to empirically estimate the recurrent state capacity in language modeling and passkey retrieval. Then, three SC mitigation methods are proposed to improve Mamba-2's length generalizability, allowing the model to process more than 1M tokens without SC. We also find that the recurrent state capacity in passkey retrieval scales exponentially to the state size, and we empirically train a Mamba-2 370M with near-perfect passkey retrieval accuracy on 256K context length. This suggests a promising future for RNN-based long-context modeling.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "21 pages, 18 figures"
    },
    {
        "paper id": "2410.07168",
        "abstract url": "https://arxiv.org/abs/2410.07168",
        "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
        "rating": "1",
        "keywords": [
            [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Syllables are compositional units of spoken language that play a crucial role in human speech perception and production. However, current neural speech representations lack structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised model that regresses features on syllabic segments distilled from a teacher model which is an exponential moving average of the model in training. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) syllabic units better suited for lexical and syntactic understanding. We also train token-to-speech generative models with our syllabic units and show that fully intelligible speech can be reconstructed from these tokens. Lastly, we observe that categorical perception, a linguistic phenomenon of speech perception, emerges naturally in our model, making the embedding space more categorical and sparse than previous self-supervised learning approaches. Together, we present a novel self-supervised approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
        "subjects": [
            "cs.CL",
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07170",
        "abstract url": "https://arxiv.org/abs/2410.07170",
        "title": "One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Foundation models (FMs) are pre-trained on large-scale datasets and then fine-tuned on a downstream task for a specific application. The most successful and most commonly used fine-tuning method is to update the pre-trained weights via a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are usually initialized at random with a uniform rank distribution across model weights. Recent works focus on weight-driven initialization or learning of adaptive ranks during training. Both approaches have only been investigated in isolation, resulting in slow convergence or a uniform rank distribution, in turn leading to sub-optimal performance. We propose to enhance LoRA by initializing the new weights in a data-driven manner by computing singular value decomposition on minibatches of activation vectors. Then, we initialize the LoRA matrices with the obtained right-singular vectors and re-distribute ranks among all weight matrices to explain the maximal amount of variance and continue the standard LoRA fine-tuning procedure. This results in our new method Explained Variance Adaptation (EVA). We apply EVA to a variety of fine-tuning tasks ranging from language generation and understanding to image classification and reinforcement learning. EVA exhibits faster convergence than competitors and attains the highest average score across a multitude of tasks per domain.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "stat.ML"
        ],
        "comment": "10 pages + references and appendix, code available at https://github.com/ml-jku/EVA"
    },
    {
        "paper id": "2410.07173",
        "abstract url": "https://arxiv.org/abs/2410.07173",
        "title": "Do better language models have crisper vision?",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "How well do text-only Large Language Models (LLMs) grasp the visual world? As LLMs are increasingly used in computer vision, addressing this question becomes both fundamental and pertinent. However, existing studies have primarily focused on limited scenarios, such as their ability to generate visual content or cluster multimodal data. To this end, we propose the Visual Text Representation Benchmark (ViTeRB) to isolate key properties that make language models well-aligned with the visual world. With this, we identify large-scale decoder-based LLMs as ideal candidates for representing text in vision-centric contexts, counter to the current practice of utilizing text encoders. Building on these findings, we propose ShareLock, an ultra-lightweight CLIP-like model. By leveraging precomputable frozen features from strong vision and language models, ShareLock achieves an impressive 51% accuracy on ImageNet despite utilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU hour (or 10 hours including the precomputation of features) - orders of magnitude less than prior methods. Code will be released.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07176",
        "abstract url": "https://arxiv.org/abs/2410.07176",
        "title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Retrieval-Augmented Generation (RAG), while effective in integrating external knowledge to address the limitations of large language models (LLMs), can be undermined by imperfect retrieval, which may introduce irrelevant, misleading, or even malicious information. Despite its importance, previous studies have rarely explored the behavior of RAG through joint analysis on how errors from imperfect retrieval attribute and propagate, and how potential conflicts arise between the LLMs' internal knowledge and external sources. We find that imperfect retrieval augmentation might be inevitable and quite harmful, through controlled analysis under realistic conditions. We identify the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome in the post-retrieval stage of RAG. To render LLMs resilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach that adaptively elicits essential information from LLMs' internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Our experiments using Gemini and Claude demonstrate that Astute RAG significantly outperforms previous robustness-enhanced RAG methods. Notably, Astute RAG is the only approach that matches or exceeds the performance of LLMs without RAG under worst-case scenarios. Further analysis reveals that Astute RAG effectively resolves knowledge conflicts, improving the reliability and trustworthiness of RAG systems.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Preprint"
    },
    {
        "paper id": "2410.07177",
        "abstract url": "https://arxiv.org/abs/2410.07177",
        "title": "MM-Ego: Towards Building Egocentric Multimodal LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "This research aims to comprehensively explore building a multimodal foundation model for egocentric video understanding. To achieve this goal, we work on three fronts. First, as there is a lack of QA data for egocentric video understanding, we develop a data engine that efficiently generates 7M high-quality QA samples for egocentric videos ranging from 30 seconds to one hour long, based on human-annotated data. This is currently the largest egocentric QA dataset. Second, we contribute a challenging egocentric QA benchmark with 629 videos and 7,026 questions to evaluate the models' ability in recognizing and memorizing visual details across videos of varying lengths. We introduce a new de-biasing evaluation method to help mitigate the unavoidable language bias present in the models being evaluated. Third, we propose a specialized multimodal architecture featuring a novel \"Memory Pointer Prompting\" mechanism. This design includes a global glimpse step to gain an overarching understanding of the entire video and identify key visual information, followed by a fallback step that utilizes the key visual information to generate responses. This enables the model to more effectively comprehend extended video content. With the data, benchmark, and model, we successfully build MM-Ego, an egocentric multimodal LLM that shows powerful performance on egocentric video understanding.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Technical Report"
    },
    {
        "paper id": "2410.07277",
        "abstract url": "https://arxiv.org/abs/2410.07277",
        "title": "Swin-BERT: A Feature Fusion System designed for Speech-based Alzheimer's Dementia Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Speech is usually used for constructing an automatic Alzheimer's dementia (AD) detection system, as the acoustic and linguistic abilities show a decline in people living with AD at the early stages. However, speech includes not only AD-related local and global information but also other information unrelated to cognitive status, such as age and gender. In this paper, we propose a speech-based system named Swin-BERT for automatic dementia detection. For the acoustic part, the shifted windows multi-head attention that proposed to extract local and global information from images, is used for designing our acoustic-based system. To decouple the effect of age and gender on acoustic feature extraction, they are used as an extra input of the designed acoustic system. For the linguistic part, the rhythm-related information, which varies significantly between people living with and without AD, is removed while transcribing the audio recordings into transcripts. To compensate for the removed rhythm-related information, the character-level transcripts are proposed to be used as the extra input of a word-level BERT-style system. Finally, the Swin-BERT combines the acoustic features learned from our proposed acoustic-based system with our linguistic-based system. The experiments are based on the two datasets provided by the international dementia detection challenges: the ADReSS and ADReSSo. The results show that both the proposed acoustic and linguistic systems can be better or comparable with previous research on the two datasets. Superior results are achieved by the proposed Swin-BERT system on the ADReSS and ADReSSo datasets, which are 85.58\\% F-score and 87.32\\% F-score respectively.",
        "subjects": [
            "eess.AS",
            "cs.AI",
            "cs.CL",
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07278",
        "abstract url": "https://arxiv.org/abs/2410.07278",
        "title": "Retrieval Replace Reduction: An effective visual token reduction method via semantic match",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Multimodal large language models (MLLMs) have demonstrated strong performance across various tasks without requiring training from scratch. However, they face significant computational and memory constraints, particularly when processing multimodal inputs that exceed context length, limiting their scalability. In this paper, we introduce a new approach, \\textbf{TRSM} (\\textbf{T}oken \\textbf{R}eduction via \\textbf{S}emantic \\textbf{M}atch), which effectively reduces the number of visual tokens without compromising MLLM performance. Inspired by how humans process multimodal tasks, TRSM leverages semantic information from one modality to match relevant semantics in another, reducing the number of visual tokens.Specifically, to retain task relevant visual tokens, we use the text prompt as a query vector to retrieve the most similar vectors from the visual prompt and merge them with the text tokens. Based on experimental results, when applied to LLaVA-1.5\\cite{liu2023}, our approach compresses the visual tokens by 20\\%, achieving comparable performance across diverse visual question-answering and reasoning tasks.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "8 pages, 2 figures,3 tables"
    },
    {
        "paper id": "2410.07394",
        "abstract url": "https://arxiv.org/abs/2410.07394",
        "title": "Structured Spatial Reasoning with Open Vocabulary Object Detectors",
        "rating": "1",
        "keywords": [
            [
                "VLMs"
            ],
            [
                "3D",
                "RGB-D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Reasoning about spatial relationships between objects is essential for many real-world robotic tasks, such as fetch-and-delivery, object rearrangement, and object search. The ability to detect and disambiguate different objects and identify their location is key to successful completion of these tasks. Several recent works have used powerful Vision and Language Models (VLMs) to unlock this capability in robotic agents. In this paper we introduce a structured probabilistic approach that integrates rich 3D geometric features with state-of-the-art open-vocabulary object detectors to enhance spatial reasoning for robotic perception. The approach is evaluated and compared against zero-shot performance of the state-of-the-art Vision and Language Models (VLMs) on spatial reasoning tasks. To enable this comparison, we annotate spatial clauses in real-world RGB-D Active Vision Dataset [1] and conduct experiments on this and the synthetic Semantic Abstraction [2] dataset. Results demonstrate the effectiveness of the proposed method, showing superior performance of grounding spatial relations over state of the art open-source VLMs by more than 20%.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07400",
        "abstract url": "https://arxiv.org/abs/2410.07400",
        "title": "Advocating Character Error Rate for Multilingual ASR Evaluation",
        "rating": "1",
        "keywords": [
            [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Automatic speech recognition (ASR) systems have traditionally been evaluated using English datasets, with the word error rate (WER) serving as the predominant metric. WER's simplicity and ease of interpretation have contributed to its widespread adoption, particularly for English. However, as ASR systems expand to multilingual contexts, WER fails in various ways, particularly with morphologically complex languages or those without clear word boundaries. Our work documents the limitations of WER as an evaluation metric and advocates for the character error rate (CER) as the primary metric in multilingual ASR evaluation. We show that CER avoids many of the challenges WER faces and exhibits greater consistency across writing systems. We support our proposition by conducting human evaluations of ASR transcriptions in three languages: Malayalam, English, and Arabic, which exhibit distinct morphological characteristics. We show that CER correlates more closely with human judgments than WER, even for English. To facilitate further research, we release our human evaluation dataset for future benchmarking of ASR metrics. Our findings suggest that CER should be prioritized, or at least supplemented, in multilingual ASR evaluations to account for the varying linguistic characteristics of different languages.",
        "subjects": [
            "cs.CL",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2410.07405",
        "abstract url": "https://arxiv.org/abs/2410.07405",
        "title": "Exploring Efficient Foundational Multi-modal Models for Video Summarization",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Foundational models are able to generate text outputs given prompt instructions and text, audio, or image inputs. Recently these models have been combined to perform tasks on video, such as video summarization. Such video foundation models perform pre-training by aligning outputs from each modality-specific model into the same embedding space. Then the embeddings from each model are used within a language model, which is fine-tuned on a desired instruction set. Aligning each modality during pre-training is computationally expensive and prevents rapid testing of different base modality models. During fine-tuning, evaluation is carried out within in-domain videos where it is hard to understand the generalizability and data efficiency of these methods. To alleviate these issues we propose a plug-and-play video language model. It directly uses the texts generated from each input modality into the language model, avoiding pre-training alignment overhead. Instead of fine-tuning we leverage few-shot instruction adaptation strategies. We compare the performance versus the computational costs for our plug-and-play style method and baseline tuning methods. Finally, we explore the generalizability of each method during domain shift and present insights on what data is useful when training data is limited. Through this analysis, we present practical insights on how to leverage multi-modal foundational models for effective results given realistic compute and data limitations.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "11 pages, 4 figures"
    },
    {
        "paper id": "2410.07410",
        "abstract url": "https://arxiv.org/abs/2410.07410",
        "title": "Aligning Motion-Blurred Images Using Contrastive Learning on Overcomplete Pixels",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We propose a new contrastive objective for learning overcomplete pixel-level features that are invariant to motion blur. Other invariances (e.g., pose, illumination, or weather) can be learned by applying the corresponding transformations on unlabeled images during self-supervised training. We showcase that a simple U-Net trained with our objective can produce local features useful for aligning the frames of an unseen video captured with a moving camera under realistic and challenging conditions. Using a carefully designed toy example, we also show that the overcomplete pixels can encode the identity of objects in an image and the pixel coordinates relative to these objects.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages, 3 figures"
    },
    {
        "paper id": "2410.07428",
        "abstract url": "https://arxiv.org/abs/2410.07428",
        "title": "The First VoicePrivacy Attacker Challenge Evaluation Plan",
        "rating": "1",
        "keywords": [
            [
                "cs.CL",
                "eess.AS"
            ]
        ],
        "abstract": "The First VoicePrivacy Attacker Challenge is a new kind of challenge organized as part of the VoicePrivacy initiative and supported by ICASSP 2025 as the SP Grand Challenge It focuses on developing attacker systems against voice anonymization, which will be evaluated against a set of anonymization systems submitted to the VoicePrivacy 2024 Challenge. Training, development, and evaluation datasets are provided along with a baseline attacker system. Participants shall develop their attacker systems in the form of automatic speaker verification systems and submit their scores on the development and evaluation data to the organizers. To do so, they can use any additional training data and models, provided that they are openly available and declared before the specified deadline. The metric for evaluation is equal error rate (EER). Results will be presented at the ICASSP 2025 special session to which 5 selected top-ranked participants will be invited to submit and present their challenge systems.",
        "subjects": [
            "eess.AS",
            "cs.CL",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07441",
        "abstract url": "https://arxiv.org/abs/2410.07441",
        "title": "Zero-Shot Generalization of Vision-Based RL Without Data Augmentation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Generalizing vision-based reinforcement learning (RL) agents to novel environments remains a difficult and open challenge. Current trends are to collect large-scale datasets or use data augmentation techniques to prevent overfitting and improve downstream generalization. However, the computational and data collection costs increase exponentially with the number of task variations and can destabilize the already difficult task of training RL agents. In this work, we take inspiration from recent advances in computational neuroscience and propose a model, Associative Latent DisentAnglement (ALDA), that builds on standard off-policy RL towards zero-shot generalization. Specifically, we revisit the role of latent disentanglement in RL and show how combining it with a model of associative memory achieves zero-shot generalization on difficult task variations without relying on data augmentation. Finally, we formally show that data augmentation techniques are a form of weak disentanglement and discuss the implications of this insight.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07463",
        "abstract url": "https://arxiv.org/abs/2410.07463",
        "title": "Language-Guided Joint Audio-Visual Editing via One-Shot Adaptation",
        "rating": "1",
        "keywords": [
            [
                "Audio-Visual"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we introduce a novel task called language-guided joint audio-visual editing. Given an audio and image pair of a sounding event, this task aims at generating new audio-visual content by editing the given sounding event conditioned on the language guidance. For instance, we can alter the background environment of a sounding object while keeping its appearance unchanged, or we can add new sounds contextualized to the visual content. To address this task, we propose a new diffusion-based framework for joint audio-visual editing and introduce two key ideas. Firstly, we propose a one-shot adaptation approach to tailor generative diffusion models for audio-visual content editing. With as few as one audio-visual sample, we jointly transfer the audio and vision diffusion models to the target domain. After fine-tuning, our model enables consistent generation of this audio-visual sample. Secondly, we introduce a cross-modal semantic enhancement approach. We observe that when using language as content editing guidance, the vision branch may overlook editing requirements. This phenomenon, termed catastrophic neglect, hampers audio-visual alignment during content editing. We therefore enhance semantic consistency between language and vision to mitigate this issue. Extensive experiments validate the effectiveness of our method in language-based audio-visual editing and highlight its superiority over several baseline approaches. We recommend that readers visit our project page for more details: https://liangsusan-git.github.io/project/avedit/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "ACCV 2024"
    },
    {
        "paper id": "2410.07471",
        "abstract url": "https://arxiv.org/abs/2410.07471",
        "title": "SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Fine-tuning on task-specific data to boost downstream performance is a crucial step for leveraging Large Language Models (LLMs). However, previous studies have demonstrated that fine-tuning the models on several adversarial samples or even benign data can greatly comprise the model's pre-equipped alignment and safety capabilities. In this work, we propose SEAL, a novel framework to enhance safety in LLM fine-tuning. SEAL learns a data ranker based on the bilevel optimization to up rank the safe and high-quality fine-tuning data and down rank the unsafe or low-quality ones. Models trained with SEAL demonstrate superior quality over multiple baselines, with 8.5% and 9.7% win rate increase compared to random selection respectively on Llama-3-8b-Instruct and Merlinite-7b models. Our code is available on github https://github.com/hanshen95/SEAL.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07473",
        "abstract url": "https://arxiv.org/abs/2410.07473",
        "title": "Localizing Factual Inconsistencies in Attributable Text Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "There has been an increasing interest in detecting hallucinations in model-generated texts, both manually and automatically, at varying levels of granularity. However, most existing methods fail to precisely pinpoint the errors. In this work, we introduce QASemConsistency, a new formalism for localizing factual inconsistencies in attributable text generation, at a fine-grained level. Drawing inspiration from Neo-Davidsonian formal semantics, we propose decomposing the generated text into minimal predicate-argument level propositions, expressed as simple question-answer (QA) pairs, and assess whether each individual QA pair is supported by a trusted reference text. As each QA pair corresponds to a single semantic relation between a predicate and an argument, QASemConsistency effectively localizes the unsupported information. We first demonstrate the effectiveness of the QASemConsistency methodology for human annotation, by collecting crowdsourced annotations of granular consistency errors, while achieving a substantial inter-annotator agreement ($\u03ba> 0.7)$. Then, we implement several methods for automatically detecting localized factual inconsistencies, with both supervised entailment models and open-source LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07491",
        "abstract url": "https://arxiv.org/abs/2410.07491",
        "title": "Transducer Consistency Regularization for Speech to Text Applications",
        "rating": "1",
        "keywords": [
            [
                "cs.CL",
                "eess.AS"
            ]
        ],
        "abstract": "Consistency regularization is a commonly used practice to encourage the model to generate consistent representation from distorted input features and improve model generalization. It shows significant improvement on various speech applications that are optimized with cross entropy criterion. However, it is not straightforward to apply consistency regularization for the transducer-based approaches, which are widely adopted for speech applications due to the competitive performance and streaming characteristic. The main challenge is from the vast alignment space of the transducer optimization criterion and not all the alignments within the space contribute to the model optimization equally. In this study, we present Transducer Consistency Regularization (TCR), a consistency regularization method for transducer models. We apply distortions such as spec augmentation and dropout to create different data views and minimize the distribution difference. We utilize occupational probabilities to give different weights on transducer output distributions, thus only alignments close to oracle alignments would contribute to the model learning. Our experiments show the proposed method is superior to other consistency regularization implementations and could effectively reduce word error rate (WER) by 4.3\\% relatively comparing with a strong baseline on the \\textsc{Librispeech} dataset.",
        "subjects": [
            "cs.CL",
            "eess.AS"
        ],
        "comment": "8 pages, 4 figures. Accepted in IEEE Spoken Language Technology Workshop 2024"
    },
    {
        "paper id": "2410.07495",
        "abstract url": "https://arxiv.org/abs/2410.07495",
        "title": "PublicHearingBR: A Brazilian Portuguese Dataset of Public Hearing Transcripts for Summarization of Long Documents",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces PublicHearingBR, a Brazilian Portuguese dataset designed for summarizing long documents. The dataset consists of transcripts of public hearings held by the Brazilian Chamber of Deputies, paired with news articles and structured summaries containing the individuals participating in the hearing and their statements or opinions. The dataset supports the development and evaluation of long document summarization systems in Portuguese. Our contributions include the dataset, a hybrid summarization system to establish a baseline for future studies, and a discussion on evaluation metrics for summarization involving large language models, addressing the challenge of hallucination in the generated summaries. As a result of this discussion, the dataset also provides annotated data that can be used in Natural Language Inference tasks in Portuguese.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "26 pages"
    },
    {
        "paper id": "2410.07504",
        "abstract url": "https://arxiv.org/abs/2410.07504",
        "title": "Using LLMs to Discover Legal Factors",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Factors are a foundational component of legal analysis and computational models of legal reasoning. These factor-based representations enable lawyers, judges, and AI and Law researchers to reason about legal cases. In this paper, we introduce a methodology that leverages large language models (LLMs) to discover lists of factors that effectively represent a legal domain. Our method takes as input raw court opinions and produces a set of factors and associated definitions. We demonstrate that a semi-automated approach, incorporating minimal human involvement, produces factor representations that can predict case outcomes with moderate success, if not yet as well as expert-defined factors can.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07513",
        "abstract url": "https://arxiv.org/abs/2410.07513",
        "title": "Evolutionary Contrastive Distillation for Language Model Alignment",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The ability of large language models (LLMs) to execute complex instructions is essential for their real-world applications. However, several recent studies indicate that LLMs struggle with challenging instructions. In this paper, we propose Evolutionary Contrastive Distillation (ECD), a novel method for generating high-quality synthetic preference data designed to enhance the complex instruction-following capability of language models. ECD generates data that specifically illustrates the difference between a response that successfully follows a set of complex instructions and a response that is high-quality, but nevertheless makes some subtle mistakes. This is done by prompting LLMs to progressively evolve simple instructions to more complex instructions. When the complexity of an instruction is increased, the original successful response to the original instruction becomes a \"hard negative\" response for the new instruction, mostly meeting requirements of the new instruction, but barely missing one or two. By pairing a good response with such a hard negative response, and employing contrastive learning algorithms such as DPO, we improve language models' ability to follow complex instructions. Empirically, we observe that our method yields a 7B model that exceeds the complex instruction-following performance of current SOTA 7B models and is competitive even with open-source 70B models.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07514",
        "abstract url": "https://arxiv.org/abs/2410.07514",
        "title": "O1O: Grouping of Known Classes to Identify Unknown Objects as Odd-One-Out",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Object detection methods trained on a fixed set of known classes struggle to detect objects of unknown classes in the open-world setting. Current fixes involve adding approximate supervision with pseudo-labels corresponding to candidate locations of objects, typically obtained in a class-agnostic manner. While previous approaches mainly rely on the appearance of objects, we find that geometric cues improve unknown recall. Although additional supervision from pseudo-labels helps to detect unknown objects, it also introduces confusion for known classes. We observed a notable decline in the model's performance for detecting known objects in the presence of noisy pseudo-labels. Drawing inspiration from studies on human cognition, we propose to group known classes into superclasses. By identifying similarities between classes within a superclass, we can identify unknown classes through an odd-one-out scoring mechanism. Our experiments on open-world detection benchmarks demonstrate significant improvements in unknown recall, consistently across all tasks. Crucially, we achieve this without compromising known performance, thanks to better partitioning of the feature space with superclasses.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at ACCV 2024 (Oral)"
    },
    {
        "paper id": "2410.07523",
        "abstract url": "https://arxiv.org/abs/2410.07523",
        "title": "DemoShapley: Valuation of Demonstrations for In-Context Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) leveraging in-context learning (ICL) have set new benchmarks in few-shot learning across various tasks without needing task-specific fine-tuning. However, extensive research has demonstrated that the effectiveness of ICL is significantly influenced by the selection and ordering of demonstrations. Considering the critical role of demonstration selection in ICL, we introduce DemoShapley which is inspired by the Data Shapley valuation theorem. This approach assesses the influence of individual demonstration instances, distinguishing between those that contribute positively and those that may hinder performance. Our findings reveal that DemoShapley not only enhances model performance in terms of accuracy and fairness but also generalizes queries from domains distinct from those of the in-context demonstrations, highlighting its versatility and effectiveness in optimizing ICL demonstration selection. Last but not least, DemoShapley demonstrates its ability to aid in identifying noisy data within the demonstration set.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07524",
        "abstract url": "https://arxiv.org/abs/2410.07524",
        "title": "Upcycling Large Language Models into Mixture of Experts",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Upcycling pre-trained dense language models into sparse mixture-of-experts (MoE) models is an efficient approach to increase the model capacity of already trained models. However, optimal techniques for upcycling at scale remain unclear. In this work, we conduct an extensive study of upcycling methods and hyperparameters for billion-parameter scale language models. We propose a novel \"virtual group\" initialization scheme and weight scaling approach to enable upcycling into fine-grained MoE architectures. Through ablations, we find that upcycling outperforms continued dense model training. In addition, we show that softmax-then-topK expert routing improves over topK-then-softmax approach and higher granularity MoEs can help improve accuracy. Finally, we upcycled Nemotron-4 15B on 1T tokens and compared it to a continuously trained version of the same model on the same 1T tokens: the continuous trained model achieved 65.3% MMLU, whereas the upcycled model achieved 67.6%. Our results offer insights and best practices to effectively leverage upcycling for building MoE language models.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07528",
        "abstract url": "https://arxiv.org/abs/2410.07528",
        "title": "CountMamba: Exploring Multi-directional Selective State-Space Models for Plant Counting",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Plant counting is essential in every stage of agriculture, including seed breeding, germination, cultivation, fertilization, pollination yield estimation, and harvesting. Inspired by the fact that humans count objects in high-resolution images by sequential scanning, we explore the potential of handling plant counting tasks via state space models (SSMs) for generating counting results. In this paper, we propose a new counting approach named CountMamba that constructs multiple counting experts to scan from various directions simultaneously. Specifically, we design a Multi-directional State-Space Group to process the image patch sequences in multiple orders and aim to simulate different counting experts. We also design Global-Local Adaptive Fusion to adaptively aggregate global features extracted from multiple directions and local features extracted from the CNN branch in a sample-wise manner. Extensive experiments demonstrate that the proposed CountMamba performs competitively on various plant counting tasks, including maize tassels, wheat ears, and sorghum head counting.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by PRCV 2024"
    },
    {
        "paper id": "2410.07530",
        "abstract url": "https://arxiv.org/abs/2410.07530",
        "title": "Audio Explanation Synthesis with Generative Foundation Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "The increasing success of audio foundation models across various tasks has led to a growing need for improved interpretability to understand their intricate decision-making processes better. Existing methods primarily focus on explaining these models by attributing importance to elements within the input space based on their influence on the final decision. In this paper, we introduce a novel audio explanation method that capitalises on the generative capacity of audio foundation models. Our method leverages the intrinsic representational power of the embedding space within these models by integrating established feature attribution techniques to identify significant features in this space. The method then generates listenable audio explanations by prioritising the most important features. Through rigorous benchmarking against standard datasets, including keyword spotting and speech emotion recognition, our model demonstrates its efficacy in producing audio explanations.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07533",
        "abstract url": "https://arxiv.org/abs/2410.07533",
        "title": "Corruption-Robust Linear Bandits: Minimax Optimality and Gap-Dependent Misspecification",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In linear bandits, how can a learner effectively learn when facing corrupted rewards? While significant work has explored this question, a holistic understanding across different adversarial models and corruption measures is lacking, as is a full characterization of the minimax regret bounds. In this work, we compare two types of corruptions commonly considered: strong corruption, where the corruption level depends on the action chosen by the learner, and weak corruption, where the corruption level does not depend on the action chosen by the learner. We provide a unified framework to analyze these corruptions. For stochastic linear bandits, we fully characterize the gap between the minimax regret under strong and weak corruptions. We also initiate the study of corrupted adversarial linear bandits, obtaining upper and lower bounds with matching dependencies on the corruption level. Next, we reveal a connection between corruption-robust learning and learning with gap-dependent mis-specification, a setting first studied by Liu et al. (2023a), where the misspecification level of an action or policy is proportional to its suboptimality. We present a general reduction that enables any corruption-robust algorithm to handle gap-dependent misspecification. This allows us to recover the results of Liu et al. (2023a) in a black-box manner and significantly generalize them to settings like linear MDPs, yielding the first results for gap-dependent misspecification in reinforcement learning. However, this general reduction does not attain the optimal rate for gap-dependent misspecification. Motivated by this, we develop a specialized algorithm that achieves optimal bounds for gap-dependent misspecification in linear bandits, thus answering an open question posed by Liu et al. (2023a).",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "NeurIPS 2024"
    },
    {
        "paper id": "2410.07548",
        "abstract url": "https://arxiv.org/abs/2410.07548",
        "title": "Hybrid Summary Statistics",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We present a way to capture high-information posteriors from training sets that are sparsely sampled over the parameter space for robust simulation-based inference. In physical inference problems, we can often apply domain knowledge to define traditional summary statistics to capture some of the information in a dataset. We show that augmenting these statistics with neural network outputs to maximise the mutual information improves information extraction compared to neural summaries alone or their concatenation to existing summaries and makes inference robust in settings with low training data. We introduce 1) two loss formalisms to achieve this and 2) apply the technique to two different cosmological datasets to extract non-Gaussian parameter information.",
        "subjects": [
            "stat.ML",
            "astro-ph.CO",
            "cs.IT",
            "cs.LG",
            "physics.data-an"
        ],
        "comment": "7 pages, 4 figures. Accepted to ML4PS2024 at NeurIPS 2024"
    },
    {
        "paper id": "2410.07561",
        "abstract url": "https://arxiv.org/abs/2410.07561",
        "title": "AI-Press: A Multi-Agent News Generating and Feedback Simulation System Powered by Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The rise of various social platforms has transformed journalism. The growing demand for news content has led to the increased use of large language models (LLMs) in news production due to their speed and cost-effectiveness. However, LLMs still encounter limitations in professionalism and ethical judgment in news generation. Additionally, predicting public feedback is usually difficult before news is released. To tackle these challenges, we introduce AI-Press, an automated news drafting and polishing system based on multi-agent collaboration and Retrieval-Augmented Generation. We develop a feedback simulation system that generates public feedback considering demographic distributions. Through extensive quantitative and qualitative evaluations, our system shows significant improvements in news-generating capabilities and verifies the effectiveness of public feedback simulation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "18 pages, 4 figures"
    },
    {
        "paper id": "2410.07563",
        "abstract url": "https://arxiv.org/abs/2410.07563",
        "title": "PLaMo-100B: A Ground-Up Language Model Designed for Japanese Proficiency",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We introduce PLaMo-100B, a large-scale language model designed for Japanese proficiency. The model was trained from scratch using 2 trillion tokens, with architecture such as QK Normalization and Z-Loss to ensure training stability during the training process. Post-training techniques, including Supervised Fine-Tuning and Direct Preference Optimization, were applied to refine the model's performance. Benchmark evaluations suggest that PLaMo-100B performs well, particularly in Japanese-specific tasks, achieving results that are competitive with frontier models like GPT-4.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07573",
        "abstract url": "https://arxiv.org/abs/2410.07573",
        "title": "RealVul: Can We Detect Vulnerabilities in Web Applications with LLM?",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The latest advancements in large language models (LLMs) have sparked interest in their potential for software vulnerability detection. However, there is currently a lack of research specifically focused on vulnerabilities in the PHP language, and challenges in extracting samples and processing persist, hindering the model's ability to effectively capture the characteristics of specific vulnerabilities. In this paper, we present RealVul, the first LLM-based framework designed for PHP vulnerability detection, addressing these issues. By vulnerability candidate detection methods and employing techniques such as normalization, we can isolate potential vulnerability triggers while streamlining the code and eliminating unnecessary semantic information, enabling the model to better understand and learn from the generated vulnerability samples. We also address the issue of insufficient PHP vulnerability samples by improving data synthesis methods. To evaluate RealVul's performance, we conduct an extensive analysis using five distinct code LLMs on vulnerability data from 180 PHP projects. The results demonstrate a significant improvement in both effectiveness and generalization compared to existing methods, effectively boosting the vulnerability detection capabilities of these models.",
        "subjects": [
            "cs.CR",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07589",
        "abstract url": "https://arxiv.org/abs/2410.07589",
        "title": "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Retrieval-Augmented Generation (RAG) is widely adopted for its effectiveness and cost-efficiency in mitigating hallucinations and enhancing the domain-specific generation capabilities of large language models (LLMs). However, is this effectiveness and cost-efficiency truly a free lunch? In this study, we comprehensively investigate the fairness costs associated with RAG by proposing a practical three-level threat model from the perspective of user awareness of fairness. Specifically, varying levels of user fairness awareness result in different degrees of fairness censorship on the external dataset. We examine the fairness implications of RAG using uncensored, partially censored, and fully censored datasets. Our experiments demonstrate that fairness alignment can be easily undermined through RAG without the need for fine-tuning or retraining. Even with fully censored and supposedly unbiased external datasets, RAG can lead to biased outputs. Our findings underscore the limitations of current alignment methods in the context of RAG-based LLMs and highlight the urgent need for new strategies to ensure fairness. We propose potential mitigations and call for further research to develop robust fairness safeguards in RAG-based LLMs.",
        "subjects": [
            "cs.IR",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07590",
        "abstract url": "https://arxiv.org/abs/2410.07590",
        "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed KV Caches for Chunked Text",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process numerous retrieved document chunks for prefill which requires a large volume of computation, therefore leading to significant latency in time-to-first-token (TTFT). To reduce the computation overhead as well as TTFT, we introduce TurboRAG, a novel RAG system that redesigns the inference paradigm of the current RAG system by first pre-computing and storing the key-value (KV) caches of documents offline, and then directly retrieving the saved KV cache for prefill. Hence, online computation of KV caches is eliminated during inference. In addition, we provide a number of insights into the mask matrix and positional embedding mechanisms, plus fine-tune a pretrained language model to maintain model accuracy of TurboRAG. Our approach is applicable to most existing large language models and their applications without any requirement in modification of models and inference systems. Experimental results across a suite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x compared to the conventional RAG systems (on an average of 8.6x), but reserving comparable performance to the standard RAG systems.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07599",
        "abstract url": "https://arxiv.org/abs/2410.07599",
        "title": "Causal Image Modeling for Efficient Visual Understanding",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this work, we present a comprehensive analysis of causal image modeling and introduce the Adventurer series models where we treat images as sequences of patch tokens and employ uni-directional language models to learn visual representations. This modeling paradigm allows us to process images in a recurrent formulation with linear complexity relative to the sequence length, which can effectively address the memory and computation explosion issues posed by high-resolution and fine-grained images. In detail, we introduce two simple designs that seamlessly integrate image inputs into the causal inference framework: a global pooling token placed at the beginning of the sequence and a flipping operation between every two layers. Extensive empirical studies demonstrate the significant efficiency and effectiveness of this causal image modeling paradigm. For example, our base-sized Adventurer model attains a competitive test accuracy of 84.0% on the standard ImageNet-1k benchmark with 216 images/s training throughput, which is 5.3 times more efficient than vision transformers to achieve the same result.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07602",
        "abstract url": "https://arxiv.org/abs/2410.07602",
        "title": "Packed Acyclic Deterministic Finite Automata",
        "rating": "1",
        "keywords": [
            [
                "time efficiency"
            ]
        ],
        "abstract": "An acyclic deterministic finite automaton (ADFA) is a data structure that represents a set of strings (i.e., a dictionary) and facilitates a pattern searching problem of determining whether a given pattern string is present in the dictionary. We introduce the packed ADFA (PADFA), a compact variant of ADFA, which is designed to achieve more efficient pattern searching by encoding specific paths as packed strings stored in contiguous memory. We theoretically demonstrate that pattern searching in PADFA is near time-optimal with a small additional overhead and becomes fully time-optimal for sufficiently long patterns. Moreover, we prove that a PADFA requires fewer bits than a trie when the dictionary size is relatively smaller than the number of states in the PADFA. Lastly, we empirically show that PADFAs improve both the space and time efficiency of pattern searching on real-world datasets.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07605",
        "abstract url": "https://arxiv.org/abs/2410.07605",
        "title": "A Variational Bayesian Inference Theory of Elasticity and Its Mixed Probabilistic Finite Element Method for Inverse Deformation Solutions in Any Dimension",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "In this work, we have developed a variational Bayesian inference theory of elasticity, which is accomplished by using a mixed Variational Bayesian inference Finite Element Method (VBI-FEM) that can be used to solve the inverse deformation problems of continua. In the proposed variational Bayesian inference theory of continuum mechanics, the elastic strain energy is used as a prior in a Bayesian inference network, which can intelligently recover the detailed continuum deformation mappings with only given the information on the deformed and undeformed continuum body shapes without knowing the interior deformation and the precise actual boundary conditions, both traction as well as displacement boundary conditions, and the actual material constitutive relation. Moreover, we have implemented the related finite element formulation in a computational probabilistic mechanics framework. To numerically solve mixed variational problem, we developed an operator splitting or staggered algorithm that consists of the finite element (FE) step and the Bayesian learning (BL) step as an analogue of the well-known the Expectation-Maximization (EM) algorithm. By solving the mixed probabilistic Galerkin variational problem, we demonstrated that the proposed method is able to inversely predict continuum deformation mappings with strong discontinuity or fracture without knowing the external load conditions. The proposed method provides a robust machine intelligent solution for the long-sought-after inverse problem solution, which has been a major challenge in structure failure forensic pattern analysis in past several decades. The proposed method may become a promising artificial intelligence-based inverse method for solving general partial differential equations.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08035",
        "abstract url": "https://arxiv.org/abs/2410.08035",
        "title": "IntrinsicVoice: Empowering LLMs with Intrinsic Real-time Voice Interaction Abilities",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.SD"
            ]
        ],
        "abstract": "Current methods of building LLMs with voice interaction capabilities rely heavily on explicit text autoregressive generation before or during speech response generation to maintain content quality, which unfortunately brings computational overhead and increases latency in multi-turn interactions. To address this, we introduce IntrinsicVoic,e an LLM designed with intrinsic real-time voice interaction capabilities. IntrinsicVoice aims to facilitate the transfer of textual capabilities of pre-trained LLMs to the speech modality by mitigating the modality gap between text and speech. Our novelty architecture, GroupFormer, can reduce speech sequences to lengths comparable to text sequences while generating high-quality audio, significantly reducing the length difference between speech and text, speeding up inference, and alleviating long-text modeling issues. Additionally, we construct a multi-turn speech-to-speech dialogue dataset named \\method-500k which includes nearly 500k turns of speech-to-speech dialogues, and a cross-modality training strategy to enhance the semantic alignment between speech and text. Experimental results demonstrate that IntrinsicVoice can generate high-quality speech response with latency lower than 100ms in multi-turn dialogue scenarios. Demos are available at https://instrinsicvoice.github.io/.",
        "subjects": [
            "cs.SD",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06561",
        "abstract url": "https://arxiv.org/abs/2410.06561",
        "title": "Efficient and Robust Knowledge Distillation from A Stronger Teacher Based on Correlation Matching",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Knowledge Distillation (KD) has emerged as a pivotal technique for neural network compression and performance enhancement. Most KD methods aim to transfer dark knowledge from a cumbersome teacher model to a lightweight student model based on Kullback-Leibler (KL) divergence loss. However, the student performance improvements achieved through KD exhibit diminishing marginal returns, where a stronger teacher model does not necessarily lead to a proportionally stronger student model. To address this issue, we empirically find that the KL-based KD method may implicitly change the inter-class relationships learned by the student model, resulting in a more complex and ambiguous decision boundary, which in turn reduces the model's accuracy and generalization ability. Therefore, this study argues that the student model should learn not only the probability values from the teacher's output but also the relative ranking of classes, and proposes a novel Correlation Matching Knowledge Distillation (CMKD) method that combines the Pearson and Spearman correlation coefficients-based KD loss to achieve more efficient and robust distillation from a stronger teacher model. Moreover, considering that samples vary in difficulty, CMKD dynamically adjusts the weights of the Pearson-based loss and Spearman-based loss. CMKD is simple yet practical, and extensive experiments demonstrate that it can consistently achieve state-of-the-art performance on CIRAR-100 and ImageNet, and adapts well to various teacher architectures, sizes, and other KD methods.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "12 pages, 10 figures"
    },
    {
        "paper id": "2410.06567",
        "abstract url": "https://arxiv.org/abs/2410.06567",
        "title": "Convex Distillation: Efficient Compression of Deep Networks via Convex Optimization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deploying large and complex deep neural networks on resource-constrained edge devices poses significant challenges due to their computational demands and the complexities of non-convex optimization. Traditional compression methods such as distillation and pruning often retain non-convexity that complicates fine-tuning in real-time on such devices. Moreover, these methods often necessitate extensive end-to-end network fine-tuning after compression to preserve model performance, which is not only time-consuming but also requires fully annotated datasets, thus potentially negating the benefits of efficient network compression. In this paper, we introduce a novel distillation technique that efficiently compresses the model via convex optimization -- eliminating intermediate non-convex activation functions and using only intermediate activations from the original model. Our approach enables distillation in a label-free data setting and achieves performance comparable to the original model without requiring any post-compression fine-tuning. We demonstrate the effectiveness of our method for image classification models on multiple standard datasets, and further show that in the data limited regime, our method can outperform standard non-convex distillation approaches. Our method promises significant advantages for deploying high-efficiency, low-footprint models on edge devices, making it a practical choice for real-world applications. We show that convex neural networks, when provided with rich feature representations from a large pre-trained non-convex model, can achieve performance comparable to their non-convex counterparts, opening up avenues for future research at the intersection of convex optimization and deep learning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "10 Pages, 7 figures, 2 tables"
    },
    {
        "paper id": "2410.06581",
        "abstract url": "https://arxiv.org/abs/2410.06581",
        "title": "Enhancing Legal Case Retrieval via Scaling High-quality Synthetic Query-Candidate Pairs",
        "rating": "0.5",
        "keywords": [
            [
                "EMNLP"
            ]
        ],
        "abstract": "Legal case retrieval (LCR) aims to provide similar cases as references for a given fact description. This task is crucial for promoting consistent judgments in similar cases, effectively enhancing judicial fairness and improving work efficiency for judges. However, existing works face two main challenges for real-world applications: existing works mainly focus on case-to-case retrieval using lengthy queries, which does not match real-world scenarios; and the limited data scale, with current datasets containing only hundreds of queries, is insufficient to satisfy the training requirements of existing data-hungry neural models. To address these issues, we introduce an automated method to construct synthetic query-candidate pairs and build the largest LCR dataset to date, LEAD, which is hundreds of times larger than existing datasets. This data construction method can provide ample training signals for LCR models. Experimental results demonstrate that model training with our constructed data can achieve state-of-the-art results on two widely-used LCR benchmarks. Besides, the construction method can also be applied to civil cases and achieve promising results. The data and codes can be found in https://github.com/thunlp/LEAD.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "15 pages, 3 figures, accepted by EMNLP 2024"
    },
    {
        "paper id": "2410.06606",
        "abstract url": "https://arxiv.org/abs/2410.06606",
        "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
        "rating": "0.5",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "cs.LG",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Fine-tuning-based unlearning methods prevail for preventing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of these methods is unclear. In this paper, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model's knowledge retrieval process, rather than genuinely erasing the problematic knowledge embedded in the model parameters. Furthermore, behavioral tests demonstrate that the unlearning mechanisms inevitably impact the global behavior of the models, affecting unrelated knowledge or capabilities. Our work advocates the development of more resilient unlearning techniques for truly erasing knowledge. Our code is released at https://github.com/yihuaihong/Dissecting-FT-Unlearning.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Accepted in EMNLP 2024 Main (Short paper)"
    },
    {
        "paper id": "2410.06621",
        "abstract url": "https://arxiv.org/abs/2410.06621",
        "title": "Effective Exploration Based on the Structural Information Principles",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Traditional information theory provides a valuable foundation for Reinforcement Learning, particularly through representation learning and entropy maximization for agent exploration. However, existing methods primarily concentrate on modeling the uncertainty associated with RL's random variables, neglecting the inherent structure within the state and action spaces. In this paper, we propose a novel Structural Information principles-based Effective Exploration framework, namely SI2E. Structural mutual information between two variables is defined to address the single-variable limitation in structural information, and an innovative embedding principle is presented to capture dynamics-relevant state-action representations. The SI2E analyzes value differences in the agent's policy between state-action pairs and minimizes structural entropy to derive the hierarchical state-action structure, referred to as the encoding tree. Under this tree structure, value-conditional structural entropy is defined and maximized to design an intrinsic reward mechanism that avoids redundant transitions and promotes enhanced coverage in the state-action space. Theoretical connections are established between SI2E and classical information-theoretic methodologies, highlighting our framework's rationality and advantage. Comprehensive evaluations in the MiniGrid, MetaWorld, and DeepMind Control Suite benchmarks demonstrate that SI2E significantly outperforms state-of-the-art exploration baselines regarding final performance and sample efficiency, with maximum improvements of 37.63% and 60.25%, respectively.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "10 pages in main paper and 15 pages in appendix"
    },
    {
        "paper id": "2410.06656",
        "abstract url": "https://arxiv.org/abs/2410.06656",
        "title": "WardropNet: Traffic Flow Predictions via Equilibrium-Augmented Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "When optimizing transportation systems, anticipating traffic flows is a central element. Yet, computing such traffic equilibria remains computationally expensive. Against this background, we introduce a novel combinatorial optimization augmented neural network architecture that allows for fast and accurate traffic flow predictions. We propose WardropNet, a neural network that combines classical layers with a subsequent equilibrium layer: the first ones inform the latter by predicting the parameterization of the equilibrium problem's latency functions. Using supervised learning we minimize the difference between the actual traffic flow and the predicted output. We show how to leverage a Bregman divergence fitting the geometry of the equilibria, which allows for end-to-end learning. WardropNet outperforms pure learning-based approaches in predicting traffic equilibria for realistic and stylized traffic scenarios. On realistic scenarios, WardropNet improves on average for time-invariant predictions by up to 72% and for time-variant predictions by up to 23% over pure learning-based approaches.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "40 pages, 15 figures"
    },
    {
        "paper id": "2410.06663",
        "abstract url": "https://arxiv.org/abs/2410.06663",
        "title": "Data-informed modeling of the formation, persistence, and evolution of social norms and conventions",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Social norms and conventions are commonly accepted and adopted behaviors and practices within a social group that guide interactions -- e.g., how to spell a word or how to greet people -- and are central to a group's culture and identity. Understanding the key mechanisms that govern the formation, persistence, and evolution of social norms and conventions in social communities is a problem of paramount importance for a broad range of real-world applications, spanning from preparedness for future emergencies to promotion of sustainable practices. In the past decades, mathematical modeling has emerged as a powerful tool to reproduce and study the complex dynamics of norm and convention change, gaining insights into their mechanisms, and ultimately deriving tools to predict their evolution. The first goal of this chapter is to introduce some of the main mathematical approaches for modeling social norms and conventions, including population models and agent-based models relying on the theories of dynamical systems, evolutionary dynamics, and game theory. The second goal of the chapter is to illustrate how quantitative observations and empirical data can be incorporated into these mathematical models in a systematic manner, establishing a data-based approach to mathematical modeling of formation, persistence, and evolution of social norms and conventions. Finally, current challenges and future opportunities in this growing field of research are discussed.",
        "subjects": [
            "cs.SI",
            "eess.SY",
            "math.DS",
            "physics.soc-ph"
        ],
        "comment": "This is an author's (preprint) version of a book chapter that is part of the Handbook of Visual, Experimental and Computational Mathematics - Bridges through Data"
    },
    {
        "paper id": "2410.06671",
        "abstract url": "https://arxiv.org/abs/2410.06671",
        "title": "GLA-DA: Global-Local Alignment Domain Adaptation for Multivariate Time Series",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Unlike images and natural language tokens, time series data is highly semantically sparse, resulting in labor-intensive label annotations. Unsupervised and Semi-supervised Domain Adaptation (UDA and SSDA) have demonstrated efficiency in addressing this issue by utilizing pre-labeled source data to train on unlabeled or partially labeled target data. However, in domain adaptation methods designed for downstream classification tasks, directly adapting labeled source samples with unlabelled target samples often results in similar distributions across various classes, thereby compromising the performance of the target classification task. To tackle this challenge, we proposed a Global-Local Alignment Domain Adaptation (GLA-DA) method for multivariate time series data. Data from two domains were initially encoded to align in an intermediate feature space adversarially, achieving Global Feature Alignment (GFA). Subsequently, GLA-DA leveraged the consistency between similarity-based and deep learning-based models to assign pseudo labels to unlabeled target data. This process aims to preserve differences among data with distinct labels by aligning the samples with the same class labels together, achieving Local Class Alignment (LCA). We implemented GLA-DA in both UDA and SSDA scenarios, showcasing its superiority over state-of-the-art methods through extensive experiments on various public datasets. Ablation experiments underscored the significance of key components within GLA-DA.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06717",
        "abstract url": "https://arxiv.org/abs/2410.06717",
        "title": "Exact full-RSB SAT/UNSAT transition in infinitely wide two-layer neural networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We analyze the problem of storing random pattern-label associations using two classes of continuous non-convex weights models, namely the perceptron with negative margin and an infinite width two layer neural network with non-overlapping receptive fields and generic activation function. Using a full-RSB ansatz we compute the exact value of the SAT/UNSAT transition. Furthermore, in the case of the negative perceptron model we show that, depending on the value of the margin and the constrained density, there is a line separating a phase in which the distribution of overlaps of typical states does not possess a gap from one in which it does. Our results show that the hypothesis underlying some recently developed theorems claiming that Approximate Message Passing (AMP) based algorithms are able to reach capacity, does not hold in general. Finally, we show that Gradient Descent is not able to reach the maximal capacity both in cases where there is and there is not a non-overlap gap phase for the typical states. This, similarly to what occurs in binary weight models, suggests that gradient-based algorithms are biased towards highly atypical states, whose inaccessibility determines the algorithmic threshold.",
        "subjects": [
            "cond-mat.dis-nn",
            "cs.LG",
            "math.PR"
        ],
        "comment": "38 pages, 11 figures"
    },
    {
        "paper id": "2410.06726",
        "abstract url": "https://arxiv.org/abs/2410.06726",
        "title": "Sharp Bounds of the Causal Effect Under MNAR Confounding",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We report bounds for any contrast between the probabilities of the counterfactual outcome under exposure and non-exposure when the confounders are missing not at random. We assume that the missingness mechanism is outcome-independent, and prove that our bounds are arbitrarily sharp, i.e., practically attainable or logically possible.",
        "subjects": [
            "stat.ME",
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06771",
        "abstract url": "https://arxiv.org/abs/2410.06771",
        "title": "Safe and High-Performance Learning of Model Predicitve Control using Kernel-Based Interpolation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present a method, which allows efficient and safe approximation of model predictive controllers using kernel interpolation. Since the computational complexity of the approximating function scales linearly with the number of data points, we propose to use a scoring function which chooses the most promising data. To further reduce the complexity of the approximation, we restrict our considerations to the set of closed-loop reachable states. That is, the approximating function only has to be accurate within this set. This makes our method especially suited for systems, where the set of initial conditions is small. In order to guarantee safety and high performance of the designed approximated controller, we use reachability analysis based on Monte Carlo methods.",
        "subjects": [
            "eess.SY",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06800",
        "abstract url": "https://arxiv.org/abs/2410.06800",
        "title": "Efficient Weight-Space Laplace-Gaussian Filtering and Smoothing for Sequential Deep Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Efficiently learning a sequence of related tasks, such as in continual learning, poses a significant challenge for neural nets due to the delicate trade-off between catastrophic forgetting and loss of plasticity. We address this challenge with a grounded framework for sequentially learning related tasks based on Bayesian inference. Specifically, we treat the model's parameters as a nonlinear Gaussian state-space model and perform efficient inference using Gaussian filtering and smoothing. This general formalism subsumes existing continual learning approaches, while also offering a clearer conceptual understanding of its components. Leveraging Laplace approximations during filtering, we construct Gaussian posterior measures on the weight space of a neural network for each task. We use it as an efficient regularizer by exploiting the structure of the generalized Gauss-Newton matrix (GGN) to construct diagonal plus low-rank approximations. The dynamics model allows targeted control of the learning process and the incorporation of domain-specific knowledge, such as modeling the type of shift between tasks. Additionally, using Bayesian approximate smoothing can enhance the performance of task-specific models without needing to re-access any data.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "20 pages, 8 figures"
    },
    {
        "paper id": "2410.06804",
        "abstract url": "https://arxiv.org/abs/2410.06804",
        "title": "The Clear Sky Corridor: Insights Towards Aerosol Formation in Exoplanets Using An AI-based Survey of Exoplanet Atmospheres",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Producing optimized and accurate transmission spectra of exoplanets from telescope data has traditionally been a manual and labor-intensive procedure. Here we present the results of the first attempt to improve and standardize this procedure using artificial intelligence (AI) based processing of light curves and spectroscopic data from transiting exoplanets observed with the Hubble Space Telescope's (HST) Wide Field Camera 3 (WFC3) instrument. We implement an AI-based parameter optimizer that autonomously operates the Eureka pipeline to produce homogeneous transmission spectra of publicly available HST WFC3 datasets, spanning exoplanet types from hot Jupiters to sub-Neptunes. Surveying 43 exoplanets with temperatures between 280 and 2580 Kelvin, we confirm modeled relationships between the amplitude of the water band at 1.4um in hot Jupiters and their equilibrium temperatures. We also identify a similar, novel trend in Neptune/sub-Neptune atmospheres, but shifted to cooler temperatures. Excitingly, a planet mass versus equilibrium temperature diagram reveals a \"Clear Sky Corridor,\" where planets between 700 and 1700 Kelvin (depending on the mass) show stronger 1.4um H2O band measurements. This novel trend points to metallicity as a potentially important driver of aerosol formation. As we unveil and include these new discoveries into our understanding of aerosol formation, we enter a thrilling future for the study of exoplanet atmospheres. With HST sculpting this foundational understanding for aerosol formation in various exoplanet types, ranging from Jupiters to sub-Neptunes, we present a compelling platform for the James Webb Space Telescope (JWST) to discover similar atmospheric trends for more planets across a broader wavelength range.",
        "subjects": [
            "astro-ph.EP",
            "astro-ph.IM",
            "cs.LG"
        ],
        "comment": "Accepted to AJ. 14 pages, 5 figures, 3 tables"
    },
    {
        "paper id": "2410.06816",
        "abstract url": "https://arxiv.org/abs/2410.06816",
        "title": "Multi-Neuron Unleashes Expressivity of ReLU Networks Under Convex Relaxation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Neural work certification has established itself as a crucial tool for ensuring the robustness of neural networks. Certification methods typically rely on convex relaxations of the feasible output set to provide sound bounds. However, complete certification requires exact bounds, which strongly limits the expressivity of ReLU networks: even for the simple ``$\\max$'' function in $\\mathbb{R}^2$, there does not exist a ReLU network that expresses this function and can be exactly bounded by single-neuron relaxation methods. This raises the question whether there exists a convex relaxation that can provide exact bounds for general continuous piecewise linear functions in $\\mathbb{R}^n$. In this work, we answer this question affirmatively by showing that (layer-wise) multi-neuron relaxation provides complete certification for general ReLU networks. Based on this novel result, we show that the expressivity of ReLU networks is no longer limited under multi-neuron relaxation. To the best of our knowledge, this is the first positive result on the completeness of convex relaxations, shedding light on the practice of certified robustness.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06828",
        "abstract url": "https://arxiv.org/abs/2410.06828",
        "title": "Transfer Learning for a Class of Cascade Dynamical Systems",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This work considers the problem of transfer learning in the context of reinforcement learning. Specifically, we consider training a policy in a reduced order system and deploying it in the full state system. The motivation for this training strategy is that running simulations in the full-state system may take excessive time if the dynamics are complex. While transfer learning alleviates the computational issue, the transfer guarantees depend on the discrepancy between the two systems. In this work, we consider a class of cascade dynamical systems, where the dynamics of a subset of the state-space influence the rest of the states but not vice-versa. The reinforcement learning policy learns in a model that ignores the dynamics of these states and treats them as commanded inputs. In the full-state system, these dynamics are handled using a classic controller (e.g., a PID). These systems have vast applications in the control literature and their structure allows us to provide transfer guarantees that depend on the stability of the inner loop controller. Numerical experiments on a quadrotor support the theoretical findings.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2410.06833",
        "abstract url": "https://arxiv.org/abs/2410.06833",
        "title": "Dynamic metastability in the self-attention model",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider the self-attention model - an interacting particle system on the unit sphere, which serves as a toy model for Transformers, the deep neural network architecture behind the recent successes of large language models. We prove the appearance of dynamic metastability conjectured in [GLPR23] - although particles collapse to a single cluster in infinite time, they remain trapped near a configuration of several clusters for an exponentially long period of time. By leveraging a gradient flow interpretation of the system, we also connect our result to an overarching framework of slow motion of gradient flows proposed by Otto and Reznikoff [OR07] in the context of coarsening and the Allen-Cahn equation. We finally probe the dynamics beyond the exponentially long period of metastability, and illustrate that, under an appropriate time-rescaling, the energy reaches its global maximum in finite time and has a staircase profile, with trajectories manifesting saddle-to-saddle-like behavior, reminiscent of recent works in the analysis of training dynamics via gradient descent for two-layer neural networks.",
        "subjects": [
            "cs.LG",
            "math.AP",
            "math.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06865",
        "abstract url": "https://arxiv.org/abs/2410.06865",
        "title": "Students' Perceptions and Use of Generative AI Tools for Programming Across Different Computing Courses",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "Investigation of students' perceptions and opinions on the use of generative artificial intelligence (GenAI) in education is a topic gaining much interest. Studies addressing this are typically conducted with large heterogeneous groups, at one moment in time. However, how students perceive and use GenAI tools can potentially depend on many factors, including their background knowledge, familiarity with the tools, and the learning goals and policies of the courses they are taking. In this study we explore how students following computing courses use GenAI for programming-related tasks across different programs and courses: Bachelor and Master, in courses in which learning programming is the learning goal, courses that require programming as a means to achieve another goal, and in courses in which programming is optional, but can be useful. We are also interested in changes over time, since GenAI capabilities are changing at a fast pace, and users are adopting GenAI increasingly. We conducted three consecutive surveys (fall `23, winter `23, and spring `24) among students of all computing programs of a large European research university. We asked questions on the use in education, ethics, and job prospects, and we included specific questions on the (dis)allowed use of GenAI tools in the courses they were taking at the time. We received 264 responses, which we quantitatively and qualitatively analyzed, to find out how students have employed GenAI tools across 59 different computing courses, and whether the opinion of an average student about these tools evolves over time. Our study contributes to the emerging discussion of how to differentiate GenAI use across different courses, and how to align its use with the learning goals of a computing course.",
        "subjects": [
            "cs.CY",
            "cs.AI"
        ],
        "comment": "Accepted to Koli Calling 24"
    },
    {
        "paper id": "2410.06875",
        "abstract url": "https://arxiv.org/abs/2410.06875",
        "title": "Group Shapley Value and Counterfactual Simulations in a Structural Model",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose a variant of the Shapley value, the group Shapley value, to interpret counterfactual simulations in structural economic models by quantifying the importance of different components. Our framework compares two sets of parameters, partitioned into multiple groups, and applying group Shapley value decomposition yields unique additive contributions to the changes between these sets. The relative contributions sum to one, enabling us to generate an importance table that is as easily interpretable as a regression table. The group Shapley value can be characterized as the solution to a constrained weighted least squares problem. Using this property, we develop robust decomposition methods to address scenarios where inputs for the group Shapley value are missing. We first apply our methodology to a simple Roy model and then illustrate its usefulness by revisiting two published papers.",
        "subjects": [
            "econ.EM",
            "cs.LG",
            "stat.ME"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06884",
        "abstract url": "https://arxiv.org/abs/2410.06884",
        "title": "Adaptive Refinement Protocols for Distributed Distribution Estimation under $\\ell^p$-Losses",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Consider the communication-constrained estimation of discrete distributions under $\\ell^p$ losses, where each distributed terminal holds multiple independent samples and uses limited number of bits to describe the samples. We obtain the minimax optimal rates of the problem in most parameter regimes. An elbow effect of the optimal rates at $p=2$ is clearly identified. To show the optimal rates, we first design estimation protocols to achieve them. The key ingredient of these protocols is to introduce adaptive refinement mechanisms, which first generate rough estimate by partial information and then establish refined estimate in subsequent steps guided by the rough estimate. The protocols leverage successive refinement, sample compression and thresholding methods to achieve the optimal rates in different parameter regimes. The optimality of the protocols is shown by deriving compatible minimax lower bounds.",
        "subjects": [
            "cs.LG",
            "cs.IT",
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06920",
        "abstract url": "https://arxiv.org/abs/2410.06920",
        "title": "To Be or Not to Be (in the EU): Measurement of Discrepancies Presented in Cookie Paywalls",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Cookie paywalls allow visitors to access the content of a website only after making a choice between paying a fee (paying option) or accepting tracking (cookie option). The practice has been studied in previous research in regard to its prevalence and legal standing, but the effects of the clients' device and geographic location remain unexplored. To address these questions, this study explores the effects of three factors: 1) the clients' browser, 2) the device type (desktop or mobile), and 3) the geographic location on the presence and behavior of cookie paywalls and the handling of users' data. Using an automatic crawler on our dataset composed of 804 websites that present a cookie paywall, we observed that the presence of a cookie paywall was most affected by the geographic location of the user. We further showed that both the behavior of a cookie paywall and the processing of user data are impacted by all three factors, but no patterns of significance could be found. Finally, an additional type of paywall was discovered to be used on approximately 11% of the studied websites, coined the \"double paywall\", which consists of a cookie paywall complemented by another paywall once tracking is accepted.",
        "subjects": [
            "cs.CY",
            "cs.ET"
        ],
        "comment": "Submitted to ICISSP 2025"
    },
    {
        "paper id": "2410.06921",
        "abstract url": "https://arxiv.org/abs/2410.06921",
        "title": "Adversarial Vulnerability as a Consequence of On-Manifold Inseparibility",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent works have shown theoretically and empirically that redundant data dimensions are a source of adversarial vulnerability. However, the inverse doesn't seem to hold in practice; employing dimension-reduction techniques doesn't exhibit robustness as expected. In this work, we consider classification tasks and characterize the data distribution as a low-dimensional manifold, with high/low variance features defining the on/off manifold direction. We argue that clean training experiences poor convergence in the off-manifold direction caused by the ill-conditioning in widely used first-order optimizers like gradient descent. The poor convergence then acts as a source of adversarial vulnerability when the dataset is inseparable in the on-manifold direction. We provide theoretical results for logistic regression and a 2-layer linear network on the considered data distribution. Furthermore, we advocate using second-order methods that are immune to ill-conditioning and lead to better robustness. We perform experiments and exhibit tremendous robustness improvements in clean training through long training and the employment of second-order methods, corroborating our framework. Additionally, we find the inclusion of batch-norm layers hinders such robustness gains. We attribute this to differing implicit biases between traditional and batch-normalized neural networks.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06922",
        "abstract url": "https://arxiv.org/abs/2410.06922",
        "title": "Estimating Exoplanet Mass using Machine Learning on Incomplete Datasets",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The exoplanet archive is an incredible resource of information on the properties of discovered extrasolar planets, but statistical analysis has been limited by the number of missing values. One of the most informative bulk properties is planet mass, which is particularly challenging to measure with more than 70\\% of discovered planets with no measured value. We compare the capabilities of five different machine learning algorithms that can utilize multidimensional incomplete datasets to estimate missing properties for imputing planet mass. The results are compared when using a partial subset of the archive with a complete set of six planet properties, and where all planet discoveries are leveraged in an incomplete set of six and eight planet properties. We find that imputation results improve with more data even when the additional data is incomplete, and allows a mass prediction for any planet regardless of which properties are known. Our favored algorithm is the newly developed $k$NN$\\times$KDE, which can return a probability distribution for the imputed properties. The shape of this distribution can indicate the algorithm's level of confidence, and also inform on the underlying demographics of the exoplanet population. We demonstrate how the distributions can be interpreted with a series of examples for planets where the discovery was made with either the transit method, or radial velocity method. Finally, we test the generative capability of the $k$NN$\\times$KDE to create a large synthetic population of planets based on the archive, and identify potential categories of planets from groups of properties in the multidimensional space. All codes are Open Source.",
        "subjects": [
            "astro-ph.EP",
            "astro-ph.IM",
            "cs.LG"
        ],
        "comment": "30 pages, 14 figures, 1 table. Accepted for publication in the Open Journal of Astrophysics"
    },
    {
        "paper id": "2410.06932",
        "abstract url": "https://arxiv.org/abs/2410.06932",
        "title": "Reproducing and Extending Experiments in Behavioral Strategy with Large Language Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In this study, we propose LLM agents as a novel approach in behavioral strategy research, complementing simulations and laboratory experiments to advance our understanding of cognitive processes in decision-making. Specifically, we reproduce a human laboratory experiment in behavioral strategy using large language model (LLM) generated agents and investigate how LLM agents compare to observed human behavior. Our results show that LLM agents effectively reproduce search behavior and decision-making comparable to humans. Extending our experiment, we analyze LLM agents' simulated \"thoughts,\" discovering that more forward-looking thoughts correlate with favoring exploitation over exploration to maximize wealth. We show how this new approach can be leveraged in behavioral strategy research and address limitations.",
        "subjects": [
            "econ.GN",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06943",
        "abstract url": "https://arxiv.org/abs/2410.06943",
        "title": "AutoFeedback: An LLM-based Framework for Efficient and Accurate API Request Generation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large Language Models (LLMs) leverage external tools primarily through generating the API request to enhance task completion efficiency. The accuracy of API request generation significantly determines the capability of LLMs to accomplish tasks. Due to the inherent hallucinations within the LLM, it is difficult to efficiently and accurately generate the correct API request. Current research uses prompt-based feedback to facilitate the LLM-based API request generation. However, existing methods lack factual information and are insufficiently detailed. To address these issues, we propose AutoFeedback, an LLM-based framework for efficient and accurate API request generation, with a Static Scanning Component (SSC) and a Dynamic Analysis Component (DAC). SSC incorporates errors detected in the API requests as pseudo-facts into the feedback, enriching the factual information. DAC retrieves information from API documentation, enhancing the level of detail in feedback. Based on this two components, Autofeedback implementes two feedback loops during the process of generating API requests by the LLM. Extensive experiments demonstrate that it significantly improves accuracy of API request generation and reduces the interaction cost. AutoFeedback achieves an accuracy of 100.00\\% on a real-world API dataset and reduces the cost of interaction with GPT-3.5 Turbo by 23.44\\%, and GPT-4 Turbo by 11.85\\%.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": "17 pages"
    },
    {
        "paper id": "2410.06944",
        "abstract url": "https://arxiv.org/abs/2410.06944",
        "title": "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages",
        "rating": "0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Neural dependency parsing has achieved remarkable performance for low resource morphologically rich languages. It has also been well-studied that morphologically rich languages exhibit relatively free word order. This prompts a fundamental investigation: Is there a way to enhance dependency parsing performance, making the model robust to word order variations utilizing the relatively free word order nature of morphologically rich languages? In this work, we examine the robustness of graph-based parsing architectures on 7 relatively free word order languages. We focus on scrutinizing essential modifications such as data augmentation and the removal of position encoding required to adapt these architectures accordingly. To this end, we propose a contrastive self-supervised learning method to make the model robust to word order variations. Furthermore, our proposed modification demonstrates a substantial average gain of 3.03/2.95 points in 7 relatively free word order languages, as measured by the UAS/LAS Score metric when compared to the best performing baseline.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at EMNLP 2024 Main (Short), 9 pages, 3 figures, 4 Tables"
    },
    {
        "paper id": "2410.06954",
        "abstract url": "https://arxiv.org/abs/2410.06954",
        "title": "How Unique is Whose Web Browser? The role of demographics in browser fingerprinting among US users",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Browser fingerprinting can be used to identify and track users across the Web, even without cookies, by collecting attributes from users' devices to create unique \"fingerprints\". This technique and resulting privacy risks have been studied for over a decade. Yet further research is limited because prior studies used data not publicly available. Additionally, data in prior studies lacked user demographics. Here we provide a first-of-its-kind dataset to enable further research. It includes browser attributes with users' demographics and survey responses, collected with informed consent from 8,400 US study participants. We use this dataset to demonstrate how fingerprinting risks differ across demographic groups. For example, we find lower income users are more at risk, and find that as users' age increases, they are both more likely to be concerned about fingerprinting and at real risk of fingerprinting. Furthermore, we demonstrate an overlooked risk: user demographics, such as gender, age, income level and race, can be inferred from browser attributes commonly used for fingerprinting, and we identify which browser attributes most contribute to this risk. Our data collection process also conducted an experiment to study what impacts users' likelihood to share browser data for open research, in order to inform future data collection efforts, with responses from 12,461 total participants. Female participants were significantly less likely to share their browser data, as were participants who were shown the browser data we asked to collect. Overall, we show the important role of user demographics in the ongoing work that intends to assess fingerprinting risks and improve user privacy, with findings to inform future privacy enhancing browser developments. The dataset and data collection tool we provide can be used to further study research questions not addressed in this work.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "Accepted to 25th Privacy Enhancing Technologies Symposium (PETS 2025)"
    },
    {
        "paper id": "2410.06964",
        "abstract url": "https://arxiv.org/abs/2410.06964",
        "title": "Bridge the Points: Graph-based Few-shot Segment Anything Semantically",
        "rating": "0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "The recent advancements in large-scale pre-training techniques have significantly enhanced the capabilities of vision foundation models, notably the Segment Anything Model (SAM), which can generate precise masks based on point and box prompts. Recent studies extend SAM to Few-shot Semantic Segmentation (FSS), focusing on prompt generation for SAM-based automatic semantic segmentation. However, these methods struggle with selecting suitable prompts, require specific hyperparameter settings for different scenarios, and experience prolonged one-shot inference times due to the overuse of SAM, resulting in low efficiency and limited automation ability. To address these issues, we propose a simple yet effective approach based on graph analysis. In particular, a Positive-Negative Alignment module dynamically selects the point prompts for generating masks, especially uncovering the potential of the background context as the negative reference. Another subsequent Point-Mask Clustering module aligns the granularity of masks and selected points as a directed graph, based on mask coverage over points. These points are then aggregated by decomposing the weakly connected components of the directed graph in an efficient manner, constructing distinct natural clusters. Finally, the positive and overshooting gating, benefiting from graph-based granularity alignment, aggregate high-confident masks and filter out the false-positive masks for final prediction, reducing the usage of additional hyperparameters and redundant mask generation. Extensive experimental analysis across standard FSS, One-shot Part Segmentation, and Cross Domain FSS datasets validate the effectiveness and efficiency of the proposed approach, surpassing state-of-the-art generalist models with a mIoU of 58.7% on COCO-20i and 35.2% on LVIS-92i. The code is available in https://andyzaq.github.io/GF-SAM/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to NeurIPS 2024 as Spotlight"
    },
    {
        "paper id": "2410.06993",
        "abstract url": "https://arxiv.org/abs/2410.06993",
        "title": "Efficient Distribution Matching of Representations via Noise-Injected Deep InfoMax",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep InfoMax (DIM) is a well-established method for self-supervised representation learning (SSRL) based on maximization of the mutual information between the input and the output of a deep neural network encoder. Despite the DIM and contrastive SSRL in general being well-explored, the task of learning representations conforming to a specific distribution (i.e., distribution matching, DM) is still under-addressed. Motivated by the importance of DM to several downstream tasks (including generative modeling, disentanglement, outliers detection and other), we enhance DIM to enable automatic matching of learned representations to a selected prior distribution. To achieve this, we propose injecting an independent noise into the normalized outputs of the encoder, while keeping the same InfoMax training objective. We show that such modification allows for learning uniformly and normally distributed representations, as well as representations of other absolutely continuous distributions. Our approach is tested on various downstream tasks. The results indicate a moderate trade-off between the performance on the downstream tasks and quality of DM.",
        "subjects": [
            "cs.LG",
            "cs.IT",
            "stat.ML"
        ],
        "comment": "22 pages, 3 fugures"
    },
    {
        "paper id": "2410.07003",
        "abstract url": "https://arxiv.org/abs/2410.07003",
        "title": "Through the Looking Glass: Mirror Schr\u00f6dinger Bridges",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Resampling from a target measure whose density is unknown is a fundamental problem in mathematical statistics and machine learning. A setting that dominates the machine learning literature consists of learning a map from an easy-to-sample prior, such as the Gaussian distribution, to a target measure. Under this model, samples from the prior are pushed forward to generate a new sample on the target measure, which is often difficult to sample from directly. In this paper, we propose a new model for conditional resampling called mirror Schr\u00f6dinger bridges. Our key observation is that solving the Schr\u00f6dinger bridge problem between a distribution and itself provides a natural way to produce new samples from conditional distributions, giving in-distribution variations of an input data point. We show how to efficiently solve this largely overlooked version of the Schr\u00f6dinger bridge problem. We prove that our proposed method leads to significant algorithmic simplifications over existing alternatives, in addition to providing control over in-distribution variation. Empirically, we demonstrate how these benefits can be leveraged to produce proximal samples in a number of application domains.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07014",
        "abstract url": "https://arxiv.org/abs/2410.07014",
        "title": "Optimizing Estimators of Squared Calibration Errors in Classification",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this work, we propose a mean-squared error-based risk that enables the comparison and optimization of estimators of squared calibration errors in practical settings. Improving the calibration of classifiers is crucial for enhancing the trustworthiness and interpretability of machine learning models, especially in sensitive decision-making scenarios. Although various calibration (error) estimators exist in the current literature, there is a lack of guidance on selecting the appropriate estimator and tuning its hyperparameters. By leveraging the bilinear structure of squared calibration errors, we reformulate calibration estimation as a regression problem with independent and identically distributed (i.i.d.) input pairs. This reformulation allows us to quantify the performance of different estimators even for the most challenging calibration criterion, known as canonical calibration. Our approach advocates for a training-validation-testing pipeline when estimating a calibration error on an evaluation dataset. We demonstrate the effectiveness of our pipeline by optimizing existing calibration estimators and comparing them with novel kernel ridge regression-based estimators on standard image classification tasks.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "Preprint"
    },
    {
        "paper id": "2410.07021",
        "abstract url": "https://arxiv.org/abs/2410.07021",
        "title": "Do Contemporary CATE Models Capture Real-World Heterogeneity? Findings from a Large-Scale Benchmark",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present unexpected findings from a large-scale benchmark study evaluating Conditional Average Treatment Effect (CATE) estimation algorithms. By running 16 modern CATE models across 43,200 datasets, we find that: (a) 62\\% of CATE estimates have a higher Mean Squared Error (MSE) than a trivial zero-effect predictor, rendering them ineffective; (b) in datasets with at least one useful CATE estimate, 80\\% still have higher MSE than a constant-effect model; and (c) Orthogonality-based models outperform other models only 30\\% of the time, despite widespread optimism about their performance. These findings expose significant limitations in current CATE models and suggest ample opportunities for further research. Our findings stem from a novel application of \\textit{observational sampling}, originally developed to evaluate Average Treatment Effect (ATE) estimates from observational methods with experiment data. To adapt observational sampling for CATE evaluation, we introduce a statistical parameter, $Q$, equal to MSE minus a constant and preserves the ranking of models by their MSE. We then derive a family of sample statistics, collectively called $\\hat{Q}$, that can be computed from real-world data. We prove that $\\hat{Q}$ is a consistent estimator of $Q$ under mild technical conditions. When used in observational sampling, $\\hat{Q}$ is unbiased and asymptotically selects the model with the smallest MSE. To ensure the benchmark reflects real-world heterogeneity, we handpick datasets where outcomes come from field rather than simulation. By combining the new observational sampling method, new statistics, and real-world datasets, the benchmark provides a unique perspective on CATE estimator performance and uncover gaps in capturing real-world heterogeneity.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07041",
        "abstract url": "https://arxiv.org/abs/2410.07041",
        "title": "Emergent properties with repeated examples",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We study the performance of transformers as a function of the number of repetitions of training examples with algorithmically generated datasets. On three problems of mathematics: the greatest common divisor, modular multiplication, and matrix eigenvalues, we show that for a fixed number of training steps, models trained on smaller sets of repeated examples outperform models trained on larger sets of single-use examples. We also demonstrate that two-set training - repeated use of a small random subset of examples, along normal sampling on the rest of the training set - provides for faster learning and better performance. This highlights that the benefits of repetition can outweigh those of data diversity. These datasets and problems provide a controlled setting to shed light on the still poorly understood interplay between generalization and memorization in deep learning.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07046",
        "abstract url": "https://arxiv.org/abs/2410.07046",
        "title": "S2HPruner: Soft-to-Hard Distillation Bridges the Discretization Gap in Pruning",
        "rating": "0.5",
        "keywords": [
            [
                "architecture search"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Recently, differentiable mask pruning methods optimize the continuous relaxation architecture (soft network) as the proxy of the pruned discrete network (hard network) for superior sub-architecture search. However, due to the agnostic impact of the discretization process, the hard network struggles with the equivalent representational capacity as the soft network, namely discretization gap, which severely spoils the pruning performance. In this paper, we first investigate the discretization gap and propose a novel structural differentiable mask pruning framework named S2HPruner to bridge the discretization gap in a one-stage manner. In the training procedure, SH2Pruner forwards both the soft network and its corresponding hard network, then distills the hard network under the supervision of the soft network. To optimize the mask and prevent performance degradation, we propose a decoupled bidirectional knowledge distillation. It blocks the weight updating from the hard to the soft network while maintaining the gradient corresponding to the mask. Compared with existing pruning arts, S2HPruner achieves surpassing pruning performance without fine-tuning on comprehensive benchmarks, including CIFAR-100, Tiny ImageNet, and ImageNet with a variety of network architectures. Besides, investigation and analysis experiments explain the effectiveness of S2HPruner. Codes will be released soon.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "NeurIPS 2024 accepted"
    },
    {
        "paper id": "2410.07054",
        "abstract url": "https://arxiv.org/abs/2410.07054",
        "title": "Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing",
        "rating": "0.5",
        "keywords": [
            [
                "Model Editing"
            ],
            [
                "cs.LG",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have recently revolutionized the NLP field, while they still fall short in some specific down-stream tasks. In the work, we focus on utilizing LLMs to perform machine translation, where we observe that two patterns of errors frequently occur and drastically affect the translation quality: language mismatch and repetition. The work sets out to explore the potential for mitigating these two issues by leveraging model editing methods, e.g., by locating Feed-Forward Network (FFN) neurons or something that are responsible for the errors and deactivating them in the inference time. We find that directly applying such methods either limited effect on the targeted errors or has significant negative side-effect on the general translation quality, indicating that the located components may also be crucial for ensuring machine translation with LLMs on the rails. To this end, we propose to refine the located components by fetching the intersection of the locating results under different language settings, filtering out the aforementioned information that is irrelevant to targeted errors. The experiment results empirically demonstrate that our methods can effectively reduce the language mismatch and repetition ratios and meanwhile enhance or keep the general translation quality in most cases.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "20 pages, EMNLP'2024 Main Conference"
    },
    {
        "paper id": "2410.07059",
        "abstract url": "https://arxiv.org/abs/2410.07059",
        "title": "Online Epsilon Net and Piercing Set for Geometric Concepts",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "VC-dimension and $\\varepsilon$-nets are key concepts in Statistical Learning Theory. Intuitively, VC-dimension is a measure of the size of a class of sets. The famous $\\varepsilon$-net theorem, a fundamental result in Discrete Geometry, asserts that if the VC-dimension of a set system is bounded, then a small sample exists that intersects all sufficiently large sets. In online learning scenarios where data arrives sequentially, the VC-dimension helps to bound the complexity of the set system, and $\\varepsilon$-nets ensure the selection of a small representative set. This sampling framework is crucial in various domains, including spatial data analysis, motion planning in dynamic environments, optimization of sensor networks, and feature extraction in computer vision, among others. Motivated by these applications, we study the online $\\varepsilon$-net problem for geometric concepts with bounded VC-dimension. While the offline version of this problem has been extensively studied, surprisingly, there are no known theoretical results for the online version to date. We present the first deterministic online algorithm with an optimal competitive ratio for intervals in $\\mathbb{R}$. Next, we give a randomized online algorithm with a near-optimal competitive ratio for axis-aligned boxes in $\\mathbb{R}^d$, for $d\\le 3$. Furthermore, we introduce a novel technique to analyze similar-sized objects of constant description complexity in $\\mathbb{R}^d$, which may be of independent interest. Next, we focus on the continuous version of this problem, where ranges of the set system are geometric concepts in $\\mathbb{R}^d$ arriving in an online manner, but the universe is the entire space, and the objective is to choose a small sample that intersects all the ranges.",
        "subjects": [
            "cs.LG",
            "cs.CG"
        ],
        "comment": "18 pages, 4 Figures"
    },
    {
        "paper id": "2410.07063",
        "abstract url": "https://arxiv.org/abs/2410.07063",
        "title": "InAttention: Linear Context Scaling for Transformers",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "VRAM requirements for transformer models scale quadratically with context length due to the self-attention mechanism. In this paper we modify the decoder-only transformer, replacing self-attention with InAttention, which scales linearly with context length during inference by having tokens attend only to initial states. Benchmarking shows that InAttention significantly reduces VRAM usage during inference, enabling handling of long sequences on consumer GPUs. We corroborate that fine-tuning extends context length efficiently, improving performance on long sequences without high training costs. InAttention offers a scalable solution for long-range dependencies in transformer models, paving the way for further optimization.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07066",
        "abstract url": "https://arxiv.org/abs/2410.07066",
        "title": "A Gentle Introduction and Tutorial on Deep Generative Models in Transportation Research",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep Generative Models (DGMs) have rapidly advanced in recent years, becoming essential tools in various fields due to their ability to learn complex data distributions and generate synthetic data. Their importance in transportation research is increasingly recognized, particularly for applications like traffic data generation, prediction, and feature extraction. This paper offers a comprehensive introduction and tutorial on DGMs, with a focus on their applications in transportation. It begins with an overview of generative models, followed by detailed explanations of fundamental models, a systematic review of the literature, and practical tutorial code to aid implementation. The paper also discusses current challenges and opportunities, highlighting how these models can be effectively utilized and further developed in transportation research. This paper serves as a valuable reference, guiding researchers and practitioners from foundational knowledge to advanced applications of DGMs in transportation research.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "64 pages, 21 figures, 4 tables"
    },
    {
        "paper id": "2410.07072",
        "abstract url": "https://arxiv.org/abs/2410.07072",
        "title": "Towards xAI: Configuring RNN Weights using Domain Knowledge for MIMO Receive Processing",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep learning is making a profound impact in the physical layer of wireless communications. Despite exhibiting outstanding empirical performance in tasks such as MIMO receive processing, the reasons behind the demonstrated superior performance improvement remain largely unclear. In this work, we advance the field of Explainable AI (xAI) in the physical layer of wireless communications utilizing signal processing principles. Specifically, we focus on the task of MIMO-OFDM receive processing (e.g., symbol detection) using reservoir computing (RC), a framework within recurrent neural networks (RNNs), which outperforms both conventional and other learning-based MIMO detectors. Our analysis provides a signal processing-based, first-principles understanding of the corresponding operation of the RC. Building on this fundamental understanding, we are able to systematically incorporate the domain knowledge of wireless systems (e.g., channel statistics) into the design of the underlying RNN by directly configuring the untrained RNN weights for MIMO-OFDM symbol detection. The introduced RNN weight configuration has been validated through extensive simulations demonstrating significant performance improvements. This establishes a foundation for explainable RC-based architectures in MIMO-OFDM receive processing and provides a roadmap for incorporating domain knowledge into the design of neural networks for NextG systems.",
        "subjects": [
            "eess.SP",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07096",
        "abstract url": "https://arxiv.org/abs/2410.07096",
        "title": "Identifying and Addressing Delusions for Target-Directed Decision-Making",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "We are interested in target-directed agents, which produce targets during decision-time planning, to guide their behaviors and achieve better generalization during evaluation. Improper training of these agents can result in delusions: the agent may come to hold false beliefs about the targets, which cannot be properly rejected, leading to unwanted behaviors and damaging out-of-distribution generalization. We identify different types of delusions by using intuitive examples in carefully controlled environments, and investigate their causes. We demonstrate how delusions can be addressed for agents trained by hindsight relabeling, a mainstream approach in for training target-directed RL agents. We validate empirically the effectiveness of the proposed solutions in correcting delusional behaviors and improving out-of-distribution generalization.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Changed the template file to correctly display \"PREPRINT\""
    },
    {
        "paper id": "2410.07153",
        "abstract url": "https://arxiv.org/abs/2410.07153",
        "title": "CHASE: Learning Convex Hull Adaptive Shift for Skeleton-based Multi-Entity Action Recognition",
        "rating": "0.5",
        "keywords": [
            [
                "Skeleton"
            ],
            [
                "cs.LG",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Skeleton-based multi-entity action recognition is a challenging task aiming to identify interactive actions or group activities involving multiple diverse entities. Existing models for individuals often fall short in this task due to the inherent distribution discrepancies among entity skeletons, leading to suboptimal backbone optimization. To this end, we introduce a Convex Hull Adaptive Shift based multi-Entity action recognition method (CHASE), which mitigates inter-entity distribution gaps and unbiases subsequent backbones. Specifically, CHASE comprises a learnable parameterized network and an auxiliary objective. The parameterized network achieves plausible, sample-adaptive repositioning of skeleton sequences through two key components. First, the Implicit Convex Hull Constrained Adaptive Shift ensures that the new origin of the coordinate system is within the skeleton convex hull. Second, the Coefficient Learning Block provides a lightweight parameterization of the mapping from skeleton sequences to their specific coefficients in convex combinations. Moreover, to guide the optimization of this network for discrepancy minimization, we propose the Mini-batch Pair-wise Maximum Mean Discrepancy as the additional objective. CHASE operates as a sample-adaptive normalization method to mitigate inter-entity distribution discrepancies, thereby reducing data bias and improving the subsequent classifier's multi-entity action recognition performance. Extensive experiments on six datasets, including NTU Mutual 11/26, H2O, Assembly101, Collective Activity and Volleyball, consistently verify our approach by seamlessly adapting to single-entity backbones and boosting their performance in multi-entity scenarios. Our code is publicly available at https://github.com/Necolizer/CHASE .",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "NeurIPS 2024 Camera-ready Version"
    },
    {
        "paper id": "2410.07158",
        "abstract url": "https://arxiv.org/abs/2410.07158",
        "title": "Quanda: An Interpretability Toolkit for Training Data Attribution Evaluation and Beyond",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In recent years, training data attribution (TDA) methods have emerged as a promising direction for the interpretability of neural networks. While research around TDA is thriving, limited effort has been dedicated to the evaluation of attributions. Similar to the development of evaluation metrics for traditional feature attribution approaches, several standalone metrics have been proposed to evaluate the quality of TDA methods across various contexts. However, the lack of a unified framework that allows for systematic comparison limits trust in TDA methods and stunts their widespread adoption. To address this research gap, we introduce Quanda, a Python toolkit designed to facilitate the evaluation of TDA methods. Beyond offering a comprehensive set of evaluation metrics, Quanda provides a uniform interface for seamless integration with existing TDA implementations across different repositories, thus enabling systematic benchmarking. The toolkit is user-friendly, thoroughly tested, well-documented, and available as an open-source library on PyPi and under https://github.com/dilyabareeva/quanda.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07172",
        "abstract url": "https://arxiv.org/abs/2410.07172",
        "title": "Glider: Global and Local Instruction-Driven Expert Router",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The availability of performant pre-trained models has led to a proliferation of fine-tuned expert models that are specialized to particular domains. This has enabled the creation of powerful and adaptive routing-based \"Model MoErging\" methods with the goal of using expert modules to create an aggregate system with improved performance or generalization. However, existing MoErging methods often prioritize generalization to unseen tasks at the expense of performance on held-in tasks, which limits its practical applicability in real-world deployment scenarios. We observe that current token-level routing mechanisms neglect the global semantic context of the input task. This token-wise independence hinders effective expert selection for held-in tasks, as routing decisions fail to incorporate the semantic properties of the task. To address this, we propose, Global and Local Instruction Driven Expert Router (GLIDER) that integrates a multi-scale routing mechanism, encompassing a semantic global router and a learned local router. The global router leverages LLM's advanced reasoning capabilities for semantic-related contexts to enhance expert selection. Given the input query and LLM, the router generates semantic task instructions that guide the retrieval of the most relevant experts across all layers. This global guidance is complemented by a local router that facilitates token-level routing decisions within each module, enabling finer control and enhanced performance on unseen tasks. Our experiments using T5-based models for T0 and FLAN tasks demonstrate that GLIDER achieves substantially improved held-in performance while maintaining strong generalization on held-out tasks. We also perform ablations experiments to dive deeper into the components of GLIDER. Our experiments highlight the importance of our multi-scale routing that leverages LLM-driven semantic reasoning for MoErging methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Our code is available at https://github.com/UNITES-Lab/glider"
    },
    {
        "paper id": "2410.07273",
        "abstract url": "https://arxiv.org/abs/2410.07273",
        "title": "BELM: Bidirectional Explicit Linear Multi-step Sampler for Exact Inversion in Diffusion Models",
        "rating": "0.5",
        "keywords": [
            [
                "Diffusion",
                "image editing"
            ],
            [
                "cs.LG",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "The inversion of diffusion model sampling, which aims to find the corresponding initial noise of a sample, plays a critical role in various tasks. Recently, several heuristic exact inversion samplers have been proposed to address the inexact inversion issue in a training-free manner. However, the theoretical properties of these heuristic samplers remain unknown and they often exhibit mediocre sampling quality. In this paper, we introduce a generic formulation, \\emph{Bidirectional Explicit Linear Multi-step} (BELM) samplers, of the exact inversion samplers, which includes all previously proposed heuristic exact inversion samplers as special cases. The BELM formulation is derived from the variable-stepsize-variable-formula linear multi-step method via integrating a bidirectional explicit constraint. We highlight this bidirectional explicit constraint is the key of mathematically exact inversion. We systematically investigate the Local Truncation Error (LTE) within the BELM framework and show that the existing heuristic designs of exact inversion samplers yield sub-optimal LTE. Consequently, we propose the Optimal BELM (O-BELM) sampler through the LTE minimization approach. We conduct additional analysis to substantiate the theoretical stability and global convergence property of the proposed optimal sampler. Comprehensive experiments demonstrate our O-BELM sampler establishes the exact inversion property while achieving high-quality sampling. Additional experiments in image editing and image interpolation highlight the extensive potential of applying O-BELM in varying applications.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "accepted paper by NeurIPS"
    },
    {
        "paper id": "2410.07282",
        "abstract url": "https://arxiv.org/abs/2410.07282",
        "title": "A Utility-Mining-Driven Active Learning Approach for Analyzing Clickstream Sequences",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In rapidly evolving e-commerce industry, the capability of selecting high-quality data for model training is essential. This study introduces the High-Utility Sequential Pattern Mining using SHAP values (HUSPM-SHAP) model, a utility mining-based active learning strategy to tackle this challenge. We found that the parameter settings for positive and negative SHAP values impact the model's mining outcomes, introducing a key consideration into the active learning framework. Through extensive experiments aimed at predicting behaviors that do lead to purchases or not, the designed HUSPM-SHAP model demonstrates its superiority across diverse scenarios. The model's ability to mitigate labeling needs while maintaining high predictive performance is highlighted. Our findings demonstrate the model's capability to refine e-commerce data processing, steering towards more streamlined, cost-effective prediction modeling.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "7 pages, 2 figures, preprint version"
    },
    {
        "paper id": "2410.07289",
        "abstract url": "https://arxiv.org/abs/2410.07289",
        "title": "Principal Orthogonal Latent Components Analysis (POLCA Net)",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Representation learning is a pivotal area in the field of machine learning, focusing on the development of methods to automatically discover the representations or features needed for a given task from raw data. Unlike traditional feature engineering, which requires manual crafting of features, representation learning aims to learn features that are more useful and relevant for tasks such as classification, prediction, and clustering. We introduce Principal Orthogonal Latent Components Analysis Network (POLCA Net), an approach to mimic and extend PCA and LDA capabilities to non-linear domains. POLCA Net combines an autoencoder framework with a set of specialized loss functions to achieve effective dimensionality reduction, orthogonality, variance-based feature sorting, high-fidelity reconstructions, and additionally, when used with classification labels, a latent representation well suited for linear classifiers and low dimensional visualization of class distribution as well.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07302",
        "abstract url": "https://arxiv.org/abs/2410.07302",
        "title": "Examining the Prevalence and Dynamics of AI-Generated Media in Art Subreddits",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.SI",
                "cs.CY"
            ]
        ],
        "abstract": "Broadly accessible generative AI models like Dall-E have made it possible for anyone to create compelling visual art. In online communities, the introduction of AI-generated content (AIGC) may impact community dynamics by shifting the kinds of content being posted or the responses to content suspected of being generated by AI. We take steps towards examining the potential impact of AIGC on art-related communities on Reddit. We distinguish between communities that disallow AI content and those without a direct policy. We look at image-based posts made to these communities that are transparently created by AI, or comments in these communities that suspect authors of using generative AI. We find that AI posts (and accusations) have played a very small part in these communities through the end of 2023, accounting for fewer than 0.2% of the image-based posts. Even as the absolute number of author-labelled AI posts dwindles over time, accusations of AI use remain more persistent. We show that AI content is more readily used by newcomers and may help increase participation if it aligns with community rules. However, the tone of comments suspecting AI use by others have become more negative over time, especially in communities that do not have explicit rules about AI. Overall, the results show the changing norms and interactions around AIGC in online communities designated for creativity.",
        "subjects": [
            "cs.AI",
            "cs.CY",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07304",
        "abstract url": "https://arxiv.org/abs/2410.07304",
        "title": "The Moral Turing Test: Evaluating Human-LLM Alignment in Moral Decision-Making",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "As large language models (LLMs) become increasingly integrated into society, their alignment with human morals is crucial. To better understand this alignment, we created a large corpus of human- and LLM-generated responses to various moral scenarios. We found a misalignment between human and LLM moral assessments; although both LLMs and humans tended to reject morally complex utilitarian dilemmas, LLMs were more sensitive to personal framing. We then conducted a quantitative user study involving 230 participants (N=230), who evaluated these responses by determining whether they were AI-generated and assessed their agreement with the responses. Human evaluators preferred LLMs' assessments in moral scenarios, though a systematic anti-AI bias was observed: participants were less likely to agree with judgments they believed to be machine-generated. Statistical and NLP-based analyses revealed subtle linguistic differences in responses, influencing detection and agreement. Overall, our findings highlight the complexities of human-AI perception in morally charged decision-making.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07305",
        "abstract url": "https://arxiv.org/abs/2410.07305",
        "title": "A Blockchain and Artificial Intelligence based System for Halal Food Traceability",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The demand of the halal food products is increasing rapidly around the world. The consumption of halal food product is just not among the Muslims but also among non-Muslims, due to the purity of the halal food products. However, there are several challenges that are faced by the halal food consumers. The challenges raise a doubt among the halal food consumers about the authenticity of the product being halal. Therefore, a solution that can address these issues and can establish trust between consumers and producers. Blockchain technology can provide a distributed ledger of an immutable record of the information. Artificial intelligence supports developing a solution for pattern identification. The proposed research utilizes blockchain an artificial intelligence-based system for developing a system that ensure the authenticity of the halal food products by providing the traceability related to all the operations and processes of the supply chain and sourcing the raw material. The proposed system has been tested with a local supermarket. The results and tests of the developed solution seemed effective and the testers expressed interest in real-world implementation of the proposed system.",
        "subjects": [
            "cs.DC",
            "cs.AI"
        ],
        "comment": "13 pages"
    },
    {
        "paper id": "2410.07348",
        "abstract url": "https://arxiv.org/abs/2410.07348",
        "title": "MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In this work, we aim to simultaneously enhance the effectiveness and efficiency of Mixture-of-Experts (MoE) methods. To achieve this, we propose MoE++, a general and heterogeneous MoE framework that integrates both Feed-Forward Network~(FFN) and zero-computation experts. Specifically, we introduce three types of zero-computation experts: the zero expert, copy expert, and constant expert, which correspond to discard, skip, and replace operations, respectively. This design offers three key advantages: (i) Low Computing Overhead: Unlike the uniform mixing mechanism for all tokens within vanilla MoE, MoE++ allows each token to engage with a dynamic number of FFNs, be adjusted by constant vectors, or even skip the MoE layer entirely. (ii) High Performance: By enabling simple tokens to utilize fewer FFN experts, MoE++ allows more experts to focus on challenging tokens, thereby unlocking greater performance potential than vanilla MoE. (iii) Deployment Friendly: Given that zero-computation experts have negligible parameters, we can deploy all zero-computation experts on each GPU, eliminating the significant communication overhead and expert load imbalance associated with FFN experts distributed across different GPUs. Moreover, we leverage gating residuals, enabling each token to consider the pathway taken in the previous layer when selecting the appropriate experts. Extensive experimental results demonstrate that MoE++ achieves better performance while delivering 1.1-2.1x expert forward throughput compared to a vanilla MoE model of the same size, which lays a solid foundation for developing advanced and efficient MoE-related models.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "23 pages, Code: https://github.com/SkyworkAI/MoE-plus-plus"
    },
    {
        "paper id": "2410.07352",
        "abstract url": "https://arxiv.org/abs/2410.07352",
        "title": "Generating Origin-Destination Matrices in Neural Spatial Interaction Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Agent-based models (ABMs) are proliferating as decision-making tools across policy areas in transportation, economics, and epidemiology. In these models, a central object of interest is the discrete origin-destination matrix which captures spatial interactions and agent trip counts between locations. Existing approaches resort to continuous approximations of this matrix and subsequent ad-hoc discretisations in order to perform ABM simulation and calibration. This impedes conditioning on partially observed summary statistics, fails to explore the multimodal matrix distribution over a discrete combinatorial support, and incurs discretisation errors. To address these challenges, we introduce a computationally efficient framework that scales linearly with the number of origin-destination pairs, operates directly on the discrete combinatorial space, and learns the agents' trip intensity through a neural differential equation that embeds spatial interactions. Our approach outperforms the prior art in terms of reconstruction error and ground truth matrix coverage, at a fraction of the computational cost. We demonstrate these benefits in large-scale spatial mobility ABMs in Cambridge, UK and Washington, DC, USA.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07358",
        "abstract url": "https://arxiv.org/abs/2410.07358",
        "title": "Improving the portability of predicting students performance models by using ontologies",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "One of the main current challenges in Educational Data Mining and Learning Analytics is the portability or transferability of predictive models obtained for a particular course so that they can be applied to other different courses. To handle this challenge, one of the foremost problems is the models excessive dependence on the low-level attributes used to train them, which reduces the models portability. To solve this issue, the use of high level attributes with more semantic meaning, such as ontologies, may be very useful. Along this line, we propose the utilization of an ontology that uses a taxonomy of actions that summarises students interactions with the Moodle learning management system. We compare the results of this proposed approach against our previous results when we used low-level raw attributes obtained directly from Moodle logs. The results indicate that the use of the proposed ontology improves the portability of the models in terms of predictive accuracy. The main contribution of this paper is to show that the ontological models obtained in one source course can be applied to other different target courses with similar usage levels without losing prediction accuracy.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07388",
        "abstract url": "https://arxiv.org/abs/2410.07388",
        "title": "On Densest $k$-Subgraph Mining and Diagonal Loading",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "The Densest $k$-Subgraph (D$k$S) problem aims to find a subgraph comprising $k$ vertices with the maximum number of edges between them. A continuous reformulation of the binary quadratic D$k$S problem is considered, which incorporates a diagonal loading term. It is shown that this non-convex, continuous relaxation is tight for a range of diagonal loading parameters, and the impact of the diagonal loading parameter on the optimization landscape is studied. On the algorithmic side, two projection-free algorithms are proposed to tackle the relaxed problem, based on Frank-Wolfe and explicit constraint parametrization, respectively. Experiments suggest that both algorithms have merits relative to the state-of-art, while the Frank-Wolfe-based algorithm stands out in terms of subgraph density, computational complexity, and ability to scale up to very large datasets.",
        "subjects": [
            "cs.SI",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07397",
        "abstract url": "https://arxiv.org/abs/2410.07397",
        "title": "Aligning AI-driven discovery with human intuition",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "As data-driven modeling of physical dynamical systems becomes more prevalent, a new challenge is emerging: making these models more compatible and aligned with existing human knowledge. AI-driven scientific modeling processes typically begin with identifying hidden state variables, then deriving governing equations, followed by predicting and analyzing future behaviors. The critical initial step of identification of an appropriate set of state variables remains challenging for two reasons. First, finding a compact set of meaningfully predictive variables is mathematically difficult and under-defined. A second reason is that variables found often lack physical significance, and are therefore difficult for human scientists to interpret. We propose a new general principle for distilling representations that are naturally more aligned with human intuition, without relying on prior physical knowledge. We demonstrate our approach on a number of experimental and simulated system where the variables generated by the AI closely resemble those chosen independently by human scientists. We suggest that this principle can help make human-AI collaboration more fruitful, as well as shed light on how humans make scientific modeling choices.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07404",
        "abstract url": "https://arxiv.org/abs/2410.07404",
        "title": "Fostering Intrinsic Motivation in Reinforcement Learning with Pretrained Foundation Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Exploration remains a significant challenge in reinforcement learning, especially in environments where extrinsic rewards are sparse or non-existent. The recent rise of foundation models, such as CLIP, offers an opportunity to leverage pretrained, semantically rich embeddings that encapsulate broad and reusable knowledge. In this work we explore the potential of these foundation models not just to drive exploration, but also to analyze the critical role of the episodic novelty term in enhancing exploration effectiveness of the agent. We also investigate whether providing the intrinsic module with complete state information -- rather than just partial observations -- can improve exploration, despite the difficulties in handling small variations within large state spaces. Our experiments in the MiniGrid domain reveal that intrinsic modules can effectively utilize full state information, significantly increasing sample efficiency while learning an optimal policy. Moreover, we show that the embeddings provided by foundation models are sometimes even better than those constructed by the agent during training, further accelerating the learning process, especially when coupled with the episodic novelty term to enhance exploration.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07426",
        "abstract url": "https://arxiv.org/abs/2410.07426",
        "title": "CAFEEN: A Cooperative Approach for Energy Efficient NoCs with Multi-Agent Reinforcement Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In emerging high-performance Network-on-Chip (NoC) architectures, efficient power management is crucial to minimize energy consumption. We propose a novel framework called CAFEEN that employs both heuristic-based fine-grained and machine learning-based coarse-grained power-gating for energy-efficient NoCs. CAFEEN uses a fine-grained method to activate only essential NoC buffers during lower network loads. It switches to a coarse-grained method at peak loads to minimize compounding wake-up overhead using multi-agent reinforcement learning. Results show that CAFEEN adaptively balances power-efficiency with performance, reducing total energy by 2.60x for single application workloads and 4.37x for multi-application workloads, compared to state-of-the-art NoC power-gating frameworks.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07427",
        "abstract url": "https://arxiv.org/abs/2410.07427",
        "title": "A Generalization Bound for a Family of Implicit Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Implicit networks are a class of neural networks whose outputs are defined by the fixed point of a parameterized operator. They have enjoyed success in many applications including natural language processing, image processing, and numerous other applications. While they have found abundant empirical success, theoretical work on its generalization is still under-explored. In this work, we consider a large family of implicit networks defined parameterized contractive fixed point operators. We show a generalization bound for this class based on a covering number argument for the Rademacher complexity of these architectures.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07432",
        "abstract url": "https://arxiv.org/abs/2410.07432",
        "title": "Can Transformers Reason Logically? A Study in SAT Solving",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We theoretically and empirically study the logical reasoning capabilities of LLMs in the context of the Boolean satisfiability (SAT) problem. First, we construct a decoder-only Transformer that can solve SAT using backtracking and deduction via Chain-of-Thought (CoT). We prove its correctness by showing trace equivalence to the well-known DPLL SAT-solving algorithm. Second, to support the implementation of this abstract construction, we design a compiler $\\texttt{PARAT}$ that takes as input a procedural specification and outputs a transformer model implementing this specification. Third, rather than $\\textit{programming}$ a transformer to reason, we evaluate empirically whether it can be $\\textit{trained}$ to do so by learning directly from algorithmic traces (\"reasoning paths\") of the DPLL algorithm.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.LO"
        ],
        "comment": "29 pages, 4 Figures"
    },
    {
        "paper id": "2410.07456",
        "abstract url": "https://arxiv.org/abs/2410.07456",
        "title": "SAGE: Scalable Ground Truth Evaluations for Large Sparse Autoencoders",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "A key challenge in interpretability is to decompose model activations into meaningful features. Sparse autoencoders (SAEs) have emerged as a promising tool for this task. However, a central problem in evaluating the quality of SAEs is the absence of ground truth features to serve as an evaluation gold standard. Current evaluation methods for SAEs are therefore confronted with a significant trade-off: SAEs can either leverage toy models or other proxies with predefined ground truth features; or they use extensive prior knowledge of realistic task circuits. The former limits the generalizability of the evaluation results, while the latter limits the range of models and tasks that can be used for evaluations. We introduce SAGE: Scalable Autoencoder Ground-truth Evaluation, a ground truth evaluation framework for SAEs that scales to large state-of-the-art SAEs and models. We demonstrate that our method can automatically identify task-specific activations and compute ground truth features at these points. Compared to previous methods we reduce the training overhead by introducing a novel reconstruction method that allows to apply residual stream SAEs to sublayer activations. This eliminates the need for SAEs trained on every task-specific activation location. Then we validate the scalability of our framework, by evaluating SAEs on novel tasks on Pythia70M, GPT-2 Small, and Gemma-2-2. Our framework therefore paves the way for generalizable, large-scale evaluations of SAEs in interpretability research.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07458",
        "abstract url": "https://arxiv.org/abs/2410.07458",
        "title": "Systematic Feature Design for Cycle Life Prediction of Lithium-Ion Batteries During Formation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Optimization of the formation step in lithium-ion battery manufacturing is challenging due to limited physical understanding of solid electrolyte interphase formation and the long testing time (~100 days) for cells to reach the end of life. We propose a systematic feature design framework that requires minimal domain knowledge for accurate cycle life prediction during formation. Two simple Q(V) features designed from our framework, extracted from formation data without any additional diagnostic cycles, achieved a median of 9.20% error for cycle life prediction, outperforming thousands of autoML models using pre-defined features. We attribute the strong performance of our designed features to their physical origins - the voltage ranges identified by our framework capture the effects of formation temperature and microscopic particle resistance heterogeneity. By designing highly interpretable features, our approach can accelerate formation research, leveraging the interplay between data-driven feature design and mechanistic understanding.",
        "subjects": [
            "cs.LG",
            "stat.AP"
        ],
        "comment": "Main: 27 pages, 6 figures. SI: 13 pages, 9 figures"
    },
    {
        "paper id": "2410.07476",
        "abstract url": "https://arxiv.org/abs/2410.07476",
        "title": "Unifying and Verifying Mechanistic Interpretations: A Case Study with Group Operations",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "A recent line of work in mechanistic interpretability has focused on reverse-engineering the computation performed by neural networks trained on the binary operation of finite groups. We investigate the internals of one-hidden-layer neural networks trained on this task, revealing previously unidentified structure and producing a more complete description of such models that unifies the explanations of previous works. Notably, these models approximate equivariance in each input argument. We verify that our explanation applies to a large fraction of networks trained on this task by translating it into a compact proof of model performance, a quantitative evaluation of model understanding. In particular, our explanation yields a guarantee of model accuracy that runs in 30% the time of brute force and gives a >=95% accuracy bound for 45% of the models we trained. We were unable to obtain nontrivial non-vacuous accuracy bounds using only explanations from previous works.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "23 pages, 4 figures"
    },
    {
        "paper id": "2410.07484",
        "abstract url": "https://arxiv.org/abs/2410.07484",
        "title": "WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Can large language models (LLMs) directly serve as powerful world models for model-based agents? While the gaps between the prior knowledge of LLMs and the specified environment's dynamics do exist, our study reveals that the gaps can be bridged by aligning an LLM with its deployed environment and such \"world alignment\" can be efficiently achieved by rule learning on LLMs. Given the rich prior knowledge of LLMs, only a few additional rules suffice to align LLM predictions with the specified environment dynamics. To this end, we propose a neurosymbolic approach to learn these rules gradient-free through LLMs, by inducing, updating, and pruning rules based on comparisons of agent-explored trajectories and world model predictions. The resulting world model is composed of the LLM and the learned rules. Our embodied LLM agent \"WALL-E\" is built upon model-predictive control (MPC). By optimizing look-ahead actions based on the precise world model, MPC significantly improves exploration and learning efficiency. Compared to existing LLM agents, WALL-E's reasoning only requires a few principal rules rather than verbose buffered trajectories being included in the LLM input. On open-world challenges in Minecraft and ALFWorld, WALL-E achieves higher success rates than existing methods, with lower costs on replanning time and the number of tokens used for reasoning. In Minecraft, WALL-E exceeds baselines by 15-30% in success rate while costing 8-20 fewer replanning rounds and only 60-80% of tokens. In ALFWorld, its success rate surges to a new record high of 95% only after 6 iterations.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "35 pages, including references and appendix"
    },
    {
        "paper id": "2410.07485",
        "abstract url": "https://arxiv.org/abs/2410.07485",
        "title": "Gem: Gaussian Mixture Model Embeddings for Numerical Feature Distributions",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Embeddings are now used to underpin a wide variety of data management tasks, including entity resolution, dataset search and semantic type detection. Such applications often involve datasets with numerical columns, but there has been more emphasis placed on the semantics of categorical data in embeddings than on the distinctive features of numerical data. In this paper, we propose a method called Gem (Gaussian mixture model embeddings) that creates embeddings that build on numerical value distributions from columns. The proposed method specializes a Gaussian Mixture Model (GMM) to identify and cluster columns with similar value distributions. We introduce a signature mechanism that generates a probability matrix for each column, indicating its likelihood of belonging to specific Gaussian components, which can be used for different applications, such as to determine semantic types. Finally, we generate embeddings for three numerical data properties: distributional, statistical, and contextual. Our core method focuses solely on numerical columns without using table names or neighboring columns for context. However, the method can be combined with other types of evidence, and we later integrate attribute names with the Gaussian embeddings to evaluate the method's contribution to improving overall performance. We compare Gem with several baseline methods for numeric only and numeric + context tasks, showing that Gem consistently outperforms the baselines on four benchmark datasets.",
        "subjects": [
            "cs.DB",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07502",
        "abstract url": "https://arxiv.org/abs/2410.07502",
        "title": "Adaptive Batch Size for Privately Finding Second-Order Stationary Points",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "There is a gap between finding a first-order stationary point (FOSP) and a second-order stationary point (SOSP) under differential privacy constraints, and it remains unclear whether privately finding an SOSP is more challenging than finding an FOSP. Specifically, Ganesh et al. (2023) demonstrated that an $\u03b1$-SOSP can be found with $\u03b1=O(\\frac{1}{n^{1/3}}+(\\frac{\\sqrt{d}}{n\u03b5})^{3/7})$, where $n$ is the dataset size, $d$ is the dimension, and $\u03b5$ is the differential privacy parameter. Building on the SpiderBoost algorithm framework, we propose a new approach that uses adaptive batch sizes and incorporates the binary tree mechanism. Our method improves the results for privately finding an SOSP, achieving $\u03b1=O(\\frac{1}{n^{1/3}}+(\\frac{\\sqrt{d}}{n\u03b5})^{1/2})$. This improved bound matches the state-of-the-art for finding an FOSP, suggesting that privately finding an SOSP may be achievable at no additional cost.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "cs.DS",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07505",
        "abstract url": "https://arxiv.org/abs/2410.07505",
        "title": "CrossQuant: A Post-Training Quantization Method with Smaller Quantization Kernel for Precise Large Language Model Compression",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Post-Training Quantization (PTQ) is an effective technique for compressing Large Language Models (LLMs). While many studies focus on quantizing both weights and activations, it is still a challenge to maintain the accuracy of LLM after activating quantization. To investigate the primary cause, we extend the concept of kernel from linear algebra to quantization functions to define a new term, \"quantization kernel\", which refers to the set of elements in activations that are quantized to zero. Through quantitative analysis of the quantization kernel, we find that these elements are crucial for maintaining the accuracy of quantized LLMs. With the decrease of quantization kernel, the precision of quantized LLMs increases. If the quantization kernel proportion is kept below 19% for OPT models and below 1% for LLaMA models, the precision loss from quantizing activations to INT8 becomes negligible. Motivated by the goal of developing a quantization method with small quantization kernel, we propose CrossQuant: a simple yet effective method for quantizing activations. CrossQuant cross-quantizes elements using row and column-wise absolute maximum vectors, achieving a quantization kernel of approximately 16% for OPT models and less than 0.1% for LLaMA models. Experimental results on LLMs (LLaMA, OPT) ranging from 6.7B to 70B parameters demonstrate that CrossQuant improves or maintains perplexity and accuracy in language modeling, zero-shot, and few-shot tasks.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07526",
        "abstract url": "https://arxiv.org/abs/2410.07526",
        "title": "MKGL: Mastery of a Three-Word Language",
        "rating": "0.5",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Large language models (LLMs) have significantly advanced performance across a spectrum of natural language processing (NLP) tasks. Yet, their application to knowledge graphs (KGs), which describe facts in the form of triplets and allow minimal hallucinations, remains an underexplored frontier. In this paper, we investigate the integration of LLMs with KGs by introducing a specialized KG Language (KGL), where a sentence precisely consists of an entity noun, a relation verb, and ends with another entity noun. Despite KGL's unfamiliar vocabulary to the LLM, we facilitate its learning through a tailored dictionary and illustrative sentences, and enhance context understanding via real-time KG context retrieval and KGL token embedding augmentation. Our results reveal that LLMs can achieve fluency in KGL, drastically reducing errors compared to conventional KG embedding methods on KG completion. Furthermore, our enhanced LLM shows exceptional competence in generating accurate three-word sentences from an initial entity and interpreting new unseen terms out of KGs.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "NeurIPS 2024 (spotlight)"
    },
    {
        "paper id": "2410.07531",
        "abstract url": "https://arxiv.org/abs/2410.07531",
        "title": "Reducing the Cost of Dropout in Flash-Attention by Hiding RNG with GEMM",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Dropout, a network operator, when enabled is likely to dramatically impact the performance of Flash-Attention, which in turn increases the end-to-end training time of Large-Language-Models (LLMs). The main contributor to such performance degradation is the Random Number Generation (RNG) phase that is traditionally fused into the Flash-Attention kernel. As RNG and Attention have the same hardware bottlenecks, RNG latency can hardly be hidden within the Attention kernel. We propose overlapping RNG with previous GEMM layers in the network to hide RNG runtime and improve end-to-end performance. RNG and GEMM have distinct resource requirements and hardware bottlenecks, so they can run in parallel without compromising each other's performance. Our fine-grained performance model, cross-validated by silicon results, shows 1.14x speedup on one transformer block (including multi-head attention and feed-forward layers) for Llama2, and up to 1.23x speedup when varying workload sizes, on GH100 GPUs with FP8 precision. Further, we extend our theoretical model to different RNG implementations and hardware architectures, and discuss the widely applicable benefits for overlapping RNG with GEMM layers.",
        "subjects": [
            "cs.AR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07539",
        "abstract url": "https://arxiv.org/abs/2410.07539",
        "title": "Efficient Generation of Molecular Clusters with Dual-Scale Equivariant Flow Matching",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Amorphous molecular solids offer a promising alternative to inorganic semiconductors, owing to their mechanical flexibility and solution processability. The packing structure of these materials plays a crucial role in determining their electronic and transport properties, which are key to enhancing the efficiency of devices like organic solar cells (OSCs). However, obtaining these optoelectronic properties computationally requires molecular dynamics (MD) simulations to generate a conformational ensemble, a process that can be computationally expensive due to the large system sizes involved. Recent advances have focused on using generative models, particularly flow-based models as Boltzmann generators, to improve the efficiency of MD sampling. In this work, we developed a dual-scale flow matching method that separates training and inference into coarse-grained and all-atom stages and enhances both the accuracy and efficiency of standard flow matching samplers. We demonstrate the effectiveness of this method on a dataset of Y6 molecular clusters obtained through MD simulations, and we benchmark its efficiency and accuracy against single-scale flow matching methods.",
        "subjects": [
            "cond-mat.mtrl-sci",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07553",
        "abstract url": "https://arxiv.org/abs/2410.07553",
        "title": "COMMA: A Communicative Multimodal Multi-Agent Benchmark",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The rapid advances of multi-modal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. This oversight presents a critical gap in understanding their effectiveness in real-world deployments, particularly when communicating with humans. Existing agentic benchmarks fail to address key aspects of inter-agent communication and collaboration, particularly in scenarios where agents have unequal access to information and must work together to achieve tasks beyond the scope of individual capabilities. To fill this gap, we introduce a novel benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication. Our benchmark features a variety of scenarios, providing a comprehensive evaluation across four key categories of agentic capability in a communicative collaboration setting. By testing both agent-agent and agent-human collaborations using open-source and closed-source models, our findings reveal surprising weaknesses in state-of-the-art models, including proprietary models like GPT-4o. These models struggle to outperform even a simple random agent baseline in agent-agent collaboration and only surpass the random baseline when a human is involved.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07560",
        "abstract url": "https://arxiv.org/abs/2410.07560",
        "title": "From student to working professional: A graduate survey",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "This paper reports on the results of a 2023 survey that explores the Work Integrated Learning (WiL) experiences of thirty recent Computer Science (CS) graduates. The graduates had all completed their undergraduate bachelors degree within the last five years and were currently employed in a CS industry role. The survey asked about the graduates' perceptions within a continuum of WiL experiences from final year capstone projects to professional development in their first industry-based role. Most respondents had taken a capstone course involving a team project. Only two respondents had participated in an internship program. Our results indicate that graduates value their capstone experiences and believe that they provide transferable skills including teamwork, managing client relations, exposure to technologies and methods, and time management. When entering their first industry role less than fifty percent of graduates were allocated a mentor. Overwhelmingly, these graduates noted the importance of those mentors in their transition from student to working professional. Very few of the surveyed graduates were provided with ongoing professional development opportunities. Those who did noted significant gains including growth of leadership skills and accelerated career progression. Our survey highlights a gap and an opportunity for tertiary institutions to work with industry to provide graduate onboarding and novice/early-career professional development opportunities.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "6 pages, 2 figures"
    },
    {
        "paper id": "2410.07564",
        "abstract url": "https://arxiv.org/abs/2410.07564",
        "title": "Boosting Deep Ensembles with Learning Rate Tuning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The Learning Rate (LR) has a high impact on deep learning training performance. A common practice is to train a Deep Neural Network (DNN) multiple times with different LR policies to find the optimal LR policy, which has been widely recognized as a daunting and costly task. Moreover, multiple times of DNN training has not been effectively utilized. In practice, often only the optimal LR is adopted, which misses the opportunities to further enhance the overall accuracy of the deep learning system and results in a huge waste of both computing resources and training time. This paper presents a novel framework, LREnsemble, to effectively leverage effective learning rate tuning to boost deep ensemble performance. We make three original contributions. First, we show that the LR tuning with different LR policies can produce highly diverse DNNs, which can be supplied as base models for deep ensembles. Second, we leverage different ensemble selection algorithms to identify high-quality deep ensembles from the large pool of base models with significant accuracy improvements over the best single base model. Third, we propose LREnsemble, a framework that utilizes the synergy of LR tuning and deep ensemble techniques to enhance deep learning performance. The experiments on multiple benchmark datasets have demonstrated the effectiveness of LREnsemble, generating up to 2.34% accuracy improvements over well-optimized baselines.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07947",
        "abstract url": "https://arxiv.org/abs/2410.07947",
        "title": "Exploring the core-periphery and community structure in the financial networks through random matrix theory",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "In finance, Random Matrix Theory (RMT) is an important tool for filtering out noise from large datasets, revealing true correlations among stocks, enhancing risk management and portfolio optimization. In this study, we use RMT to filter out noise from the full cross-correlation matrix of stock price returns for the NIFTY 200 and NIFTY 500 indices on the National Stock Exchange of India. In addition, we applied network theory tools to analyze market and sector modes as filtered correlation structures to study local interactions within financial networks. This allows us to study the very fundamental properties of networks, such as the core-periphery and the community structure of constructed networks over these filtered modes, and compare the results with the network constructed over the full cross-correlation matrix. The results suggest that the core-periphery structure is contained in the market mode, while the community structure is in the sector mode. Thus, both modes outperform the full cross-correlation in terms of capturing the essential respective structure of the network. Furthermore, we used these insights to build portfolios based on communities of the networks corresponding to the sector mode and the network corresponding to the full cross-correlation matrix. The results suggest that the portfolio constructed on the complete cross-correlation-based matrix performs better than the sector mode. These insights provide a greater understanding of RMT application in the financial market.",
        "subjects": [
            "cs.SI",
            "physics.soc-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06576",
        "abstract url": "https://arxiv.org/abs/2410.06576",
        "title": "On The Relationship between Visual Anomaly-free and Anomalous Representations",
        "rating": "0",
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Anomaly Detection is an important problem within computer vision, having variety of real-life applications. Yet, the current set of solutions to this problem entail known, systematic shortcomings. Specifically, contemporary surface Anomaly Detection task assumes the presence of multiple specific anomaly classes e.g. cracks, rusting etc., unlike one-class classification model of past. However, building a deep learning model in such setup remains a challenge because anomalies arise rarely, and hence anomaly samples are quite scarce. Transfer learning has been a preferred paradigm in such situations. But the typical source domains with large dataset sizes e.g. ImageNet, JFT-300M, LAION-2B do not correlate well with the domain of surfaces and materials, an important premise of transfer learning. In this paper, we make an important hypothesis and show, by exhaustive experimentation, that the space of anomaly-free visual patterns of the normal samples correlates well with each of the various spaces of anomalous patterns of the class-specific anomaly samples. The first results of using this hypothesis in transfer learning have indeed been quite encouraging. We expect that finding such a simple closeby domain that readily entails large number of samples, and which also oftentimes shows interclass separability though with narrow margins, will be a useful discovery. Especially, it is expected to improve domain adaptation for anomaly detection, and few-shot learning for anomaly detection, making in-the-wild anomaly detection realistically possible in future.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06600",
        "abstract url": "https://arxiv.org/abs/2410.06600",
        "title": "DDRN:a Data Distribution Reconstruction Network for Occluded Person Re-Identification",
        "rating": "0",
        "keywords": [
            [
                "Re-Identification"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In occluded person re-identification(ReID), severe occlusions lead to a significant amount of irrelevant information that hinders the accurate identification of individuals. These irrelevant cues primarily stem from background interference and occluding interference, adversely affecting the final retrieval results. Traditional discriminative models, which rely on the specific content and positions of the images, often misclassify in cases of occlusion. To address these limitations, we propose the Data Distribution Reconstruction Network (DDRN), a generative model that leverages data distribution to filter out irrelevant details, enhancing overall feature perception ability and reducing irrelevant feature interference. Additionally, severe occlusions lead to the complexity of the feature space. To effectively handle this, we design a multi-center approach through the proposed Hierarchical SubcenterArcface (HS-Arcface) loss function, which can better approximate complex feature spaces. On the Occluded-Duke dataset, we achieved a mAP of 62.4\\% (+1.1\\%) and a rank-1 accuracy of 71.3\\% (+0.6\\%), surpassing the latest state-of-the-art methods(FRT) significantly.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06614",
        "abstract url": "https://arxiv.org/abs/2410.06614",
        "title": "Pair-VPR: Place-Aware Pre-training and Contrastive Pair Classification for Visual Place Recognition with Vision Transformers",
        "rating": "0",
        "keywords": [
            [
                "robotics"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "In this work we propose a novel joint training method for Visual Place Recognition (VPR), which simultaneously learns a global descriptor and a pair classifier for re-ranking. The pair classifier can predict whether a given pair of images are from the same place or not. The network only comprises Vision Transformer components for both the encoder and the pair classifier, and both components are trained using their respective class tokens. In existing VPR methods, typically the network is initialized using pre-trained weights from a generic image dataset such as ImageNet. In this work we propose an alternative pre-training strategy, by using Siamese Masked Image Modelling as a pre-training task. We propose a Place-aware image sampling procedure from a collection of large VPR datasets for pre-training our model, to learn visual features tuned specifically for VPR. By re-using the Mask Image Modelling encoder and decoder weights in the second stage of training, Pair-VPR can achieve state-of-the-art VPR performance across five benchmark datasets with a ViT-B encoder, along with further improvements in localization recall with larger encoders. The Pair-VPR website is: https://csiro-robotics.github.io/Pair-VPR.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06626",
        "abstract url": "https://arxiv.org/abs/2410.06626",
        "title": "Open-RGBT: Open-vocabulary RGB-T Zero-shot Semantic Segmentation in Open-world Environments",
        "rating": "0",
        "keywords": [
            [
                "Visual Language",
                "VLMs"
            ],
            [
                "thermal"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Semantic segmentation is a critical technique for effective scene understanding. Traditional RGB-T semantic segmentation models often struggle to generalize across diverse scenarios due to their reliance on pretrained models and predefined categories. Recent advancements in Visual Language Models (VLMs) have facilitated a shift from closed-set to open-vocabulary semantic segmentation methods. However, these models face challenges in dealing with intricate scenes, primarily due to the heterogeneity between RGB and thermal modalities. To address this gap, we present Open-RGBT, a novel open-vocabulary RGB-T semantic segmentation model. Specifically, we obtain instance-level detection proposals by incorporating visual prompts to enhance category understanding. Additionally, we employ the CLIP model to assess image-text similarity, which helps correct semantic consistency and mitigates ambiguities in category identification. Empirical evaluations demonstrate that Open-RGBT achieves superior performance in diverse and challenging real-world scenarios, even in the wild, significantly advancing the field of RGB-T semantic segmentation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06634",
        "abstract url": "https://arxiv.org/abs/2410.06634",
        "title": "Tree of Problems: Improving structured problem solving with compositionality",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across multiple tasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition performs better than CoT on complex reasoning tasks. All code for this paper is publicly available here: https://github.com/ArmelRandy/tree-of-problems.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06704",
        "abstract url": "https://arxiv.org/abs/2410.06704",
        "title": "PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "In this work, we introduce PII-Scope, a comprehensive benchmark designed to evaluate state-of-the-art methodologies for PII extraction attacks targeting LLMs across diverse threat settings. Our study provides a deeper understanding of these attacks by uncovering several hyperparameters (e.g., demonstration selection) crucial to their effectiveness. Building on this understanding, we extend our study to more realistic attack scenarios, exploring PII attacks that employ advanced adversarial strategies, including repeated and diverse querying, and leveraging iterative learning for continual PII extraction. Through extensive experimentation, our results reveal a notable underestimation of PII leakage in existing single-query attacks. In fact, we show that with sophisticated adversarial capabilities and a limited query budget, PII extraction rates can increase by up to fivefold when targeting the pretrained model. Moreover, we evaluate PII leakage on finetuned models, showing that they are more vulnerable to leakage than pretrained models. Overall, our work establishes a rigorous empirical benchmark for PII extraction attacks in realistic threat scenarios and provides a strong foundation for developing effective mitigation strategies.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06711",
        "abstract url": "https://arxiv.org/abs/2410.06711",
        "title": "Analysis of different disparity estimation techniques on aerial stereo image datasets",
        "rating": "0",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "With the advent of aerial image datasets, dense stereo matching has gained tremendous progress. This work analyses dense stereo correspondence analysis on aerial images using different techniques. Traditional methods, optimization based methods and learning based methods have been implemented and compared here for aerial images. For traditional methods, we implemented the architecture of Stereo SGBM while using different cost functions to get an understanding of their performance on aerial datasets. Analysis of most of the methods in standard datasets has shown good performance, however in case of aerial dataset, not much benchmarking is available. Visual qualitative and quantitative analysis has been carried out for two stereo aerial datasets in order to compare different cost functions and techniques for the purpose of depth estimation from stereo images. Using existing pre-trained models, recent learning based architectures have also been tested on stereo pairs along with different cost functions in SGBM. The outputs and given ground truth are compared using MSE, SSIM and other error metrics.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06719",
        "abstract url": "https://arxiv.org/abs/2410.06719",
        "title": "Suppress Content Shift: Better Diffusion Features via Off-the-Shelf Generation Techniques",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion models are powerful generative models, and this capability can also be applied to discrimination. The inner activations of a pre-trained diffusion model can serve as features for discriminative tasks, namely, diffusion feature. We discover that diffusion feature has been hindered by a hidden yet universal phenomenon that we call content shift. To be specific, there are content differences between features and the input image, such as the exact shape of a certain object. We locate the cause of content shift as one inherent characteristic of diffusion models, which suggests the broad existence of this phenomenon in diffusion feature. Further empirical study also indicates that its negative impact is not negligible even when content shift is not visually perceivable. Hence, we propose to suppress content shift to enhance the overall quality of diffusion features. Specifically, content shift is related to the information drift during the process of recovering an image from the noisy input, pointing out the possibility of turning off-the-shelf generation techniques into tools for content shift suppression. We further propose a practical guideline named GATE to efficiently evaluate the potential benefit of a technique and provide an implementation of our methodology. Despite the simplicity, the proposed approach has achieved superior results on various tasks and datasets, validating its potential as a generic booster for diffusion features. Our code is available at https://github.com/Darkbblue/diffusion-content-shift.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2410.03558"
    },
    {
        "paper id": "2410.06725",
        "abstract url": "https://arxiv.org/abs/2410.06725",
        "title": "Evaluating the Impact of Point Cloud Colorization on Semantic Segmentation Accuracy",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Point cloud semantic segmentation, the process of classifying each point into predefined categories, is essential for 3D scene understanding. While image-based segmentation is widely adopted due to its maturity, methods relying solely on RGB information often suffer from degraded performance due to color inaccuracies. Recent advancements have incorporated additional features such as intensity and geometric information, yet RGB channels continue to negatively impact segmentation accuracy when errors in colorization occur. Despite this, previous studies have not rigorously quantified the effects of erroneous colorization on segmentation performance. In this paper, we propose a novel statistical approach to evaluate the impact of inaccurate RGB information on image-based point cloud segmentation. We categorize RGB inaccuracies into two types: incorrect color information and similar color information. Our results demonstrate that both types of color inaccuracies significantly degrade segmentation accuracy, with similar color errors particularly affecting the extraction of geometric features. These findings highlight the critical need to reassess the role of RGB information in point cloud segmentation and its implications for future algorithm design.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.MM"
        ],
        "comment": "Accepted by 2024 IEEE 8th International Conference on Vision, Image and Signal Processing"
    },
    {
        "paper id": "2410.06735",
        "abstract url": "https://arxiv.org/abs/2410.06735",
        "title": "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?",
        "rating": "0",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Recent large language models (LLMs) have demonstrated remarkable generalization abilities in mathematics and logical reasoning tasks. Prior research indicates that LLMs pre-trained with programming language data exhibit high mathematical and reasoning abilities; however, this causal relationship has not been rigorously tested. Our research aims to verify which programming languages and features during pre-training affect logical inference performance. Specifically, we pre-trained decoder-based language models from scratch using datasets from ten programming languages (e.g., Python, C, Java) and three natural language datasets (Wikipedia, Fineweb, C4) under identical conditions. Thereafter, we evaluated the trained models in a few-shot in-context learning setting on logical reasoning tasks: FLD and bAbi, which do not require commonsense or world knowledge. The results demonstrate that nearly all models trained with programming languages consistently outperform those trained with natural languages, indicating that programming languages contain factors that elicit logic inference performance. In addition, we found that models trained with programming languages exhibit a better ability to follow instructions compared to those trained with natural languages. Further analysis reveals that the depth of Abstract Syntax Trees representing parsed results of programs also affects logical reasoning performance. These findings will offer insights into the essential elements of pre-training for acquiring the foundational abilities of LLMs.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06811",
        "abstract url": "https://arxiv.org/abs/2410.06811",
        "title": "Rethinking the Evaluation of Visible and Infrared Image Fusion",
        "rating": "0",
        "keywords": [
            [
                "Infrared"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Visible and Infrared Image Fusion (VIF) has garnered significant interest across a wide range of high-level vision tasks, such as object detection and semantic segmentation. However, the evaluation of VIF methods remains challenging due to the absence of ground truth. This paper proposes a Segmentation-oriented Evaluation Approach (SEA) to assess VIF methods by incorporating the semantic segmentation task and leveraging segmentation labels available in latest VIF datasets. Specifically, SEA utilizes universal segmentation models, capable of handling diverse images and classes, to predict segmentation outputs from fused images and compare these outputs with segmentation labels. Our evaluation of recent VIF methods using SEA reveals that their performance is comparable or even inferior to using visible images only, despite nearly half of the infrared images demonstrating better performance than visible images. Further analysis indicates that the two metrics most correlated to our SEA are the gradient-based fusion metric $Q_{\\text{ABF}}$ and the visual information fidelity metric $Q_{\\text{VIFF}}$ in conventional VIF evaluation metrics, which can serve as proxies when segmentation labels are unavailable. We hope that our evaluation will guide the development of novel and practical VIF methods. The code has been released in \\url{https://github.com/Yixuan-2002/SEA/}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The code has been released in \\url{https://github.com/Yixuan-2002/SEA/}"
    },
    {
        "paper id": "2410.06841",
        "abstract url": "https://arxiv.org/abs/2410.06841",
        "title": "Boosting Few-Shot Detection with Large Language Models and Layout-to-Image Synthesis",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in diffusion models have enabled a wide range of works exploiting their ability to generate high-volume, high-quality data for use in various downstream tasks. One subclass of such models, dubbed Layout-to-Image Synthesis (LIS), learns to generate images conditioned on a spatial layout (bounding boxes, masks, poses, etc.) and has shown a promising ability to generate realistic images, albeit with limited layout-adherence. Moreover, the question of how to effectively transfer those models for scalable augmentation of few-shot detection data remains unanswered. Thus, we propose a collaborative framework employing a Large Language Model (LLM) and an LIS model for enhancing few-shot detection beyond state-of-the-art generative augmentation approaches. We leverage LLM's reasoning ability to extrapolate the spatial prior of the annotation space by generating new bounding boxes given only a few example annotations. Additionally, we introduce our novel layout-aware CLIP score for sample ranking, enabling tight coupling between generated layouts and images. Significant improvements on COCO few-shot benchmarks are observed. With our approach, a YOLOX-S baseline is boosted by more than 140%, 50%, 35% in mAP on the COCO 5-,10-, and 30-shot settings, respectively.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "This paper has been accepted at the Asian Conference on Computer Vision (ACCV), 2024"
    },
    {
        "paper id": "2410.06846",
        "abstract url": "https://arxiv.org/abs/2410.06846",
        "title": "Joint Fine-tuning and Conversion of Pretrained Speech and Language Models towards Linear Complexity",
        "rating": "0",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Architectures such as Linformer and Mamba have recently emerged as competitive linear time replacements for transformers. However, corresponding large pretrained models are often unavailable, especially in non-text domains. To remedy this, we present a Cross-Architecture Layerwise Distillation (CALD) approach that jointly converts a transformer model to a linear time substitute and fine-tunes it to a target task. We also compare several means to guide the fine-tuning to optimally retain the desired inference capability from the original model. The methods differ in their use of the target model and the trajectory of the parameters. In a series of empirical studies on language processing, language modeling, and speech processing, we show that CALD can effectively recover the result of the original model, and that the guiding strategy contributes to the result. Some reasons for the variation are suggested.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "15 pages, 4 figures"
    },
    {
        "paper id": "2410.06893",
        "abstract url": "https://arxiv.org/abs/2410.06893",
        "title": "Learning from Spatio-temporal Correlation for Semi-Supervised LiDAR Semantic Segmentation",
        "rating": "0",
        "keywords": [
            [
                "LiDAR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We address the challenges of the semi-supervised LiDAR segmentation (SSLS) problem, particularly in low-budget scenarios. The two main issues in low-budget SSLS are the poor-quality pseudo-labels for unlabeled data, and the performance drops due to the significant imbalance between ground-truth and pseudo-labels. This imbalance leads to a vicious training cycle. To overcome these challenges, we leverage the spatio-temporal prior by recognizing the substantial overlap between temporally adjacent LiDAR scans. We propose a proximity-based label estimation, which generates highly accurate pseudo-labels for unlabeled data by utilizing semantic consistency with adjacent labeled data. Additionally, we enhance this method by progressively expanding the pseudo-labels from the nearest unlabeled scans, which helps significantly reduce errors linked to dynamic classes. Additionally, we employ a dual-branch structure to mitigate performance degradation caused by data imbalance. Experimental results demonstrate remarkable performance in low-budget settings (i.e., <= 5%) and meaningful improvements in normal budget settings (i.e., 5 - 50%). Finally, our method has achieved new state-of-the-art results on SemanticKITTI and nuScenes in semi-supervised LiDAR segmentation. With only 5% labeled data, it offers competitive results against fully-supervised counterparts. Moreover, it surpasses the performance of the previous state-of-the-art at 100% labeled data (75.2%) using only 20% of labeled data (76.0%) on nuScenes. The code is available on https://github.com/halbielee/PLE.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06905",
        "abstract url": "https://arxiv.org/abs/2410.06905",
        "title": "Reliable Probabilistic Human Trajectory Prediction for Autonomous Applications",
        "rating": "0",
        "keywords": [
            [
                "Trajectory",
                "vehicle"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Autonomous systems, like vehicles or robots, require reliable, accurate, fast, resource-efficient, scalable, and low-latency trajectory predictions to get initial knowledge about future locations and movements of surrounding objects for safe human-machine interaction. Furthermore, they need to know the uncertainty of the predictions for risk assessment to provide safe path planning. This paper presents a lightweight method to address these requirements, combining Long Short-Term Memory and Mixture Density Networks. Our method predicts probability distributions, including confidence level estimations for positional uncertainty to support subsequent risk management applications and runs on a low-power embedded platform. We discuss essential requirements for human trajectory prediction in autonomous vehicle applications and demonstrate our method's performance using multiple traffic-related datasets. Furthermore, we explain reliability and sharpness metrics and show how important they are to guarantee the correctness and robustness of a model's predictions and uncertainty assessments. These essential evaluations have so far received little attention for no good reason. Our approach focuses entirely on real-world applicability. Verifying prediction uncertainties and a model's reliability are central to autonomous real-world applications. Our framework and code are available at: https://github.com/kav-institute/mdn_trajectory_forecasting.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06977",
        "abstract url": "https://arxiv.org/abs/2410.06977",
        "title": "Adaptive High-Frequency Transformer for Diverse Wildlife Re-Identification",
        "rating": "0",
        "keywords": [
            [
                "Re-Identification"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Wildlife ReID involves utilizing visual technology to identify specific individuals of wild animals in different scenarios, holding significant importance for wildlife conservation, ecological research, and environmental monitoring. Existing wildlife ReID methods are predominantly tailored to specific species, exhibiting limited applicability. Although some approaches leverage extensively studied person ReID techniques, they struggle to address the unique challenges posed by wildlife. Therefore, in this paper, we present a unified, multi-species general framework for wildlife ReID. Given that high-frequency information is a consistent representation of unique features in various species, significantly aiding in identifying contours and details such as fur textures, we propose the Adaptive High-Frequency Transformer model with the goal of enhancing high-frequency information learning. To mitigate the inevitable high-frequency interference in the wilderness environment, we introduce an object-aware high-frequency selection strategy to adaptively capture more valuable high-frequency components. Notably, we unify the experimental settings of multiple wildlife datasets for ReID, achieving superior performance over state-of-the-art ReID methods. In domain generalization scenarios, our approach demonstrates robust generalization to unknown species.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06985",
        "abstract url": "https://arxiv.org/abs/2410.06985",
        "title": "Jointly Generating Multi-view Consistent PBR Textures using Collaborative Control",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Multi-view consistency remains a challenge for image diffusion models. Even within the Text-to-Texture problem, where perfect geometric correspondences are known a priori, many methods fail to yield aligned predictions across views, necessitating non-trivial fusion methods to incorporate the results onto the original mesh. We explore this issue for a Collaborative Control workflow specifically in PBR Text-to-Texture. Collaborative Control directly models PBR image probability distributions, including normal bump maps; to our knowledge, the only diffusion model to directly output full PBR stacks. We discuss the design decisions involved in making this model multi-view consistent, and demonstrate the effectiveness of our approach in ablation studies, as well as practical applications.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "19 pages, 13 figures"
    },
    {
        "paper id": "2410.07025",
        "abstract url": "https://arxiv.org/abs/2410.07025",
        "title": "Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation Models Without Human Feedback",
        "rating": "0",
        "keywords": [
            [
                "vision-language",
                "VLMs"
            ],
            [
                "medical",
                "X-Ray",
                "radiology"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Radiologists play a crucial role by translating medical images into medical reports. However, the field faces staffing shortages and increasing workloads. While automated approaches using vision-language models (VLMs) show promise as assistants, they require exceptionally high accuracy. Most current VLMs in radiology rely solely on supervised fine-tuning (SFT). Meanwhile, in the general domain, additional preference fine-tuning has become standard practice. The challenge in radiology lies in the prohibitive cost of obtaining radiologist feedback. We propose a scalable automated preference alignment technique for VLMs in radiology, focusing on chest X-ray (CXR) report generation. Our method leverages publicly available datasets with an LLM-as-a-Judge mechanism, eliminating the need for additional expert radiologist feedback. We evaluate and benchmark five direct alignment algorithms (DAAs). Our results show up to a 57.4% improvement in average GREEN scores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase in an average across six metrics (domain specific and general), compared to the SFT baseline. We study reward overoptimization via length exploitation, with reports lengthening by up to 3.2x. To assess a potential alignment tax, we benchmark on six additional diverse tasks, finding no significant degradations. A reader study involving four board-certified radiologists indicates win rates of up to 0.62 over the SFT baseline, while significantly penalizing verbosity. Our analysis provides actionable insights for the development of VLMs in high-stakes fields like radiology.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07081",
        "abstract url": "https://arxiv.org/abs/2410.07081",
        "title": "JPEG Inspired Deep Learning",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Although it is traditionally believed that lossy image compression, such as JPEG compression, has a negative impact on the performance of deep neural networks (DNNs), it is shown by recent works that well-crafted JPEG compression can actually improve the performance of deep learning (DL). Inspired by this, we propose JPEG-DL, a novel DL framework that prepends any underlying DNN architecture with a trainable JPEG compression layer. To make the quantization operation in JPEG compression trainable, a new differentiable soft quantizer is employed at the JPEG layer, and then the quantization operation and underlying DNN are jointly trained. Extensive experiments show that in comparison with the standard DL, JPEG-DL delivers significant accuracy improvements across various datasets and model architectures while enhancing robustness against adversarial attacks. Particularly, on some fine-grained image classification datasets, JPEG-DL can increase prediction accuracy by as much as 20.9%. Our code is available on https://github.com/JpegInspiredDl/JPEG-Inspired-DL.git.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07119",
        "abstract url": "https://arxiv.org/abs/2410.07119",
        "title": "Thing2Reality: Transforming 2D Content into Conditioned Multiviews and 3D Gaussian Objects for XR Communication",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "During remote communication, participants often share both digital and physical content, such as product designs, digital assets, and environments, to enhance mutual understanding. Recent advances in augmented communication have facilitated users to swiftly create and share digital 2D copies of physical objects from video feeds into a shared space. However, conventional 2D representations of digital objects restricts users' ability to spatially reference items in a shared immersive environment. To address this, we propose Thing2Reality, an Extended Reality (XR) communication platform that enhances spontaneous discussions of both digital and physical items during remote sessions. With Thing2Reality, users can quickly materialize ideas or physical objects in immersive environments and share them as conditioned multiview renderings or 3D Gaussians. Thing2Reality enables users to interact with remote objects or discuss concepts in a collaborative manner. Our user study revealed that the ability to interact with and manipulate 3D representations of objects significantly enhances the efficiency of discussions, with the potential to augment discussion of 2D artifacts.",
        "subjects": [
            "cs.HC",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "18 pages (15 pages without references), 13 figures"
    },
    {
        "paper id": "2410.07157",
        "abstract url": "https://arxiv.org/abs/2410.07157",
        "title": "InstructG2I: Synthesizing Images from Multimodal Attributed Graphs",
        "rating": "0",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "diffusion"
            ],
            [
                "Graphs"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.SI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we approach an overlooked yet critical task Graph2Image: generating images from multimodal attributed graphs (MMAGs). This task poses significant challenges due to the explosion in graph size, dependencies among graph entities, and the need for controllability in graph conditions. To address these challenges, we propose a graph context-conditioned diffusion model called InstructG2I. InstructG2I first exploits the graph structure and multimodal information to conduct informative neighbor sampling by combining personalized page rank and re-ranking based on vision-language features. Then, a Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliary set of graph prompts to guide the denoising process of diffusion. Finally, we propose graph classifier-free guidance, enabling controllable generation by varying the strength of graph guidance and multiple connected edges to a node. Extensive experiments conducted on three datasets from different domains demonstrate the effectiveness and controllability of our approach. The code is available at https://github.com/PeterGriffinJin/InstructG2I.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG",
            "cs.SI"
        ],
        "comment": "16 pages"
    },
    {
        "paper id": "2410.07164",
        "abstract url": "https://arxiv.org/abs/2410.07164",
        "title": "AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in diffusion models have led to significant improvements in the generation and animation of 4D full-body human-object interactions (HOI). Nevertheless, existing methods primarily focus on SMPL-based motion generation, which is limited by the scarcity of realistic large-scale interaction data. This constraint affects their ability to create everyday HOI scenes. This paper addresses this challenge using a zero-shot approach with a pre-trained diffusion model. Despite this potential, achieving our goals is difficult due to the diffusion model's lack of understanding of ''where'' and ''how'' objects interact with the human body. To tackle these issues, we introduce AvatarGO, a novel framework designed to generate animatable 4D HOI scenes directly from textual inputs. Specifically, 1) for the ''where'' challenge, we propose LLM-guided contact retargeting, which employs Lang-SAM to identify the contact body part from text prompts, ensuring precise representation of human-object spatial relations. 2) For the ''how'' challenge, we introduce correspondence-aware motion optimization that constructs motion fields for both human and object models using the linear blend skinning function from SMPL-X. Our framework not only generates coherent compositional motions, but also exhibits greater robustness in handling penetration issues. Extensive experiments with existing methods validate AvatarGO's superior generation and animation capabilities on a variety of human-object pairs and diverse poses. As the first attempt to synthesize 4D avatars with object interactions, we hope AvatarGO could open new doors for human-centric 4D content creation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://yukangcao.github.io/AvatarGO/"
    },
    {
        "paper id": "2410.07171",
        "abstract url": "https://arxiv.org/abs/2410.07171",
        "title": "IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation",
        "rating": "0",
        "keywords": [
            [
                "diffusion",
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relationships. This disparity highlights the need for an approach that can leverage the complementary strengths of various models to comprehensively improve the composition capability. To this end, we introduce IterComp, a novel framework that aggregates composition-aware model preferences from multiple models and employs an iterative feedback learning approach to enhance compositional generation. Specifically, we curate a gallery of six powerful open-source diffusion models and evaluate their three key compositional metrics: attribute binding, spatial relationships, and non-spatial relationships. Based on these metrics, we develop a composition-aware model preference dataset comprising numerous image-rank pairs to train composition-aware reward models. Then, we propose an iterative feedback learning method to enhance compositionality in a closed-loop manner, enabling the progressive self-refinement of both the base diffusion model and reward models over multiple iterations. Theoretical proof demonstrates the effectiveness and extensive experiments show our significant superiority over previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category object composition and complex semantic alignment. IterComp opens new research avenues in reward feedback learning for diffusion models and compositional generation. Code: https://github.com/YangLing0818/IterComp",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project: https://github.com/YangLing0818/IterComp"
    },
    {
        "paper id": "2410.07286",
        "abstract url": "https://arxiv.org/abs/2410.07286",
        "title": "Benchmarking Data Heterogeneity Evaluation Approaches for Personalized Federated Learning",
        "rating": "0",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "There is growing research interest in measuring the statistical heterogeneity of clients' local datasets. Such measurements are used to estimate the suitability for collaborative training of personalized federated learning (PFL) models. Currently, these research endeavors are taking place in silos and there is a lack of a unified benchmark to provide a fair and convenient comparison among various approaches in common settings. We aim to bridge this important gap in this paper. The proposed benchmarking framework currently includes six representative approaches. Extensive experiments have been conducted to compare these approaches under five standard non-IID FL settings, providing much needed insights into which approaches are advantageous under which settings. The proposed framework offers useful guidance on the suitability of various data divergence measures in FL systems. It is beneficial for keeping related research activities on the right track in terms of: (1) designing PFL schemes, (2) selecting appropriate data heterogeneity evaluation approaches for specific FL application scenarios, and (3) addressing fairness issues in collaborative model training. The code is available at https://github.com/Xiaoni-61/DH-Benchmark.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Accepted to FL@FM-NeurIPS'24"
    },
    {
        "paper id": "2410.07298",
        "abstract url": "https://arxiv.org/abs/2410.07298",
        "title": "Enhancing Performance of Point Cloud Completion Networks with Consistency Loss",
        "rating": "0",
        "keywords": [
            [
                "Point Cloud"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Point cloud completion networks are conventionally trained to minimize the disparities between the completed point cloud and the ground-truth counterpart. However, an incomplete object-level point cloud can have multiple valid completion solutions when it is examined in isolation. This one-to-many mapping issue can cause contradictory supervision signals to the network because the loss function may produce different values for identical input-output pairs of the network. In many cases, this issue could adversely affect the network optimization process. In this work, we propose to enhance the conventional learning objective using a novel completion consistency loss to mitigate the one-to-many mapping problem. Specifically, the proposed consistency loss ensure that a point cloud completion network generates a coherent completion solution for incomplete objects originating from the same source point cloud. Experimental results across multiple well-established datasets and benchmarks demonstrated the proposed completion consistency loss have excellent capability to enhance the completion performance of various existing networks without any modification to the design of the networks. The proposed consistency loss enhances the performance of the point completion network without affecting the inference speed, thereby increasing the accuracy of point cloud completion. Notably, a state-of-the-art point completion network trained with the proposed consistency loss can achieve state-of-the-art accuracy on the challenging new MVP dataset. The code and result of experiment various point completion models using proposed consistency loss will be available at: https://github.com/kaist-avelab/ConsistencyLoss .",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "First version of Paper \"Enhancing Performance of Point Cloud Completion Networks with Consistency Loss\" by Kevin Tirta Wijaya and Christofel Rio Goenawan. In process submission to Neurocomputing Journal 2024"
    },
    {
        "paper id": "2410.07303",
        "abstract url": "https://arxiv.org/abs/2410.07303",
        "title": "Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion models have greatly improved visual generation but are hindered by slow generation speed due to the computationally intensive nature of solving generative ODEs. Rectified flow, a widely recognized solution, improves generation speed by straightening the ODE path. Its key components include: 1) using the diffusion form of flow-matching, 2) employing $\\boldsymbol v$-prediction, and 3) performing rectification (a.k.a. reflow). In this paper, we argue that the success of rectification primarily lies in using a pretrained diffusion model to obtain matched pairs of noise and samples, followed by retraining with these matched noise-sample pairs. Based on this, components 1) and 2) are unnecessary. Furthermore, we highlight that straightness is not an essential training target for rectification; rather, it is a specific case of flow-matching models. The more critical training target is to achieve a first-order approximate ODE path, which is inherently curved for models like DDPM and Sub-VP. Building on this insight, we propose Rectified Diffusion, which generalizes the design space and application scope of rectification to encompass the broader category of diffusion models, rather than being restricted to flow-matching models. We validate our method on Stable Diffusion v1-5 and Stable Diffusion XL. Our method not only greatly simplifies the training procedure of rectified flow-based previous works (e.g., InstaFlow) but also achieves superior performance with even lower training cost. Our code is available at https://github.com/G-U-N/Rectified-Diffusion.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07379",
        "abstract url": "https://arxiv.org/abs/2410.07379",
        "title": "Learn from Real: Reality Defender's Submission to ASVspoof5 Challenge",
        "rating": "0",
        "keywords": [
            [
                "deepfake"
            ],
            [
                "cs.AI",
                "cs.CL",
                "eess.AS"
            ]
        ],
        "abstract": "Audio deepfake detection is crucial to combat the malicious use of AI-synthesized speech. Among many efforts undertaken by the community, the ASVspoof challenge has become one of the benchmarks to evaluate the generalizability and robustness of detection models. In this paper, we present Reality Defender's submission to the ASVspoof5 challenge, highlighting a novel pretraining strategy which significantly improves generalizability while maintaining low computational cost during training. Our system SLIM learns the style-linguistics dependency embeddings from various types of bonafide speech using self-supervised contrastive learning. The learned embeddings help to discriminate spoof from bonafide speech by focusing on the relationship between the style and linguistics aspects. We evaluated our system on ASVspoof5, ASV2019, and In-the-wild. Our submission achieved minDCF of 0.1499 and EER of 5.5% on ASVspoof5 Track 1, and EER of 7.4% and 10.8% on ASV2019 and In-the-wild respectively.",
        "subjects": [
            "eess.AS",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "Accepted into ASVspoof5 workshop"
    },
    {
        "paper id": "2410.07401",
        "abstract url": "https://arxiv.org/abs/2410.07401",
        "title": "Enhancing Soccer Camera Calibration Through Keypoint Exploitation",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Accurate camera calibration is essential for transforming 2D images from camera sensors into 3D world coordinates, enabling precise scene geometry interpretation and supporting sports analytics tasks such as player tracking, offside detection, and performance analysis. However, obtaining a sufficient number of high-quality point pairs remains a significant challenge for both traditional and deep learning-based calibration methods. This paper introduces a multi-stage pipeline that addresses this challenge by leveraging the structural features of the football pitch. Our approach significantly increases the number of usable points for calibration by exploiting line-line and line-conic intersections, points on the conics, and other geometric features. To mitigate the impact of imperfect annotations, we employ data fitting techniques. Our pipeline utilizes deep learning for keypoint and line detection and incorporates geometric constraints based on real-world pitch dimensions. A voter algorithm iteratively selects the most reliable keypoints, further enhancing calibration accuracy. We evaluated our approach on the largest football broadcast camera calibration dataset available, and secured the top position in the SoccerNet Camera Calibration Challenge 2023 [arXiv:2309.06006], which demonstrates the effectiveness of our method in real-world scenarios. The project code is available at https://github.com/NikolasEnt/soccernet-calibration-sportlight .",
        "subjects": [
            "cs.CV"
        ],
        "comment": "7th ACM International Workshop on Multimedia Content Analysis in Sports"
    },
    {
        "paper id": "2410.07436",
        "abstract url": "https://arxiv.org/abs/2410.07436",
        "title": "Toward Robust Real-World Audio Deepfake Detection: Closing the Explainability Gap",
        "rating": "0",
        "keywords": [
            [
                "Deepfake"
            ],
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "The rapid proliferation of AI-manipulated or generated audio deepfakes poses serious challenges to media integrity and election security. Current AI-driven detection solutions lack explainability and underperform in real-world settings. In this paper, we introduce novel explainability methods for state-of-the-art transformer-based audio deepfake detectors and open-source a novel benchmark for real-world generalizability. By narrowing the explainability gap between transformer-based audio deepfake detectors and traditional methods, our results not only build trust with human experts, but also pave the way for unlocking the potential of citizen intelligence to overcome the scalability issue in audio deepfake detection.",
        "subjects": [
            "cs.LG",
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07437",
        "abstract url": "https://arxiv.org/abs/2410.07437",
        "title": "Robust infrared small target detection using self-supervised and a contrario paradigms",
        "rating": "0",
        "keywords": [
            [
                "infrared"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Detecting small targets in infrared images poses significant challenges in defense applications due to the presence of complex backgrounds and the small size of the targets. Traditional object detection methods often struggle to balance high detection rates with low false alarm rates, especially when dealing with small objects. In this paper, we introduce a novel approach that combines a contrario paradigm with Self-Supervised Learning (SSL) to improve Infrared Small Target Detection (IRSTD). On the one hand, the integration of an a contrario criterion into a YOLO detection head enhances feature map responses for small and unexpected objects while effectively controlling false alarms. On the other hand, we explore SSL techniques to overcome the challenges of limited annotated data, common in IRSTD tasks. Specifically, we benchmark several representative SSL strategies for their effectiveness in improving small object detection performance. Our findings show that instance discrimination methods outperform masked image modeling strategies when applied to YOLO-based small object detection. Moreover, the combination of the a contrario and SSL paradigms leads to significant performance improvements, narrowing the gap with state-of-the-art segmentation methods and even outperforming them in frugal settings. This two-pronged approach offers a robust solution for improving IRSTD performance, particularly under challenging conditions.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07499",
        "abstract url": "https://arxiv.org/abs/2410.07499",
        "title": "Dense Optimizer : An Information Entropy-Guided Structural Search Method for Dense-like Neural Network Design",
        "rating": "0",
        "keywords": [
            [
                "architecture search"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Dense Convolutional Network has been continuously refined to adopt a highly efficient and compact architecture, owing to its lightweight and efficient structure. However, the current Dense-like architectures are mainly designed manually, it becomes increasingly difficult to adjust the channels and reuse level based on past experience. As such, we propose an architecture search method called Dense Optimizer that can search high-performance dense-like network automatically. In Dense Optimizer, we view the dense network as a hierarchical information system, maximize the network's information entropy while constraining the distribution of the entropy across each stage via a power law, thereby constructing an optimization problem. We also propose a branch-and-bound optimization algorithm, tightly integrates power-law principle with search space scaling to solve the optimization problem efficiently. The superiority of Dense Optimizer has been validated on different computer vision benchmark datasets. Specifically, Dense Optimizer completes high-quality search but only costs 4 hours with one CPU. Our searched model DenseNet-OPT achieved a top 1 accuracy of 84.3% on CIFAR-100, which is 5.97% higher than the original one.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "7 pages,3 figures"
    },
    {
        "paper id": "2410.07536",
        "abstract url": "https://arxiv.org/abs/2410.07536",
        "title": "I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow Transformers with Projected Flow",
        "rating": "0",
        "keywords": [
            [
                "diffusion",
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Rectified Flow Transformers (RFTs) offer superior training and inference efficiency, making them likely the most viable direction for scaling up diffusion models. However, progress in generation resolution has been relatively slow due to data quality and training costs. Tuning-free resolution extrapolation presents an alternative, but current methods often reduce generative stability, limiting practical application. In this paper, we review existing resolution extrapolation methods and introduce the I-Max framework to maximize the resolution potential of Text-to-Image RFTs. I-Max features: (i) a novel Projected Flow strategy for stable extrapolation and (ii) an advanced inference toolkit for generalizing model knowledge to higher resolutions. Experiments with Lumina-Next-2K and Flux.1-dev demonstrate I-Max's ability to enhance stability in resolution extrapolation and show that it can bring image detail emergence and artifact correction, confirming the practical value of tuning-free resolution extrapolation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07545",
        "abstract url": "https://arxiv.org/abs/2410.07545",
        "title": "Calibration of 3D Single-pixel Imaging Systems with a Calibration Field",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "3D single-pixel imaging (SPI) is a promising imaging technique that can be ffexibly applied to various wavebands. The main challenge in 3D SPI is that the calibration usually requires a large number of standard points as references, which are tricky to capture using single-pixel detectors. Conventional solutions involve sophisticated device deployment and cumbersome operations, resulting in hundreds of images needed for calibration. In our work, we construct a Calibration Field (CaliF) to efffciently generate the standard points from one single image. A high accuracy of the CaliF is guaranteed by the technique of deep learning and digital twin. We perform experiments with our new method to verify its validity and accuracy. We believe our work holds great potential in 3D SPI systems or even general imaging systems.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07551",
        "abstract url": "https://arxiv.org/abs/2410.07551",
        "title": "KRAG Framework for Enhancing LLMs in the Legal Domain",
        "rating": "0",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces Knowledge Representation Augmented Generation (KRAG), a novel framework designed to enhance the capabilities of Large Language Models (LLMs) within domain-specific applications. KRAG points to the strategic inclusion of critical knowledge entities and relationships that are typically absent in standard data sets and which LLMs do not inherently learn. In the context of legal applications, we present Soft PROLEG, an implementation model under KRAG, which uses inference graphs to aid LLMs in delivering structured legal reasoning, argumentation, and explanations tailored to user inquiries. The integration of KRAG, either as a standalone framework or in tandem with retrieval augmented generation (RAG), markedly improves the ability of language models to navigate and solve the intricate challenges posed by legal texts and terminologies. This paper details KRAG's methodology, its implementation through Soft PROLEG, and potential broader applications, underscoring its significant role in advancing natural language understanding and processing in specialized knowledge domains.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)"
    },
    {
        "paper id": "2410.07567",
        "abstract url": "https://arxiv.org/abs/2410.07567",
        "title": "When and Where Did it Happen? An Encoder-Decoder Model to Identify Scenario Context",
        "rating": "0",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "We introduce a neural architecture finetuned for the task of scenario context generation: The relevant location and time of an event or entity mentioned in text. Contextualizing information extraction helps to scope the validity of automated finings when aggregating them as knowledge graphs. Our approach uses a high-quality curated dataset of time and location annotations in a corpus of epidemiology papers to train an encoder-decoder architecture. We also explored the use of data augmentation techniques during training. Our findings suggest that a relatively small fine-tuned encoder-decoder model performs better than out-of-the-box LLMs and semantic role labeling parsers to accurate predict the relevant scenario information of a particular entity or event.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "9 pages, 7 figures"
    },
    {
        "paper id": "2410.07582",
        "abstract url": "https://arxiv.org/abs/2410.07582",
        "title": "Detecting Training Data of Large Language Models via Expectation Maximization",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The widespread deployment of large language models (LLMs) has led to impressive advancements, yet information about their training data, a critical factor in their performance, remains undisclosed. Membership inference attacks (MIAs) aim to determine whether a specific instance was part of a target model's training data. MIAs can offer insights into LLM outputs and help detect and address concerns such as data contamination and compliance with privacy and copyright standards. However, applying MIAs to LLMs presents unique challenges due to the massive scale of pre-training data and the ambiguous nature of membership. Additionally, creating appropriate benchmarks to evaluate MIA methods is not straightforward, as training and test data distributions are often unknown. In this paper, we introduce EM-MIA, a novel MIA method for LLMs that iteratively refines membership scores and prefix scores via an expectation-maximization algorithm, leveraging the duality that the estimates of these scores can be improved by each other. Membership scores and prefix scores assess how each instance is likely to be a member and discriminative as a prefix, respectively. Our method achieves state-of-the-art results on the WikiMIA dataset. To further evaluate EM-MIA, we present OLMoMIA, a benchmark built from OLMo resources, which allows us to control the difficulty of MIA tasks with varying degrees of overlap between training and test data distributions. We believe that EM-MIA serves as a robust MIA method for LLMs and that OLMoMIA provides a valuable resource for comprehensively evaluating MIA approaches, thereby driving future research in this critical area.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CR",
            "cs.LG"
        ],
        "comment": "14 pages"
    },
    {
        "paper id": "2410.07597",
        "abstract url": "https://arxiv.org/abs/2410.07597",
        "title": "Fine-detailed Neural Indoor Scene Reconstruction using multi-level importance sampling and multi-view consistency",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently, neural implicit 3D reconstruction in indoor scenarios has become popular due to its simplicity and impressive performance. Previous works could produce complete results leveraging monocular priors of normal or depth. However, they may suffer from over-smoothed reconstructions and long-time optimization due to unbiased sampling and inaccurate monocular priors. In this paper, we propose a novel neural implicit surface reconstruction method, named FD-NeuS, to learn fine-detailed 3D models using multi-level importance sampling strategy and multi-view consistency methodology. Specifically, we leverage segmentation priors to guide region-based ray sampling, and use piecewise exponential functions as weights to pilot 3D points sampling along the rays, ensuring more attention on important regions. In addition, we introduce multi-view feature consistency and multi-view normal consistency as supervision and uncertainty respectively, which further improve the reconstruction of details. Extensive quantitative and qualitative results show that FD-NeuS outperforms existing methods in various scenes.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "7 pages, 3 figures, International Conference on Image Processing"
    },
    {
        "paper id": "2410.07600",
        "abstract url": "https://arxiv.org/abs/2410.07600",
        "title": "RNA: Video Editing with ROI-based Neural Atlas",
        "rating": "0",
        "keywords": [
            [
                "Video Editing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the recent growth of video-based Social Network Service (SNS) platforms, the demand for video editing among common users has increased. However, video editing can be challenging due to the temporally-varying factors such as camera movement and moving objects. While modern atlas-based video editing methods have addressed these issues, they often fail to edit videos including complex motion or multiple moving objects, and demand excessive computational cost, even for very simple edits. In this paper, we propose a novel region-of-interest (ROI)-based video editing framework: ROI-based Neural Atlas (RNA). Unlike prior work, RNA allows users to specify editing regions, simplifying the editing process by removing the need for foreground separation and atlas modeling for foreground objects. However, this simplification presents a unique challenge: acquiring a mask that effectively handles occlusions in the edited area caused by moving objects, without relying on an additional segmentation model. To tackle this, we propose a novel mask refinement approach designed for this specific challenge. Moreover, we introduce a soft neural atlas model for video reconstruction to ensure high-quality editing results. Extensive experiments show that RNA offers a more practical and efficient editing solution, applicable to a wider range of videos with superior quality compared to prior methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "ACCV2024"
    },
    {
        "paper id": "2410.07610",
        "abstract url": "https://arxiv.org/abs/2410.07610",
        "title": "CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features",
        "rating": "0",
        "keywords": [
            [
                "lidar"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Multimodal encoders like CLIP excel in tasks such as zero-shot image classification and cross-modal retrieval. However, they require excessive training data. We propose canonical similarity analysis (CSA), which uses two unimodal encoders to replicate multimodal encoders using limited data. CSA maps unimodal features into a multimodal space, using a new similarity score to retain only the multimodal information. CSA only involves the inference of unimodal encoders and a cubic-complexity matrix decomposition, eliminating the need for extensive GPU-based model training. Experiments show that CSA outperforms CLIP while requiring $300,000\\times$ fewer multimodal data pairs and $6\\times$ fewer unimodal data for ImageNet classification and misinformative news captions detection. CSA surpasses the state-of-the-art method to map unimodal features to multimodal features. We also demonstrate the ability of CSA with modalities beyond image and text, paving the way for future modality pairs with limited paired multimodal data but abundant unpaired unimodal data, such as lidar and text.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06648",
        "abstract url": "https://arxiv.org/abs/2410.06648",
        "title": "Q-WSL:Leveraging Dynamic Programming for Weighted Supervised Learning in Goal-conditioned RL",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "A novel class of advanced algorithms, termed Goal-Conditioned Weighted Supervised Learning (GCWSL), has recently emerged to tackle the challenges posed by sparse rewards in goal-conditioned reinforcement learning (RL). GCWSL consistently delivers strong performance across a diverse set of goal-reaching tasks due to its simplicity, effectiveness, and stability. However, GCWSL methods lack a crucial capability known as trajectory stitching, which is essential for learning optimal policies when faced with unseen skills during testing. This limitation becomes particularly pronounced when the replay buffer is predominantly filled with sub-optimal trajectories. In contrast, traditional TD-based RL methods, such as Q-learning, which utilize Dynamic Programming, do not face this issue but often experience instability due to the inherent difficulties in value function approximation. In this paper, we propose Q-learning Weighted Supervised Learning (Q-WSL), a novel framework designed to overcome the limitations of GCWSL by incorporating the strengths of Dynamic Programming found in Q-learning. Q-WSL leverages Dynamic Programming results to output the optimal action of (state, goal) pairs across different trajectories within the replay buffer. This approach synergizes the strengths of both Q-learning and GCWSL, effectively mitigating their respective weaknesses and enhancing overall performance. Empirical evaluations on challenging goal-reaching tasks demonstrate that Q-WSL surpasses other goal-conditioned approaches in terms of both performance and sample efficiency. Additionally, Q-WSL exhibits notable robustness in environments characterized by binary reward structures and environmental stochasticity.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06675",
        "abstract url": "https://arxiv.org/abs/2410.06675",
        "title": "SCOREQ: Speech Quality Assessment with Contrastive Regression",
        "rating": "-0.5",
        "keywords": [
            [
                "Quality Assessment"
            ],
            [
                "cs.SD",
                "eess.AS"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In this paper, we present SCOREQ, a novel approach for speech quality prediction. SCOREQ is a triplet loss function for contrastive regression that addresses the domain generalisation shortcoming exhibited by state of the art no-reference speech quality metrics. In the paper we: (i) illustrate the problem of L2 loss training failing at capturing the continuous nature of the mean opinion score (MOS) labels; (ii) demonstrate the lack of generalisation through a benchmarking evaluation across several speech domains; (iii) outline our approach and explore the impact of the architectural design decisions through incremental evaluation; (iv) evaluate the final model against state of the art models for a wide variety of data and domains. The results show that the lack of generalisation observed in state of the art speech quality metrics is addressed by SCOREQ. We conclude that using a triplet loss function for contrastive regression improves generalisation for speech quality prediction models but also has potential utility across a wide range of applications using regression-based predictive models.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": "Accepted NeurIPS 2024"
    },
    {
        "paper id": "2410.06703",
        "abstract url": "https://arxiv.org/abs/2410.06703",
        "title": "ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness in Web Agents",
        "rating": "-0.5",
        "keywords": [
            [
                "navigation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Recent advancements in LLM-based web agents have introduced novel architectures and benchmarks showcasing progress in autonomous web navigation and interaction. However, most existing benchmarks prioritize effectiveness and accuracy, overlooking crucial factors like safety and trustworthiness which are essential for deploying web agents in enterprise settings. The risks of unsafe web agent behavior, such as accidentally deleting user accounts or performing unintended actions in critical business operations, pose significant barriers to widespread adoption. In this paper, we present ST-WebAgentBench, a new online benchmark specifically designed to evaluate the safety and trustworthiness of web agents in enterprise contexts. This benchmark is grounded in a detailed framework that defines safe and trustworthy (ST) agent behavior, outlines how ST policies should be structured and introduces the Completion under Policies metric to assess agent performance. Our evaluation reveals that current SOTA agents struggle with policy adherence and cannot yet be relied upon for critical business applications. Additionally, we propose architectural principles aimed at improving policy awareness and compliance in web agents. We open-source this benchmark and invite the community to contribute, with the goal of fostering a new generation of safer, more trustworthy AI agents. All code, data, environment reproduction resources, and video demonstrations are available at https://sites.google.com/view/st-webagentbench/home.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06742",
        "abstract url": "https://arxiv.org/abs/2410.06742",
        "title": "Inference over Unseen Entities, Relations and Literals on Knowledge Graphs",
        "rating": "-0.5",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In recent years, knowledge graph embedding models have been successfully applied in the transductive setting to tackle various challenging tasks including link prediction, and query answering. Yet, the transductive setting does not allow for reasoning over unseen entities, relations, let alone numerical or non-numerical literals. Although increasing efforts are put into exploring inductive scenarios, inference over unseen entities, relations, and literals has yet to come. This limitation prohibits the existing methods from handling real-world dynamic knowledge graphs involving heterogeneous information about the world. Here, we propose a remedy to this limitation. We propose the attentive byte-pair encoding layer (BytE) to construct a triple embedding from a sequence of byte-pair encoded subword units of entities and relations. Compared to the conventional setting, BytE leads to massive feature reuse via weight tying, since it forces a knowledge graph embedding model to learn embeddings for subword units instead of entities and relations directly. Consequently, the size of the embedding matrices are not anymore bound to the unique number of entities and relations of a knowledge graph. Experimental results show that BytE improves the link prediction performance of 4 knowledge graph embedding models on datasets where the syntactic representations of triples are semantically meaningful. However, benefits of training a knowledge graph embedding model with BytE dissipate on knowledge graphs where entities and relations are represented with plain numbers or URIs. We provide an open source implementation of BytE to foster reproducible research.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "8 pages, 4 figures, ECAI 2024 Workshops (CompAI)"
    },
    {
        "paper id": "2410.06756",
        "abstract url": "https://arxiv.org/abs/2410.06756",
        "title": "DreamMesh4D: Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh Hybrid Representation",
        "rating": "-0.5",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "NeRF",
                "radiance fields"
            ],
            [
                "graph"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Recent advancements in 2D/3D generative techniques have facilitated the generation of dynamic 3D objects from monocular videos. Previous methods mainly rely on the implicit neural radiance fields (NeRF) or explicit Gaussian Splatting as the underlying representation, and struggle to achieve satisfactory spatial-temporal consistency and surface appearance. Drawing inspiration from modern 3D animation pipelines, we introduce DreamMesh4D, a novel framework combining mesh representation with geometric skinning technique to generate high-quality 4D object from a monocular video. Instead of utilizing classical texture map for appearance, we bind Gaussian splats to triangle face of mesh for differentiable optimization of both the texture and mesh vertices. In particular, DreamMesh4D begins with a coarse mesh obtained through an image-to-3D generation procedure. Sparse points are then uniformly sampled across the mesh surface, and are used to build a deformation graph to drive the motion of the 3D object for the sake of computational efficiency and providing additional constraint. For each step, transformations of sparse control points are predicted using a deformation network, and the mesh vertices as well as the surface Gaussians are deformed via a novel geometric skinning algorithm, which is a hybrid approach combining LBS (linear blending skinning) and DQS (dual-quaternion skinning), mitigating drawbacks associated with both approaches. The static surface Gaussians and mesh vertices as well as the deformation network are learned via reference view photometric loss, score distillation loss as well as other regularizers in a two-stage manner. Extensive experiments demonstrate superior performance of our method. Furthermore, our method is compatible with modern graphic pipelines, showcasing its potential in the 3D gaming and film industry.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "NeurIPS 2024"
    },
    {
        "paper id": "2410.06814",
        "abstract url": "https://arxiv.org/abs/2410.06814",
        "title": "Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning",
        "rating": "-0.5",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Over-parameterized models are typically vulnerable to membership inference attacks, which aim to determine whether a specific sample is included in the training of a given model. Previous Weight regularizations (e.g., L1 regularization) typically impose uniform penalties on all parameters, leading to a suboptimal tradeoff between model utility and privacy. In this work, we first show that only a small fraction of parameters substantially impact the privacy risk. In light of this, we propose Privacy-aware Sparsity Tuning (PAST), a simple fix to the L1 Regularization, by employing adaptive penalties to different parameters. Our key idea behind PAST is to promote sparsity in parameters that significantly contribute to privacy leakage. In particular, we construct the adaptive weight for each parameter based on its privacy sensitivity, i.e., the gradient of the loss gap with respect to the parameter. Using PAST, the network shrinks the loss gap between members and non-members, leading to strong resistance to privacy attacks. Extensive experiments demonstrate the superiority of PAST, achieving a state-of-the-art balance in the privacy-utility trade-off.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06851",
        "abstract url": "https://arxiv.org/abs/2410.06851",
        "title": "Understanding Model Ensemble in Transferable Adversarial Attack",
        "rating": "-0.5",
        "keywords": [
            [
                "Attack"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Model ensemble adversarial attack has become a powerful method for generating transferable adversarial examples that can target even unknown models, but its theoretical foundation remains underexplored. To address this gap, we provide early theoretical insights that serve as a roadmap for advancing model ensemble adversarial attack. We first define transferability error to measure the error in adversarial transferability, alongside concepts of diversity and empirical model ensemble Rademacher complexity. We then decompose the transferability error into vulnerability, diversity, and a constant, which rigidly explains the origin of transferability error in model ensemble attack: the vulnerability of an adversarial example to ensemble components, and the diversity of ensemble components. Furthermore, we apply the latest mathematical tools in information theory to bound the transferability error using complexity and generalization terms, contributing to three practical guidelines for reducing transferability error: (1) incorporating more surrogate models, (2) increasing their diversity, and (3) reducing their complexity in cases of overfitting. Finally, extensive experiments with 54 models validate our theoretical framework, representing a significant step forward in understanding transferable model ensemble adversarial attacks.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06878",
        "abstract url": "https://arxiv.org/abs/2410.06878",
        "title": "Noise is All You Need: Private Second-Order Convergence of Noisy SGD",
        "rating": "-0.5",
        "keywords": [
            [
                "unlearning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Private optimization is a topic of major interest in machine learning, with differentially private stochastic gradient descent (DP-SGD) playing a key role in both theory and practice. Furthermore, DP-SGD is known to be a powerful tool in contexts beyond privacy, including robustness, machine unlearning, etc. Existing analyses of DP-SGD either make relatively strong assumptions (e.g., Lipschitz continuity of the loss function, or even convexity) or prove only first-order convergence (and thus might end at a saddle point in the non-convex setting). At the same time, there has been progress in proving second-order convergence of the non-private version of ``noisy SGD'', as well as progress in designing algorithms that are more complex than DP-SGD and do guarantee second-order convergence. We revisit DP-SGD and show that ``noise is all you need'': the noise necessary for privacy already implies second-order convergence under the standard smoothness assumptions, even for non-Lipschitz loss functions. Hence, we get second-order convergence essentially for free: DP-SGD, the workhorse of modern private optimization, under minimal assumptions can be used to find a second-order stationary point.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "30 pages"
    },
    {
        "paper id": "2410.06883",
        "abstract url": "https://arxiv.org/abs/2410.06883",
        "title": "Degree Distribution based Spiking Graph Networks for Domain Adaptation",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Spiking Graph Networks (SGNs) have garnered significant attraction from both researchers and industry due to their ability to address energy consumption challenges in graph classification. However, SGNs are only effective for in-distribution data and cannot tackle out-of-distribution data. In this paper, we first propose the domain adaptation problem in SGNs, and introduce a novel framework named Degree-aware Spiking Graph Domain Adaptation for Classification. The proposed DeSGDA addresses the spiking graph domain adaptation problem by three aspects: node degree-aware personalized spiking representation, adversarial feature distribution alignment, and pseudo-label distillation. First, we introduce the personalized spiking representation method for generating degree-dependent spiking signals. Specifically, the threshold of triggering a spike is determined by the node degree, allowing this personalized approach to capture more expressive information for classification. Then, we propose the graph feature distribution alignment module that is adversarially trained using membrane potential against a domain discriminator. Such an alignment module can efficiently maintain high performance and low energy consumption in the case of inconsistent distribution. Additionally, we extract consistent predictions across two spaces to create reliable pseudo-labels, effectively leveraging unlabeled data to enhance graph classification performance. Extensive experiments on benchmark datasets validate the superiority of the proposed DeSGDA compared with competitive baselines.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06895",
        "abstract url": "https://arxiv.org/abs/2410.06895",
        "title": "Average Certified Radius is a Poor Metric for Randomized Smoothing",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Randomized smoothing is a popular approach for providing certified robustness guarantees against adversarial attacks, and has become a very active area of research. Over the past years, the average certified radius (ACR) has emerged as the single most important metric for comparing methods and tracking progress in the field. However, in this work, we show that ACR is an exceptionally poor metric for evaluating robustness guarantees provided by randomized smoothing. We theoretically show not only that a trivial classifier can have arbitrarily large ACR, but also that ACR is much more sensitive to improvements on easy samples than on hard ones. Empirically, we confirm that existing training strategies that improve ACR reduce the model's robustness on hard samples. Further, we show that by focusing on easy samples, we can effectively replicate the increase in ACR. We develop strategies, including explicitly discarding hard samples, reweighing the dataset with certified radius, and extreme optimization for easy samples, to achieve state-of-the-art ACR, although these strategies ignore robustness for the general data distribution. Overall, our results suggest that ACR has introduced a strong undesired bias to the field, and better metrics are required to holistically evaluate randomized smoothing.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06950",
        "abstract url": "https://arxiv.org/abs/2410.06950",
        "title": "Faithful Interpretation for Graph Neural Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Currently, attention mechanisms have garnered increasing attention in Graph Neural Networks (GNNs), such as Graph Attention Networks (GATs) and Graph Transformers (GTs). It is not only due to the commendable boost in performance they offer but also its capacity to provide a more lucid rationale for model behaviors, which are often viewed as inscrutable. However, Attention-based GNNs have demonstrated instability in interpretability when subjected to various sources of perturbations during both training and testing phases, including factors like additional edges or nodes. In this paper, we propose a solution to this problem by introducing a novel notion called Faithful Graph Attention-based Interpretation (FGAI). In particular, FGAI has four crucial properties regarding stability and sensitivity to interpretation and final output distribution. Built upon this notion, we propose an efficient methodology for obtaining FGAI, which can be viewed as an ad hoc modification to the canonical Attention-based GNNs. To validate our proposed solution, we introduce two novel metrics tailored for graph interpretation assessment. Experimental results demonstrate that FGAI exhibits superior stability and preserves the interpretability of attention under various forms of perturbations and randomness, which makes FGAI a more faithful and reliable explanation tool.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2410.06957",
        "abstract url": "https://arxiv.org/abs/2410.06957",
        "title": "Support Vector Boosting Machine (SVBM): Enhancing Classification Performance with AdaBoost and Residual Connections",
        "rating": "-0.5",
        "keywords": [
            [
                "SVM",
                "Support Vector Machine"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In traditional boosting algorithms, the focus on misclassified training samples emphasizes their importance based on difficulty during the learning process. While using a standard Support Vector Machine (SVM) as a weak learner in an AdaBoost framework can enhance model performance by concentrating on error samples, this approach introduces significant challenges. Specifically, SVMs, characterized by their stability and robustness, may require destabilization to fit the boosting paradigm, which in turn can constrain performance due to reliance on the weighted results from preceding iterations. To address these challenges, we propose the Support Vector Boosting Machine (SVBM), which integrates a novel subsampling process with SVM algorithms and residual connection techniques. This method updates sample weights by considering both the current model's predictions and the outputs from prior rounds, allowing for effective sparsity control. The SVBM framework enhances the ability to form complex decision boundaries, thereby improving classification performance. The MATLAB source code for SVBM can be accessed at https://github.com/junbolian/SVBM.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "The MATLAB source code for SVBM can be accessed at https://github.com/junbolian/SVBM"
    },
    {
        "paper id": "2410.06976",
        "abstract url": "https://arxiv.org/abs/2410.06976",
        "title": "AdaRC: Mitigating Graph Structure Shifts during Test-Time",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Powerful as they are, graph neural networks (GNNs) are known to be vulnerable to distribution shifts. Recently, test-time adaptation (TTA) has attracted attention due to its ability to adapt a pre-trained model to a target domain without re-accessing the source domain. However, existing TTA algorithms are primarily designed for attribute shifts in vision tasks, where samples are independent. These methods perform poorly on graph data that experience structure shifts, where node connectivity differs between source and target graphs. We attribute this performance gap to the distinct impact of node attribute shifts versus graph structure shifts: the latter significantly degrades the quality of node representations and blurs the boundaries between different node categories. To address structure shifts in graphs, we propose AdaRC, an innovative framework designed for effective and efficient adaptation to structure shifts by adjusting the hop-aggregation parameters in GNNs. To enhance the representation quality, we design a prediction-informed clustering loss to encourage the formation of distinct clusters for different node categories. Additionally, AdaRC seamlessly integrates with existing TTA algorithms, allowing it to handle attribute shifts effectively while improving overall performance under combined structure and attribute shifts. We validate the effectiveness of AdaRC on both synthetic and real-world datasets, demonstrating its robustness across various combinations of structure and attribute shifts.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06986",
        "abstract url": "https://arxiv.org/abs/2410.06986",
        "title": "Diffusion Density Estimators",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We investigate the use of diffusion models as neural density estimators. The current approach to this problem involves converting the generative process to a smooth flow, known as the Probability Flow ODE. The log density at a given sample can be obtained by solving the ODE with a black-box solver. We introduce a new, highly parallelizable method that computes log densities without the need to solve a flow. Our approach is based on estimating a path integral by Monte Carlo, in a manner identical to the simulation-free training of diffusion models. We also study how different training parameters affect the accuracy of the density calculation, and offer insights into how these models can be made more scalable and efficient.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "20 pages + references, 7 figures"
    },
    {
        "paper id": "2410.07013",
        "abstract url": "https://arxiv.org/abs/2410.07013",
        "title": "Causal Representation Learning in Temporal Data via Single-Parent Decoding",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Scientific research often seeks to understand the causal structure underlying high-level variables in a system. For example, climate scientists study how phenomena, such as El Ni\u00f1o, affect other climate processes at remote locations across the globe. However, scientists typically collect low-level measurements, such as geographically distributed temperature readings. From these, one needs to learn both a mapping to causally-relevant latent variables, such as a high-level representation of the El Ni\u00f1o phenomenon and other processes, as well as the causal model over them. The challenge is that this task, called causal representation learning, is highly underdetermined from observational data alone, requiring other constraints during learning to resolve the indeterminacies. In this work, we consider a temporal model with a sparsity assumption, namely single-parent decoding: each observed low-level variable is only affected by a single latent variable. Such an assumption is reasonable in many scientific applications that require finding groups of low-level variables, such as extracting regions from geographically gridded measurement data in climate research or capturing brain regions from neural activity data. We demonstrate the identifiability of the resulting model and propose a differentiable method, Causal Discovery with Single-parent Decoding (CDSD), that simultaneously learns the underlying latents and a causal graph over them. We assess the validity of our theoretical results using simulated data and showcase the practical validity of our method in an application to real-world data from the climate science field.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "33 pages, 17 figures"
    },
    {
        "paper id": "2410.07071",
        "abstract url": "https://arxiv.org/abs/2410.07071",
        "title": "Retrieval-Augmented Decision Transformer: External Memory for In-context RL",
        "rating": "-0.5",
        "keywords": [
            [
                "robotics"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In-context learning (ICL) is the ability of a model to learn a new task by observing a few exemplars in its context. While prevalent in NLP, this capability has recently also been observed in Reinforcement Learning (RL) settings. Prior in-context RL methods, however, require entire episodes in the agent's context. Given that complex environments typically lead to long episodes with sparse rewards, these methods are constrained to simple environments with short episodes. To address these challenges, we introduce Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external memory mechanism to store past experiences from which it retrieves only sub-trajectories relevant for the current situation. The retrieval component in RA-DT does not require training and can be entirely domain-agnostic. We evaluate the capabilities of RA-DT on grid-world environments, robotics simulations, and procedurally-generated video games. On grid-worlds, RA-DT outperforms baselines, while using only a fraction of their context length. Furthermore, we illuminate the limitations of current in-context RL methods on complex environments and discuss future directions. To facilitate future research, we release datasets for four of the considered environments.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07074",
        "abstract url": "https://arxiv.org/abs/2410.07074",
        "title": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world systems, yet leveraging large language models (LLMs) for TAGs presents unique challenges due to the gap between sequential text processing and graph-structured data. We introduce AskGNN, a novel approach that bridges this gap by leveraging In-Context Learning (ICL) to integrate graph data and task-specific information into LLMs. AskGNN employs a Graph Neural Network (GNN)-powered structure-enhanced retriever to select labeled nodes across graphs, incorporating complex graph structures and their supervision signals. Our learning-to-retrieve algorithm optimizes the retriever to select example nodes that maximize LLM performance on graph. Experiments across three tasks and seven LLMs demonstrate AskGNN's superior effectiveness in graph task performance, opening new avenues for applying LLMs to graph-structured data without extensive fine-tuning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07091",
        "abstract url": "https://arxiv.org/abs/2410.07091",
        "title": "Collusion Detection with Graph Neural Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Collusion is a complex phenomenon in which companies secretly collaborate to engage in fraudulent practices. This paper presents an innovative methodology for detecting and predicting collusion patterns in different national markets using neural networks (NNs) and graph neural networks (GNNs). GNNs are particularly well suited to this task because they can exploit the inherent network structures present in collusion and many other economic problems. Our approach consists of two phases: In Phase I, we develop and train models on individual market datasets from Japan, the United States, two regions in Switzerland, Italy, and Brazil, focusing on predicting collusion in single markets. In Phase II, we extend the models' applicability through zero-shot learning, employing a transfer learning approach that can detect collusion in markets in which training data is unavailable. This phase also incorporates out-of-distribution (OOD) generalization to evaluate the models' performance on unseen datasets from other countries and regions. In our empirical study, we show that GNNs outperform NNs in detecting complex collusive patterns. This research contributes to the ongoing discourse on preventing collusion and optimizing detection methodologies, providing valuable guidance on the use of NNs and GNNs in economic applications to enhance market fairness and economic welfare.",
        "subjects": [
            "econ.EM",
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07147",
        "abstract url": "https://arxiv.org/abs/2410.07147",
        "title": "Taking a turn for the better: Conversation redirection throughout the course of mental-health therapy",
        "rating": "-0.5",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.AI",
                "cs.CY",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Mental-health therapy involves a complex conversation flow in which patients and therapists continuously negotiate what should be talked about next. For example, therapists might try to shift the conversation's direction to keep the therapeutic process on track and avoid stagnation, or patients might push the discussion towards issues they want to focus on. How do such patient and therapist redirections relate to the development and quality of their relationship? To answer this question, we introduce a probabilistic measure of the extent to which a certain utterance immediately redirects the flow of the conversation, accounting for both the intention and the actual realization of such a change. We apply this new measure to characterize the development of patient-therapist relationships over multiple sessions in a very large, widely-used online therapy platform. Our analysis reveals that (1) patient control of the conversation's direction generally increases relative to that of the therapist as their relationship progresses; and (2) patients who have less control in the first few sessions are significantly more likely to eventually express dissatisfaction with their therapist and terminate the relationship.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CY"
        ],
        "comment": "To appear in the Proceedings of EMNLP (Findings) 2024. Code available at https://convokit.cornell.edu"
    },
    {
        "paper id": "2410.07272",
        "abstract url": "https://arxiv.org/abs/2410.07272",
        "title": "Boosting the Performance of Decentralized Federated Learning via Catalyst Acceleration",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Decentralized Federated Learning has emerged as an alternative to centralized architectures due to its faster training, privacy preservation, and reduced communication overhead. In decentralized communication, the server aggregation phase in Centralized Federated Learning shifts to the client side, which means that clients connect with each other in a peer-to-peer manner. However, compared to the centralized mode, data heterogeneity in Decentralized Federated Learning will cause larger variances between aggregated models, which leads to slow convergence in training and poor generalization performance in tests. To address these issues, we introduce Catalyst Acceleration and propose an acceleration Decentralized Federated Learning algorithm called DFedCata. It consists of two main components: the Moreau envelope function, which primarily addresses parameter inconsistencies among clients caused by data heterogeneity, and Nesterov's extrapolation step, which accelerates the aggregation phase. Theoretically, we prove the optimization error bound and generalization error bound of the algorithm, providing a further understanding of the nature of the algorithm and the theoretical perspectives on the hyperparameter choice. Empirically, we demonstrate the advantages of the proposed algorithm in both convergence speed and generalization performance on CIFAR10/100 with various non-iid data distributions. Furthermore, we also experimentally verify the theoretical properties of DFedCata.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2410.06482"
    },
    {
        "paper id": "2410.07283",
        "abstract url": "https://arxiv.org/abs/2410.07283",
        "title": "Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "As Large Language Models (LLMs) grow increasingly powerful, multi-agent systems are becoming more prevalent in modern AI applications. Most safety research, however, has focused on vulnerabilities in single-agent LLMs. These include prompt injection attacks, where malicious prompts embedded in external content trick the LLM into executing unintended or harmful actions, compromising the victim's application. In this paper, we reveal a more dangerous vector: LLM-to-LLM prompt injection within multi-agent systems. We introduce Prompt Infection, a novel attack where malicious prompts self-replicate across interconnected agents, behaving much like a computer virus. This attack poses severe threats, including data theft, scams, misinformation, and system-wide disruption, all while propagating silently through the system. Our extensive experiments demonstrate that multi-agent systems are highly susceptible, even when agents do not publicly share all communications. To address this, we propose LLM Tagging, a defense mechanism that, when combined with existing safeguards, significantly mitigates infection spread. This work underscores the urgent need for advanced security measures as multi-agent LLM systems become more widely adopted.",
        "subjects": [
            "cs.MA",
            "cs.AI",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07409",
        "abstract url": "https://arxiv.org/abs/2410.07409",
        "title": "Learning responsibility allocations for multi-agent interactions: A differentiable optimization approach with control barrier functions",
        "rating": "-0.5",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "From autonomous driving to package delivery, ensuring safe yet efficient multi-agent interaction is challenging as the interaction dynamics are influenced by hard-to-model factors such as social norms and contextual cues. Understanding these influences can aid in the design and evaluation of socially-aware autonomous agents whose behaviors are aligned with human values. In this work, we seek to codify factors governing safe multi-agent interactions via the lens of responsibility, i.e., an agent's willingness to deviate from their desired control to accommodate safe interaction with others. Specifically, we propose a data-driven modeling approach based on control barrier functions and differentiable optimization that efficiently learns agents' responsibility allocation from data. We demonstrate on synthetic and real-world datasets that we can obtain an interpretable and quantitative understanding of how much agents adjust their behavior to ensure the safety of others given their current environment.",
        "subjects": [
            "eess.SY",
            "cs.LG",
            "cs.MA",
            "cs.RO"
        ],
        "comment": "8 pages, 7 figures"
    },
    {
        "paper id": "2410.07451",
        "abstract url": "https://arxiv.org/abs/2410.07451",
        "title": "Collective variables of neural networks: empirical time evolution and scaling laws",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This work presents a novel means for understanding learning dynamics and scaling relations in neural networks. We show that certain measures on the spectrum of the empirical neural tangent kernel, specifically entropy and trace, yield insight into the representations learned by a neural network and how these can be improved through architecture scaling. These results are demonstrated first on test cases before being shown on more complex networks, including transformers, auto-encoders, graph neural networks, and reinforcement learning studies. In testing on a wide range of architectures, we highlight the universal nature of training dynamics and further discuss how it can be used to understand the mechanisms behind learning in neural networks. We identify two such dominant mechanisms present throughout machine learning training. The first, information compression, is seen through a reduction in the entropy of the NTK spectrum during training, and occurs predominantly in small neural networks. The second, coined structure formation, is seen through an increasing entropy and thus, the creation of structure in the neural network representations beyond the prior established by the network at initialization. Due to the ubiquity of the latter in deep neural network architectures and its flexibility in the creation of feature-rich representations, we argue that this form of evolution of the network's entropy be considered the onset of a deep learning regime.",
        "subjects": [
            "cs.LG",
            "physics.comp-ph"
        ],
        "comment": "11 pages, 3 figures"
    },
    {
        "paper id": "2410.07519",
        "abstract url": "https://arxiv.org/abs/2410.07519",
        "title": "MEMS Gyroscope Multi-Feature Calibration Using Machine Learning Technique",
        "rating": "-0.5",
        "keywords": [
            [
                "navigation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Gyroscopes are crucial for accurate angular velocity measurements in navigation, stabilization, and control systems. MEMS gyroscopes offer advantages like compact size and low cost but suffer from errors and inaccuracies that are complex and time varying. This study leverages machine learning (ML) and uses multiple signals of the MEMS resonator gyroscope to improve its calibration. XGBoost, known for its high predictive accuracy and ability to handle complex, non-linear relationships, and MLP, recognized for its capability to model intricate patterns through multiple layers and hidden dimensions, are employed to enhance the calibration process. Our findings show that both XGBoost and MLP models significantly reduce noise and enhance accuracy and stability, outperforming the traditional calibration techniques. Despite higher computational costs, DL models are ideal for high-stakes applications, while ML models are efficient for consumer electronics and environmental monitoring. Both ML and DL models demonstrate the potential of advanced calibration techniques in enhancing MEMS gyroscope performance and calibration efficiency.",
        "subjects": [
            "cs.LG",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07543",
        "abstract url": "https://arxiv.org/abs/2410.07543",
        "title": "Generalization Ability Analysis of Through-the-Wall Radar Human Activity Recognition",
        "rating": "-0.5",
        "keywords": [
            [
                "Radar"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Through-the-Wall radar (TWR) human activity recognition (HAR) is a technology that uses low-frequency ultra-wideband (UWB) signal to detect and analyze indoor human motion. However, the high dependence of existing end-to-end recognition models on the distribution of TWR training data makes it difficult to achieve good generalization across different indoor testers. In this regard, the generalization ability of TWR HAR is analyzed in this paper. In detail, an end-to-end linear neural network method for TWR HAR and its generalization error bound are first discussed. Second, a micro-Doppler corner representation method and the change of the generalization error before and after dimension reduction are presented. The appropriateness of the theoretical generalization errors is proved through numerical simulations and experiments. The results demonstrate that feature dimension reduction is effective in allowing recognition models to generalize across different indoor testers.",
        "subjects": [
            "eess.SP",
            "cs.AI"
        ],
        "comment": "6 pages, 4 figures, 0 table, in Proc. IEEE International Conference on Signal, Information and Data Processing (ICSIDP), 2024"
    },
    {
        "paper id": "2410.07550",
        "abstract url": "https://arxiv.org/abs/2410.07550",
        "title": "Conditional Lagrangian Wasserstein Flow for Time Series Imputation",
        "rating": "-0.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Time series imputation is important for numerous real-world applications. To overcome the limitations of diffusion model-based imputation methods, e.g., slow convergence in inference, we propose a novel method for time series imputation in this work, called Conditional Lagrangian Wasserstein Flow. The proposed method leverages the (conditional) optimal transport theory to learn the probability flow in a simulation-free manner, in which the initial noise, missing data, and observations are treated as the source distribution, target distribution, and conditional information, respectively. According to the principle of least action in Lagrangian mechanics, we learn the velocity by minimizing the corresponding kinetic energy. Moreover, to incorporate more prior information into the model, we parameterize the derivative of a task-specific potential function via a variational autoencoder, and combine it with the base estimator to formulate a Rao-Blackwellized sampler. The propose model allows us to take less intermediate steps to produce high-quality samples for inference compared to existing diffusion methods. Finally, the experimental results on the real-word datasets show that the proposed method achieves competitive performance on time series imputation compared to the state-of-the-art methods.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "13 pages, 2 figures"
    },
    {
        "paper id": "2410.07588",
        "abstract url": "https://arxiv.org/abs/2410.07588",
        "title": "Careful About What App Promotion Ads Recommend! Detecting and Explaining Malware Promotion via App Promotion Graph",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "In Android apps, their developers frequently place app promotion ads, namely advertisements to promote other apps. Unfortunately, the inadequate vetting of ad content allows malicious developers to exploit app promotion ads as a new distribution channel for malware. To help detect malware distributed via app promotion ads, in this paper, we propose a novel approach, named ADGPE, that synergistically integrates app user interface (UI) exploration with graph learning to automatically collect app promotion ads, detect malware promoted by these ads, and explain the promotion mechanisms employed by the detected malware. Our evaluation on 18, 627 app promotion ads demonstrates the substantial risks in the app promotion ecosystem.",
        "subjects": [
            "cs.CR",
            "cs.CY"
        ],
        "comment": "NDSS Symposium 2025 Accepted Papers"
    },
    {
        "paper id": "2410.07592",
        "abstract url": "https://arxiv.org/abs/2410.07592",
        "title": "Diversified and Adaptive Negative Sampling on Knowledge Graphs",
        "rating": "-0.5",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In knowledge graph embedding, aside from positive triplets (ie: facts in the knowledge graph), the negative triplets used for training also have a direct influence on the model performance. In reality, since knowledge graphs are sparse and incomplete, negative triplets often lack explicit labels, and thus they are often obtained from various sampling strategies (eg: randomly replacing an entity in a positive triplet). An ideal sampled negative triplet should be informative enough to help the model train better. However, existing methods often ignore diversity and adaptiveness in their sampling process, which harms the informativeness of negative triplets. As such, we propose a generative adversarial approach called Diversified and Adaptive Negative Sampling DANS on knowledge graphs. DANS is equipped with a two-way generator that generates more diverse negative triplets through two pathways, and an adaptive mechanism that produces more fine-grained examples by localizing the global generator for different entities and relations. On the one hand, the two-way generator increase the overall informativeness with more diverse negative examples; on the other hand, the adaptive mechanism increases the individual sample-wise informativeness with more fine-grained sampling. Finally, we evaluate the performance of DANS on three benchmark knowledge graphs to demonstrate its effectiveness through quantitative and qualitative experiments.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "30 pages, 7 figures, Journal"
    },
    {
        "paper id": "2410.07611",
        "abstract url": "https://arxiv.org/abs/2410.07611",
        "title": "Parallel Digital Twin-driven Deep Reinforcement Learning for User Association and Load Balancing in Dynamic Wireless Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "diffusion"
            ],
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Optimization of user association in a densely deployed heterogeneous cellular network is usually challenging and even more complicated due to the dynamic nature of user mobility and fluctuation in user counts. While deep reinforcement learning (DRL) emerges as a promising solution, its application in practice is hindered by high trial-and-error costs in real world and unsatisfactory physical network performance during training. In addition, existing DRL-based user association methods are usually only applicable to scenarios with a fixed number of users due to convergence and compatibility challenges. In this paper, we propose a parallel digital twin (DT)-driven DRL method for user association and load balancing in networks with both dynamic user counts, distribution, and mobility patterns. Our method employs a distributed DRL strategy to handle varying user numbers and exploits a refined neural network structure for faster convergence. To address these DRL training-related challenges, we devise a high-fidelity DT construction technique, featuring a zero-shot generative user mobility model, named Map2Traj, based on a diffusion model. Map2Traj estimates user trajectory patterns and spatial distributions solely from street maps. Armed with this DT environment, DRL agents are enabled to be trained without the need for interactions with the physical network. To enhance the generalization ability of DRL models for dynamic scenarios, a parallel DT framework is further established to alleviate strong correlation and non-stationarity in single-environment training and improve the training efficiency. Numerical results show that the proposed parallel DT-driven DRL method achieves closely comparable performance to real environment training, and even outperforms those trained in a single real-world environment with nearly 20% gain in terms of cell-edge user performance.",
        "subjects": [
            "cs.LG",
            "eess.SY"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2407.19765"
    },
    {
        "paper id": "2410.06566",
        "abstract url": "https://arxiv.org/abs/2410.06566",
        "title": "Detecting Bias and Enhancing Diagnostic Accuracy in Large Language Models for Healthcare",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "health",
                "Healthcare",
                "clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Biased AI-generated medical advice and misdiagnoses can jeopardize patient safety, making the integrity of AI in healthcare more critical than ever. As Large Language Models (LLMs) take on a growing role in medical decision-making, addressing their biases and enhancing their accuracy is key to delivering safe, reliable care. This study addresses these challenges head-on by introducing new resources designed to promote ethical and precise AI in healthcare. We present two datasets: BiasMD, featuring 6,007 question-answer pairs crafted to evaluate and mitigate biases in health-related LLM outputs, and DiseaseMatcher, with 32,000 clinical question-answer pairs spanning 700 diseases, aimed at assessing symptom-based diagnostic accuracy. Using these datasets, we developed the EthiClinician, a fine-tuned model built on the ChatDoctor framework, which outperforms GPT-4 in both ethical reasoning and clinical judgment. By exposing and correcting hidden biases in existing models for healthcare, our work sets a new benchmark for safer, more reliable patient outcomes.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06608",
        "abstract url": "https://arxiv.org/abs/2410.06608",
        "title": "Bahasa Harmony: A Comprehensive Dataset for Bahasa Text-to-Speech Synthesis with Discrete Codec Modeling of EnGen-TTS",
        "rating": "-1",
        "keywords": [
            [
                "Text-to-Speech"
            ],
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "This research introduces a comprehensive Bahasa text-to-speech (TTS) dataset and a novel TTS model, EnGen-TTS, designed to enhance the quality and versatility of synthetic speech in the Bahasa language. The dataset, spanning \\textasciitilde55.0 hours and 52K audio recordings, integrates diverse textual sources, ensuring linguistic richness. A meticulous recording setup captures the nuances of Bahasa phonetics, employing professional equipment to ensure high-fidelity audio samples. Statistical analysis reveals the dataset's scale and diversity, laying the foundation for model training and evaluation. The proposed EnGen-TTS model performs better than established baselines, achieving a Mean Opinion Score (MOS) of 4.45 $\\pm$ 0.13. Additionally, our investigation on real-time factor and model size highlights EnGen-TTS as a compelling choice, with efficient performance. This research marks a significant advancement in Bahasa TTS technology, with implications for diverse language applications. Link to Generated Samples: \\url{https://bahasa-harmony-comp.vercel.app/}",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06620",
        "abstract url": "https://arxiv.org/abs/2410.06620",
        "title": "Task Coordination and Trajectory Optimization for Multi-Aerial Systems via Signal Temporal Logic: A Wind Turbine Inspection Study",
        "rating": "-1",
        "keywords": [
            [
                "Trajectory",
                "vehicle",
                "flight"
            ]
        ],
        "abstract": "This paper presents a method for task allocation and trajectory generation in cooperative inspection missions using a fleet of multirotor drones, with a focus on wind turbine inspection. The approach generates safe, feasible flight paths that adhere to time-sensitive constraints and vehicle limitations by formulating an optimization problem based on Signal Temporal Logic (STL) specifications. An event-triggered replanning mechanism addresses unexpected events and delays, while a generalized robustness scoring method incorporates user preferences and minimizes task conflicts. The approach is validated through simulations in MATLAB and Gazebo, as well as field experiments in a mock-up scenario.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "2 pages, Accepted for discussion at the workshop session \"Formal methods techniques in robotics systems: Design and control\" at IROS'24 in Abu Dhabi, UAE"
    },
    {
        "paper id": "2410.06624",
        "abstract url": "https://arxiv.org/abs/2410.06624",
        "title": "Optimized Magnetic Resonance Fingerprinting Using Ziv-Zakai Bound",
        "rating": "-1",
        "keywords": [
            [
                "MRI"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Magnetic Resonance Fingerprinting (MRF) has emerged as a promising quantitative imaging technique within the field of Magnetic Resonance Imaging (MRI), offers comprehensive insights into tissue properties by simultaneously acquiring multiple tissue parameter maps in a single acquisition. Sequence optimization is crucial for improving the accuracy and efficiency of MRF. In this work, a novel framework for MRF sequence optimization is proposed based on the Ziv-Zakai bound (ZZB). Unlike the Cram\u00e9r-Rao bound (CRB), which aims to enhance the quality of a single fingerprint signal with deterministic parameters, ZZB provides insights into evaluating the minimum mismatch probability for pairs of fingerprint signals within the specified parameter range in MRF. Specifically, the explicit ZZB is derived to establish a lower bound for the discrimination error in the fingerprint signal matching process within MRF. This bound illuminates the intrinsic limitations of MRF sequences, thereby fostering a deeper understanding of existing sequence performance. Subsequently, an optimal experiment design problem based on ZZB was formulated to ascertain the optimal scheme of acquisition parameters, maximizing discrimination power of MRF between different tissue types. Preliminary numerical experiments show that the optimized ZZB scheme outperforms both the conventional and CRB schemes in terms of the reconstruction accuracy of multiple parameter maps.",
        "subjects": [
            "eess.IV",
            "q-bio.QM",
            "stat.AP"
        ],
        "comment": "Accepted at 2024 IEEE International Conference on Imaging Systems and Techniques (IST 2024)"
    },
    {
        "paper id": "2410.06628",
        "abstract url": "https://arxiv.org/abs/2410.06628",
        "title": "Does Vec2Text Pose a New Corpus Poisoning Threat?",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "The emergence of Vec2Text -- a method for text embedding inversion -- has raised serious privacy concerns for dense retrieval systems which use text embeddings. This threat comes from the ability for an attacker with access to embeddings to reconstruct the original text. In this paper, we take a new look at Vec2Text and investigate how much of a threat it poses to the different attacks of corpus poisoning, whereby an attacker injects adversarial passages into a retrieval corpus with the intention of misleading dense retrievers. Theoretically, Vec2Text is far more dangerous than previous attack methods because it does not need access to the embedding model's weights and it can efficiently generate many adversarial passages. We show that under certain conditions, corpus poisoning with Vec2Text can pose a serious threat to dense retriever system integrity and user experience by injecting adversarial passaged into top ranked positions. Code and data are made available at https://github.com/ielab/vec2text-corpus-poisoning",
        "subjects": [
            "cs.IR"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2402.12784"
    },
    {
        "paper id": "2410.06692",
        "abstract url": "https://arxiv.org/abs/2410.06692",
        "title": "How hard can it be? Quantifying MITRE attack campaigns with attack trees and cATM logic",
        "rating": "-1",
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "The landscape of cyber threats grows more complex by the day. Advanced Persistent Threats carry out systematic attack campaigns against which cybersecurity practitioners must defend. Examples of such organized attacks are operations Dream Job, Wocao, WannaCry or the SolarWinds Compromise. To evaluate which risks are most threatening, and which campaigns to prioritize against when defending, cybersecurity experts must be equipped with the right toolbox. In particular, they must be able to (a) obtain likelihood values for each attack campaign recorded in the wild and (b) reliably and transparently operationalize these values to carry out quantitative comparisons among campaigns. This will allow security experts to perform quantitatively-informed decision making that is transparent and accountable. In this paper we construct such a framework by: (1) quantifying the likelihood of attack campaigns via data-driven procedures on the MITRE knowledge base and (2) introducing a methodology for automatic modelling of MITRE intelligence data: this is complete in the sense that it captures any attack campaign via template attack tree models. (3) We further propose a computational framework to carry out this comparisons based on the cATM formal logic, and implement this into an open-source Python tool. Finally, we validate our approach by quantifying the likelihood of all MITRE campaigns, and comparing the likelihood of the Wocao and Dream Job MITRE campaigns -- generated with our proposed approach -- against \"ad hoc\" traditionally-built attack tree models, demonstrating how our methodology is substantially lighter in modelling effort, and still capable of capturing all the quantitative relevant data.",
        "subjects": [
            "cs.CR",
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06720",
        "abstract url": "https://arxiv.org/abs/2410.06720",
        "title": "Collective perception for tracking people with a robot swarm",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Swarm perception refers to the ability of a robot swarm to utilize the perception capabilities of each individual robot, forming a collective understanding of the environment. Their distributed nature enables robot swarms to continuously monitor dynamic environments by maintaining a constant presence throughout the space.In this study, we present a preliminary experiment on the collective tracking of people using a robot swarm. The experiment was conducted in simulation across four different office environments, with swarms of varying sizes. The robots were provided with images sampled from a dataset of real-world office environment pictures.We measured the time distribution required for a robot to detect a person changing location and to propagate this information to increasing fractions of the swarm. The results indicate that robot swarms show significant promise in monitoring dynamic environments.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Presented at ICRA@40, Rotterdam"
    },
    {
        "paper id": "2410.06723",
        "abstract url": "https://arxiv.org/abs/2410.06723",
        "title": "Evaluating Computational Pathology Foundation Models for Prostate Cancer Grading under Distribution Shifts",
        "rating": "-1",
        "keywords": [
            [
                "whole-slide",
                "Cancer"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Foundation models have recently become a popular research direction within computational pathology. They are intended to be general-purpose feature extractors, promising to achieve good performance on a range of downstream tasks. Real-world pathology image data does however exhibit considerable variability. Foundation models should be robust to these variations and other distribution shifts which might be encountered in practice. We evaluate two computational pathology foundation models: UNI (trained on more than 100,000 whole-slide images) and CONCH (trained on more than 1.1 million image-caption pairs), by utilizing them as feature extractors within prostate cancer grading models. We find that while UNI and CONCH perform well relative to baselines, the absolute performance can still be far from satisfactory in certain settings. The fact that foundation models have been trained on large and varied datasets does not guarantee that downstream models always will be robust to common distribution shifts.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Preprint, work in progress"
    },
    {
        "paper id": "2410.06743",
        "abstract url": "https://arxiv.org/abs/2410.06743",
        "title": "Utilizing Transfer Learning and pre-trained Models for Effective Forest Fire Detection: A Case Study of Uttarakhand",
        "rating": "-1",
        "keywords": [
            [
                "satellite"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Forest fires pose a significant threat to the environment, human life, and property. Early detection and response are crucial to mitigating the impact of these disasters. However, traditional forest fire detection methods are often hindered by our reliability on manual observation and satellite imagery with low spatial resolution. This paper emphasizes the role of transfer learning in enhancing forest fire detection in India, particularly in overcoming data collection challenges and improving model accuracy across various regions. We compare traditional learning methods with transfer learning, focusing on the unique challenges posed by regional differences in terrain, climate, and vegetation. Transfer learning can be categorized into several types based on the similarity between the source and target tasks, as well as the type of knowledge transferred. One key method is utilizing pre-trained models for efficient transfer learning, which significantly reduces the need for extensive labeled data. We outline the transfer learning process, demonstrating how researchers can adapt pre-trained models like MobileNetV2 for specific tasks such as forest fire detection. Finally, we present experimental results from training and evaluating a deep learning model using the Uttarakhand forest fire dataset, showcasing the effectiveness of transfer learning in this context.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "15 pages, 6 figures"
    },
    {
        "paper id": "2410.06746",
        "abstract url": "https://arxiv.org/abs/2410.06746",
        "title": "Cluster-wise Graph Transformer with Dual-granularity Kernelized Attention",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ],
            [
                "Kernel Learning"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In the realm of graph learning, there is a category of methods that conceptualize graphs as hierarchical structures, utilizing node clustering to capture broader structural information. While generally effective, these methods often rely on a fixed graph coarsening routine, leading to overly homogeneous cluster representations and loss of node-level information. In this paper, we envision the graph as a network of interconnected node sets without compressing each cluster into a single embedding. To enable effective information transfer among these node sets, we propose the Node-to-Cluster Attention (N2C-Attn) mechanism. N2C-Attn incorporates techniques from Multiple Kernel Learning into the kernelized attention framework, effectively capturing information at both node and cluster levels. We then devise an efficient form for N2C-Attn using the cluster-wise message-passing framework, achieving linear time complexity. We further analyze how N2C-Attn combines bi-level feature maps of queries and keys, demonstrating its capability to merge dual-granularity information. The resulting architecture, Cluster-wise Graph Transformer (Cluster-GT), which uses node clusters as tokens and employs our proposed N2C-Attn module, shows superior performance on various graph-level tasks. Code is available at https://github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted as NeurIPS 2024 Spotlight"
    },
    {
        "paper id": "2410.06764",
        "abstract url": "https://arxiv.org/abs/2410.06764",
        "title": "An Optimal Algorithm for the Stacker Crane Problem on Fixed Topologies",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "The Stacker Crane Problem (SCP) is a variant of the Traveling Salesman Problem. In SCP, pairs of pickup and delivery points are designated on a graph, and a crane must visit these points to move objects from each pickup location to its respective delivery point. The goal is to minimize the total distance traveled. SCP is known to be NP-hard, even on tree structures. The only positive results, in terms of polynomial-time solvability, apply to graphs that are topologically equivalent to a path or a cycle. We propose an algorithm that is optimal for each fixed topology, running in near-linear time. This is achieved by demonstrating that the problem is fixed-parameter tractable (FPT) when parameterized by both the cycle rank and the number of branch vertices.",
        "subjects": [
            "cs.DS",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06768",
        "abstract url": "https://arxiv.org/abs/2410.06768",
        "title": "Patterns of Creativity: How User Input Shapes AI-Generated Visual Diversity",
        "rating": "-1",
        "keywords": [
            [
                "Text-to-Image"
            ]
        ],
        "abstract": "Recent critiques of Artificial-intelligence (AI)-generated visual content highlight concerns about the erosion of artistic originality, as these systems often replicate patterns from their training datasets, leading to significant uniformity and reduced diversity. Our research adopts a novel approach by focusing on user behavior during interactions with Text-to-Image models. Instead of solely analyzing training data patterns, we examine how users' tendencies to create original prompts or rely on common templates influence content homogenization. We developed three originality metrics -- lexical, thematic, and word-sequence originality -- and applied them to user-generated prompts from two datasets, DiffusionDB and Civiverse. Additionally, we explored how characteristics such as topic choice, language originality, and the presence of NSFW content affect image popularity, using a linear regression model to predict user engagement. Our research enhances the discourse on AI's impact on creativity by emphasizing the critical role of user behavior in shaping the diversity of AI-generated visual content.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06782",
        "abstract url": "https://arxiv.org/abs/2410.06782",
        "title": "Mind Your Questions! Towards Backdoor Attacks on Text-to-Visualization Models",
        "rating": "-1",
        "keywords": [
            [
                "Attacks"
            ]
        ],
        "abstract": "Text-to-visualization (text-to-vis) models have become valuable tools in the era of big data, enabling users to generate data visualizations and make informed decisions through natural language queries (NLQs). Despite their widespread application, the security vulnerabilities of these models have been largely overlooked. To address this gap, we propose VisPoison, a novel framework designed to identify these vulnerabilities of current text-to-vis models systematically. VisPoison introduces two types of triggers that activate three distinct backdoor attacks, potentially leading to data exposure, misleading visualizations, or denial-of-service (DoS) incidents. The framework features both proactive and passive attack mechanisms: proactive attacks leverage rare-word triggers to access confidential data, while passive attacks, triggered unintentionally by users, exploit a first-word trigger method, causing errors or DoS events in visualizations. Through extensive experiments on both trainable and in-context learning (ICL)-based text-to-vis models, \\textit{VisPoison} achieves attack success rates of over 90\\%, highlighting the security problem of current text-to-vis models. Additionally, we explore two types of defense mechanisms against these attacks, but the results show that existing countermeasures are insufficient, underscoring the pressing need for more robust security solutions in text-to-vis systems.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "11 pages, 4 figures"
    },
    {
        "paper id": "2410.06787",
        "abstract url": "https://arxiv.org/abs/2410.06787",
        "title": "Efficient training strategies for natural sounding speech synthesis and speaker adaptation based on FastPitch",
        "rating": "-1",
        "keywords": [
            [
                "text-to-speech"
            ],
            [
                "eess.AS"
            ]
        ],
        "abstract": "This paper focuses on adapting the functionalities of the FastPitch model to the Romanian language; extending the set of speakers from one to eighteen; synthesising speech using an anonymous identity; and replicating the identities of new, unseen speakers. During this work, the effects of various configurations and training strategies were tested and discussed, along with their advantages and weaknesses. Finally, we settled on a new configuration, built on top of the FastPitch architecture, capable of producing natural speech synthesis, for both known (identities from the training dataset) and unknown (identities learnt through short reference samples) speakers. The anonymous speaker can be used for text-to-speech synthesis, if one wants to cancel out the identity information while keeping the semantic content whole and clear. At last, we discussed possible limitations of our work, which will form the basis for future investigations and advancements.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "Accepted at 2024 IEEE 20th International Conference on Intelligent Computer Communication and Processing (ICCP 2024)"
    },
    {
        "paper id": "2410.06793",
        "abstract url": "https://arxiv.org/abs/2410.06793",
        "title": "A Polynomial Time Algorithm for Steiner Tree when Terminals Avoid a $K_4$-Minor",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "We study a special case of the Steiner Tree problem in which the input graph does not have a minor model of a complete graph on 4 vertices for which all branch sets contain a terminal. We show that this problem can be solved in $O(n^4)$ time, where $n$ denotes the number of vertices in the input graph. This generalizes a seminal paper by Erickson et al. [Math. Oper. Res., 1987] that solves Steiner tree on planar graphs with all terminals on one face in polynomial time.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06796",
        "abstract url": "https://arxiv.org/abs/2410.06796",
        "title": "Diffuse or Confuse: A Diffusion Deepfake Speech Dataset",
        "rating": "-1",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Deepfake"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.SD"
            ]
        ],
        "abstract": "Advancements in artificial intelligence and machine learning have significantly improved synthetic speech generation. This paper explores diffusion models, a novel method for creating realistic synthetic speech. We create a diffusion dataset using available tools and pretrained models. Additionally, this study assesses the quality of diffusion-generated deepfakes versus non-diffusion ones and their potential threat to current deepfake detection systems. Findings indicate that the detection of diffusion-based deepfakes is generally comparable to non-diffusion deepfakes, with some variability based on detector architecture. Re-vocoding with diffusion vocoders shows minimal impact, and the overall speech quality is comparable to non-diffusion methods.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.LG",
            "cs.SD"
        ],
        "comment": "Presented at International Conference of the Biometrics Special Interest Group (BIOSIG 2024)"
    },
    {
        "paper id": "2410.06825",
        "abstract url": "https://arxiv.org/abs/2410.06825",
        "title": "K-SAM: A Prompting Method Using Pretrained U-Net to Improve Zero Shot Performance of SAM on Lung Segmentation in CXR Images",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "X-ray",
                "clinical"
            ],
            [
                "cs.LG",
                "eess.IV"
            ]
        ],
        "abstract": "In clinical procedures, precise localization of the target area is an essential step for clinical diagnosis and screening. For many diagnostic applications, lung segmentation of chest X-ray images is an essential first step that significantly reduces the image size to speed up the subsequent analysis. One of the primary difficulties with this task is segmenting the lung regions covered by dense abnormalities also known as opacities due to diseases like pneumonia and tuberculosis. SAM has astonishing generalization capabilities for category agnostic segmentation. In this study we propose an algorithm to improve zero shot performance of SAM on lung region segmentation task by automatic prompt selection. Two separate UNet models were trained, one for predicting lung segments and another for heart segment. Though these predictions lack fine details around the edges, they provide positive and negative points as prompt for SAM. Using proposed prompting method zero shot performance of SAM is evaluated on two benchmark datasets. ViT-l version of the model achieved slightly better performance compared to other two versions, ViTh and ViTb. It yields an average Dice score of 95.5 percent and 94.9 percent on hold out data for two datasets respectively. Though, for most of the images, SAM did outstanding segmentation, its prediction was way off for some of the images. After careful inspection it is found that all of these images either had extreme abnormality or distorted shape. Unlike most of the research performed so far on lung segmentation from CXR images using SAM, this study proposes a fully automated prompt selection process only from the input image. Our finding indicates that using pretrained models for prompt selection can utilize SAM impressive generalization capability to its full extent.",
        "subjects": [
            "eess.IV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06845",
        "abstract url": "https://arxiv.org/abs/2410.06845",
        "title": "MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders",
        "rating": "-1",
        "keywords": [
            [
                "biomedicalQA",
                "Health",
                "Diagnosis",
                "disease"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Mental health disorders are one of the most serious diseases in the world. Most people with such a disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health disorders. However, in the mental health domain, privacy concerns limit the accessibility of personalized treatment data, making it challenging to build powerful models. In this paper, we introduce MentalArena, a self-play framework to train language models by generating domain-specific personalized data, where we obtain a better model capable of making a personalized diagnosis and treatment (as a therapist) and providing information (as a patient). To accurately model human-like mental health patients, we devise Symptom Encoder, which simulates a real patient from both cognition and behavior perspectives. To address intent bias during patient-therapist interactions, we propose Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and dynamically manage the dialogue between patient and therapist according to the identified deviations. We evaluated MentalArena against 6 benchmarks, including biomedicalQA and mental health tasks, compared to 6 advanced models. Our models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform their counterparts, including GPT-4o. We hope that our work can inspire future research on personalized care. Code is available in https://github.com/Scarelette/MentalArena/tree/main",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.MA"
        ],
        "comment": "Technical Report; 27 pages"
    },
    {
        "paper id": "2410.06852",
        "abstract url": "https://arxiv.org/abs/2410.06852",
        "title": "Safe Reinforcement Learning Filter for Multicopter Collision-Free Tracking under disturbances",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "This paper proposes a safe reinforcement learning filter (SRLF) to realize multicopter collision-free trajectory tracking with input disturbance. A novel robust control barrier function (RCBF) with its analysis techniques is introduced to avoid collisions with unknown disturbances during tracking. To ensure the system state remains within the safe set, the RCBF gain is designed in control action. A safety filter is introduced to transform unsafe reinforcement learning (RL) control inputs into safe ones, allowing RL training to proceed without explicitly considering safety constraints. The SRLF obtains rigorous guaranteed safe control action by solving a quadratic programming (QP) problem that incorporates forward invariance of RCBF and input saturation constraints. Both simulation and real-world experiments on multicopters demonstrate the effectiveness and excellent performance of SRLF in achieving collision-free tracking under input disturbances and saturation.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06854",
        "abstract url": "https://arxiv.org/abs/2410.06854",
        "title": "Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "depth"
            ]
        ],
        "abstract": "Computer-Generated Holography (CGH) is a set of algorithmic methods for identifying holograms that reconstruct Three-Dimensi-onal (3D) scenes in holographic displays. CGH algorithms decompose 3D scenes into multiplanes at different depth levels and rely on simulations of light that propagated from a source plane to a targeted plane. Thus, for n planes, CGH typically optimizes holograms using n plane-to-plane light transport simulations, leading to major time and computational demands. Our work replaces multiple planes with a focal surface and introduces a learned light transport model that could propagate a light field from a source plane to the focal surface in a single inference. Our learned light transport model leverages spatially adaptive convolution to achieve depth-varying propagation demanded by targeted focal surfaces. The proposed model reduces the hologram optimization process up to 1.5x, which contributes to hologram dataset generation and the training of future learned CGH models.",
        "subjects": [
            "cs.GR",
            "cs.HC"
        ],
        "comment": "SIGGRAPH Asia 2024 Technical Communications"
    },
    {
        "paper id": "2410.06855",
        "abstract url": "https://arxiv.org/abs/2410.06855",
        "title": "RIS-Assisted ISAC: Precoding and Phase-Shift Optimization for Mono-Static Target Detection",
        "rating": "-1",
        "keywords": [
            [
                "radar"
            ]
        ],
        "abstract": "The reconfigurable intelligent surface (RIS) technology emerges as a highly useful component of the rapidly evolving integrated sensing and communications paradigm, primarily owing to its remarkable signal-to-noise ratio enhancement capabilities. In this paper, our focus is on mono-static target detection while considering the communication requirement of a user equipment. Both sensing and communication benefit from the presence of an RIS, which makes the channels richer and stronger. Diverging from prior research, we comprehensively examine three target echo paths: the direct (static) channel path, the path via the RIS, and a combination of these, each characterized by distinct radar cross sections (RCSs). We take both the line-of-sight (LOS) and the non-line-of-sight (NLOS) paths into account under a clutter for which the distribution is not known, but the low-rank subspace it resides. We derive the generalized likelihood ratio test (GLRT) detector and introduce a novel approach for jointly optimizing the configuration of RIS phase-shifts and precoding. Our simulation results underscore the paramount importance of this combined design in terms of enhancing detection probability. Moreover, it becomes evident that the derived clutter-aware target detection significantly enhances detection performance, especially when the clutter is strong.",
        "subjects": [
            "eess.SP",
            "cs.ET"
        ],
        "comment": "6 pages, 3 figures, accepted to be presented at IEEE GLOBECOM 2024"
    },
    {
        "paper id": "2410.06892",
        "abstract url": "https://arxiv.org/abs/2410.06892",
        "title": "Selecting the Best Sequential Transfer Path for Medical Image Segmentation with Limited Labeled Data",
        "rating": "-1",
        "keywords": [
            [
                "Medical",
                "MRI"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The medical image processing field often encounters the critical issue of scarce annotated data. Transfer learning has emerged as a solution, yet how to select an adequate source task and effectively transfer the knowledge to the target task remains challenging. To address this, we propose a novel sequential transfer scheme with a task affinity metric tailored for medical images. Considering the characteristics of medical image segmentation tasks, we analyze the image and label similarity between tasks and compute the task affinity scores, which assess the relatedness among tasks. Based on this, we select appropriate source tasks and develop an effective sequential transfer strategy by incorporating intermediate source tasks to gradually narrow the domain discrepancy and minimize the transfer cost. Thereby we identify the best sequential transfer path for the given target task. Extensive experiments on three MRI medical datasets, FeTS 2022, iSeg-2019, and WMH, demonstrate the efficacy of our method in finding the best source sequence. Compared with directly transferring from a single source task, the sequential transfer results underline a significant improvement in target task performance, achieving an average of 2.58% gain in terms of segmentation Dice score, notably, 6.00% for FeTS 2022. Code is available at the git repository.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06917",
        "abstract url": "https://arxiv.org/abs/2410.06917",
        "title": "A structural description of Zykov and Blanche Descartes graphs",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "In 1949, Zykov proposed the first explicit construction of triangle-free graphs with arbitrarily large chromatic number. We define a Zykov graph as any induced subgraph of a graph created using Zykov's construction. We give a structural characterization of Zykov graphs based on a specific type of stable set, that we call splitting stable set. It implies that recognizing this class is NP-complete, while being FPT in the treewidth of the input graph. We provide similar results for the Blanche Descartes construction.",
        "subjects": [
            "math.CO",
            "cs.CC",
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06934",
        "abstract url": "https://arxiv.org/abs/2410.06934",
        "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and Computation Offloading Policies in Vehicular Edge Networks",
        "rating": "-1",
        "keywords": [
            [
                "trajectory",
                "vehicle"
            ]
        ],
        "abstract": "Computer simulation platforms offer an alternative solution by emulating complex systems in a controlled manner. However, existing Edge Computing (EC) simulators, as well as general-purpose vehicular network simulators, are not tailored for VEC and lack dedicated support for modeling the distinct access pattern, entity mobility trajectory and other unique characteristics of VEC networks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation platform for in-depth evaluation and analysis of various service caching and computation offloading policies in VEC networks. VEC-Sim incorporates realistic mechanisms to replicate real-world access patterns, including service feature vector, vehicle mobility modeling, evolving service popularity, new service upload and user preference shifts, etc. Moreover, its modular architecture and extensive Application Programming Interfaces (APIs) allow seamless integration of customized scheduling policies and user-defined metrics. A comprehensive evaluation of VEC-Sim's capabilities is undertaken in comparison to real-world ground truths. Results prove it to be accurate in reproducing classical scheduling algorithms and extremely effective in conducting case studies.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06963",
        "abstract url": "https://arxiv.org/abs/2410.06963",
        "title": "ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling",
        "rating": "-1",
        "keywords": [
            [
                "point cloud",
                "skeleton"
            ],
            [
                "LiDAR"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "This paper introduces ELMO, a real-time upsampling motion capture framework designed for a single LiDAR sensor. Modeled as a conditional autoregressive transformer-based upsampling motion generator, ELMO achieves 60 fps motion capture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is the coupling of the self-attention mechanism with thoughtfully designed embedding modules for motion and point clouds, significantly elevating the motion quality. To facilitate accurate motion capture, we develop a one-time skeleton calibration model capable of predicting user skeleton offsets from a single-frame point cloud. Additionally, we introduce a novel data augmentation technique utilizing a LiDAR simulator, which enhances global root tracking to improve environmental understanding. To demonstrate the effectiveness of our method, we compare ELMO with state-of-the-art methods in both image-based and point cloud-based motion capture. We further conduct an ablation study to validate our design principles. ELMO's fast inference time makes it well-suited for real-time applications, exemplified in our demo video featuring live streaming and interactive gaming scenarios. Furthermore, we contribute a high-quality LiDAR-mocap synchronized dataset comprising 20 different subjects performing a range of motions, which can serve as a valuable resource for future research. The dataset and evaluation code are available at {\\blue \\url{https://movin3d.github.io/ELMO_SIGASIA2024/}}",
        "subjects": [
            "cs.GR",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "published at ACM Transactions on Graphics (Proc. SIGGRAPH ASIA), 2024"
    },
    {
        "paper id": "2410.06974",
        "abstract url": "https://arxiv.org/abs/2410.06974",
        "title": "Diagnosis of Malignant Lymphoma Cancer Using Hybrid Optimized Techniques Based on Dense Neural Networks",
        "rating": "-1",
        "keywords": [
            [
                "biopsy",
                "Diagnosis",
                "Cancer",
                "clinical"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Lymphoma diagnosis, particularly distinguishing between subtypes, is critical for effective treatment but remains challenging due to the subtle morphological differences in histopathological images. This study presents a novel hybrid deep learning framework that combines DenseNet201 for feature extraction with a Dense Neural Network (DNN) for classification, optimized using the Harris Hawks Optimization (HHO) algorithm. The model was trained on a dataset of 15,000 biopsy images, spanning three lymphoma subtypes: Chronic Lymphocytic Leukemia (CLL), Follicular Lymphoma (FL), and Mantle Cell Lymphoma (MCL). Our approach achieved a testing accuracy of 99.33\\%, demonstrating significant improvements in both accuracy and model interpretability. Comprehensive evaluation using precision, recall, F1-score, and ROC-AUC underscores the model's robustness and potential for clinical adoption. This framework offers a scalable solution for improving diagnostic accuracy and efficiency in oncology.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "6 pages, 5 figures, 4 tables, IEEE ICCA"
    },
    {
        "paper id": "2410.07001",
        "abstract url": "https://arxiv.org/abs/2410.07001",
        "title": "WebTigerPython -- A Low-Floor High-Ceiling Python IDE for the Browser",
        "rating": "-1",
        "keywords": [
            [
                "robotics"
            ]
        ],
        "abstract": "With the large diversity of platforms and devices used by students, web applications increasingly suggest themselves as the solution of choice. Developing adequate educational programming environments in the browser, however, remains a challenge and often involves trade-offs between desired functionalities and navigating the limitations of web applications, in particular the blocking single-threaded execution model. We introduce a fully browser-based Python programming environment that explores the possibilities and demonstrates that a web application can indeed support a rich and mature set of features, ranging from Turtle graphics over educational robotics to data processing.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07009",
        "abstract url": "https://arxiv.org/abs/2410.07009",
        "title": "Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based Outline-guided Generation",
        "rating": "-1",
        "keywords": [
            [
                "Patent"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The patent domain is gaining attention in natural language processing research, offering practical applications in streamlining the patenting process and providing challenging benchmarks for large language models (LLMs). However, the generation of the description sections of patents, which constitute more than 90% of the patent document, has not been studied to date. We address this gap by introducing the task of outline-guided paper-to-patent generation, where an academic paper provides the technical specification of the invention and an outline conveys the desired patent structure. We present PAP2PAT, a new challenging benchmark of 1.8k patent-paper pairs with document outlines, collected using heuristics that reflect typical research lab practices. Our experiments with current open-weight LLMs and outline-guided chunk-based generation show that they can effectively use information from the paper but struggle with repetitions, likely due to the inherent repetitiveness of patent language. We release our data and code.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07022",
        "abstract url": "https://arxiv.org/abs/2410.07022",
        "title": "Exploiting Distribution Constraints for Scalable and Efficient Image Retrieval",
        "rating": "-1",
        "keywords": [
            [
                "robotics",
                "robot"
            ]
        ],
        "abstract": "Image retrieval is crucial in robotics and computer vision, with downstream applications in robot place recognition and vision-based product recommendations. Modern retrieval systems face two key challenges: scalability and efficiency. State-of-the-art image retrieval systems train specific neural networks for each dataset, an approach that lacks scalability. Furthermore, since retrieval speed is directly proportional to embedding size, existing systems that use large embeddings lack efficiency. To tackle scalability, recent works propose using off-the-shelf foundation models. However, these models, though applicable across datasets, fall short in achieving performance comparable to that of dataset-specific models. Our key observation is that, while foundation models capture necessary subtleties for effective retrieval, the underlying distribution of their embedding space can negatively impact cosine similarity searches. We introduce Autoencoders with Strong Variance Constraints (AE-SVC), which, when used for projection, significantly improves the performance of foundation models. We provide an in-depth theoretical analysis of AE-SVC. Addressing efficiency, we introduce Single-shot Similarity Space Distillation ((SS)$_2$D), a novel approach to learn embeddings with adaptive sizes that offers a better trade-off between size and performance. We conducted extensive experiments on four retrieval datasets, including Stanford Online Products (SoP) and Pittsburgh30k, using four different off-the-shelf foundation models, including DinoV2 and CLIP. AE-SVC demonstrates up to a $16\\%$ improvement in retrieval performance, while (SS)$_2$D shows a further $10\\%$ improvement for smaller embedding sizes.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07060",
        "abstract url": "https://arxiv.org/abs/2410.07060",
        "title": "Token sliding independent set reconfiguration on block graphs",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "Let $S$ be an independent set of a simple undirected graph $G$. Suppose that each vertex of $S$ has a token placed on it. The tokens are allowed to be moved, one at a time, by sliding along the edges of $G$, so that after each move, the vertices having tokens always form an independent set of $G$. We would like to determine whether the tokens can be eventually brought to stay on the vertices of another independent set $S'$ of $G$ in this manner. In other words, we would like to decide if we can transform $S$ into $S'$ through a sequence of steps, each of which involves substituting a vertex in the current independent set with one of its neighbours to obtain another independent set. This problem of determining if one independent set of a graph ``is reachable'' from another independent set of it is known to be PSPACE-hard even for split graphs, planar graphs, and graphs of bounded treewidth. Polynomial time algorithms have been obtained for certain graph classes like trees, interval graphs, claw-free graphs, and bipartite permutation graphs. We present a polynomial time algorithm for the problem on block graphs, which are the graphs in which every maximal 2-connected subgraph is a clique. Our algorithm is the first generalization of the known polynomial time algorithm for trees to a larger class of graphs (note that trees form a proper subclass of block graphs).",
        "subjects": [
            "cs.DM"
        ],
        "comment": "36 pages, 5 figures"
    },
    {
        "paper id": "2410.07076",
        "abstract url": "https://arxiv.org/abs/2410.07076",
        "title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses",
        "rating": "-1",
        "keywords": [
            [
                "Chemistry"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Scientific discovery contributes largely to human society's prosperity, and recent progress shows that LLMs could potentially catalyze this process. However, it is still unclear whether LLMs can discover novel and valid hypotheses in chemistry. In this work, we investigate this central research question: Can LLMs automatically discover novel and valid chemistry research hypotheses given only a chemistry research background (consisting of a research question and/or a background survey), without limitation on the domain of the research question? After extensive discussions with chemistry experts, we propose an assumption that a majority of chemistry hypotheses can be resulted from a research background and several inspirations. With this key insight, we break the central question into three smaller fundamental questions. In brief, they are: (1) given a background question, whether LLMs can retrieve good inspirations; (2) with background and inspirations, whether LLMs can lead to hypothesis; and (3) whether LLMs can identify good hypotheses to rank them higher. To investigate these questions, we construct a benchmark consisting of 51 chemistry papers published in Nature, Science, or a similar level in 2024 (all papers are only available online since 2024). Every paper is divided by chemistry PhD students into three components: background, inspirations, and hypothesis. The goal is to rediscover the hypothesis, given only the background and a large randomly selected chemistry literature corpus consisting the ground truth inspiration papers, with LLMs trained with data up to 2023. We also develop an LLM-based multi-agent framework that leverages the assumption, consisting of three stages reflecting the three smaller questions. The proposed method can rediscover many hypotheses with very high similarity with the ground truth ones, covering the main innovations.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Code and Benchmark are available at https://github.com/ZonglinY/MOOSE-Chem.git"
    },
    {
        "paper id": "2410.07078",
        "abstract url": "https://arxiv.org/abs/2410.07078",
        "title": "FlowBotHD: History-Aware Diffuser Handling Ambiguities in Articulated Objects Manipulation",
        "rating": "-1",
        "keywords": [
            [
                "diffusion"
            ]
        ],
        "abstract": "We introduce a novel approach to manipulate articulated objects with ambiguities, such as opening a door, in which multi-modality and occlusions create ambiguities about the opening side and direction. Multi-modality occurs when the method to open a fully closed door (push, pull, slide) is uncertain, or the side from which it should be opened is uncertain. Occlusions further obscure the door's shape from certain angles, creating further ambiguities during the occlusion. To tackle these challenges, we propose a history-aware diffusion network that models the multi-modal distribution of the articulated object and uses history to disambiguate actions and make stable predictions under occlusions. Experiments and analysis demonstrate the state-of-art performance of our method and specifically improvements in ambiguity-caused failure modes. Our project website is available at https://flowbothd.github.io/.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted to CoRL 2024"
    },
    {
        "paper id": "2410.07080",
        "abstract url": "https://arxiv.org/abs/2410.07080",
        "title": "Gaussian to log-normal transition for independent sets in a percolated hypercube",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "Independent sets in graphs, i.e., subsets of vertices where no two are adjacent, have long been studied, for instance as a model of hard-core gas. The $d$-dimensional hypercube, $\\{0,1\\}^d$, with the nearest neighbor structure, has been a particularly appealing choice for the base graph, owing in part to its many symmetries. Results go back to the work of Korshunov and Sapozhenko who proved sharp results on the count of such sets as well as structure theorems for random samples drawn uniformly. Of much interest is the behavior of such Gibbs measures in the presence of disorder. In this direction, Kronenberg and Spinka [KS] initiated the study of independent sets in a random subgraph of the hypercube obtained by considering an instance of bond percolation with probability $p$. Relying on tools from statistical mechanics they obtained a detailed understanding of the moments of the partition function, say $\\mathcal{Z}$, of the hard-core model on such random graphs and consequently deduced certain fluctuation information, as well as posed a series of interesting questions. In particular, they showed in the uniform case that there is a natural phase transition at $p=2/3$ where $\\mathcal{Z}$ transitions from being concentrated for $p>2/3$ to not concentrated at $p=2/3$. In this article, developing a probabilistic framework, as well as relying on certain cluster expansion inputs from [KS], we present a detailed picture of both the fluctuations of $\\mathcal{Z}$ as well as the geometry of a randomly sampled independent set. In particular, we establish that $\\mathcal{Z}$, properly centered and scaled, converges to a standard Gaussian for $p>2/3$, and to a sum of two i.i.d. log-normals at $p=2/3$. A particular step in the proof which could be of independent interest involves a non-uniform birthday problem for which collisions emerge at $p=2/3$.",
        "subjects": [
            "math.PR",
            "cond-mat.stat-mech",
            "cs.DM",
            "math-ph",
            "math.CO"
        ],
        "comment": "35 pages, 1 figure. Abstract shortened to meet arXiv requirements"
    },
    {
        "paper id": "2410.07129",
        "abstract url": "https://arxiv.org/abs/2410.07129",
        "title": "Mental Disorders Detection in the Era of Large Language Models",
        "rating": "-1",
        "keywords": [
            [
                "clinical"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "This paper compares the effectiveness of traditional machine learning methods, encoder-based models, and large language models (LLMs) on the task of detecting depression and anxiety. Five datasets were considered, each differing in format and the method used to define the target pathology class. We tested AutoML models based on linguistic features, several variations of encoder-based Transformers such as BERT, and state-of-the-art LLMs as pathology classification models. The results demonstrated that LLMs outperform traditional methods, particularly on noisy and small datasets where training examples vary significantly in text length and genre. However, psycholinguistic features and encoder-based models can achieve performance comparable to language models when trained on texts from individuals with clinically confirmed depression, highlighting their potential effectiveness in targeted clinical applications.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07131",
        "abstract url": "https://arxiv.org/abs/2410.07131",
        "title": "Stochastic Process Turing Machines",
        "rating": "-1",
        "keywords": [
            [
                "depth"
            ]
        ],
        "abstract": "Computer science theory provides many different measures of complexity of a system including Kolmogorov complexity, logical depth, computational depth, and Levin complexity. However, these measures are all defined only for deterministic Turing machines, i.e., deterministic dynamics of the underlying generative process whose output we are interested in. Therefore, by construction they cannot capture complexity of the output of stochastic processes - like those in the real world. Motivated by this observation, we combine probabilistic Turing machines with a prior over the inputs to the Turing machine to define a complete stochastic process of Turing machines. We call this a stochastic process Turing machine. Stochastic process Turing machines allow us to formalize a stochastic generalization of logical depth called stochastic depth, and also to apply stochastic thermodynamics to the analysis of Turing machines. Stochastic process Turing machines and stochastic depth allow us to study the complex, stochastic systems like the human brain, societies, and evolution all from within the framework of formal computation.",
        "subjects": [
            "cs.CC",
            "cs.FL",
            "cs.LO"
        ],
        "comment": "30 pages. arXiv admin note: text overlap with arXiv:2404.16050, arXiv:1905.05669"
    },
    {
        "paper id": "2410.07163",
        "abstract url": "https://arxiv.org/abs/2410.07163",
        "title": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning",
        "rating": "-1",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "In this work, we address the problem of large language model (LLM) unlearning, aiming to remove unwanted data influences and associated model capabilities (e.g., copyrighted data or harmful content generation) while preserving essential model utilities, without the need for retraining from scratch. Despite the growing need for LLM unlearning, a principled optimization framework remains lacking. To this end, we revisit the state-of-the-art approach, negative preference optimization (NPO), and identify the issue of reference model bias, which could undermine NPO's effectiveness, particularly when unlearning forget data of varying difficulty. Given that, we propose a simple yet effective unlearning optimization framework, called SimNPO, showing that 'simplicity' in removing the reliance on a reference model (through the lens of simple preference optimization) benefits unlearning. We also provide deeper insights into SimNPO's advantages, supported by analysis using mixtures of Markov chains. Furthermore, we present extensive experiments validating SimNPO's superiority over existing unlearning baselines in benchmarks like TOFU and MUSE, and robustness against relearning attacks. Codes are available at https://github.com/OPTML-Group/Unlearn-Simple.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07169",
        "abstract url": "https://arxiv.org/abs/2410.07169",
        "title": "VIRT: Vision Instructed Transformer for Robotic Manipulation",
        "rating": "-1",
        "keywords": [
            [
                "robot",
                "Robotic Manipulation"
            ]
        ],
        "abstract": "Robotic manipulation, owing to its multi-modal nature, often faces significant training ambiguity, necessitating explicit instructions to clearly delineate the manipulation details in tasks. In this work, we highlight that vision instruction is naturally more comprehensible to recent robotic policies than the commonly adopted text instruction, as these policies are born with some vision understanding ability like human infants. Building on this premise and drawing inspiration from cognitive science, we introduce the robotic imagery paradigm, which realizes large-scale robotic data pre-training without text annotations. Additionally, we propose the robotic gaze strategy that emulates the human eye gaze mechanism, thereby guiding subsequent actions and focusing the attention of the policy on the manipulated object. Leveraging these innovations, we develop VIRT, a fully Transformer-based policy. We design comprehensive tasks using both a physical robot and simulated environments to assess the efficacy of VIRT. The results indicate that VIRT can complete very competitive tasks like ``opening the lid of a tightly sealed bottle'', and the proposed techniques boost the success rates of the baseline policy on diverse challenging tasks from nearly 0% to more than 65%.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07274",
        "abstract url": "https://arxiv.org/abs/2410.07274",
        "title": "Mitigation of gender bias in automatic facial non-verbal behaviors generation",
        "rating": "-1",
        "keywords": [
            [
                "facial"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Research on non-verbal behavior generation for social interactive agents focuses mainly on the believability and synchronization of non-verbal cues with speech. However, existing models, predominantly based on deep learning architectures, often perpetuate biases inherent in the training data. This raises ethical concerns, depending on the intended application of these agents. This paper addresses these issues by first examining the influence of gender on facial non-verbal behaviors. We concentrate on gaze, head movements, and facial expressions. We introduce a classifier capable of discerning the gender of a speaker from their non-verbal cues. This classifier achieves high accuracy on both real behavior data, extracted using state-of-the-art tools, and synthetic data, generated from a model developed in previous work.Building upon this work, we present a new model, FairGenderGen, which integrates a gender discriminator and a gradient reversal layer into our previous behavior generation model. This new model generates facial non-verbal behaviors from speech features, mitigating gender sensitivity in the generated behaviors. Our experiments demonstrate that the classifier, developed in the initial phase, is no longer effective in distinguishing the gender of the speaker from the generated non-verbal behaviors.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.HC",
            "cs.LG",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07297",
        "abstract url": "https://arxiv.org/abs/2410.07297",
        "title": "Autonomous Navigation and Collision Avoidance for Mobile Robots: Classification and Review",
        "rating": "-1",
        "keywords": [
            [
                "Navigation"
            ]
        ],
        "abstract": "This paper introduces a novel classification for Autonomous Mobile Robots (AMRs), into three phases and five steps, focusing on autonomous collision-free navigation. Additionally, it presents the main methods and widely accepted technologies for each phase of the proposed classification. The purpose of this classification is to facilitate understanding and establish connections between the independent input variables of the system (hardware, software) and autonomous navigation. By analyzing well-established technologies in terms of sensors and methods used for autonomous navigation, this paper aims to provide a foundation of knowledge that can be applied in future projects of mobile robots.",
        "subjects": [
            "cs.RO",
            "cs.SE"
        ],
        "comment": "This paper was presented at the JAR Congress in Buenos Aires, Argentina, and published as ID 27 at 9:20 on June 5, 2024. You can find more details on the conference at the following link: https://jar.com.ar/programa.html#programa. Additionally, the content of the presentation was re-recorded and uploaded to YouTube for better understanding: https://www.youtube.com/watch?v=TU6EkT43VfE&t=4s"
    },
    {
        "paper id": "2410.07299",
        "abstract url": "https://arxiv.org/abs/2410.07299",
        "title": "Towards Generalisable Time Series Understanding Across Domains",
        "rating": "-1",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "In natural language processing and computer vision, self-supervised pre-training on large datasets unlocks foundational model capabilities across domains and tasks. However, this potential has not yet been realised in time series analysis, where existing methods disregard the heterogeneous nature of time series characteristics. Time series are prevalent in many domains, including medicine, engineering, natural sciences, and finance, but their characteristics vary significantly in terms of variate count, inter-variate relationships, temporal dynamics, and sampling frequency. This inherent heterogeneity across domains prevents effective pre-training on large time series corpora. To address this issue, we introduce OTiS, an open model for general time series analysis, that has been specifically designed to handle multi-domain heterogeneity. We propose a novel pre-training paradigm including a tokeniser with learnable domain-specific signatures, a dual masking strategy to capture temporal causality, and a normalised cross-correlation loss to model long-range dependencies. Our model is pre-trained on a large corpus of 640,187 samples and 11 billion time points spanning 8 distinct domains, enabling it to analyse time series from any (unseen) domain. In comprehensive experiments across 15 diverse applications - including classification, regression, and forecasting - OTiS showcases its ability to accurately capture domain-specific data characteristics and demonstrates its competitiveness against state-of-the-art baselines. Our code and pre-trained weights are publicly available at https://github.com/oetu/otis.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07336",
        "abstract url": "https://arxiv.org/abs/2410.07336",
        "title": "Positive-Augmented Contrastive Learning for Vision-and-Language Evaluation and Training",
        "rating": "-1",
        "keywords": [
            [
                "grammatical"
            ],
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Despite significant advancements in caption generation, existing evaluation metrics often fail to capture the full quality or fine-grained details of captions. This is mainly due to their reliance on non-specific human-written references or noisy pre-training data. Still, finding an effective metric is crucial not only for captions evaluation but also for the generation phase. Metrics can indeed play a key role in the fine-tuning stage of captioning models, ultimately enhancing the quality of the generated captions. In this paper, we propose PAC-S++, a learnable metric that leverages the CLIP model, pre-trained on both web-collected and cleaned data and regularized through additional pairs of generated visual and textual positive samples. Exploiting this stronger and curated pre-training, we also apply PAC-S++ as a reward in the Self-Critical Sequence Training (SCST) stage typically employed to fine-tune captioning models. Extensive experiments on different image and video datasets highlight the effectiveness of PAC-S++ compared to popular metrics for the task, including its sensitivity to object hallucinations. Furthermore, we show that integrating PAC-S++ into the fine-tuning stage of a captioning model results in semantically richer captions with fewer repetitions and grammatical errors. Evaluations on out-of-domain benchmarks further demonstrate the efficacy of our fine-tuning approach in enhancing model capabilities. Source code and trained models are publicly available at: https://github.com/aimagelab/pacscore.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07413",
        "abstract url": "https://arxiv.org/abs/2410.07413",
        "title": "A Rapid Trajectory Optimization and Control Framework for Resource-Constrained Applications",
        "rating": "-1",
        "keywords": [
            [
                "Trajectory"
            ]
        ],
        "abstract": "This paper presents a computationally efficient model predictive control formulation that uses an integral Chebyshev collocation method to enable rapid operations of autonomous agents. By posing the finite-horizon optimal control problem and recursive re-evaluation of the optimal trajectories, minimization of the L2 norms of the state and control errors are transcribed into a quadratic program. Control and state variable constraints are parameterized using Chebyshev polynomials and are accommodated in the optimal trajectory generation programs to incorporate the actuator limits and keepout constraints. Differentiable collision detection of polytopes is leveraged for optimal collision avoidance. Results obtained from the collocation methods are benchmarked against the existing approaches on an edge computer to outline the performance improvements. Finally, collaborative control scenarios involving multi-agent space systems are considered to demonstrate the technical merits of the proposed work.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": "This work has been submitted to the IEEE ACC 2025 for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2410.07419",
        "abstract url": "https://arxiv.org/abs/2410.07419",
        "title": "Reconfigurations of Plane Caterpillars and Paths",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "Let $S$ be a point set in the plane, $\\mathcal{P}(S)$ and $\\mathcal{C}(S)$ sets of all plane spanning paths and caterpillars on $S$. We study reconfiguration operations on $\\mathcal{P}(S)$ and $\\mathcal{C}(S)$. In particular, we prove that all of the commonly studied reconfigurations on plane spanning trees still yield connected reconfiguration graphs for caterpillars when $S$ is in convex position. If $S$ is in general position, we show that the rotation, compatible flip and flip graphs of $\\mathcal{C}(S)$ are connected while the slide graph is disconnected. For paths, we prove the existence of a connected component of size at least $2^{n-1}$ and that no component of size at most $7$ can exist in the flip graph on $\\mathcal{P}(S)$.",
        "subjects": [
            "math.CO",
            "cs.CG"
        ],
        "comment": "17 pages, 13 figures"
    },
    {
        "paper id": "2410.07447",
        "abstract url": "https://arxiv.org/abs/2410.07447",
        "title": "TinyLidarNet: 2D LiDAR-based End-to-End Deep Learning Model for F1TENTH Autonomous Racing",
        "rating": "-1",
        "keywords": [
            [
                "LiDAR",
                "vehicle"
            ],
            [
                "navigation"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Prior research has demonstrated the effectiveness of end-to-end deep learning for robotic navigation, where the control signals are directly derived from raw sensory data. However, the majority of existing end-to-end navigation solutions are predominantly camera-based. In this paper, we introduce TinyLidarNet, a lightweight 2D LiDAR-based end-to-end deep learning model for autonomous racing. An F1TENTH vehicle using TinyLidarNet won 3rd place in the 12th F1TENTH Autonomous Grand Prix competition, demonstrating its competitive performance. We systematically analyze its performance on untrained tracks and computing requirements for real-time processing. We find that TinyLidarNet's 1D Convolutional Neural Network (CNN) based architecture significantly outperforms widely used Multi-Layer Perceptron (MLP) based architecture. In addition, we show that it can be processed in real-time on low-end micro-controller units (MCUs).",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07460",
        "abstract url": "https://arxiv.org/abs/2410.07460",
        "title": "Generalizing Segmentation Foundation Model Under Sim-to-real Domain-shift for Guidewire Segmentation in X-ray Fluoroscopy",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "X-ray",
                "Cardiac"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Guidewire segmentation during endovascular interventions holds the potential to significantly enhance procedural accuracy, improving visualization and providing critical feedback that can support both physicians and robotic systems in navigating complex vascular pathways. Unlike supervised segmentation networks, which need many expensive expert-annotated labels, sim-to-real domain adaptation approaches utilize synthetic data from simulations, offering a cost-effective solution. The success of models like Segment-Anything (SAM) has driven advancements in image segmentation foundation models with strong zero/few-shot generalization through prompt engineering. However, they struggle with medical images like X-ray fluoroscopy and the domain-shifts of the data. Given the challenges of acquiring annotation and the accessibility of labeled simulation data, we propose a sim-to-real domain adaption framework with a coarse-to-fine strategy to adapt SAM to X-ray fluoroscopy guidewire segmentation without any annotation on the target domain. We first generate the pseudo-labels by utilizing a simple source image style transfer technique that preserves the guidewire structure. Then, we develop a weakly supervised self-training architecture to fine-tune an end-to-end student SAM with the coarse labels by imposing consistency regularization and supervision from the teacher SAM network. We validate the effectiveness of the proposed method on a publicly available Cardiac dataset and an in-house Neurovascular dataset, where our method surpasses both pre-trained SAM and many state-of-the-art domain adaptation techniques by a large margin. Our code will be made public on GitHub soon.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07490",
        "abstract url": "https://arxiv.org/abs/2410.07490",
        "title": "MoDEM: Mixture of Domain Expert Models",
        "rating": "-1",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "We propose a novel approach to enhancing the performance and efficiency of large language models (LLMs) by combining domain prompt routing with domain-specialized models. We introduce a system that utilizes a BERT-based router to direct incoming prompts to the most appropriate domain expert model. These expert models are specifically tuned for domains such as health, mathematics and science. Our research demonstrates that this approach can significantly outperform general-purpose models of comparable size, leading to a superior performance-to-cost ratio across various benchmarks. The implications of this study suggest a potential paradigm shift in LLM development and deployment. Rather than focusing solely on creating increasingly large, general-purpose models, the future of AI may lie in developing ecosystems of smaller, highly specialized models coupled with sophisticated routing systems. This approach could lead to more efficient resource utilization, reduced computational costs, and superior overall performance.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07494",
        "abstract url": "https://arxiv.org/abs/2410.07494",
        "title": "G$^{2}$TR: Generalized Grounded Temporal Reasoning for Robot Instruction Following by Combining Large Pre-trained Models",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "Consider the scenario where a human cleans a table and a robot observing the scene is instructed with the task \"Remove the cloth using which I wiped the table\". Instruction following with temporal reasoning requires the robot to identify the relevant past object interaction, ground the object of interest in the present scene, and execute the task according to the human's instruction. Directly grounding utterances referencing past interactions to grounded objects is challenging due to the multi-hop nature of references to past interactions and large space of object groundings in a video stream observing the robot's workspace. Our key insight is to factor the temporal reasoning task as (i) estimating the video interval associated with event reference, (ii) performing spatial reasoning over the interaction frames to infer the intended object (iii) semantically track the object's location till the current scene to enable future robot interactions. Our approach leverages existing large pre-trained models (which possess inherent generalization capabilities) and combines them appropriately for temporal grounding tasks. Evaluation on a video-language corpus acquired with a robot manipulator displaying rich temporal interactions in spatially-complex scenes displays an average accuracy of 70.10%. The dataset, code, and videos are available at https://reail-iitdelhi.github.io/temporalreasoning.github.io/ .",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07503",
        "abstract url": "https://arxiv.org/abs/2410.07503",
        "title": "Modeling Alzheimer's Disease: From Memory Loss to Plaque & Tangles Formation",
        "rating": "-1",
        "keywords": [
            [
                "biochemical",
                "Disease"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "We employ the Hopfield model as a simplified framework to explore both the memory deficits and the biochemical processes characteristic of Alzheimer's disease. By simulating neuronal death and synaptic degradation through increasing the number of stored patterns and introducing noise into the synaptic weights, we demonstrate hallmark symptoms of dementia, including memory loss, confusion, and delayed retrieval times. As the network's capacity is exceeded, retrieval errors increase, mirroring the cognitive confusion observed in Alzheimer's patients. Additionally, we simulate the impact of synaptic degradation by varying the sparsity of the weight matrix, showing impaired memory recall and reduced retrieval success as noise levels increase. Furthermore, we extend our model to connect memory loss with biochemical processes linked to Alzheimer's. By simulating the role of reduced insulin sensitivity over time, we show how it can trigger increased calcium influx into mitochondria, leading to misfolded proteins and the formation of amyloid plaques. These findings, modeled over time, suggest that both neuronal degradation and metabolic factors contribute to the progressive decline seen in Alzheimer's disease. Our work offers a computational framework for understanding the dual impact of synaptic and metabolic dysfunction in neurodegenerative diseases.",
        "subjects": [
            "q-bio.NC",
            "cs.CV",
            "eess.IV"
        ],
        "comment": "8 pages, 4 figures"
    },
    {
        "paper id": "2410.07507",
        "abstract url": "https://arxiv.org/abs/2410.07507",
        "title": "Thought2Text: Text Generation from EEG Signal using Large Language Models (LLMs)",
        "rating": "-1",
        "keywords": [
            [
                "EEG"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Decoding and expressing brain activity in a comprehensible form is a challenging frontier in AI. This paper presents Thought2Text, which uses instruction-tuned Large Language Models (LLMs) fine-tuned with EEG data to achieve this goal. The approach involves three stages: (1) training an EEG encoder for visual feature extraction, (2) fine-tuning LLMs on image and text data, enabling multimodal description generation, and (3) further fine-tuning on EEG embeddings to generate text directly from EEG during inference. Experiments on a public EEG dataset collected for six subjects with image stimuli demonstrate the efficacy of multimodal LLMs (LLaMa-v3, Mistral-v0.3, Qwen2.5), validated using traditional language generation evaluation metrics, GPT-4 based assessments, and evaluations by human expert. This approach marks a significant advancement towards portable, low-cost \"thoughts-to-text\" technology with potential applications in both neuroscience and natural language processing (NLP).",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07518",
        "abstract url": "https://arxiv.org/abs/2410.07518",
        "title": "Exploring the Landscape of Distributed Graph Sketching",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "Recent work has initiated the study of dense graph processing using graph sketching methods, which drastically reduce space costs by lossily compressing information about the input graph. In this paper, we explore the strange and surprising performance landscape of sketching algorithms. We highlight both their surprising advantages for processing dense graphs that were previously prohibitively expensive to study, as well as the current limitations of the technique. Most notably, we show how sketching can avoid bottlenecks that limit conventional graph processing methods. Single-machine streaming graph processing systems are typically bottlenecked by CPU performance, and distributed graph processing systems are typically bottlenecked by network latency. We present Landscape, a distributed graph-stream processing system that uses linear sketching to distribute the CPU work of computing graph properties to distributed workers with no need for worker-to-worker communication. As a result, it overcomes the CPU and network bottlenecks that limit other systems. In fact, for the connected components problem, Landscape achieves a stream ingestion rate one-fourth that of maximum sustained RAM bandwidth, and is four times faster than random access RAM bandwidth. Additionally, we prove that for any sequence of graph updates and queries Landscape consumes at most a constant factor more network bandwidth than is required to receive the input stream. We show that this system can ingest up to 332 million stream updates per second on a graph with $2^{17}$ vertices. We show that it scales well with more distributed compute power: given a cluster of 40 distributed worker machines, it can ingest updates 35 times as fast as with 1 distributed worker machine. Landscape uses heuristics to reduce its query latency by up to four orders of magnitude over the prior state of the art.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07527",
        "abstract url": "https://arxiv.org/abs/2410.07527",
        "title": "Enhanced physics-informed neural networks (PINNs) for high-order power grid dynamics",
        "rating": "-1",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We develop improved physics-informed neural networks (PINNs) for high-order and high-dimensional power system models described by nonlinear ordinary differential equations. We propose some novel enhancements to improve PINN training and accuracy and also implement several other recently proposed ideas from the literature. We successfully apply these to study the transient dynamics of synchronous generators. We also make progress towards applying PINNs to advanced inverter models. Such enhanced PINNs can allow us to accelerate high-fidelity simulations needed to ensure a stable and reliable renewables-rich future grid.",
        "subjects": [
            "cs.LG",
            "eess.SY"
        ],
        "comment": "Accepted to the Tackling Climate Change with Machine Learning workshop at NeurIPS 2024"
    },
    {
        "paper id": "2410.07537",
        "abstract url": "https://arxiv.org/abs/2410.07537",
        "title": "Understanding the AI-powered Binary Code Similarity Detection",
        "rating": "-1",
        "keywords": [
            [
                "GNN"
            ]
        ],
        "abstract": "AI-powered binary code similarity detection (BinSD), which transforms intricate binary code comparison to the distance measure of code embedding through neural networks, has been widely applied to program analysis. However, due to the diversity of the adopted embedding strategies, evaluation methodologies, running environments, and/or benchmarks, it is difficult to quantitatively understand to what extent the BinSD problem has been solved, especially in realworld applications. Moreover, the lack of an in-depth investigation of the increasingly complex embedding neural networks and various evaluation methodologies has become the key factor hindering the development of AI-powered BinSD. To fill these research gaps, in this paper, we present a systematic evaluation of state-of-the-art AI-powered BinSD approaches by conducting a comprehensive comparison of BinSD systems on similar function detection and two downstream applications, namely vulnerability search and license violation detection. Building upon this evaluation, we perform the first investigation of embedding neural networks and evaluation methodologies. The experimental results yield several findings, which provide valuable insights in the BinSD domain, including (1) despite the GNN-based BinSD systems currently achieving the best performance in similar function detection, there still exists considerable space for improvements;(2) the capability of AI-powered BinSD approaches exhibits significant variation when applied to different downstream applications;(3) existing evaluation methodologies still need substantial adjustments. For instance, the evaluation metrics (such as the widely adopted ROC and AUC) usually fall short of accurately representing the model performance of the practical use in realworld scenarios. Based on the extensive experiments and analysis, we further provide several promising future research directions.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07540",
        "abstract url": "https://arxiv.org/abs/2410.07540",
        "title": "CoPESD: A Multi-Level Surgical Motion Dataset for Training Large Vision-Language Models to Co-Pilot Endoscopic Submucosal Dissection",
        "rating": "-1",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "robot"
            ],
            [
                "Surgical",
                "survival",
                "Endoscopic"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "submucosal dissection (ESD) enables rapid resection of large lesions, minimizing recurrence rates and improving long-term overall survival. Despite these advantages, ESD is technically challenging and carries high risks of complications, necessitating skilled surgeons and precise instruments. Recent advancements in Large Visual-Language Models (LVLMs) offer promising decision support and predictive planning capabilities for robotic systems, which can augment the accuracy of ESD and reduce procedural risks. However, existing datasets for multi-level fine-grained ESD surgical motion understanding are scarce and lack detailed annotations. In this paper, we design a hierarchical decomposition of ESD motion granularity and introduce a multi-level surgical motion dataset (CoPESD) for training LVLMs as the robotic \\textbf{Co}-\\textbf{P}ilot of \\textbf{E}ndoscopic \\textbf{S}ubmucosal \\textbf{D}issection. CoPESD includes 17,679 images with 32,699 bounding boxes and 88,395 multi-level motions, from over 35 hours of ESD videos for both robot-assisted and conventional surgeries. CoPESD enables granular analysis of ESD motions, focusing on the complex task of submucosal dissection. Extensive experiments on the LVLMs demonstrate the effectiveness of CoPESD in training LVLMs to predict following surgical robotic motions. As the first multimodal ESD motion dataset, CoPESD supports advanced research in ESD instruction-following and surgical automation. The dataset is available at \\href{https://github.com/gkw0010/CoPESD}{https://github.com/gkw0010/CoPESD.}}",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07575",
        "abstract url": "https://arxiv.org/abs/2410.07575",
        "title": "Self-Supervised Meta-Learning for All-Layer DNN-Based Adaptive Control with Stability Guarantees",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "A critical goal of adaptive control is enabling robots to rapidly adapt in dynamic environments. Recent studies have developed a meta-learning-based adaptive control scheme, which uses meta-learning to extract nonlinear features (represented by Deep Neural Networks (DNNs)) from offline data, and uses adaptive control to update linear coefficients online. However, such a scheme is fundamentally limited by the linear parameterization of uncertainties and does not fully unleash the capability of DNNs. This paper introduces a novel learning-based adaptive control framework that pretrains a DNN via self-supervised meta-learning (SSML) from offline trajectories and online adapts the full DNN via composite adaptation. In particular, the offline SSML stage leverages the time consistency in trajectory data to train the DNN to predict future disturbances from history, in a self-supervised manner without environment condition labels. The online stage carefully designs a control law and an adaptation law to update the full DNN with stability guarantees. Empirically, the proposed framework significantly outperforms (19-39%) various classic and learning-based adaptive control baselines, in challenging real-world quadrotor tracking problems under large dynamic wind disturbance.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07577",
        "abstract url": "https://arxiv.org/abs/2410.07577",
        "title": "3D Vision-Language Gaussian Splatting",
        "rating": "-1",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "autonomous driving"
            ],
            [
                "robotics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in 3D reconstruction methods and vision-language models have propelled the development of multi-modal 3D scene understanding, which has vital applications in robotics, autonomous driving, and virtual/augmented reality. However, current multi-modal scene understanding approaches have naively embedded semantic representations into 3D reconstruction methods without striking a balance between visual and language modalities, which leads to unsatisfying semantic rasterization of translucent or reflective objects, as well as over-fitting on color modality. To alleviate these limitations, we propose a solution that adequately handles the distinct visual and semantic modalities, i.e., a 3D vision-language Gaussian splatting model for scene understanding, to put emphasis on the representation learning of language modality. We propose a novel cross-modal rasterizer, using modality fusion along with a smoothed semantic indicator for enhancing semantic rasterization. We also employ a camera-view blending technique to improve semantic consistency between existing and synthesized views, thereby effectively mitigating over-fitting. Extensive experiments demonstrate that our method achieves state-of-the-art performance in open-vocabulary semantic segmentation, surpassing existing methods by a significant margin.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "main paper + supplementary material"
    },
    {
        "paper id": "2410.06651",
        "abstract url": "https://arxiv.org/abs/2410.06651",
        "title": "Toward Physics-guided Time Series Embedding",
        "rating": "-1.5",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In various scientific and engineering fields, the primary research areas have revolved around physics-based dynamical systems modeling and data-driven time series analysis. According to the embedding theory, dynamical systems and time series can be mutually transformed using observation functions and physical reconstruction techniques. Based on this, we propose Embedding Duality Theory, where the parameterized embedding layer essentially provides a linear estimation of the non-linear time series dynamics. This theory enables us to bypass the parameterized embedding layer and directly employ physical reconstruction techniques to acquire a data embedding representation. Utilizing physical priors results in a 10X reduction in parameters, a 3X increase in speed, and maximum performance boosts of 18% in expert, 22% in few-shot, and 53\\% in zero-shot tasks without any hyper-parameter tuning. All methods are encapsulated as a plug-and-play module",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06665",
        "abstract url": "https://arxiv.org/abs/2410.06665",
        "title": "Revisiting Multi-Permutation Equivariance through the Lens of Irreducible Representations",
        "rating": "-1.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "anomaly detection"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "This paper explores the characterization of equivariant linear layers for representations of permutations and related groups. Unlike traditional approaches, which address these problems using parameter-sharing, we consider an alternative methodology based on irreducible representations and Schur's lemma. Using this methodology, we obtain an alternative derivation for existing models like DeepSets, 2-IGN graph equivariant networks, and Deep Weight Space (DWS) networks. The derivation for DWS networks is significantly simpler than that of previous results. Next, we extend our approach to unaligned symmetric sets, where equivariance to the wreath product of groups is required. Previous works have addressed this problem in a rather restrictive setting, in which almost all wreath equivariant layers are Siamese. In contrast, we give a full characterization of layers in this case and show that there is a vast number of additional non-Siamese layers in some settings. We also show empirically that these additional non-Siamese layers can improve performance in tasks like graph anomaly detection, weight space alignment, and learning Wasserstein distances. Our code is available at \\href{https://github.com/yonatansverdlov/Irreducible-Representations-of-Deep-Weight-Spaces}{GitHub}.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06731",
        "abstract url": "https://arxiv.org/abs/2410.06731",
        "title": "Gridded Transformer Neural Processes for Large Unstructured Spatio-Temporal Data",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Many important problems require modelling large-scale spatio-temporal datasets, with one prevalent example being weather forecasting. Recently, transformer-based approaches have shown great promise in a range of weather forecasting problems. However, these have mostly focused on gridded data sources, neglecting the wealth of unstructured, off-the-grid data from observational measurements such as those at weather stations. A promising family of models suitable for such tasks are neural processes (NPs), notably the family of transformer neural processes (TNPs). Although TNPs have shown promise on small spatio-temporal datasets, they are unable to scale to the quantities of data used by state-of-the-art weather and climate models. This limitation stems from their lack of efficient attention mechanisms. We address this shortcoming through the introduction of gridded pseudo-token TNPs which employ specialised encoders and decoders to handle unstructured observations and utilise a processor containing gridded pseudo-tokens that leverage efficient attention mechanisms. Our method consistently outperforms a range of strong baselines on various synthetic and real-world regression tasks involving large-scale data, while maintaining competitive computational efficiency. The real-life experiments are performed on weather data, demonstrating the potential of our approach to bring performance and computational benefits when applied at scale in a weather modelling pipeline.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06734",
        "abstract url": "https://arxiv.org/abs/2410.06734",
        "title": "MimicTalk: Mimicking a personalized and expressive 3D talking face in minutes",
        "rating": "-1.5",
        "keywords": [
            [
                "3D",
                "NeRF"
            ],
            [
                "facial"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Talking face generation (TFG) aims to animate a target identity's face to create realistic talking videos. Personalized TFG is a variant that emphasizes the perceptual identity similarity of the synthesized result (from the perspective of appearance and talking style). While previous works typically solve this problem by learning an individual neural radiance field (NeRF) for each identity to implicitly store its static and dynamic information, we find it inefficient and non-generalized due to the per-identity-per-training framework and the limited training data. To this end, we propose MimicTalk, the first attempt that exploits the rich knowledge from a NeRF-based person-agnostic generic model for improving the efficiency and robustness of personalized TFG. To be specific, (1) we first come up with a person-agnostic 3D TFG model as the base model and propose to adapt it into a specific identity; (2) we propose a static-dynamic-hybrid adaptation pipeline to help the model learn the personalized static appearance and facial dynamic features; (3) To generate the facial motion of the personalized talking style, we propose an in-context stylized audio-to-motion model that mimics the implicit talking style provided in the reference video without information loss by an explicit style representation. The adaptation process to an unseen identity can be performed in 15 minutes, which is 47 times faster than previous person-dependent methods. Experiments show that our MimicTalk surpasses previous baselines regarding video quality, efficiency, and expressiveness. Source code and video samples are available at https://mimictalk.github.io .",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by NeurIPS 2024"
    },
    {
        "paper id": "2410.06786",
        "abstract url": "https://arxiv.org/abs/2410.06786",
        "title": "Deep End-to-End Survival Analysis with Temporal Consistency",
        "rating": "-1.5",
        "keywords": [
            [
                "Survival"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this study, we present a novel Survival Analysis algorithm designed to efficiently handle large-scale longitudinal data. Our approach draws inspiration from Reinforcement Learning principles, particularly the Deep Q-Network paradigm, extending Temporal Learning concepts to Survival Regression. A central idea in our method is temporal consistency, a hypothesis that past and future outcomes in the data evolve smoothly over time. Our framework uniquely incorporates temporal consistency into large datasets by providing a stable training signal that captures long-term temporal relationships and ensures reliable updates. Additionally, the method supports arbitrarily complex architectures, enabling the modeling of intricate temporal dependencies, and allows for end-to-end training. Through numerous experiments we provide empirical evidence demonstrating our framework's ability to exploit temporal consistency across datasets of varying sizes. Moreover, our algorithm outperforms benchmarks on datasets with long sequences, demonstrating its ability to capture long-term patterns. Finally, ablation studies show how our method enhances training stability.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06815",
        "abstract url": "https://arxiv.org/abs/2410.06815",
        "title": "Shap-Select: Lightweight Feature Selection Using SHAP Values and Regression",
        "rating": "-1.5",
        "keywords": [
            [
                "tabular"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Feature selection is an essential process in machine learning, especially when dealing with high-dimensional datasets. It helps reduce the complexity of machine learning models, improve performance, mitigate overfitting, and decrease computation time. This paper presents a novel feature selection framework, shap-select. The framework conducts a linear or logistic regression of the target on the Shapley values of the features, on the validation set, and uses the signs and significance levels of the regression coefficients to implement an efficient heuristic for feature selection in tabular regression and classification tasks. We evaluate shap-select on the Kaggle credit card fraud dataset, demonstrating its effectiveness compared to established methods such as Recursive Feature Elimination (RFE), HISEL (a mutual information-based feature selection method), Boruta and a simpler Shapley value-based method. Our findings show that shap-select combines interpretability, computational efficiency, and performance, offering a robust solution for feature selection.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "13 pages, 1 figure"
    },
    {
        "paper id": "2410.06819",
        "abstract url": "https://arxiv.org/abs/2410.06819",
        "title": "Dynamic Neural Potential Field: Online Trajectory Optimization in Presence of Moving Obstacles",
        "rating": "-1.5",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "We address a task of local trajectory planning for the mobile robot in the presence of static and dynamic obstacles. Local trajectory is obtained as a numerical solution of the Model Predictive Control (MPC) problem. Collision avoidance may be provided by adding repulsive potential of the obstacles to the cost function of MPC. We develop an approach, where repulsive potential is estimated by the neural model. We propose and explore three possible strategies of handling dynamic obstacles. First, environment with dynamic obstacles is considered as a sequence of static environments. Second, the neural model predict a sequence of repulsive potential at once. Third, the neural model predict future repulsive potential step by step in autoregressive mode. We implement these strategies and compare it with CIAO* and MPPI using BenchMR framework. First two strategies showed higher performance than CIAO* and MPPI while preserving safety constraints. The third strategy was a bit slower, however it still satisfy time limits. We deploy our approach on Husky UGV mobile platform, which move through the office corridors under proposed MPC local trajectory planner. The code and trained models are available at \\url{https://github.com/CognitiveAISystems/Dynamic-Neural-Potential-Field}.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06820",
        "abstract url": "https://arxiv.org/abs/2410.06820",
        "title": "Learning a Neural Solver for Parametric PDE to Enhance Physics-Informed Methods",
        "rating": "-1.5",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Physics-informed deep learning often faces optimization challenges due to the complexity of solving partial differential equations (PDEs), which involve exploring large solution spaces, require numerous iterations, and can lead to unstable training. These challenges arise particularly from the ill-conditioning of the optimization problem, caused by the differential terms in the loss function. To address these issues, we propose learning a solver, i.e., solving PDEs using a physics-informed iterative algorithm trained on data. Our method learns to condition a gradient descent algorithm that automatically adapts to each PDE instance, significantly accelerating and stabilizing the optimization process and enabling faster convergence of physics-aware models. Furthermore, while traditional physics-informed methods solve for a single PDE instance, our approach addresses parametric PDEs. Specifically, our method integrates the physical loss gradient with the PDE parameters to solve over a distribution of PDE parameters, including coefficients, initial conditions, or boundary conditions. We demonstrate the effectiveness of our method through empirical experiments on multiple datasets, comparing training and test-time optimization performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06847",
        "abstract url": "https://arxiv.org/abs/2410.06847",
        "title": "A Safety Modulator Actor-Critic Method in Model-Free Safe Reinforcement Learning and Application in UAV Hovering",
        "rating": "-1.5",
        "keywords": [
            [
                "UAV"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "This paper proposes a safety modulator actor-critic (SMAC) method to address safety constraint and overestimation mitigation in model-free safe reinforcement learning (RL). A safety modulator is developed to satisfy safety constraints by modulating actions, allowing the policy to ignore safety constraint and focus on maximizing reward. Additionally, a distributional critic with a theoretical update rule for SMAC is proposed to mitigate the overestimation of Q-values with safety constraints. Both simulation and real-world scenarios experiments on Unmanned Aerial Vehicles (UAVs) hovering confirm that the SMAC can effectively maintain safety constraints and outperform mainstream baseline algorithms.",
        "subjects": [
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06848",
        "abstract url": "https://arxiv.org/abs/2410.06848",
        "title": "Forgetting Through Transforming: Enabling Federated Unlearning via Class-Aware Representation Transformation",
        "rating": "-1.5",
        "keywords": [
            [
                "federated learning"
            ],
            [
                "Unlearning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated Unlearning (FU) enables clients to selectively remove the influence of specific data from a trained federated learning model, addressing privacy concerns and regulatory requirements. However, existing FU methods often struggle to balance effective erasure with model utility preservation, especially for class-level unlearning in non-IID settings. We propose Federated Unlearning via Class-aware Representation Transformation (FUCRT), a novel method that achieves unlearning through class-aware representation transformation. FUCRT employs two key components: (1) a transformation class selection strategy to identify optimal forgetting directions, and (2) a transformation alignment technique using dual class-aware contrastive learning to ensure consistent transformations across clients. Extensive experiments on four datasets demonstrate FUCRT's superior performance in terms of erasure guarantee, model utility preservation, and efficiency. FUCRT achieves complete (100\\%) erasure of unlearning classes while maintaining or improving performance on remaining classes, outperforming state-of-the-art baselines across both IID and Non-IID settings. Analysis of the representation space reveals FUCRT's ability to effectively merge unlearning class representations with the transformation class from remaining classes, closely mimicking the model retrained from scratch.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06911",
        "abstract url": "https://arxiv.org/abs/2410.06911",
        "title": "Combining Planning and Diffusion for Mobility with Unknown Dynamics",
        "rating": "-1.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "robot",
                "navigation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Manipulation of large objects over long horizons (such as carts in a warehouse) is an essential skill for deployable robotic systems. Large objects require mobile manipulation which involves simultaneous manipulation, navigation, and movement with the object in tow. In many real-world situations, object dynamics are incredibly complex, such as the interaction of an office chair (with a rotating base and five caster wheels) and the ground. We present a hierarchical algorithm for long-horizon robot manipulation problems in which the dynamics are partially unknown. We observe that diffusion-based behavior cloning is highly effective for short-horizon problems with unknown dynamics, so we decompose the problem into an abstract high-level, obstacle-aware motion-planning problem that produces a waypoint sequence. We use a short-horizon, relative-motion diffusion policy to achieve the waypoints in sequence. We train mobile manipulation policies on a Spot robot that has to push and pull an office chair. Our hierarchical manipulation policy performs consistently better, especially when the horizon increases, compared to a diffusion policy trained on long-horizon demonstrations or motion planning assuming a rigidly-attached object (success rate of 8 (versus 0 and 5 respectively) out of 10 runs). Importantly, our learned policy generalizes to new layouts, grasps, chairs, and flooring that induces more friction, without any further training, showing promise for other complex mobile manipulation problems. Project Page: https://yravan.github.io/plannerorderedpolicy/",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "Submitted to ICRA 2025"
    },
    {
        "paper id": "2410.06935",
        "abstract url": "https://arxiv.org/abs/2410.06935",
        "title": "Predicting Bitcoin Market Trends with Enhanced Technical Indicator Integration and Classification Models",
        "rating": "-1.5",
        "keywords": [
            [
                "forecast"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Thanks to the high potential for profit, trading has become increasingly attractive to investors as the cryptocurrency and stock markets rapidly expand. However, because financial markets are intricate and dynamic, accurately predicting prices remains a significant challenge. The volatile nature of the cryptocurrency market makes it even harder for traders and investors to make decisions. This study presents a machine learning model based on classification to forecast the direction of the cryptocurrency market, i.e., whether prices will increase or decrease. The model is trained using historical data and important technical indicators such as the Moving Average Convergence Divergence, the Relative Strength Index, and Bollinger Bands. We illustrate our approach with an empirical study of the closing price of Bitcoin. Several simulations, including a confusion matrix and Receiver Operating Characteristic curve, are used to assess the model's performance, and the results show a buy/sell signal accuracy of over 92%. These findings demonstrate how machine learning models can assist investors and traders of cryptocurrencies in making wise/informed decisions in a very volatile market.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "12 pages, 8 figures, and 6 tables"
    },
    {
        "paper id": "2410.06946",
        "abstract url": "https://arxiv.org/abs/2410.06946",
        "title": "A Trilogy of AI Safety Frameworks: Paths from Facts and Knowledge Gaps to Reliable Predictions and New Knowledge",
        "rating": "-1.5",
        "keywords": [
            [
                "biomedical"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "AI Safety has become a vital front-line concern of many scientists within and outside the AI community. There are many immediate and long term anticipated risks that range from existential risk to human existence to deep fakes and bias in machine learning systems [1-5]. In this paper, we reduce the full scope and immense complexity of AI safety concerns to a trilogy of three important but tractable opportunities for advances that have the short-term potential to improve AI safety and reliability without reducing AI innovation in critical domains. In this perspective, we discuss this vision based on several case studies that already produced proofs of concept in critical ML applications in biomedical science.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07097",
        "abstract url": "https://arxiv.org/abs/2410.07097",
        "title": "A Law of Large Numbers for SIR on the Stochastic Block Model: A Proof via Herd Immunity",
        "rating": "-1.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "graph"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "In this paper, we study the dynamics of the susceptible-infected-recovered (SIR) model on a network with community structure, namely the stochastic block model (SBM). As usual, the SIR model is a stochastic model for an epidemic where infected vertices infect susceptible neighbors at some rate $\u03b7$ and recover at rate $\u03b3$, and the SBM is a random graph model where vertices are partitioned into a finite number of communities. The connection probability between two vertices depends on their community affiliation, here scaled so that the average degrees have a finite limit as the network grows. We prove laws of large numbers (LLN) for the epidemic's trajectory to a system of ordinary differential equations over any time horizon (finite or infinite), including in particular a LLN for the final size of the infection. Our proofs rely on two main ingredients: (i) a new coupling of the SIR epidemic and the randomness of the SBM, revealing a vector-valued random variable that drives the epidemic (related to what is usually called the ``force of the infection'' via a linear transformation), and (ii) a novel technique for analyzing the limiting behavior of the infinite time horizon for the infection, using the fact that once the infection passes the herd immunity threshold it dies out quickly and has a negligible impact on the overall size of the infection.",
        "subjects": [
            "math.PR",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07120",
        "abstract url": "https://arxiv.org/abs/2410.07120",
        "title": "Sequential Decoding of Multiple Traces Over the Syndrome Trellis for Synchronization Errors",
        "rating": "-1.5",
        "keywords": [
            [
                "DNA"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Standard decoding approaches for convolutional codes, such as the Viterbi and BCJR algorithms, entail significant complexity when correcting synchronization errors. The situation worsens when multiple received sequences should be jointly decoded, as in DNA storage. Previous work has attempted to address this via separate-BCJR decoding, i.e., combining the results of decoding each received sequence separately. Another attempt to reduce complexity adapted sequential decoders for use over channels with insertion and deletion errors. However, these decoding alternatives remain prohibitively expensive for high-rate convolutional codes. To address this, we adapt sequential decoders to decode multiple received sequences jointly over the syndrome trellis. For the short blocklength regime, this decoding strategy can outperform separate-BCJR decoding under certain channel conditions, in addition to reducing decoding complexity. To mitigate the occurrence of a decoding timeout, formally called erasure, we also extend this approach to work bidirectionally, i.e., deploying two independent stack decoders that simultaneously operate in the forward and backward directions.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "Submitted to ICASSP"
    },
    {
        "paper id": "2410.07287",
        "abstract url": "https://arxiv.org/abs/2410.07287",
        "title": "Crafting desirable climate trajectories with RL explored socio-environmental simulations",
        "rating": "-1.5",
        "keywords": [
            [
                "forecast"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Climate change poses an existential threat, necessitating effective climate policies to enact impactful change. Decisions in this domain are incredibly complex, involving conflicting entities and evidence. In the last decades, policymakers increasingly use simulations and computational methods to guide some of their decisions. Integrated Assessment Models (IAMs) are one of such methods, which combine social, economic, and environmental simulations to forecast potential policy effects. For example, the UN uses outputs of IAMs for their recent Intergovernmental Panel on Climate Change (IPCC) reports. Traditionally these have been solved using recursive equation solvers, but have several shortcomings, e.g. struggling at decision making under uncertainty. Recent preliminary work using Reinforcement Learning (RL) to replace the traditional solvers shows promising results in decision making in uncertain and noisy scenarios. We extend on this work by introducing multiple interacting RL agents as a preliminary analysis on modelling the complex interplay of socio-interactions between various stakeholders or nations that drives much of the current climate crisis. Our findings show that cooperative agents in this framework can consistently chart pathways towards more desirable futures in terms of reduced carbon emissions and improved economy. However, upon introducing competition between agents, for instance by using opposing reward functions, desirable climate futures are rarely reached. Modelling competition is key to increased realism in these simulations, as such we employ policy interpretation by visualising what states lead to more uncertain behaviour, to understand algorithm failure. Finally, we highlight the current limitations and avenues for further work to ensure future technology uptake for policy derivation.",
        "subjects": [
            "physics.soc-ph",
            "cs.AI"
        ],
        "comment": "23 pages, 13 Figures"
    },
    {
        "paper id": "2410.07296",
        "abstract url": "https://arxiv.org/abs/2410.07296",
        "title": "ReinDiffuse: Crafting Physically Plausible Motions with Reinforced Diffusion Model",
        "rating": "-1.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "physics"
            ],
            [
                "cs.CV"
            ],
            [
                "WACV"
            ]
        ],
        "abstract": "Generating human motion from textual descriptions is a challenging task. Existing methods either struggle with physical credibility or are limited by the complexities of physics simulations. In this paper, we present \\emph{ReinDiffuse} that combines reinforcement learning with motion diffusion model to generate physically credible human motions that align with textual descriptions. Our method adapts Motion Diffusion Model to output a parameterized distribution of actions, making them compatible with reinforcement learning paradigms. We employ reinforcement learning with the objective of maximizing physically plausible rewards to optimize motion generation for physical fidelity. Our approach outperforms existing state-of-the-art models on two major datasets, HumanML3D and KIT-ML, achieving significant improvements in physical plausibility and motion quality. Project: \\url{https://reindiffuse.github.io/}",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by WACV 2025 in Round 1"
    },
    {
        "paper id": "2410.07387",
        "abstract url": "https://arxiv.org/abs/2410.07387",
        "title": "Siamese networks for Poincar\u00e9 embeddings and the reconstruction of evolutionary trees",
        "rating": "-1.5",
        "keywords": [
            [
                "song"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present a method for reconstructing evolutionary trees from high-dimensional data, with a specific application to bird song spectrograms. We address the challenge of inferring phylogenetic relationships from phenotypic traits, like vocalizations, without predefined acoustic properties. Our approach combines two main components: Poincar\u00e9 embeddings for dimensionality reduction and distance computation, and the neighbor joining algorithm for tree reconstruction. Unlike previous work, we employ Siamese networks to learn embeddings from only leaf node samples of the latent tree. We demonstrate our method's effectiveness on both synthetic data and spectrograms from six species of finches.",
        "subjects": [
            "q-bio.PE",
            "cs.LG"
        ],
        "comment": "17 pages, 10 figures"
    },
    {
        "paper id": "2410.07395",
        "abstract url": "https://arxiv.org/abs/2410.07395",
        "title": "LLM Embeddings Improve Test-time Adaptation to Tabular $Y|X$-Shifts",
        "rating": "-1.5",
        "keywords": [
            [
                "Tabular"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "For tabular datasets, the change in the relationship between the label and covariates ($Y|X$-shifts) is common due to missing variables (a.k.a. confounders). Since it is impossible to generalize to a completely new and unknown domain, we study models that are easy to adapt to the target domain even with few labeled examples. We focus on building more informative representations of tabular data that can mitigate $Y|X$-shifts, and propose to leverage the prior world knowledge in LLMs by serializing (write down) the tabular data to encode it. We find LLM embeddings alone provide inconsistent improvements in robustness, but models trained on them can be well adapted/finetuned to the target domain even using 32 labeled observations. Our finding is based on a comprehensive and systematic study consisting of 7650 source-target pairs and benchmark against 261,000 model configurations trained by 22 algorithms. Our observation holds when ablating the size of accessible target data and different adaptation strategies. The code is available at https://github.com/namkoong-lab/LLM-Tabular-Shifts.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "math.OC",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07508",
        "abstract url": "https://arxiv.org/abs/2410.07508",
        "title": "MOLA: Enhancing Industrial Process Monitoring Using Multi-Block Orthogonal Long Short-Term Memory Autoencoder",
        "rating": "-1.5",
        "keywords": [
            [
                "Industrial"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this work, we introduce MOLA: a Multi-block Orthogonal Long short-term memory Autoencoder paradigm, to conduct accurate, reliable fault detection of industrial processes. To achieve this, MOLA effectively extracts dynamic orthogonal features by introducing an orthogonality-based loss function to constrain the latent space output. This helps eliminate the redundancy in the features identified, thereby improving the overall monitoring performance. On top of this, a multi-block monitoring structure is proposed, which categorizes the process variables into multiple blocks by leveraging expert process knowledge about their associations with the overall process. Each block is associated with its specific Orthogonal Long short-term memory Autoencoder model, whose extracted dynamic orthogonal features are monitored by distance-based Hotelling's $T^2$ statistics and quantile-based cumulative sum (CUSUM) designed for multivariate data streams that are nonparametric, heterogeneous in nature. Compared to having a single model accounting for all process variables, such a multi-block structure improves the overall process monitoring performance significantly, especially for large-scale industrial processes. Finally, we propose an adaptive weight-based Bayesian fusion (W-BF) framework to aggregate all block-wise monitoring statistics into a global statistic that we monitor for faults, with the goal of improving fault detection speed by assigning weights to blocks based on the sequential order where alarms are raised. We demonstrate the efficiency and effectiveness of our MOLA framework by applying it to the Tennessee Eastman Process and comparing the performance with various benchmark methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "21 pages, 9 figures, 9 tables. Submitted to Processes"
    },
    {
        "paper id": "2410.07525",
        "abstract url": "https://arxiv.org/abs/2410.07525",
        "title": "Offline Inverse Constrained Reinforcement Learning for Safe-Critical Decision Making in Healthcare",
        "rating": "-1.5",
        "keywords": [
            [
                "medical",
                "Healthcare",
                "CT"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Reinforcement Learning (RL) applied in healthcare can lead to unsafe medical decisions and treatment, such as excessive dosages or abrupt changes, often due to agents overlooking common-sense constraints. Consequently, Constrained Reinforcement Learning (CRL) is a natural choice for safe decisions. However, specifying the exact cost function is inherently difficult in healthcare. Recent Inverse Constrained Reinforcement Learning (ICRL) is a promising approach that infers constraints from expert demonstrations. ICRL algorithms model Markovian decisions in an interactive environment. These settings do not align with the practical requirement of a decision-making system in healthcare, where decisions rely on historical treatment recorded in an offline dataset. To tackle these issues, we propose the Constraint Transformer (CT). Specifically, 1) we utilize a causal attention mechanism to incorporate historical decisions and observations into the constraint modeling, while employing a Non-Markovian layer for weighted constraints to capture critical states. 2) A generative world model is used to perform exploratory data augmentation, enabling offline RL methods to simulate unsafe decision sequences. In multiple medical scenarios, empirical results demonstrate that CT can capture unsafe states and achieve strategies that approximate lower mortality rates, reducing the occurrence probability of unsafe behaviors.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07538",
        "abstract url": "https://arxiv.org/abs/2410.07538",
        "title": "Rank Aggregation in Crowdsourcing for Listwise Annotations",
        "rating": "-1.5",
        "keywords": [
            [
                "quality assessment"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Rank aggregation through crowdsourcing has recently gained significant attention, particularly in the context of listwise ranking annotations. However, existing methods primarily focus on a single problem and partial ranks, while the aggregation of listwise full ranks across numerous problems remains largely unexplored. This scenario finds relevance in various applications, such as model quality assessment and reinforcement learning with human feedback. In light of practical needs, we propose LAC, a Listwise rank Aggregation method in Crowdsourcing, where the global position information is carefully measured and included. In our design, an especially proposed annotation quality indicator is employed to measure the discrepancy between the annotated rank and the true rank. We also take the difficulty of the ranking problem itself into consideration, as it directly impacts the performance of annotators and consequently influences the final results. To our knowledge, LAC is the first work to directly deal with the full rank aggregation problem in listwise crowdsourcing, and simultaneously infer the difficulty of problems, the ability of annotators, and the ground-truth ranks in an unsupervised way. To evaluate our method, we collect a real-world business-oriented dataset for paragraph ranking. Experimental results on both synthetic and real-world benchmark datasets demonstrate the effectiveness of our proposed LAC method.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "19 pages"
    },
    {
        "paper id": "2410.07574",
        "abstract url": "https://arxiv.org/abs/2410.07574",
        "title": "Gap-Dependent Bounds for Q-Learning using Reference-Advantage Decomposition",
        "rating": "-1.5",
        "keywords": [
            [
                "tabular"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study the gap-dependent bounds of two important algorithms for on-policy Q-learning for finite-horizon episodic tabular Markov Decision Processes (MDPs): UCB-Advantage (Zhang et al. 2020) and Q-EarlySettled-Advantage (Li et al. 2021). UCB-Advantage and Q-EarlySettled-Advantage improve upon the results based on Hoeffding-type bonuses and achieve the almost optimal $\\sqrt{T}$-type regret bound in the worst-case scenario, where $T$ is the total number of steps. However, the benign structures of the MDPs such as a strictly positive suboptimality gap can significantly improve the regret. While gap-dependent regret bounds have been obtained for Q-learning with Hoeffding-type bonuses, it remains an open question to establish gap-dependent regret bounds for Q-learning using variance estimators in their bonuses and reference-advantage decomposition for variance reduction. We develop a novel error decomposition framework to prove gap-dependent regret bounds of UCB-Advantage and Q-EarlySettled-Advantage that are logarithmic in $T$ and improve upon existing ones for Q-learning algorithms. Moreover, we establish the gap-dependent bound for the policy switching cost of UCB-Advantage and improve that under the worst-case MDPs. To our knowledge, this paper presents the first gap-dependent regret analysis for Q-learning using variance estimators and reference-advantage decomposition and also provides the first gap-dependent analysis on policy switching cost for Q-learning.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07584",
        "abstract url": "https://arxiv.org/abs/2410.07584",
        "title": "Imitation Learning with Limited Actions via Diffusion Planners and Deep Koopman Controllers",
        "rating": "-1.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "robot"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent advances in diffusion-based robot policies have demonstrated significant potential in imitating multi-modal behaviors. However, these approaches typically require large quantities of demonstration data paired with corresponding robot action labels, creating a substantial data collection burden. In this work, we propose a plan-then-control framework aimed at improving the action-data efficiency of inverse dynamics controllers by leveraging observational demonstration data. Specifically, we adopt a Deep Koopman Operator framework to model the dynamical system and utilize observation-only trajectories to learn a latent action representation. This latent representation can then be effectively mapped to real high-dimensional continuous actions using a linear action decoder, requiring minimal action-labeled data. Through experiments on simulated robot manipulation tasks and a real robot experiment with multi-modal expert demonstrations, we demonstrate that our approach significantly enhances action-data efficiency and achieves high task success rates with limited action data.",
        "subjects": [
            "cs.RO",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06551",
        "abstract url": "https://arxiv.org/abs/2410.06551",
        "title": "InstantIR: Blind Image Restoration with Instant Generative Reference",
        "rating": "-2",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "Image Restoration"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Handling test-time unknown degradation is the major challenge in Blind Image Restoration (BIR), necessitating high model generalization. An effective strategy is to incorporate prior knowledge, either from human input or generative model. In this paper, we introduce Instant-reference Image Restoration (InstantIR), a novel diffusion-based BIR method which dynamically adjusts generation condition during inference. We first extract a compact representation of the input via a pre-trained vision encoder. At each generation step, this representation is used to decode current diffusion latent and instantiate it in the generative prior. The degraded image is then encoded with this reference, providing robust generation condition. We observe the variance of generative references fluctuate with degradation intensity, which we further leverage as an indicator for developing a sampling algorithm adaptive to input quality. Extensive experiments demonstrate InstantIR achieves state-of-the-art performance and offering outstanding visual quality. Through modulating generative references with textual description, InstantIR can restore extreme degradation and additionally feature creative restoration.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06552",
        "abstract url": "https://arxiv.org/abs/2410.06552",
        "title": "Ventilator pressure prediction using recurrent neural network",
        "rating": "-2",
        "keywords": [
            [
                "medical"
            ]
        ],
        "abstract": "This paper presents a recurrent neural network approach to simulating mechanical ventilator pressure. The traditional mechanical ventilator has a control pressure that is monitored by a medical practitioner and can behave incorrectly if the proper pressure is not applied. This paper takes advantage of recent research and develops a simulator based on a deep sequence model to predict airway pressure in the respiratory circuit during the inspiratory phase of a breath given a time series of control parameters and lung attributes. This method demonstrates the effectiveness of neural network-based controllers in tracking pressure wave forms significantly better than the current industry standard and provides insights into the development of effective and robust pressure-controlled mechanical ventilators. The paper will measure as the mean absolute error between the predicted and actual pressures during the inspiratory phase of each breath.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06584",
        "abstract url": "https://arxiv.org/abs/2410.06584",
        "title": "Two Birds With One Stone: Enhancing Communication and Sensing via Multi-Functional RIS",
        "rating": "-2",
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "In this article, we propose new network architectures that integrate multi-functional reconfigurable intelligent surfaces (MF-RISs) into 6G networks to enhance both communication and sensing capabilities. Firstly, we elaborate how to leverage MF-RISs for improving communication performance in different communication modes including unicast, mulitcast, and broadcast and for different multi-access schemes. Next, we emphasize synergistic benefits of integrating MF-RISs with wireless sensing, enabling more accurate and efficient target detection in 6G networks. Furthermore, we present two schemes that utilize MF-RISs to enhance the performance of integrated sensing and communication (ISAC). We also study multi-objective optimization to achieve the optimal trade-off between communication and sensing performance. Finally, we present numerical results to show the performance improvements offered by MF-RISs compared to conventional RISs in ISAC. We also outline key research directions for MF-RIS under the ambition of 6G.",
        "subjects": [
            "eess.SP",
            "eess.SY"
        ],
        "comment": "8 pages, 5 figures, submitted to IEEE"
    },
    {
        "paper id": "2410.06594",
        "abstract url": "https://arxiv.org/abs/2410.06594",
        "title": "Assessing the Circadian Rhythm of Cats Living in a Group using Accelerometers",
        "rating": "-2",
        "keywords": [
            [
                "biological"
            ]
        ],
        "abstract": "This study explores the biological rhythms of domestic cats. Twelve cats from the AVA shelter in Cuy-Saint-Fiacre, France, participated in the experimental study, wearing collars equipped with IMU sensors for about three weeks. Recorded data were analyzed to measure the cats activity and to gain further insights into their biological rhythms. We first determined the time budget of the cats by categorizing behavior into inactivity and activity. Next, we analyzed the day/night activity repartition and the hourly distribution of activity. Results showed an average of 14.5% of global activity and a higher activity during the day in comparison with the night. Moreover, a bimodal activity pattern with increased activity at the time of the caretaker's interventions at feeding time was found.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06613",
        "abstract url": "https://arxiv.org/abs/2410.06613",
        "title": "ES-Gaussian: Gaussian Splatting Mapping via Error Space-Based Gaussian Completion",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "LiDAR"
            ],
            [
                "robot",
                "navigation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Accurate and affordable indoor 3D reconstruction is critical for effective robot navigation and interaction. Traditional LiDAR-based mapping provides high precision but is costly, heavy, and power-intensive, with limited ability for novel view rendering. Vision-based mapping, while cost-effective and capable of capturing visual data, often struggles with high-quality 3D reconstruction due to sparse point clouds. We propose ES-Gaussian, an end-to-end system using a low-altitude camera and single-line LiDAR for high-quality 3D indoor reconstruction. Our system features Visual Error Construction (VEC) to enhance sparse point clouds by identifying and correcting areas with insufficient geometric detail from 2D error maps. Additionally, we introduce a novel 3DGS initialization method guided by single-line LiDAR, overcoming the limitations of traditional multi-view setups and enabling effective reconstruction in resource-constrained environments. Extensive experimental results on our new Dreame-SR dataset and a publicly available dataset demonstrate that ES-Gaussian outperforms existing methods, particularly in challenging scenarios. The project page is available at https://chenlu-china.github.io/ES-Gaussian/.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "Project page: https://chenlu-china.github.io/ES-Gaussian/"
    },
    {
        "paper id": "2410.06652",
        "abstract url": "https://arxiv.org/abs/2410.06652",
        "title": "Task-oriented Time Series Imputation Evaluation via Generalized Representers",
        "rating": "-2",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Time series analysis is widely used in many fields such as power energy, economics, and transportation, including different tasks such as forecasting, anomaly detection, classification, etc. Missing values are widely observed in these tasks, and often leading to unpredictable negative effects on existing methods, hindering their further application. In response to this situation, existing time series imputation methods mainly focus on restoring sequences based on their data characteristics, while ignoring the performance of the restored sequences in downstream tasks. Considering different requirements of downstream tasks (e.g., forecasting), this paper proposes an efficient downstream task-oriented time series imputation evaluation approach. By combining time series imputation with neural network models used for downstream tasks, the gain of different imputation strategies on downstream tasks is estimated without retraining, and the most favorable imputation value for downstream tasks is given by combining different imputation strategies according to the estimated gain.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "22 pages, 9 figures, 38th Conference on Neural Information Processing Systems (NeurIPS 2024)"
    },
    {
        "paper id": "2410.06689",
        "abstract url": "https://arxiv.org/abs/2410.06689",
        "title": "Perceptual Quality Assessment of Trisoup-Lifting Encoded 3D Point Clouds",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "Quality Assessment"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "No-reference bitstream-layer point cloud quality assessment (PCQA) can be deployed without full decoding at any network node to achieve real-time quality monitoring. In this work, we develop the first PCQA model dedicated to Trisoup-Lifting encoded 3D point clouds by analyzing bitstreams without full decoding. Specifically, we investigate the relationship among texture bitrate per point (TBPP), texture complexity (TC) and texture quantization parameter (TQP) while geometry encoding is lossless. Subsequently, we estimate TC by utilizing TQP and TBPP. Then, we establish a texture distortion evaluation model based on TC, TBPP and TQP. Ultimately, by integrating this texture distortion model with a geometry attenuation factor, a function of trisoupNodeSizeLog2 (tNSL), we acquire a comprehensive NR bitstream-layer PCQA model named streamPCQ-TL. In addition, this work establishes a database named WPC6.0, the first and largest PCQA database dedicated to Trisoup-Lifting encoding mode, encompassing 400 distorted point clouds with both 4 geometric multiplied by 5 texture distortion levels. Experiment results on M-PCCD, ICIP2020 and the proposed WPC6.0 database suggest that the proposed streamPCQ-TL model exhibits robust and notable performance in contrast to existing advanced PCQA metrics, particularly in terms of computational cost. The dataset and source code will be publicly released at \\href{https://github.com/qdushl/Waterloo-Point-Cloud-Database-6.0}{\\textit{https://github.com/qdushl/Waterloo-Point-Cloud-Database-6.0}}",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06698",
        "abstract url": "https://arxiv.org/abs/2410.06698",
        "title": "Fourier-based Action Recognition for Wildlife Behavior Quantification with Event Cameras",
        "rating": "-2",
        "keywords": [
            [
                "Event Cameras"
            ],
            [
                "bio-inspired"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Event cameras are novel bio-inspired vision sensors that measure pixel-wise brightness changes asynchronously instead of images at a given frame rate. They offer promising advantages, namely a high dynamic range, low latency, and minimal motion blur. Modern computer vision algorithms often rely on artificial neural network approaches, which require image-like representations of the data and cannot fully exploit the characteristics of event data. We propose approaches to action recognition based on the Fourier Transform. The approaches are intended to recognize oscillating motion patterns commonly present in nature. In particular, we apply our approaches to a recent dataset of breeding penguins annotated for \"ecstatic display\", a behavior where the observed penguins flap their wings at a certain frequency. We find that our approaches are both simple and effective, producing slightly lower results than a deep neural network (DNN) while relying just on a tiny fraction of the parameters compared to the DNN (five orders of magnitude fewer parameters). They work well despite the uncontrolled, diverse data present in the dataset. We hope this work opens a new perspective on event-based processing and action recognition.",
        "subjects": [
            "cs.CV",
            "cs.ET"
        ],
        "comment": "11 pages, 10 figures, 7 tables"
    },
    {
        "paper id": "2410.06700",
        "abstract url": "https://arxiv.org/abs/2410.06700",
        "title": "Optimizing Integrated Terrestrial and Non-Terrestrial Networks Performance with Traffic-Aware Resource Management",
        "rating": "-2",
        "keywords": [
            [
                "satellite"
            ]
        ],
        "abstract": "To address an ever-increasing demand for ubiquitous high-speed connectivity, mobile networks have intensified their deployment process. However, achieving this target has proven to be a challenge and has led to a surge in overall energy consumption. In recent years, non-terrestrial networks (NTNs) have been endorsed as a potential solution to these problems by complementing the coverage of the terrestrial network in areas with limited network deployment. To this end, this paper proposes an integrated terrestrial and non-terrestrial network (TN-NTN) that utilises the overall available communication resources to expand coverage and meet Quality of Service (QoS) requirements during high-traffic hours in any deployment scenario. Importantly, our framework allows to drastically reduce the terrestrial network energy consumption during low-traffic hours. Specifically, we introduce a novel radio resource management algorithm, BLASTER (Bandwidth SpLit, User ASsociation, and PowEr ContRol), which integrates bandwidth allocation, user equipment (UE) association, power control, and base station activation within the TN-NTN. This algorithm aims to optimize network resource allocation fairness and energy consumption dynamically, demonstrating new opportunities in deploying satellite networks in legacy cellular systems. Our study offers a comprehensive analysis of the integrated network model, emphasizing the effective balance between energy saving and QoS, and proposing practical solutions to meet the fluctuating traffic demands of cellular networks.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "Submitted to IEEE Transactions on Wireless Communications"
    },
    {
        "paper id": "2410.06713",
        "abstract url": "https://arxiv.org/abs/2410.06713",
        "title": "SHRINK: Data Compression by Semantic Extraction and Residuals Encoding",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "The distributed data infrastructure in Internet of Things (IoT) ecosystems requires efficient data-series compression methods, along with the ability to feed different accuracy demands. However, the compression performance of existing compression methods degrades sharply when calling for ultra-accurate data recovery. In this paper, we introduce SHRINK, a novel highly accurate data compression method that offers a higher compression ratio and also lower runtime than prior compressors. SHRINK extracts data semantics in the form of linear segments to construct a compact knowledge base, using a dynamic error threshold that it adapts to data characteristics. Then, it captures the remaining data details as residuals to support lossy compression at diverse resolutions as well as lossless compression. As SHRINK identifies repeated semantics, its compression ratio increases with data size. Our experimental evaluation demonstrates that SHRINK outperforms state-of-art methods with an up to threefold improvement in compression ratio.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "11 pages"
    },
    {
        "paper id": "2410.06757",
        "abstract url": "https://arxiv.org/abs/2410.06757",
        "title": "Diff-FMT: Diffusion Models for Fluorescence Molecular Tomography",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "biomedical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Fluorescence molecular tomography (FMT) is a real-time, noninvasive optical imaging technology that plays a significant role in biomedical research. Nevertheless, the ill-posedness of the inverse problem poses huge challenges in FMT reconstructions. Previous various deep learning algorithms have been extensively explored to address the critical issues, but they remain faces the challenge of high data dependency with poor image quality. In this paper, we, for the first time, propose a FMT reconstruction method based on a denoising diffusion probabilistic model (DDPM), termed Diff-FMT, which is capable of obtaining high-quality reconstructed images from noisy images. Specifically, we utilize the noise addition mechanism of DDPM to generate diverse training samples. Through the step-by-step probability sampling mechanism in the inverse process, we achieve fine-grained reconstruction of the image, avoiding issues such as loss of image detail that can occur with end-to-end deep-learning methods. Additionally, we introduce the fluorescence signals as conditional information in the model training to sample a reconstructed image that is highly consistent with the input fluorescence signals from the noisy images. Numerous experimental results show that Diff-FMT can achieve high-resolution reconstruction images without relying on large-scale datasets compared with other cutting-edge algorithms.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06790",
        "abstract url": "https://arxiv.org/abs/2410.06790",
        "title": "Discrete time model predictive control for humanoid walking with step adjustment",
        "rating": "-2",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "Robot"
            ]
        ],
        "abstract": "This paper presents a Discrete-Time Model Predictive Controller (MPC) for humanoid walking with online footstep adjustment. The proposed controller utilizes a hierarchical control approach. The high-level controller uses a low-dimensional Linear Inverted Pendulum Model (LIPM) to determine desired foot placement and Center of Mass (CoM) motion, to prevent falls while maintaining the desired velocity. A Task Space Controller (TSC) then tracks the desired motion obtained from the high-level controller, exploiting the whole-body dynamics of the humanoid. Our approach differs from existing MPC methods for walking pattern generation by not relying on a predefined foot-plan or a reference center of pressure (CoP) trajectory. The overall approach is tested in simulation on a torque-controlled Humanoid Robot. Results show that proposed control approach generates stable walking and prevents fall against push disturbances.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "6 pages, 17 figures, 1 table"
    },
    {
        "paper id": "2410.06808",
        "abstract url": "https://arxiv.org/abs/2410.06808",
        "title": "Near-Optimal-Time Quantum Algorithms for Approximate Pattern Matching",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Approximate Pattern Matching is among the most fundamental string-processing tasks. Given a text $T$ of length $n$, a pattern $P$ of length $m$, and a threshold $k$, the task is to identify the fragments of $T$ that are at distance at most $k$ to $P$. We consider the two most common distances: Hamming distance (the number of character substitutions) in Pattern Matching with Mismatches and edit distance (the minimum number of character insertions, deletions, and substitutions) in Pattern Matching with Edits. We revisit the complexity of these two problems in the quantum setting. Our recent work [STOC'24] shows that $\\hat{O}(\\sqrt{nk})$ quantum queries are sufficient to solve (the decision version of) Pattern Matching with Edits. However, the quantum time complexity of the underlying solution does not provide any improvement over classical computation. On the other hand, the state-of-the-art algorithm for Pattern Matching with Mismatches [Jin and Nogler; SODA'23] achieves query complexity $\\hat{O}(\\sqrt{nk^{3/2}})$ and time complexity $\\tilde{O}(\\sqrt{nk^2})$, falling short of an unconditional lower bound of $\u03a9(\\sqrt{nk})$ queries. In this work, we present quantum algorithms with a time complexity of $\\tilde{O}(\\sqrt{nk}+\\sqrt{n/m}\\cdot k^2)$ for Pattern Matching with Mismatches and $\\hat{O}(\\sqrt{nk}+\\sqrt{n/m}\\cdot k^{3.5})$ for Pattern Matching with Edits; both solutions use $\\hat{O}(\\sqrt{nk})$ queries. The running times are near-optimal for $k\\ll m^{1/3}$ and $k\\ll m^{1/6}$, respectively, and offer advantage over classical algorithms for $k\\ll (mn)^{1/4}$ and $k\\ll (mn)^{1/7}$, respectively. Our solutions can also report the starting positions of approximate occurrences of $P$ in $T$ (represented as collections of arithmetic progressions); in this case, the unconditional lower bound and the complexities of our algorithms increase by a $\u0398(\\sqrt{n/m})$ factor.",
        "subjects": [
            "cs.DS",
            "quant-ph"
        ],
        "comment": "69 pages, 2 figures"
    },
    {
        "paper id": "2410.06866",
        "abstract url": "https://arxiv.org/abs/2410.06866",
        "title": "Secure Video Quality Assessment Resisting Adversarial Attacks",
        "rating": "-2",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "Quality Assessment"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The exponential surge in video traffic has intensified the imperative for Video Quality Assessment (VQA). Leveraging cutting-edge architectures, current VQA models have achieved human-comparable accuracy. However, recent studies have revealed the vulnerability of existing VQA models against adversarial attacks. To establish a reliable and practical assessment system, a secure VQA model capable of resisting such malicious attacks is urgently demanded. Unfortunately, no attempt has been made to explore this issue. This paper first attempts to investigate general adversarial defense principles, aiming at endowing existing VQA models with security. Specifically, we first introduce random spatial grid sampling on the video frame for intra-frame defense. Then, we design pixel-wise randomization through a guardian map, globally neutralizing adversarial perturbations. Meanwhile, we extract temporal information from the video sequence as compensation for inter-frame defense. Building upon these principles, we present a novel VQA framework from the security-oriented perspective, termed SecureVQA. Extensive experiments indicate that SecureVQA sets a new benchmark in security while achieving competitive VQA performance compared with state-of-the-art models. Ablation studies delve deeper into analyzing the principles of SecureVQA, demonstrating their generalization and contributions to the security of leading VQA models.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06870",
        "abstract url": "https://arxiv.org/abs/2410.06870",
        "title": "Balanced Space- and Time-based Duty-cycle Scheduling for Light-based IoT",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "In this work, we propose a Multiple Access Control (MAC) protocol for Light-based IoT (LIoT) networks, where the gateway node orchestrates and schedules batteryless nodes duty-cycles based on their location and sleep time. The LIoT concept represents a sustainable solution for massive indoor IoT applications, offering an alternative communication medium through Visible Light Communication (VLC). While most existing scheduling algorithms for intermittent batteryless IoT aim to maximize data collection and enhance dataset size, our solution is tailored for environmental sensing applications, such as temperature, humidity, and air quality monitoring, optimizing measurement distribution and minimizing blind spots to achieve comprehensive and uniform environmental sensing. We propose a Balanced Space and Time-based Time Division Multiple Access scheduling (BST-TDMA) algorithm, which addresses environmental sensing challenges by balancing spatial and temporal factors to improve the environmental sensing efficiency of batteryless LIoT nodes. Our measurement-based results show that BST-TDMA was able to efficiently schedule duty-cycles with given intervals.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06885",
        "abstract url": "https://arxiv.org/abs/2410.06885",
        "title": "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "text-to-speech"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "This paper introduces F5-TTS, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length as input speech, and then the denoising is performed for speech generation, which was originally proved feasible by E2 TTS. However, the original design of E2 TTS makes it hard to follow due to its slow convergence and low robustness. To address these issues, we first model the input with ConvNeXt to refine the text representation, making it easy to align with the speech. We further propose an inference-time Sway Sampling strategy, which significantly improves our model's performance and efficiency. This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining. Our design allows faster training and achieves an inference RTF of 0.15, which is greatly improved compared to state-of-the-art diffusion-based TTS models. Trained on a public 100K hours multilingual dataset, our Fairytaler Fakes Fluent and Faithful speech with Flow matching (F5-TTS) exhibits highly natural and expressive zero-shot ability, seamless code-switching capability, and speed control efficiency. Demo samples can be found at https://SWivid.github.io/F5-TTS. We release all code and checkpoints to promote community development.",
        "subjects": [
            "eess.AS",
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06938",
        "abstract url": "https://arxiv.org/abs/2410.06938",
        "title": "Optimized Resource Allocation for Cloud-Native 6G Networks: Zero-Touch ML Models in Microservices-based VNF Deployments",
        "rating": "-2",
        "keywords": [
            [
                "5G",
                "6G"
            ]
        ],
        "abstract": "6G, the next generation of mobile networks, is set to offer even higher data rates, ultra-reliability, and lower latency than 5G. New 6G services will increase the load and dynamism of the network. Network Function Virtualization (NFV) aids with this increased load and dynamism by eliminating hardware dependency. It aims to boost the flexibility and scalability of network deployment services by separating network functions from their specific proprietary forms so that they can run as virtual network functions (VNFs) on commodity hardware. It is essential to design an NFV orchestration and management framework to support these services. However, deploying bulky monolithic VNFs on the network is difficult, especially when underlying resources are scarce, resulting in ineffective resource management. To address this, microservices-based NFV approaches are proposed. In this approach, monolithic VNFs are decomposed into micro VNFs, increasing the likelihood of their successful placement and resulting in more efficient resource management. This article discusses the proposed framework for resource allocation for microservices-based services to provide end-to-end Quality of Service (QoS) using the Double Deep Q Learning (DDQL) approach. Furthermore, to enhance this resource allocation approach, we discussed and addressed two crucial sub-problems: the need for a dynamic priority technique and the presence of the low-priority starvation problem. Using the Deep Deterministic Policy Gradient (DDPG) model, an Adaptive Scheduling model is developed that effectively mitigates the starvation problem. Additionally, the impact of incorporating traffic load considerations into deployment and scheduling is thoroughly investigated.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06968",
        "abstract url": "https://arxiv.org/abs/2410.06968",
        "title": "RM4D: A Combined Reachability and Inverse Reachability Map for Common 6-/7-axis Robot Arms by Dimensionality Reduction to 4D",
        "rating": "-2",
        "keywords": [
            [
                "6D"
            ],
            [
                "robotics",
                "Robot"
            ]
        ],
        "abstract": "Knowledge of a manipulator's workspace is fundamental for a variety of tasks including robot design, grasp planning and robot base placement. Consequently, workspace representations are well studied in robotics. Two important representations are reachability maps and inverse reachability maps. The former predicts whether a given end-effector pose is reachable from where the robot currently is, and the latter suggests suitable base positions for a desired end-effector pose. Typically, the reachability map is built by discretizing the 6D space containing the robot's workspace and determining, for each cell, whether it is reachable or not. The reachability map is subsequently inverted to build the inverse map. This is a cumbersome process which restricts the applications of such maps. In this work, we exploit commonalities of existing six and seven axis robot arms to reduce the dimension of the discretization from 6D to 4D. We propose Reachability Map 4D (RM4D), a map that only requires a single 4D data structure for both forward and inverse queries. This gives a much more compact map that can be constructed by an order of magnitude faster than existing maps, with no inversion overheads and no loss in accuracy. Our experiments showcase the usefulness of RM4D for grasp planning with a mobile manipulator.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Submitted to ICRA 2025. See project page: https://mrudorfer.github.io/rm4d/"
    },
    {
        "paper id": "2410.06987",
        "abstract url": "https://arxiv.org/abs/2410.06987",
        "title": "Radio signal propagation in 5G systems equipped with RISs (PL: Propagacja sygna\u0142u radiowego w systemach 5G wyposa\u017conych w matryce IPR)",
        "rating": "-2",
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "In this paper, the characteristics of radio signal propagation within the boundaries of the city of Poznan (Poland) are analyzed. The study considers the use of a Radio Access Network (RAN) of the 5th generation wireless system (5G NR - New Radio), which includes 8 base stations (BSs) utilizing Single Input Single Output (SISO) or Multiple Input Multiple Output (MIMO) antenna technology depending on the adopted configuration of network cells. Additionally, 15 reflecting arrays known as Reconfigurable Intelligent Surfaces (RISs) were placed in the studied area, and their impact on radio signal propagation at different suspension heights was taken into account.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "The scientific paper is written in Polish. The contribution of the paper was presented during the 2nd Radiocommunications and ICT Conference (pl. II Konferencja Radiokomunikacji i Teleinformatyki - KRiT 2024) in Poznan, Poland"
    },
    {
        "paper id": "2410.07043",
        "abstract url": "https://arxiv.org/abs/2410.07043",
        "title": "Z-upscaling: Optical Flow Guided Frame Interpolation for Isotropic Reconstruction of 3D EM Volumes",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "biological"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "We propose a novel optical flow based approach to enhance the axial resolution of anisotropic 3D EM volumes to achieve isotropic 3D reconstruction. Assuming spatial continuity of 3D biological structures in well aligned EM volumes, we reasoned that optical flow estimation techniques, often applied for temporal resolution enhancement in videos, can be utilized. Pixel level motion is estimated between neighboring 2D slices along z, using spatial gradient flow estimates to interpolate and generate new 2D slices resulting in isotropic voxels. We leverage recent state-of-the-art learning methods for video frame interpolation and transfer learning techniques, and demonstrate the success of our approach on publicly available ultrastructure EM volumes.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07051",
        "abstract url": "https://arxiv.org/abs/2410.07051",
        "title": "Exponents for Shared Randomness-Assisted Channel Simulation",
        "rating": "-2",
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "We determine the exact error and strong converse exponents of shared randomness-assisted channel simulation in worst case total-variation distance. Namely, we find that these exponents can be written as simple optimizations over the R\u00e9nyi channel mutual information. Strikingly, and in stark contrast to channel coding, there are no critical rates, allowing a tight characterization for arbitrary rates below and above the simulation capacity. We derive our results by asymptotically expanding the meta-converse for channel simulation [Cao {\\it et al.}, IEEE Trans.~Inf.~Theory (2024)], which corresponds to non-signaling assisted codes. We prove this to be asymptotically tight by employing the approximation algorithms from [Berta {\\it et al.}, Proc.~IEEE ISIT (2024)], which show how to round any non-signaling assisted strategy to a strategy that only uses shared randomness. Notably, this implies that any additional quantum entanglement-assistance does not change the error or the strong converse exponents.",
        "subjects": [
            "cs.IT",
            "quant-ph"
        ],
        "comment": "27+6 pages"
    },
    {
        "paper id": "2410.07087",
        "abstract url": "https://arxiv.org/abs/2410.07087",
        "title": "Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology",
        "rating": "-2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "trajectory",
                "flight"
            ],
            [
                "Navigation"
            ],
            [
                "UAV"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Developing agents capable of navigating to a target location based on language instructions and visual information, known as vision-language navigation (VLN), has attracted widespread interest. Most research has focused on ground-based agents, while UAV-based VLN remains relatively underexplored. Recent efforts in UAV vision-language navigation predominantly adopt ground-based VLN settings, relying on predefined discrete action spaces and neglecting the inherent disparities in agent movement dynamics and the complexity of navigation tasks between ground and aerial environments. To address these disparities and challenges, we propose solutions from three perspectives: platform, benchmark, and methodology. To enable realistic UAV trajectory simulation in VLN tasks, we propose the OpenUAV platform, which features diverse environments, realistic flight control, and extensive algorithmic support. We further construct a target-oriented VLN dataset consisting of approximately 12k trajectories on this platform, serving as the first dataset specifically designed for realistic UAV VLN tasks. To tackle the challenges posed by complex aerial environments, we propose an assistant-guided UAV object search benchmark called UAV-Need-Help, which provides varying levels of guidance information to help UAVs better accomplish realistic VLN tasks. We also propose a UAV navigation LLM that, given multi-view images, task descriptions, and assistant instructions, leverages the multimodal understanding capabilities of the MLLM to jointly process visual and textual information, and performs hierarchical trajectory generation. The evaluation results of our method significantly outperform the baseline models, while there remains a considerable gap between our results and those achieved by human operators, underscoring the challenge presented by the UAV-Need-Help task.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07155",
        "abstract url": "https://arxiv.org/abs/2410.07155",
        "title": "Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis",
        "rating": "-2",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "physic"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advances in diffusion models have demonstrated exceptional capabilities in image and video generation, further improving the effectiveness of 4D synthesis. Existing 4D generation methods can generate high-quality 4D objects or scenes based on user-friendly conditions, benefiting the gaming and video industries. However, these methods struggle to synthesize significant object deformation of complex 4D transitions and interactions within scenes. To address this challenge, we propose Trans4D, a novel text-to-4D synthesis framework that enables realistic complex scene transitions. Specifically, we first use multi-modal large language models (MLLMs) to produce a physic-aware scene description for 4D scene initialization and effective transition timing planning. Then we propose a geometry-aware 4D transition network to realize a complex scene-level 4D transition based on the plan, which involves expressive geometrical object deformation. Extensive experiments demonstrate that Trans4D consistently outperforms existing state-of-the-art methods in generating 4D scenes with accurate and high-quality transitions, validating its effectiveness. Code: https://github.com/YangLing0818/Trans4D",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project: https://github.com/YangLing0818/Trans4D"
    },
    {
        "paper id": "2410.07385",
        "abstract url": "https://arxiv.org/abs/2410.07385",
        "title": "En masse scanning and automated surfacing of small objects using Micro-CT",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "CT"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Modern archaeological methods increasingly utilize 3D virtual representations of objects, computationally intensive analyses, high resolution scanning, large datasets, and machine learning. With higher resolution scans, challenges surrounding computational power, memory, and file storage quickly arise. Processing and analyzing high resolution scans often requires memory-intensive workflows, which are infeasible for most computers and increasingly necessitate the use of super-computers or innovative methods for processing on standard computers. Here we introduce a novel protocol for en-masse micro-CT scanning of small objects with a {\\em mostly-automated} processing workflow that functions in memory-limited settings. We scanned 1,112 animal bone fragments using just 10 micro-CT scans, which were post-processed into individual PLY files. Notably, our methods can be applied to any object (with discernible density from the packaging material) making this method applicable to a variety of inquiries and fields including paleontology, geology, electrical engineering, and materials science. Further, our methods may immediately be adopted by scanning institutes to pool customer orders together and offer more affordable scanning. The work presented herein is part of a larger program facilitated by the international and multi-disciplinary research consortium known as Anthropological and Mathematical Analysis of Archaeological and Zooarchaeological Evidence (AMAAZE). AMAAZE unites experts in anthropology, mathematics, and computer science to develop new methods for mass-scale virtual archaeological research. Overall, our new scanning method and processing workflows lay the groundwork and set the standard for future mass-scale, high resolution scanning studies.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": "36 pages, 12 figures, 2 tables. Source code available at https://github.com/oneil571/AMAAZE-MCT-Processing"
    },
    {
        "paper id": "2410.07389",
        "abstract url": "https://arxiv.org/abs/2410.07389",
        "title": "MIMO MAC Empowered by Reconfigurable Intelligent Surfaces: Capacity Region and Large System Analysis",
        "rating": "-2",
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "Smart wireless environments enabled by multiple distributed Reconfigurable Intelligent Surfaces (RISs) have recently attracted significant research interest as a wireless connectivity paradigm for sixth Generation (6G) networks. In this paper, using random matrix theory methods, we calculate the mean of the sum Mutual Information (MI) for the correlated Multiple-Input Multiple-Output (MIMO) Multiple Access Channel (MAC) in the presence of multiple RISs, in the large-antenna number limit. We thus obtain the capacity region boundaries, after optimizing over the tunable RISs' phase configurations. Furthermore, we obtain a closed-form expression for the variance of the sum-MI metric, which together with the mean provides a tight Gaussian approximation for the outage probability. The derived results become relevant in the presence of fast-fading, when channel estimation is extremely challenging. Our numerical investigations showcased that, when the angle-spread in the neighborhood of each RIS is small, which is expected for higher carrier frequencies, the communication link strongly improves from optimizing the ergodic MI of the multiple RISs.We also found that, increasing the number of transmitting users in such MIMO-MAC-RIS systems results to rapidly diminishing sum-MI gains, hence, providing limits on the number of users that can be efficiently served by a given RIS.",
        "subjects": [
            "cs.IT",
            "cs.ET"
        ],
        "comment": "14 pages, 5 figures, to appear in an IEEE Transactions"
    },
    {
        "paper id": "2410.07393",
        "abstract url": "https://arxiv.org/abs/2410.07393",
        "title": "How Much Power Must We Extract From a Receiver Antenna to Effect Communications?",
        "rating": "-2",
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "Subject to the laws of classical physics - the science that governs the design of today's wireless communication systems - there is no need to extract power from a receiver antenna in order to effect communications. If we dispense with a transmission line and, instead, make the front-end electronics colocated with the antenna, then a high input-impedance preamplifier can measure the open-circuit voltage directly on the antenna port without drawing either current or power. Neither Friis' concept of noise figure, nor Shannon information theory, nor electronics technology dictates that we must extract power from an antenna.",
        "subjects": [
            "eess.SP",
            "cs.IT"
        ],
        "comment": "10 pages"
    },
    {
        "paper id": "2410.07414",
        "abstract url": "https://arxiv.org/abs/2410.07414",
        "title": "Bayes-Nash Generative Privacy Protection Against Membership Inference Attacks",
        "rating": "-2",
        "keywords": [
            [
                "GAN"
            ],
            [
                "Attacks"
            ]
        ],
        "abstract": "An ability to share data, even in aggregated form, is critical to advancing both conventional and data science. However, insofar as such datasets are comprised of individuals, their membership in these datasets is often viewed as sensitive, with membership inference attacks (MIAs) threatening to violate their privacy. We propose a Bayesian game model for privacy-preserving publishing of data-sharing mechanism outputs (for example, summary statistics for sharing genomic data). In this game, the defender minimizes a combination of expected utility and privacy loss, with the latter being maximized by a Bayes-rational attacker. We propose a GAN-style algorithm to approximate a Bayes-Nash equilibrium of this game, and introduce the notions of Bayes-Nash generative privacy (BNGP) and Bayes generative privacy (BGP) risk that aims to optimally balance the defender's privacy and utility in a way that is robust to the attacker's heterogeneous preferences with respect to true and false positives. We demonstrate the properties of composition and post-processing for BGP risk and establish conditions under which BNGP and pure differential privacy (PDP) are equivalent. We apply our method to sharing summary statistics, where MIAs can re-identify individuals even from aggregated data. Theoretical analysis and empirical results demonstrate that our Bayesian game-theoretic method outperforms state-of-the-art approaches for privacy-preserving sharing of summary statistics.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2406.01811"
    },
    {
        "paper id": "2410.07415",
        "abstract url": "https://arxiv.org/abs/2410.07415",
        "title": "3D2M Dataset: A 3-Dimension diverse Mesh Dataset",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Three-dimensional (3D) reconstruction has emerged as a prominent area of research, attracting significant attention from academia and industry alike. Among the various applications of 3D reconstruction, facial reconstruction poses some of the most formidable challenges. Additionally, each individuals facial structure is unique, requiring algorithms to be robust enough to handle this variability while maintaining fidelity to the original features. This article presents a comprehensive dataset of 3D meshes featuring a diverse range of facial structures and corresponding facial landmarks. The dataset comprises 188 3D facial meshes, including 73 from female candidates and 114 from male candidates. It encompasses a broad representation of ethnic backgrounds, with contributions from 45 different ethnicities, ensuring a rich diversity in facial characteristics. Each facial mesh is accompanied by key points that accurately annotate the relevant features, facilitating precise analysis and manipulation. This dataset is particularly valuable for applications such as facial re targeting, the study of facial structure components, and real-time person representation in video streams. By providing a robust resource for researchers and developers, it aims to advance the field of 3D facial reconstruction and related technologies.",
        "subjects": [
            "cs.CV",
            "cs.MM"
        ],
        "comment": "6 pages, 1 figures, 2 tables"
    },
    {
        "paper id": "2410.07431",
        "abstract url": "https://arxiv.org/abs/2410.07431",
        "title": "Goal-oriented vessel detection with distributed computing in a LEO satellite constellation",
        "rating": "-2",
        "keywords": [
            [
                "satellite"
            ]
        ],
        "abstract": "Earth Observation (EO) has traditionally involved the transmission of a large volume of raw data to map the Earth surface. This results in congestion to the satellite network and delays in the availability of the results, invalidating the approach for timing-sensitive applications. Instead, the computation resources at the satellites can be used as an edge layer for compressing the data and/or doing inferences. In this paper, we investigate satellite edge computing for vessel detection with a LEO satellite constellation. First, we distribute the computation and inference load among the neighbouring satellites of the one taking the images, based on the VHRShips data set and YOLOv8. This semantic and fragmented information is then routed to a remote ground monitor through the whole constellation. The average and peak Age of Information (AoI) are reformulated to measure the freshness of the aggregated information at the receiver in this image-capture scenario. We then dimension the network (number of orbital planes and satellites per orbital plane) for a given target age and covered area that quantify the level of achievement of the task. The results show that 20 orbital planes with 20 satellites are necessary to keep the peak AoI below 60 seconds with a compression ratio > 23000, i.e., a size reduction of 99.996%, and for a approximately 100% probability of coverage.",
        "subjects": [
            "cs.NI",
            "astro-ph.IM"
        ],
        "comment": "This paper was presented at the ComSoc Challenge on 'Future 6G NTN Systems' held at ICCSPA'24, where it was awarded second place"
    },
    {
        "paper id": "2410.07434",
        "abstract url": "https://arxiv.org/abs/2410.07434",
        "title": "Surgical Depth Anything: Depth Estimation for Surgical Scenes using Foundation Models",
        "rating": "-2",
        "keywords": [
            [
                "Depth"
            ],
            [
                "Surgical",
                "surgery"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Monocular depth estimation is crucial for tracking and reconstruction algorithms, particularly in the context of surgical videos. However, the inherent challenges in directly obtaining ground truth depth maps during surgery render supervised learning approaches impractical. While many self-supervised methods based on Structure from Motion (SfM) have shown promising results, they rely heavily on high-quality camera motion and require optimization on a per-patient basis. These limitations can be mitigated by leveraging the current state-of-the-art foundational model for depth estimation, Depth Anything. However, when directly applied to surgical scenes, Depth Anything struggles with issues such as blurring, bleeding, and reflections, resulting in suboptimal performance. This paper presents a fine-tuning of the Depth Anything model specifically for the surgical domain, aiming to deliver more accurate pixel-wise depth maps tailored to the unique requirements and challenges of surgical environments. Our fine-tuning approach significantly improves the model's performance in surgical scenes, reducing errors related to blurring and reflections, and achieving a more reliable and precise depth estimation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07442",
        "abstract url": "https://arxiv.org/abs/2410.07442",
        "title": "Self-Supervised Learning for Real-World Object Detection: a Survey",
        "rating": "-2",
        "keywords": [
            [
                "vehicle",
                "infrared"
            ],
            [
                "remote sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Self-Supervised Learning (SSL) has emerged as a promising approach in computer vision, enabling networks to learn meaningful representations from large unlabeled datasets. SSL methods fall into two main categories: instance discrimination and Masked Image Modeling (MIM). While instance discrimination is fundamental to SSL, it was originally designed for classification and may be less effective for object detection, particularly for small objects. In this survey, we focus on SSL methods specifically tailored for real-world object detection, with an emphasis on detecting small objects in complex environments. Unlike previous surveys, we offer a detailed comparison of SSL strategies, including object-level instance discrimination and MIM methods, and assess their effectiveness for small object detection using both CNN and ViT-based architectures. Specifically, our benchmark is performed on the widely-used COCO dataset, as well as on a specialized real-world dataset focused on vehicle detection in infrared remote sensing imagery. We also assess the impact of pre-training on custom domain-specific datasets, highlighting how certain SSL strategies are better suited for handling uncurated data. Our findings highlight that instance discrimination methods perform well with CNN-based encoders, while MIM methods are better suited for ViT-based architectures and custom dataset pre-training. This survey provides a practical guide for selecting optimal SSL strategies, taking into account factors such as backbone architecture, object size, and custom pre-training requirements. Ultimately, we show that choosing an appropriate SSL pre-training strategy, along with a suitable encoder, significantly enhances performance in real-world object detection, particularly for small object detection in frugal settings.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07468",
        "abstract url": "https://arxiv.org/abs/2410.07468",
        "title": "Fast Real Evaluation Through Sound Mixed-Precision Tuning",
        "rating": "-2",
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "Evaluating a real-valued expression to high precision is a key building block in computational mathematics, physics, and numerics. A typical implementation uses a uniform precision for each operation, and doubles that precision until the real result can be bounded to some sufficiently narrow interval. However, this is wasteful: usually only a few operations really need to be performed at high precision, and the bulk of the expression could use much lower precision. Uniform precision can also waste iterations discovering the necessary precision and then still overestimate by up to a factor of two. We propose to instead use mixed-precision interval arithmetic to evaluate real-valued expressions. A key challenge is deriving the mixed-precision assignment both soundly and quickly. To do so, we introduce a sound variation of error Taylor series and condition numbers, specialized to interval arithmetic, that can be evaluated with minimal overhead thanks to an \"exponent trick\". Our implementation, Reval, achieves a speed-up of 1.25x compared to the state-of-the-art Sollya tool, with the speed-up increasing to 2.99x on the most difficult input points. An examination of the precisions used with and without precision tuning shows that the speed-up results come from quickly assigning lower precisions for the majority of operations.",
        "subjects": [
            "math.NA",
            "cs.MS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07492",
        "abstract url": "https://arxiv.org/abs/2410.07492",
        "title": "Simulating the blood transfusion system in Kenya: Modelling methods and exploratory analyses",
        "rating": "-2",
        "keywords": [
            [
                "clinical"
            ]
        ],
        "abstract": "The process of collecting blood from donors and making it available for transfusion requires a complex series of operations involving multiple actors and resources at each step. Ensuring hospitals receive adequate and safe blood for transfusion is a common challenge across low- and middle-income countries, but is rarely addressed from a system level. This paper presents the first use of discrete event simulation to study the blood system in Kenya and to explore the effect of variations and perturbations at different steps of the system on meeting patient blood demand. A process map of the Kenyan blood system was developed to capture critical steps from blood donation to transfusion using interviews with blood bank, hospital, and laboratory personnel at four public hospitals across three counties in Kenya. The blood system was simulated starting with blood collection, a blood bank where blood is tested and stored before it is issued, a major hospital attached to the blood bank, and several smaller hospitals served by the same blood bank. Values for supply-side parameters were based mainly on expert opinion; demand-side parameters were based on data from blood requisitions made in hospital wards, and dispatch of blood from the hospital laboratory. Illustrative examples demonstrate how the model can be used to explore the impacts of changes in blood collection (e.g., prioritising different donor types), blood demand (e.g., differing clinical case mix), and blood distribution (e.g., restocking strategies) on meeting demand at patient level. The model can reveal potential process impediments in the blood system and aid in choosing strategies for improving blood collection, distribution or use. Such a systems approach allows for interventions at different steps in the blood continuum to be tested on blood availability for different patients presenting at diverse hospitals across the country.",
        "subjects": [
            "physics.med-ph",
            "eess.SY"
        ],
        "comment": "38 pages, 8 figures"
    },
    {
        "paper id": "2410.07500",
        "abstract url": "https://arxiv.org/abs/2410.07500",
        "title": "Learning to Generate Diverse Pedestrian Movements from Web Videos with Noisy Labels",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "forecasting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Understanding and modeling pedestrian movements in the real world is crucial for applications like motion forecasting and scene simulation. Many factors influence pedestrian movements, such as scene context, individual characteristics, and goals, which are often ignored by the existing human generation methods. Web videos contain natural pedestrian behavior and rich motion context, but annotating them with pre-trained predictors leads to noisy labels. In this work, we propose learning diverse pedestrian movements from web videos. We first curate a large-scale dataset called CityWalkers that captures diverse real-world pedestrian movements in urban scenes. Then, based on CityWalkers, we propose a generative model called PedGen for diverse pedestrian movement generation. PedGen introduces automatic label filtering to remove the low-quality labels and a mask embedding to train with partial labels. It also contains a novel context encoder that lifts the 2D scene context to 3D and can incorporate various context factors in generating realistic pedestrian movements in urban scenes. Experiments show that PedGen outperforms existing baseline methods for pedestrian movement generation by learning from noisy labels and incorporating the context factors. In addition, PedGen achieves zero-shot generalization in both real-world and simulated environments. The code, model, and data will be made publicly available at https://genforce.github.io/PedGen/ .",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://genforce.github.io/PedGen/"
    },
    {
        "paper id": "2410.07546",
        "abstract url": "https://arxiv.org/abs/2410.07546",
        "title": "The BRAM is the Limit: Shattering Myths, Shaping Standards, and Building Scalable PIM Accelerators",
        "rating": "-2",
        "keywords": [
            [
                "FPGA"
            ]
        ],
        "abstract": "Many recent FPGA-based Processor-in-Memory (PIM) architectures have appeared with promises of impressive levels of parallelism but with performance that falls short of expectations due to reduced maximum clock frequencies, an inability to scale processing elements up to the maximum BRAM capacity, and minimal hardware support for large reduction operations. In this paper, we first establish what we believe should be a \"Gold Standard\" set of design objectives for PIM-based FPGA designs. This Gold Standard was established to serve as an absolute metric for comparing PIMs developed on different technology nodes and vendor families as well as an aspirational goal for designers. We then present IMAGine, an In-Memory Accelerated GEMV engine used as a case study to show the Gold Standard can be realized in practice. IMAGine serves as an existence proof that dispels several myths surrounding what is normally accepted as clocking and scaling FPGA performance limitations. Specifically, IMAGine clocks at the maximum frequency of the BRAM and scales to 100% of the available BRAMs. Comparative analyses are presented showing execution speeds over existing PIM-based GEMV engines on FPGAs and achieving a 2.65x - 3.2x faster clock. An AMD Alveo U55 implementation is presented that achieves a system clock speed of 737 MHz, providing 64K bit-serial multiply-accumulate (MAC) units for GEMV operation. This establishes IMAGine as the fastest PIM-based GEMV overlay, outperforming even the custom PIM-based FPGA accelerators reported to date. Additionally, it surpasses TPU v1-v2 and Alibaba Hanguang 800 in clock speed while offering an equal or greater number of MAC units.",
        "subjects": [
            "cs.AR"
        ],
        "comment": "Accepted for poster presentation in FCCM 2024. arXiv admin note: substantial text overlap with arXiv:2410.04367"
    },
    {
        "paper id": "2410.07552",
        "abstract url": "https://arxiv.org/abs/2410.07552",
        "title": "Methods for Few-View CT Image Reconstruction",
        "rating": "-2",
        "keywords": [
            [
                "CT"
            ]
        ],
        "abstract": "Computed Tomography (CT) is an essential non-destructive three dimensional imaging modality used in medicine, security screening, and inspection of manufactured components. Typical CT data acquisition entails the collection of a thousand or more projections through the object under investigation through a range of angles covering one hundred eighty degrees or more. It may be desirable or required that the number of projections angles be reduced by one or two orders of magnitude for reasons such as acquisition time or dose. Unless specialized reconstruction algorithms are applied, reconstructing with fewer views will result in streak artifacts and failure to resolve object boundaries at certain orientations. These artifacts may substantially diminish the usefulness of the reconstructed CT volumes. Here we develop constrained and regularized numerical optimization methods to reconstruct CT volumes from 4-28 projections. These methods entail utilization of novel data fidelity and convex and non-convex regularization terms. In addition, the methods outlined here are usually carried out by a sequence of two or three numerical optimization methods in sequence. The efficacy of our methods is demonstrated on four measured and three simulated few-view CT data sets. We show that these methods outperform other state of the art few-view numerical optimization methods.",
        "subjects": [
            "physics.med-ph",
            "cs.MS",
            "physics.comp-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07554",
        "abstract url": "https://arxiv.org/abs/2410.07554",
        "title": "Force-Centric Imitation Learning with Force-Motion Capture System for Contact-Rich Manipulation",
        "rating": "-2",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "In most contact-rich manipulation tasks, humans apply time-varying forces to the target object, compensating for inaccuracies in the vision-guided hand trajectory. However, current robot learning algorithms primarily focus on trajectory-based policy, with limited attention given to learning force-related skills. To address this limitation, we introduce ForceMimic, a force-centric robot learning system, providing a natural, force-aware and robot-free robotic demonstration collection system, along with a hybrid force-motion imitation learning algorithm for robust contact-rich manipulation. Using the proposed ForceCapture system, an operator can peel a zucchini in 5 minutes, while force-feedback teleoperation takes over 13 minutes and struggles with task completion. With the collected data, we propose HybridIL to train a force-centric imitation learning model, equipped with hybrid force-position control primitive to fit the predicted wrench-position parameters during robot execution. Experiments demonstrate that our approach enables the model to learn a more robust policy under the contact-rich task of vegetable peeling, increasing the success rates by 54.5% relatively compared to state-of-the-art pure-vision-based imitation learning. Hardware, code, data and more results would be open-sourced on the project website at https://forcemimic.github.io.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 7 figures, submitted to ICRA 2025, project website at https://forcemimic.github.io"
    },
    {
        "paper id": "2410.07586",
        "abstract url": "https://arxiv.org/abs/2410.07586",
        "title": "A Cloud in the Sky: Geo-Aware On-board Data Services for LEO Satellites",
        "rating": "-2",
        "keywords": [
            [
                "satellite"
            ]
        ],
        "abstract": "We propose an architecture with accompanying protocol for on-board satellite data infrastructure designed for Low Earth Orbit (LEO) constellations offering communication services, such as direct-to-cell connectivity. Our design leverages the unused or under-used computing and communication resources of LEO satellites that are orbiting over uninhabited parts of the earth, like the oceans. We show how blockchain-backed distributed transactions can be run efficiently on this architecture to offer smart contract services. A key aspect of the proposed architecture that sets it apart from other blockchain systems is that migration of the ledger is not done solely to recover from failures. Rather, migration is also performed periodically and continuously as the satellites circle around in their orbits and enter and leave the blockchain service area. We show in simulations how message and blockchain processing overhead can be contained using different sizes of dynamic geo-aware service areas.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07595",
        "abstract url": "https://arxiv.org/abs/2410.07595",
        "title": "Geometric structure and transversal logic of quantum Reed-Muller codes",
        "rating": "-2",
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "Designing efficient and noise-tolerant quantum computation protocols generally begins with an understanding of quantum error-correcting codes and their native logical operations. The simplest class of native operations are transversal gates, which are naturally fault-tolerant. In this paper, we aim to characterize the transversal gates of quantum Reed-Muller (RM) codes by exploiting the well-studied properties of their classical counterparts. We start our work by establishing a new geometric characterization of quantum RM codes via the Boolean hypercube and its associated subcube complex. More specifically, a set of stabilizer generators for a quantum RM code can be described via transversal $X$ and $Z$ operators acting on subcubes of particular dimensions. This characterization leads us to define subcube operators composed of single-qubit $\u03c0/2^k$ $Z$-rotations that act on subcubes of given dimensions. We first characterize the action of subcube operators on the code space: depending on the dimension of the subcube, these operators either (1) act as a logical identity on the code space, (2) implement non-trivial logic, or (3) rotate a state away from the code space. Second, and more remarkably, we uncover that the logic implemented by these operators corresponds to circuits of multi-controlled-$Z$ gates that have an explicit and simple combinatorial description. Overall, this suite of results yields a comprehensive understanding of a class of natural transversal operators for quantum RM codes.",
        "subjects": [
            "quant-ph",
            "cs.IT",
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06549",
        "abstract url": "https://arxiv.org/abs/2410.06549",
        "title": "DiffGAD: A Diffusion-based Unsupervised Graph Anomaly Detector",
        "rating": "-2.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Graph"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Graph Anomaly Detection (GAD) is crucial for identifying abnormal entities within networks, garnering significant attention across various fields. Traditional unsupervised methods, which decode encoded latent representations of unlabeled data with a reconstruction focus, often fail to capture critical discriminative content, leading to suboptimal anomaly detection. To address these challenges, we present a Diffusion-based Graph Anomaly Detector (DiffGAD). At the heart of DiffGAD is a novel latent space learning paradigm, meticulously designed to enhance its proficiency by guiding it with discriminative content. This innovative approach leverages diffusion sampling to infuse the latent space with discriminative content and introduces a content-preservation mechanism that retains valuable information across different scales, significantly improving its adeptness at identifying anomalies with limited time and space complexity. Our comprehensive evaluation of DiffGAD, conducted on six real-world and large-scale datasets with various metrics, demonstrated its exceptional performance.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06681",
        "abstract url": "https://arxiv.org/abs/2410.06681",
        "title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "rating": "-2.5",
        "keywords": [
            [
                "robotics"
            ],
            [
                "industrial"
            ],
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "We live in a world that is experiencing an unprecedented boom of AI applications that increasingly penetrate and enhance all sectors of private and public life, from education, media, medicine, and mobility to the industrial and professional workspace, and -- potentially particularly consequentially -- robotics. As this world is simultaneously grappling with climate change, the climate and environmental implications of the development and use of AI have become an important subject of public and academic debate. In this paper, we aim to provide guidance on the climate-related regulation for data centers and AI specifically, and discuss how to operationalize these requirements. We also highlight challenges and room for improvement, and make a number of policy proposals to this end. In particular, we propose a specific interpretation of the AI Act to bring reporting on the previously unadressed energy consumption from AI inferences back into the scope. We also find that the AI Act fails to address indirect greenhouse gas emissions from AI applications. Furthermore, for the purpose of energy consumption reporting, we compare levels of measurement within data centers and recommend measurement at the cumulative server level. We also argue for an interpretation of the AI Act that includes environmental concerns in the mandatory risk assessment (sustainability risk assessment, SIA), and provide guidance on its operationalization. The EU data center regulation proves to be a good first step but requires further development by including binding renewable energy and efficiency targets for data centers. Overall, we make twelve concrete policy proposals, in four main areas: Energy and Environmental Reporting Obligations; Legal and Regulatory Clarifications; Transparency and Accountability Mechanisms; and Future Far-Reaching Measures beyond Transparency.",
        "subjects": [
            "cs.CY",
            "cs.AI"
        ],
        "comment": "18 pages, 1 figure, preprint"
    },
    {
        "paper id": "2410.07039",
        "abstract url": "https://arxiv.org/abs/2410.07039",
        "title": "Distributionally Robust Clustered Federated Learning: A Case Study in Healthcare",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Healthcare"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we address the challenge of heterogeneous data distributions in cross-silo federated learning by introducing a novel algorithm, which we term Cross-silo Robust Clustered Federated Learning (CS-RCFL). Our approach leverages the Wasserstein distance to construct ambiguity sets around each client's empirical distribution that capture possible distribution shifts in the local data, enabling evaluation of worst-case model performance. We then propose a model-agnostic integer fractional program to determine the optimal distributionally robust clustering of clients into coalitions so that possible biases in the local models caused by statistically heterogeneous client datasets are avoided, and analyze our method for linear and logistic regression models. Finally, we discuss a federated learning protocol that ensures the privacy of client distributions, a critical consideration, for instance, when clients are healthcare institutions. We evaluate our algorithm on synthetic and real-world healthcare data.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "8 pages, 3 figures, Accepted to IEEE CDC 2024"
    },
    {
        "paper id": "2410.07271",
        "abstract url": "https://arxiv.org/abs/2410.07271",
        "title": "Multi-Task Program Error Repair and Explanatory Diagnosis",
        "rating": "-2.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "Diagnosis"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Program errors can occur in any type of programming, and can manifest in a variety of ways, such as unexpected output, crashes, or performance issues. And program error diagnosis can often be too abstract or technical for developers to understand, especially for beginners. The goal of this paper is to present a novel machine-learning approach for Multi-task Program Error Repair and Explanatory Diagnosis (mPRED). A pre-trained language model is used to encode the source code, and a downstream model is specifically designed to identify and repair errors. Programs and test cases will be augmented and optimized from several perspectives. Additionally, our approach incorporates a \"chain of thoughts\" method, which enables the models to produce intermediate reasoning explanations before providing the final correction. To aid in visualizing and analyzing the program structure, we use a graph neural network for program structure visualization. Overall, our approach offers a promising approach for repairing program errors across different programming languages and providing helpful explanations to programmers.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07454",
        "abstract url": "https://arxiv.org/abs/2410.07454",
        "title": "Representation-Enhanced Neural Knowledge Integration with Application to Large-Scale Medical Ontology Learning",
        "rating": "-2.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "biomedical",
                "Medical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "A large-scale knowledge graph enhances reproducibility in biomedical data discovery by providing a standardized, integrated framework that ensures consistent interpretation across diverse datasets. It improves generalizability by connecting data from various sources, enabling broader applicability of findings across different populations and conditions. Generating reliable knowledge graph, leveraging multi-source information from existing literature, however, is challenging especially with a large number of node sizes and heterogeneous relations. In this paper, we propose a general theoretically guaranteed statistical framework, called RENKI, to enable simultaneous learning of multiple relation types. RENKI generalizes various network models widely used in statistics and computer science. The proposed framework incorporates representation learning output into initial entity embedding of a neural network that approximates the score function for the knowledge graph and continuously trains the model to fit observed facts. We prove nonasymptotic bounds for in-sample and out-of-sample weighted MSEs in relation to the pseudo-dimension of the knowledge graph function class. Additionally, we provide pseudo-dimensions for score functions based on multilayer neural networks with ReLU activation function, in the scenarios when the embedding parameters either fixed or trainable. Finally, we complement our theoretical results with numerical studies and apply the method to learn a comprehensive medical knowledge graph combining a pretrained language model representation with knowledge graph links observed in several medical ontologies. The experiments justify our theoretical findings and demonstrate the effect of weighting in the presence of heterogeneous relations and the benefit of incorporating representation learning in nonparametric models.",
        "subjects": [
            "stat.ME",
            "cs.LG",
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07472",
        "abstract url": "https://arxiv.org/abs/2410.07472",
        "title": "Exploring the design space of deep-learning-based weather forecasting systems",
        "rating": "-2.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Despite tremendous progress in developing deep-learning-based weather forecasting systems, their design space, including the impact of different design choices, is yet to be well understood. This paper aims to fill this knowledge gap by systematically analyzing these choices including architecture, problem formulation, pretraining scheme, use of image-based pretrained models, loss functions, noise injection, multi-step inputs, additional static masks, multi-step finetuning (including larger stride models), as well as training on a larger dataset. We study fixed-grid architectures such as UNet, fully convolutional architectures, and transformer-based models, along with grid-invariant architectures, including graph-based and operator-based models. Our results show that fixed-grid architectures outperform grid-invariant architectures, indicating a need for further architectural developments in grid-invariant models such as neural operators. We therefore propose a hybrid system that combines the strong performance of fixed-grid models with the flexibility of grid-invariant architectures. We further show that multi-step fine-tuning is essential for most deep-learning models to work well in practice, which has been a common practice in the past. Pretraining objectives degrade performance in comparison to supervised training, while image-based pretrained models provide useful inductive biases in some cases in comparison to training the model from scratch. Interestingly, we see a strong positive effect of using a larger dataset when training a smaller model as compared to training on a smaller dataset for longer. Larger models, on the other hand, primarily benefit from just an increase in the computational budget. We believe that these results will aid in the design of better weather forecasting systems in the future.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07501",
        "abstract url": "https://arxiv.org/abs/2410.07501",
        "title": "Inferring biological processes with intrinsic noise from cross-sectional data",
        "rating": "-2.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "biological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Inferring dynamical models from data continues to be a significant challenge in computational biology, especially given the stochastic nature of many biological processes. We explore a common scenario in omics, where statistically independent cross-sectional samples are available at a few time points, and the goal is to infer the underlying diffusion process that generated the data. Existing inference approaches often simplify or ignore noise intrinsic to the system, compromising accuracy for the sake of optimization ease. We circumvent this compromise by inferring the phase-space probability flow that shares the same time-dependent marginal distributions as the underlying stochastic process. Our approach, probability flow inference (PFI), disentangles force from intrinsic stochasticity while retaining the algorithmic ease of ODE inference. Analytically, we prove that for Ornstein-Uhlenbeck processes the regularized PFI formalism yields a unique solution in the limit of well-sampled distributions. In practical applications, we show that PFI enables accurate parameter and force estimation in high-dimensional stochastic reaction networks, and that it allows inference of cell differentiation dynamics with molecular noise, outperforming state-of-the-art approaches.",
        "subjects": [
            "cs.LG",
            "physics.bio-ph",
            "q-bio.QM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07542",
        "abstract url": "https://arxiv.org/abs/2410.07542",
        "title": "Generalizable Indoor Human Activity Recognition Method Based on Micro-Doppler Corner Point Cloud and Dynamic Graph Learning",
        "rating": "-2.5",
        "keywords": [
            [
                "Point Cloud"
            ],
            [
                "radar"
            ],
            [
                "Graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Through-the-wall radar (TWR) human activity recognition can be achieved by fusing micro-Doppler signature extraction and intelligent decision-making algorithms. However, limited by the insufficient priori of tester in practical indoor scenarios, the trained models on one tester are commonly difficult to inference well on other testers, which causes poor generalization ability. To solve this problem, this paper proposes a generalizable indoor human activity recognition method based on micro-Doppler corner point cloud and dynamic graph learning. In the proposed method, DoG-\u03bcD-CornerDet is used for micro-Doppler corner extraction on two types of radar profiles. Then, a micro-Doppler corner filtering method based on polynomial fitting smoothing is proposed to maximize the feature distance under the constraints of the kinematic model. The extracted corners from the two types of radar profiles are concatenated together into three-dimensional point cloud. Finally, the paper proposes a dynamic graph neural network (DGNN)-based recognition method for data-to-activity label mapping. Visualization, comparison and ablation experiments are carried out to verify the effectiveness of the proposed method. The results prove that the proposed method has strong generalization ability on radar data collected from different testers.",
        "subjects": [
            "eess.SP",
            "cs.AI"
        ],
        "comment": "15 pages, 12 figures, 6 tables, in IEEE Transactions on Aerospace and Electronics Systems, 2024"
    },
    {
        "paper id": "2410.06565",
        "abstract url": "https://arxiv.org/abs/2410.06565",
        "title": "Agile Mobility with Rapid Online Adaptation via Meta-learning and Uncertainty-aware MPPI",
        "rating": "-3",
        "keywords": [
            [
                "robot"
            ],
            [
                "physics"
            ]
        ],
        "abstract": "Modern non-linear model-based controllers require an accurate physics model and model parameters to be able to control mobile robots at their limits. Also, due to surface slipping at high speeds, the friction parameters may continually change (like tire degradation in autonomous racing), and the controller may need to adapt rapidly. Many works derive a task-specific robot model with a parameter adaptation scheme that works well for the task but requires a lot of effort and tuning for each platform and task. In this work, we design a full model-learning-based controller based on meta pre-training that can very quickly adapt using few-shot dynamics data to any wheel-based robot with any model parameters, while also reasoning about model uncertainty. We demonstrate our results in small-scale numeric simulation, the large-scale Unity simulator, and on a medium-scale hardware platform with a wide range of settings. We show that our results are comparable to domain-specific well-engineered controllers, and have excellent generalization performance across all scenarios.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06572",
        "abstract url": "https://arxiv.org/abs/2410.06572",
        "title": "Can DeepFake Speech be Reliably Detected?",
        "rating": "-3",
        "keywords": [
            [
                "DeepFake"
            ],
            [
                "attacks"
            ],
            [
                "text-to-speech"
            ],
            [
                "cs.LG",
                "cs.SD"
            ]
        ],
        "abstract": "Recent advances in text-to-speech (TTS) systems, particularly those with voice cloning capabilities, have made voice impersonation readily accessible, raising ethical and legal concerns due to potential misuse for malicious activities like misinformation campaigns and fraud. While synthetic speech detectors (SSDs) exist to combat this, they are vulnerable to ``test domain shift\", exhibiting decreased performance when audio is altered through transcoding, playback, or background noise. This vulnerability is further exacerbated by deliberate manipulation of synthetic speech aimed at deceiving detectors. This work presents the first systematic study of such active malicious attacks against state-of-the-art open-source SSDs. White-box attacks, black-box attacks, and their transferability are studied from both attack effectiveness and stealthiness, using both hardcoded metrics and human ratings. The results highlight the urgent need for more robust detection methods in the face of evolving adversarial threats.",
        "subjects": [
            "cs.SD",
            "cs.CR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06627",
        "abstract url": "https://arxiv.org/abs/2410.06627",
        "title": "Variations in Multi-Agent Actor-Critic Frameworks for Joint Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and Directions",
        "rating": "-3",
        "keywords": [
            [
                "trajectory",
                "vehicle"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can effectively execute surveillance, connectivity, and computing services to ground users (GUs). These missions require trajectory planning, UAV-GUs association, task offloading, next-hop selection, and resources such as transmit power, bandwidth, caching, and computing allocation to improve network performances. Owing to the highly dynamic topology, limited resources, and non-availability of global knowledge, optimizing network performance in UAVSNs is very intricate. Hence, it requires an adaptive joint optimization framework that can tackle both discrete and continuous decision variables to ensure optimal network performance under dynamic constraints. Multi-agent deep reinforcement learning-based adaptive actor-critic framework can efficiently address these problems. This paper investigates the recent evolutions of actor-critic frameworks to deal with joint optimization problems in UAVSNs. In addition, challenges and potential solutions are addressed as research directions.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06729",
        "abstract url": "https://arxiv.org/abs/2410.06729",
        "title": "Perceptual Quality Assessment of Octree-RAHT Encoded 3D Point Clouds",
        "rating": "-3",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "Quality Assessment"
            ]
        ],
        "abstract": "No-reference bitstream-layer point cloud quality assessment (PCQA) can be deployed without full decoding at any network node to achieve real-time quality monitoring. In this work, we focus on the PCQA problem dedicated to Octree-RAHT encoding mode. First, to address the issue that existing PCQA databases have a small scale and limited distortion levels, we establish the WPC5.0 database which is the first one dedicated to Octree-RAHT encoding mode with a scale of 400 distorted point clouds (PCs) including 4 geometric multiplied by 5 attitude distortion levels. Then, we propose the first PCQA model dedicated to Octree-RAHT encoding mode by parsing PC bitstreams without full decoding. The model introduces texture bitrate (TBPP) to predict texture complexity (TC) and further derives the texture distortion factor. In addition, the Geometric Quantization Parameter (PQS) is used to estimate the geometric distortion factor, which is then integrated into the model along with the texture distortion factor to obtain the proposed PCQA model named streamPCQ-OR. The proposed model has been compared with other advanced PCQA methods on the WPC5.0, BASICS and M-PCCD databases, and experimental results show that our model has excellent performance while having very low computational complexity, providing a reliable choice for time-critical applications. To facilitate subsequent research, the database and source code will be publicly released at https://github.com/qdushl/Waterloo-Point-Cloud-Database-5.0.",
        "subjects": [
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06982",
        "abstract url": "https://arxiv.org/abs/2410.06982",
        "title": "Structure-Centric Robust Monocular Depth Estimation via Knowledge Distillation",
        "rating": "-3",
        "keywords": [
            [
                "3D",
                "Depth"
            ],
            [
                "graphs"
            ],
            [
                "industrial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Monocular depth estimation, enabled by self-supervised learning, is a key technique for 3D perception in computer vision. However, it faces significant challenges in real-world scenarios, which encompass adverse weather variations, motion blur, as well as scenes with poor lighting conditions at night. Our research reveals that we can divide monocular depth estimation into three sub-problems: depth structure consistency, local texture disambiguation, and semantic-structural correlation. Our approach tackles the non-robustness of existing self-supervised monocular depth estimation models to interference textures by adopting a structure-centered perspective and utilizing the scene structure characteristics demonstrated by semantics and illumination. We devise a novel approach to reduce over-reliance on local textures, enhancing robustness against missing or interfering patterns. Additionally, we incorporate a semantic expert model as the teacher and construct inter-model feature dependencies via learnable isomorphic graphs to enable aggregation of semantic structural knowledge. Our approach achieves state-of-the-art out-of-distribution monocular depth estimation performance across a range of public adverse scenario datasets. It demonstrates notable scalability and compatibility, without necessitating extensive model engineering. This showcases the potential for customizing models for diverse industrial applications.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "To be published in Asian Conference on Computer Vision 2024"
    },
    {
        "paper id": "2410.06990",
        "abstract url": "https://arxiv.org/abs/2410.06990",
        "title": "Structure and Control of Biology-inspired Networks",
        "rating": "-3",
        "keywords": [
            [
                "graph"
            ],
            [
                "Biology-inspired",
                "disease"
            ]
        ],
        "abstract": "There is increasing interest in developing the theoretical foundations of networked control systems that illuminate how brain networks function so as to enable sensory perception, control of movement, memory and all the operations that are needed for animals to survive. The present paper proposes a biologically inspired network model featuring dynamic connections regulated by Hebbian learning. Drawing on the machinery of graph theory and classical control we show that our novel nonlinear model exhibits such biologically plausible features as bounded evolution, stability, resilience, and a kind of structural stability -- meaning that perturbations of the model parameters leave the essential properties of the model in tact. The proposed network model involves generalized cactus graphs with multiple control input nodes, and it is shown that the properties of the network are resilient to various changes in network topology provided these changes preserve the generalized cactus structure. A particular example described in what follows is an idealized network model of the visual system of a macaque monkey. The model displays resilience to network disruptions such as might occur in a living organism due to disease or injury. A different model of the same type provides an example of a system that can perform data classification.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "12 pages"
    },
    {
        "paper id": "2410.06997",
        "abstract url": "https://arxiv.org/abs/2410.06997",
        "title": "A Diffusion-based Xray2MRI Model: Generating Pseudo-MRI Volumes From one Single X-ray",
        "rating": "-3",
        "keywords": [
            [
                "depth"
            ],
            [
                "Diffusion"
            ],
            [
                "medical",
                "diagnosis",
                "MRI",
                "X-ray"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Knee osteoarthritis (KOA) is a prevalent musculoskeletal disorder, and X-rays are commonly used for its diagnosis due to their cost-effectiveness. Magnetic Resonance Imaging (MRI), on the other hand, offers detailed soft tissue visualization and has become a valuable supplementary diagnostic tool for KOA. Unfortunately, the high cost and limited accessibility of MRI hinder its widespread use, leaving many patients with KOA reliant solely on X-ray imaging. In this study, we introduce a novel diffusion-based Xray2MRI model capable of generating pseudo-MRI volumes from one single X-ray image. In addition to using X-rays as conditional input, our model integrates target depth, KOA probability distribution, and image intensity distribution modules to guide the synthesis process, ensuring that the generated corresponding slices accurately correspond to the anatomical structures. Experimental results demonstrate that by integrating information from X-rays with additional input data, our proposed approach is capable of generating pseudo-MRI sequences that approximate real MRI scans. Moreover, by increasing the inference times, the model achieves effective interpolation, further improving the continuity and smoothness of the generated MRI sequences, representing one promising initial attempt for cost-effective medical imaging solutions.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07408",
        "abstract url": "https://arxiv.org/abs/2410.07408",
        "title": "ACDC: Automated Creation of Digital Cousins for Robust Policy Learning",
        "rating": "-3",
        "keywords": [
            [
                "robot"
            ],
            [
                "physics"
            ]
        ],
        "abstract": "Training robot policies in the real world can be unsafe, costly, and difficult to scale. Simulation serves as an inexpensive and potentially limitless source of training data, but suffers from the semantics and physics disparity beween simulated and real-world environments. These discrepancies can be minimized by training in digital twins,which serve as virtual replicas of a real scene but are expensive to generate and cannot produce cross-domain generalization. To address these limitations, we propose the concept of digital cousins, a virtual asset or scene that, unlike a digital twin,does not explicitly model a real-world counterpart but still exhibits similar geometric and semantic affordances. As a result, digital cousins simultaneously reduce the cost of generating an analogous virtual environment while also facilitating better robustness during sim-to-real domain transfer by providing a distribution of similar training scenes. Leveraging digital cousins, we introduce a novel method for the Automatic Creation of Digital Cousins (ACDC), and propose a fully automated real-to-sim-to-real pipeline for generating fully interactive scenes and training robot policies that can be deployed zero-shot in the original scene. We find that ACDC can produce digital cousin scenes that preserve geometric and semantic affordances, and can be used to train policies that outperform policies trained on digital twins, achieving 90% vs. 25% under zero-shot sim-to-real transfer. Additional details are available at https://digital-cousins.github.io/.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "CoRL 2024"
    },
    {
        "paper id": "2410.07418",
        "abstract url": "https://arxiv.org/abs/2410.07418",
        "title": "NeRF-Accelerated Ecological Monitoring in Mixed-Evergreen Redwood Forest",
        "rating": "-3",
        "keywords": [
            [
                "NeRF",
                "radiance fields"
            ],
            [
                "LiDAR"
            ],
            [
                "biomass"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Forest mapping provides critical observational data needed to understand the dynamics of forest environments. Notably, tree diameter at breast height (DBH) is a metric used to estimate forest biomass and carbon dioxide (CO$_2$) sequestration. Manual methods of forest mapping are labor intensive and time consuming, a bottleneck for large-scale mapping efforts. Automated mapping relies on acquiring dense forest reconstructions, typically in the form of point clouds. Terrestrial laser scanning (TLS) and mobile laser scanning (MLS) generate point clouds using expensive LiDAR sensing, and have been used successfully to estimate tree diameter. Neural radiance fields (NeRFs) are an emergent technology enabling photorealistic, vision-based reconstruction by training a neural network on a sparse set of input views. In this paper, we present a comparison of MLS and NeRF forest reconstructions for the purpose of trunk diameter estimation in a mixed-evergreen Redwood forest. In addition, we propose an improved DBH-estimation method using convex-hull modeling. Using this approach, we achieved 1.68 cm RMSE, which consistently outperformed standard cylinder modeling approaches. Our code contributions and forest datasets are freely available at https://github.com/harelab-ucsc/RedwoodNeRF.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07421",
        "abstract url": "https://arxiv.org/abs/2410.07421",
        "title": "Segmenting objects with Bayesian fusion of active contour models and convnet priors",
        "rating": "-3",
        "keywords": [
            [
                "GAN"
            ],
            [
                "infrared"
            ],
            [
                "remote sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Instance segmentation is a core computer vision task with great practical significance. Recent advances, driven by large-scale benchmark datasets, have yielded good general-purpose Convolutional Neural Network (CNN)-based methods. Natural Resource Monitoring (NRM) utilizes remote sensing imagery with generally known scale and containing multiple overlapping instances of the same class, wherein the object contours are jagged and highly irregular. This is in stark contrast with the regular man-made objects found in classic benchmark datasets. We address this problem and propose a novel instance segmentation method geared towards NRM imagery. We formulate the problem as Bayesian maximum a posteriori inference which, in learning the individual object contours, incorporates shape, location, and position priors from state-of-the-art CNN architectures, driving a simultaneous level-set evolution of multiple object contours. We employ loose coupling between the CNNs that supply the priors and the active contour process, allowing a drop-in replacement of new network architectures. Moreover, we introduce a novel prior for contour shape, namely, a class of Deep Shape Models based on architectures from Generative Adversarial Networks (GANs). These Deep Shape Models are in essence a non-linear generalization of the classic Eigenshape formulation. In experiments, we tackle the challenging, real-world problem of segmenting individual dead tree crowns and delineating precise contours. We compare our method to two leading general-purpose instance segmentation methods - Mask R-CNN and K-net - on color infrared aerial imagery. Results show our approach to significantly outperform both methods in terms of reconstruction quality of tree crown contours. Furthermore, use of the GAN-based deep shape model prior yields significant improvement of all results over the vanilla Eigenshape prior.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07475",
        "abstract url": "https://arxiv.org/abs/2410.07475",
        "title": "Progressive Multi-Modal Fusion for Robust 3D Object Detection",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "autonomous driving",
                "LiDAR"
            ],
            [
                "BEV"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Multi-sensor fusion is crucial for accurate 3D object detection in autonomous driving, with cameras and LiDAR being the most commonly used sensors. However, existing methods perform sensor fusion in a single view by projecting features from both modalities either in Bird's Eye View (BEV) or Perspective View (PV), thus sacrificing complementary information such as height or geometric proportions. To address this limitation, we propose ProFusion3D, a progressive fusion framework that combines features in both BEV and PV at both intermediate and object query levels. Our architecture hierarchically fuses local and global features, enhancing the robustness of 3D object detection. Additionally, we introduce a self-supervised mask modeling pre-training strategy to improve multi-modal representation learning and data efficiency through three novel objectives. Extensive experiments on nuScenes and Argoverse2 datasets conclusively demonstrate the efficacy of ProFusion3D. Moreover, ProFusion3D is robust to sensor failure, demonstrating strong performance when only one modality is available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07493",
        "abstract url": "https://arxiv.org/abs/2410.07493",
        "title": "Autonomous Robotic System with Optical Coherence Tomography Guidance for Vascular Anastomosis",
        "rating": "-3",
        "keywords": [
            [
                "Robot"
            ],
            [
                "surgical",
                "organ"
            ]
        ],
        "abstract": "Vascular anastomosis, the surgical connection of blood vessels, is essential in procedures such as organ transplants and reconstructive surgeries. The precision required limits accessibility due to the extensive training needed, with manual suturing leading to variable outcomes and revision rates up to 7.9%. Existing robotic systems, while promising, are either fully teleoperated or lack the capabilities necessary for autonomous vascular anastomosis. We present the Micro Smart Tissue Autonomous Robot (micro-STAR), an autonomous robotic system designed to perform vascular anastomosis on small-diameter vessels. The micro-STAR system integrates a novel suturing tool equipped with Optical Coherence Tomography (OCT) fiber-optic sensor and a microcamera, enabling real-time tissue detection and classification. Our system autonomously places sutures and manipulates tissue with minimal human intervention. In an ex vivo study, micro-STAR achieved outcomes competitive with experienced surgeons in terms of leak pressure, lumen reduction, and suture placement variation, completing 90% of sutures without human intervention. This represents the first instance of a robotic system autonomously performing vascular anastomosis on real tissue, offering significant potential for improving surgical precision and expanding access to high-quality care.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": "This paper was submitted to IEEE TMRB and is currently under review. There are 9 pages, 9 figures, and 2 tables"
    },
    {
        "paper id": "2410.07591",
        "abstract url": "https://arxiv.org/abs/2410.07591",
        "title": "Robustness and Security Enhancement of Radio Frequency Fingerprint Identification in Time-Varying Channels",
        "rating": "-3",
        "keywords": [
            [
                "attacks"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "Radio frequency fingerprint identification (RFFI) is becoming increasingly popular, especially in applications with constrained power, such as the Internet of Things (IoT). Due to subtle manufacturing variations, wireless devices have unique radio frequency fingerprints (RFFs). These RFFs can be used with pattern recognition algorithms to classify wireless devices. However, Implementing reliable RFFI in time-varying channels is challenging because RFFs are often distorted by channel effects, reducing the classification accuracy. This paper introduces a new channel-robust RFF, and leverages transfer learning to enhance RFFI in the time-varying channels. Experimental results show that the proposed RFFI system achieved an average classification accuracy improvement of 33.3 % in indoor environments and 34.5 % in outdoor environments. This paper also analyzes the security of the proposed RFFI system to address the security flaw in formalized impersonation attacks. Since RFF collection is being carried out in uncontrolled deployment environments, RFFI systems can be targeted with false RFFs sent by rogue devices. The resulting classifiers may classify the rogue devices as legitimate, effectively replacing their true identities. To defend against impersonation attacks, a novel keyless countermeasure is proposed, which exploits the intrinsic output of the softmax function after classifier training without sacrificing the lightweight nature of RFFI. Experimental results demonstrate an average increase of 0.3 in the area under the receiver operating characteristic curve (AUC), with a 40.0 % improvement in attack detection rate in indoor and outdoor environments.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "15 pages"
    },
    {
        "paper id": "2410.06560",
        "abstract url": "https://arxiv.org/abs/2410.06560",
        "title": "Mitigating Time Discretization Challenges with WeatherODE: A Sandwich Physics-Driven Neural ODE for Weather Forecasting",
        "rating": "-3.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "Physics"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In the field of weather forecasting, traditional models often grapple with discretization errors and time-dependent source discrepancies, which limit their predictive performance. In this paper, we present WeatherODE, a novel one-stage, physics-driven ordinary differential equation (ODE) model designed to enhance weather forecasting accuracy. By leveraging wave equation theory and integrating a time-dependent source model, WeatherODE effectively addresses the challenges associated with time-discretization error and dynamic atmospheric processes. Moreover, we design a CNN-ViT-CNN sandwich structure, facilitating efficient learning dynamics tailored for distinct yet interrelated tasks with varying optimization biases in advection equation estimation. Through rigorous experiments, WeatherODE demonstrates superior performance in both global and regional weather forecasting tasks, outperforming recent state-of-the-art approaches by significant margins of over 40.0\\% and 31.8\\% in root mean square error (RMSE), respectively. The source code is available at \\url{https://github.com/DAMO-DI-ML/WeatherODE}.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07174",
        "abstract url": "https://arxiv.org/abs/2410.07174",
        "title": "Neural Circuit Architectural Priors for Quadruped Locomotion",
        "rating": "-3.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "robot"
            ],
            [
                "biologically"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Learning-based approaches to quadruped locomotion commonly adopt generic policy architectures like fully connected MLPs. As such architectures contain few inductive biases, it is common in practice to incorporate priors in the form of rewards, training curricula, imitation data, or trajectory generators. In nature, animals are born with priors in the form of their nervous system's architecture, which has been shaped by evolution to confer innate ability and efficient learning. For instance, a horse can walk within hours of birth and can quickly improve with practice. Such architectural priors can also be useful in ANN architectures for AI. In this work, we explore the advantages of a biologically inspired ANN architecture for quadruped locomotion based on neural circuits in the limbs and spinal cord of mammals. Our architecture achieves good initial performance and comparable final performance to MLPs, while using less data and orders of magnitude fewer parameters. Our architecture also exhibits better generalization to task variations, even admitting deployment on a physical robot without standard sim-to-real methods. This work shows that neural circuits can provide valuable architectural priors for locomotion and encourages future work in other sensorimotor skills.",
        "subjects": [
            "q-bio.NC",
            "cs.AI",
            "cs.LG",
            "cs.NE",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07295",
        "abstract url": "https://arxiv.org/abs/2410.07295",
        "title": "IterGen: Iterative Structured LLM Generation",
        "rating": "-3.5",
        "keywords": [
            [
                "grammar"
            ],
            [
                "SQL"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large Language Models (LLMs) are widely used for tasks such as natural language and code generation. Still, their outputs often suffer from issues like privacy violations, and semantically inaccurate code generation. Current libraries for LLM generation rely on left-to-right decoding without systematic support for backtracking, limiting the ability to correct or refine outputs mid-generation. To address this issue, we introduce IterGen, an intuitive framework for iterative, grammar-guided LLM generation that enables users to move both forward and backward within the generated output based on grammar symbols. By leveraging a symbol-to-position mapping, IterGen ensures efficient and structured generation while allowing for corrections during the process. We demonstrate IterGen's effectiveness in two important applications: reducing privacy leakage in LLM outputs and improving the accuracy of LLM-generated SQL queries. Our code is available at https://github.com/uiuc-arc/itergen",
        "subjects": [
            "cs.SE",
            "cs.LG",
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07364",
        "abstract url": "https://arxiv.org/abs/2410.07364",
        "title": "Unlocking Real-Time Fluorescence Lifetime Imaging: Multi-Pixel Parallelism for FPGA-Accelerated Processing",
        "rating": "-3.5",
        "keywords": [
            [
                "biomedical",
                "surgery",
                "clinical"
            ],
            [
                "FPGA"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Fluorescence lifetime imaging (FLI) is a widely used technique in the biomedical field for measuring the decay times of fluorescent molecules, providing insights into metabolic states, protein interactions, and ligand-receptor bindings. However, its broader application in fast biological processes, such as dynamic activity monitoring, and clinical use, such as in guided surgery, is limited by long data acquisition times and computationally demanding data processing. While deep learning has reduced post-processing times, time-resolved data acquisition remains a bottleneck for real-time applications. To address this, we propose a method to achieve real-time FLI using an FPGA-based hardware accelerator. Specifically, we implemented a GRU-based sequence-to-sequence (Seq2Seq) model on an FPGA board compatible with time-resolved cameras. The GRU model balances accurate processing with the resource constraints of FPGAs, which have limited DSP units and BRAM. The limited memory and computational resources on the FPGA require efficient scheduling of operations and memory allocation to deploy deep learning models for low-latency applications. We address these challenges by using STOMP, a queue-based discrete-event simulator that automates and optimizes task scheduling and memory management on hardware. By integrating a GRU-based Seq2Seq model and its compressed version, called Seq2SeqLite, generated through knowledge distillation, we were able to process multiple pixels in parallel, reducing latency compared to sequential processing. We explore various levels of parallelism to achieve an optimal balance between performance and resource utilization. Our results indicate that the proposed techniques achieved a 17.7x and 52.0x speedup over manual scheduling for the Seq2Seq model and the Seq2SeqLite model, respectively.",
        "subjects": [
            "physics.optics",
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "comment": "7 pages, 6 figures"
    },
    {
        "paper id": "2410.07430",
        "abstract url": "https://arxiv.org/abs/2410.07430",
        "title": "EventFlow: Forecasting Continuous-Time Event Data with Flow Matching",
        "rating": "-3.5",
        "keywords": [
            [
                "industrial"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Continuous-time event sequences, in which events occur at irregular intervals, are ubiquitous across a wide range of industrial and scientific domains. The contemporary modeling paradigm is to treat such data as realizations of a temporal point process, and in machine learning it is common to model temporal point processes in an autoregressive fashion using a neural network. While autoregressive models are successful in predicting the time of a single subsequent event, their performance can be unsatisfactory in forecasting longer horizons due to cascading errors. We propose EventFlow, a non-autoregressive generative model for temporal point processes. Our model builds on the flow matching framework in order to directly learn joint distributions over event times, side-stepping the autoregressive process. EventFlow is likelihood-free, easy to implement and sample from, and either matches or surpasses the performance of state-of-the-art models in both unconditional and conditional generation tasks on a set of standard benchmarks",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07446",
        "abstract url": "https://arxiv.org/abs/2410.07446",
        "title": "KACQ-DCNN: Uncertainty-Aware Interpretable Kolmogorov-Arnold Classical-Quantum Dual-Channel Neural Network for Heart Disease Detection",
        "rating": "-3.5",
        "keywords": [
            [
                "health",
                "healthcare",
                "Disease"
            ],
            [
                "Quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Heart failure remains a major global health challenge, contributing significantly to the 17.8 million annual deaths from cardiovascular disease, highlighting the need for improved diagnostic tools. Current heart disease prediction models based on classical machine learning face limitations, including poor handling of high-dimensional, imbalanced data, limited performance on small datasets, and a lack of uncertainty quantification, while also being difficult for healthcare professionals to interpret. To address these issues, we introduce KACQ-DCNN, a novel classical-quantum hybrid dual-channel neural network that replaces traditional multilayer perceptrons and convolutional layers with Kolmogorov-Arnold Networks (KANs). This approach enhances function approximation with learnable univariate activation functions, reducing model complexity and improving generalization. The KACQ-DCNN 4-qubit 1-layered model significantly outperforms 37 benchmark models across multiple metrics, achieving an accuracy of 92.03%, a macro-average precision, recall, and F1 score of 92.00%, and an ROC-AUC score of 94.77%. Ablation studies demonstrate the synergistic benefits of combining classical and quantum components with KAN. Additionally, explainability techniques like LIME and SHAP provide feature-level insights, improving model transparency, while uncertainty quantification via conformal prediction ensures robust probability estimates. These results suggest that KACQ-DCNN offers a promising path toward more accurate, interpretable, and reliable heart disease predictions, paving the way for advancements in cardiovascular healthcare.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07511",
        "abstract url": "https://arxiv.org/abs/2410.07511",
        "title": "CSGDN: Contrastive Signed Graph Diffusion Network for Predicting Crop Gene-Trait Associations",
        "rating": "-3.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Graph"
            ],
            [
                "DNA",
                "physiological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Positive and negative association preidiction between gene and trait help studies for crops to perform complex physiological functions. The transcription and regulation activity of specific genes will be adjusted accordingly in different cell types, developmental stages, and physiological states to meet the needs of organisms. Determing gene-trait associations can resolve the mechanism of trait formation and benefit the improvement of crop yield and quality. There are the following two problems in obtaining the positive/negative associations between gene and trait: 1) High-throughput DNA/RNA sequencing and trait data collection are expensive and time-consuming due to the need to process large sample sizes; 2) experiments introduce both random and systematic errors, and, at the same time, calculations or predictions using software or models may produce noise. To address these two issues, we propose a Contrastive Signed Graph Diffusion Network, CSGDN, to learn robust node representations with fewer training samples to achieve higher link prediction accuracy. CSGDN employs a signed graph diffusion method to uncover the underlying regulatory associations between genes and traits. Then, stochastic perterbation strategies are used to create two views for both original and diffusive graphs. At last, a multi-view contrastive learning paradigm loss is designed to unify the node presentations learned from the two views to resist interference and reduce noise. We conduct experiments to validate the performance of CSGDN on three crop datasets: Gossypium hirsutum, Brassica napus, and Triticum turgidum. The results demonstrate that the proposed model outperforms state-of-the-art methods by up to 9.28% AUC for link sign prediction in G. hirsutum dataset.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2410.06678",
        "abstract url": "https://arxiv.org/abs/2410.06678",
        "title": "M${}^{3}$Bench: Benchmarking Whole-body Motion Generation for Mobile Manipulation in 3D Scenes",
        "rating": "-4",
        "keywords": [
            [
                "3D"
            ],
            [
                "trajectory"
            ],
            [
                "robotics",
                "robot"
            ],
            [
                "physics"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We propose M^3Bench, a new benchmark for whole-body motion generation for mobile manipulation tasks. Given a 3D scene context, M^3Bench requires an embodied agent to understand its configuration, environmental constraints and task objectives, then generate coordinated whole-body motion trajectories for object rearrangement tasks. M^3Bench features 30k object rearrangement tasks across 119 diverse scenes, providing expert demonstrations generated by our newly developed M^3BenchMaker. This automatic data generation tool produces coordinated whole-body motion trajectories from high-level task instructions, requiring only basic scene and robot information. Our benchmark incorporates various task splits to assess generalization across different dimensions and leverages realistic physics simulation for trajectory evaluation. Through extensive experimental analyses, we reveal that state-of-the-art models still struggle with coordinated base-arm motion while adhering to environment-context and task-specific constraints, highlighting the need to develop new models that address this gap. Through M^3Bench, we aim to facilitate future robotics research towards more adaptive and capable mobile manipulation in diverse, real-world environments.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06762",
        "abstract url": "https://arxiv.org/abs/2410.06762",
        "title": "Finite-Time Trajectory Tracking of a Four wheeled Mecanum Mobile Robot",
        "rating": "-4",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "Robot"
            ],
            [
                "industrial"
            ]
        ],
        "abstract": "Four Wheeled Mecanum Robot (FWMR) possess the capability to move in any direction on a plane making it a cornerstone system in modern industrial operations. Despite the extreme maneuverability offered by FWMR, the practical implementation or real-time simulation of Mecanum wheel robots encounters substantial challenges in trajectory tracking control. In this research work, we present a finite-time control law using backstepping technique to perform stabilization and trajectory tracking objectives for a FWMR system. A rigorous stability proof is presented and explicit computation of the finite-time is provided. For tracking objective, we demonstrate the results taking an S-shaped trajectory inclined towards collision avoidance applications. Simulation validation in real time using Gazebo-ROS on a Mecanum robot model is carried out which complies with the theoretical results.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06818",
        "abstract url": "https://arxiv.org/abs/2410.06818",
        "title": "An Improved Approach for Cardiac MRI Segmentation based on 3D UNet Combined with Papillary Muscle Exclusion",
        "rating": "-4",
        "keywords": [
            [
                "3D"
            ],
            [
                "MRI",
                "clinical",
                "Cardiac"
            ],
            [
                "recommendation"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Left ventricular ejection fraction (LVEF) is the most important clinical parameter of cardiovascular function. The accuracy in estimating this parameter is highly dependent upon the precise segmentation of the left ventricle (LV) structure at the end diastole and systole phases. Therefore, it is crucial to develop robust algorithms for the precise segmentation of the heart structure during different phases. Methodology: In this work, an improved 3D UNet model is introduced to segment the myocardium and LV, while excluding papillary muscles, as per the recommendation of the Society for Cardiovascular Magnetic Resonance. For the practical testing of the proposed framework, a total of 8,400 cardiac MRI images were collected and analysed from the military hospital in Tunis (HMPIT), as well as the popular ACDC public dataset. As performance metrics, we used the Dice coefficient and the F1 score for validation/testing of the LV and the myocardium segmentation. Results: The data was split into 70%, 10%, and 20% for training, validation, and testing, respectively. It is worth noting that the proposed segmentation model was tested across three axis views: basal, medio basal and apical at two different cardiac phases: end diastole and end systole instances. The experimental results showed a Dice index of 0.965 and 0.945, and an F1 score of 0.801 and 0.799, at the end diastolic and systolic phases, respectively. Additionally, clinical evaluation outcomes revealed a significant difference in the LVEF and other clinical parameters when the papillary muscles were included or excluded.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07369",
        "abstract url": "https://arxiv.org/abs/2410.07369",
        "title": "An undetectable watermark for generative image models",
        "rating": "-4",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "attacks"
            ],
            [
                "watermark"
            ]
        ],
        "abstract": "We present the first undetectable watermarking scheme for generative image models. Undetectability ensures that no efficient adversary can distinguish between watermarked and un-watermarked images, even after making many adaptive queries. In particular, an undetectable watermark does not degrade image quality under any efficiently computable metric. Our scheme works by selecting the initial latents of a diffusion model using a pseudorandom error-correcting code (Christ and Gunn, 2024), a strategy which guarantees undetectability and robustness. We experimentally demonstrate that our watermarks are quality-preserving and robust using Stable Diffusion 2.1. Our experiments verify that, in contrast to every prior scheme we tested, our watermark does not degrade image quality. Our experiments also demonstrate robustness: existing watermark removal attacks fail to remove our watermark from images without significantly degrading the quality of the images. Finally, we find that we can robustly encode 512 bits in our watermark, and up to 2500 bits when the images are not subjected to watermark removal attacks. Our code is available at https://github.com/XuandongZhao/PRC-Watermark.",
        "subjects": [
            "cs.CR",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07955",
        "abstract url": "https://arxiv.org/abs/2410.07955",
        "title": "Iterative Optimization Annotation Pipeline and ALSS-YOLO-Seg for Efficient Banana Plantation Segmentation in UAV Imagery",
        "rating": "-4",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "health"
            ],
            [
                "UAV"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Precise segmentation of Unmanned Aerial Vehicle (UAV)-captured images plays a vital role in tasks such as crop yield estimation and plant health assessment in banana plantations. By identifying and classifying planted areas, crop area can be calculated, which is indispensable for accurate yield predictions. However, segmenting banana plantation scenes requires a substantial amount of annotated data, and manual labeling of these images is both time-consuming and labor-intensive, limiting the development of large-scale datasets. Furthermore, challenges such as changing target sizes, complex ground backgrounds, limited computational resources, and correct identification of crop categories make segmentation even more difficult. To address these issues, we proposed a comprehensive solution. Firstly, we designed an iterative optimization annotation pipeline leveraging SAM2's zero-shot capabilities to generate high-quality segmentation annotations, thereby reducing the cost and time associated with data annotation significantly. Secondly, we developed ALSS-YOLO-Seg, an efficient lightweight segmentation model optimized for UAV imagery. The model's backbone includes an Adaptive Lightweight Channel Splitting and Shuffling (ALSS) module to improve information exchange between channels and optimize feature extraction, aiding accurate crop identification. Additionally, a Multi-Scale Channel Attention (MSCA) module combines multi-scale feature extraction with channel attention to tackle challenges of varying target sizes and complex ground backgrounds.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06969",
        "abstract url": "https://arxiv.org/abs/2410.06969",
        "title": "DLGNet: Hyperedge Classification through Directed Line Graphs for Chemical Reactions",
        "rating": "-4.5",
        "keywords": [
            [
                "GNN",
                "Graphs"
            ],
            [
                "biology"
            ],
            [
                "chemistry",
                "Chemical"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Graphs and hypergraphs provide powerful abstractions for modeling interactions among a set of entities of interest and have been attracting a growing interest in the literature thanks to many successful applications in several fields. In particular, they are rapidly expanding in domains such as chemistry and biology, especially in the areas of drug discovery and molecule generation. One of the areas witnessing the fasted growth is the chemical reactions field, where chemical reactions can be naturally encoded as directed hyperedges of a hypergraph. In this paper, we address the chemical reaction classification problem by introducing the notation of a Directed Line Graph (DGL) associated with a given directed hypergraph. On top of it, we build the Directed Line Graph Network (DLGNet), the first spectral-based Graph Neural Network (GNN) expressly designed to operate on a hypergraph via its DLG transformation. The foundation of DLGNet is a novel Hermitian matrix, the Directed Line Graph Laplacian, which compactly encodes the directionality of the interactions taking place within the directed hyperedges of the hypergraph thanks to the DLG representation. The Directed Line Graph Laplacian enjoys many desirable properties, including admitting an eigenvalue decomposition and being positive semidefinite, which make it well-suited for its adoption within a spectral-based GNN. Through extensive experiments on chemical reaction datasets, we show that DGLNet significantly outperforms the existing approaches, achieving on a collection of real-world datasets an average relative-percentage-difference improvement of 33.01%, with a maximum improvement of 37.71%.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06880",
        "abstract url": "https://arxiv.org/abs/2410.06880",
        "title": "Cooperative UAV-Relay based Satellite Aerial Ground Integrated Networks",
        "rating": "-5",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "5G"
            ],
            [
                "UAV",
                "Satellite"
            ]
        ],
        "abstract": "In the post-fifth generation (5G) era, escalating user quality of service (QoS) strains terrestrial network capacity, especially in urban areas with dynamic traffic distributions. This paper introduces a novel cooperative unmanned aerial vehicle relay-based deployment (CUD) framework in satellite air-ground integrated networks (SAGIN). The CUD strategy deploys an unmanned aerial vehicle-based relay (UAVr) in an amplify-andforward (AF) mode to enhance user QoS when terrestrial base stations fall short of network capacity. By combining low earth orbit (LEO) satellite and UAVr signals using cooperative diversity, the CUD framework enhances the signal to noise ratio (SNR) at the user. Comparative evaluations against existing frameworks reveal performance improvements, demonstrating the effectiveness of the CUD framework in addressing the evolving demands of next-generation networks.",
        "subjects": [
            "eess.SY",
            "eess.SP"
        ],
        "comment": "5 pages, 3 figures, to appear in IEEE 100th Vehicular Technology Conference (VTC2024-Fall)"
    },
    {
        "paper id": "2410.06556",
        "abstract url": "https://arxiv.org/abs/2410.06556",
        "title": "MPC-guided, Data-driven Fuzzy Controller Synthesis",
        "rating": "-10",
        "keywords": [],
        "abstract": "Model predictive control (MPC) is a powerful control technique for online optimization using system model-based predictions over a finite time horizon. However, the computational cost MPC requires can be prohibitive in resource-constrained computer systems. This paper presents a fuzzy controller synthesis framework guided by MPC. In the proposed framework, training data is obtained from MPC closed-loop simulations and is used to optimize a low computational complexity controller to emulate the response of MPC. In particular, autoregressive moving average (ARMA) controllers are trained using data obtained from MPC closed-loop simulations, such that each ARMA controller emulates the response of the MPC controller under particular desired conditions. Using a Takagi-Sugeno (T-S) fuzzy system, the responses of all the trained ARMA controllers are then weighted depending on the measured system conditions, resulting in the Fuzzy-Autoregressive Moving Average (F-ARMA) controller. The effectiveness of the trained F-ARMA controllers is illustrated via numerical examples.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "8 pages, 8 figures, submitted to the American Control Conference 2025"
    },
    {
        "paper id": "2410.06569",
        "abstract url": "https://arxiv.org/abs/2410.06569",
        "title": "Open loop calibration and closed loop non-perturbative estimation of the lateral errors of an adaptive optics system: examples with GRAVITY+ and CHARA experimental data",
        "rating": "-10",
        "keywords": [],
        "abstract": "Performances of an adaptive optics (AO) system are directly linked with the quality of its alignment. During the instrument calibration, having open loop fast tools with a large capture range are necessary to quickly assess the system misalignment and to drive it towards a state allowing to close the AO loop. During operation, complex systems are prone to misalignments (mechanical flexions, rotation of optical elements, etc.) that potentially degrade the AO performances, creating a need for a monitoring tool to tackle their driftage. In this work, we first present an improved perturbative method to quickly assess large lateral errors in open loop. It uses the spatial correlation of the measured interaction matrix of a limited number of 2D spatial modes with a synthetic model. Then, we introduce a novel solution to finely measure and correct these lateral errors via the closed loop telemetry. Non-perturbative, this method consequently does not impact the science output of the instrument. It is based on the temporal correlation of 2D spatial frequencies in the deformable mirror commands. It is model-free (no need of an interaction matrix model) and sparse in the Fourier space, making it fast and easily scalable to complex systems such as future extremely large telescopes. Finally, we present some results obtained on the development bench of the GRAVITY+ extreme AO system (Cartesian grid, 1432 actuators). In addition, we show with on-sky results gathered with CHARA and GRAVITY/CIAO that the method is adaptable to non-conventional AO geometries (hexagonal grids, 60 actuators).",
        "subjects": [
            "eess.SP",
            "astro-ph.IM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06570",
        "abstract url": "https://arxiv.org/abs/2410.06570",
        "title": "Disturbance Observer-based Control Barrier Functions with Residual Model Learning for Safe Reinforcement Learning",
        "rating": "-10",
        "keywords": [],
        "abstract": "Reinforcement learning (RL) agents need to explore their environment to learn optimal behaviors and achieve maximum rewards. However, exploration can be risky when training RL directly on real systems, while simulation-based training introduces the tricky issue of the sim-to-real gap. Recent approaches have leveraged safety filters, such as control barrier functions (CBFs), to penalize unsafe actions during RL training. However, the strong safety guarantees of CBFs rely on a precise dynamic model. In practice, uncertainties always exist, including internal disturbances from the errors of dynamics and external disturbances such as wind. In this work, we propose a new safe RL framework based on disturbance rejection-guarded learning, which allows for an almost model-free RL with an assumed but not necessarily precise nominal dynamic model. We demonstrate our results on the Safety-gym benchmark for Point and Car robots on all tasks where we can outperform state-of-the-art approaches that use only residual model learning or a disturbance observer (DOB). We further validate the efficacy of our framework using a physical F1/10 racing car. Videos: https://sites.google.com/view/res-dob-cbf-rl",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06583",
        "abstract url": "https://arxiv.org/abs/2410.06583",
        "title": "A short note about the learning-augmented secretary problem",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider the secretary problem through the lens of learning-augmented algorithms. As it is known that the best possible expected competitive ratio is $1/e$ in the classic setting without predictions, a natural goal is to design algorithms that are 1-consistent and $1/e$-robust. Unfortunately, [FY24] provided hardness constructions showing that such a goal is not attainable when the candidates' true values are allowed to scale with $n$. Here, we provide a simple and explicit alternative hardness construction showing that such a goal is not achievable even when the candidates' true values are constants that do not scale with $n$.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06587",
        "abstract url": "https://arxiv.org/abs/2410.06587",
        "title": "Bots can Snoop: Uncovering and Mitigating Privacy Risks of Bots in Group Chats",
        "rating": "-10",
        "keywords": [],
        "abstract": "New privacy concerns arise with chatbots on group messaging platforms. Chatbots may access information beyond their intended functionalities, such as messages unintended for chatbots or sender's identities. Chatbot operators may exploit such information to infer personal information and link users across groups, potentially leading to personal data breaches, pervasive tracking, and targeted advertising. Our analysis of conversation datasets shows that (1) chatbots often access far more messages than needed, and (2) when a user joins a new group with chatbots, there is a 3.4% chance that at least one of the chatbots can recognize and associate the user with their previous interactions in other groups. Although state-of-the-art group messaging protocols provide robust end-to-end security and some platforms have implemented policies to limit chatbot access, no platforms successfully combine these features. This paper introduces SnoopGuard, a secure group messaging protocol that ensures user privacy against chatbots while maintaining strong end-to-end security. Our method offers selective message access, preventing chatbots from accessing unrelated messages, and ensures sender anonymity within the group. SnoopGuard achieves $O(\\log n + m)$ message-sending complexity for a group of $n$ users and $m$ chatbots, compared to $O(\\log(n + m))$ in state-of-the-art protocols, with acceptable overhead for enhanced privacy. Our prototype implementation shows that sending a message in a group of 50 users and 10 chatbots takes about 30 milliseconds when integrated with Message Layer Security (MLS).",
        "subjects": [
            "cs.CR"
        ],
        "comment": "18 pages, 5 figures"
    },
    {
        "paper id": "2410.06589",
        "abstract url": "https://arxiv.org/abs/2410.06589",
        "title": "A Family of LZ78-based Universal Sequential Probability Assignments",
        "rating": "-10",
        "keywords": [],
        "abstract": "We propose and study a family of universal sequential probability assignments on individual sequences, based on the incremental parsing procedure of the Lempel-Ziv (LZ78) compression algorithm. We show that the normalized log loss under any of these models converges to the normalized LZ78 codelength, uniformly over all individual sequences. To establish the universality of these models, we consolidate a set of results from the literature relating finite-state compressibility to optimal log-loss under Markovian and finite-state models. We also consider some theoretical and computational properties of these models when viewed as probabilistic sources. Finally, we present experimental results showcasing the potential benefit of using this family -- as models and as sources -- for compression, generation, and classification.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "31 pages, 5 figures, submitted to IEEE Transactions on Information Theory"
    },
    {
        "paper id": "2410.06607",
        "abstract url": "https://arxiv.org/abs/2410.06607",
        "title": "Towards optimal algorithms for the recovery of low-dimensional models with linear rates",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider the problem of recovering elements of a low-dimensional model from linear measurements. From signal and image processing to inverse problems in data science, this question has been at the center of many applications. Lately, with the success of models and methods relying on deep neural networks leading to non-convex formulations, traditional convex variational approaches have shown their limits. Furthermore, the multiplication of algorithms and recovery results makes identifying the best methods a complex task. In this article, we study recovery with a class of widely used algorithms without considering any underlying functional. This result leads to a class of projected gradient descent algorithms that recover a given low-dimensional with linear rates. The obtained rates decouple the impact of the quality of the measurements with respect to the model from its intrinsic complexity. As a consequence, we can directly measure the performance of this class of projected gradient descents through a restricted Lipschitz constant of the projection. By optimizing this constant, we define optimal algorithms. Our general approach provides an optimality result in the case of sparse recovery. Moreover, we uncover underlying linear rates of convergence for some ''plug and play'' imaging methods relying on deep priors by interpreting our results in this context, thus linking low-dimensional recovery and recovery with deep priors under a unified theory, validated by experiments on synthetic and real data.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06647",
        "abstract url": "https://arxiv.org/abs/2410.06647",
        "title": "Achieving Interference-Free Degrees of Freedom in Cellular Networks via RIS",
        "rating": "-10",
        "keywords": [],
        "abstract": "It's widely perceived that Reconfigurable Intelligent Surfaces (RIS) cannot increase Degrees of Freedom (DoF) due to their relay nature. A notable exception is Jiang \\& Yu's work. They demonstrate via simulation that in an ideal $K$-user interference channel, passive RIS can achieve the interference-free DoF. In this paper, we investigate the DoF gain of RIS in more realistic systems, namely cellular networks, and more challenging scenarios with direct links. We prove that RIS can boost the DoF per cell to that of the interference-free scenario even \\textit{ with direct-links}. Furthermore, we \\textit{theoretically} quantify the number of RIS elements required to achieve that goal, i.e. $max\\left\\{ {2L, (\\sqrt L + c)\u03b7+L } \\right\\}$ (where $L=GM(GM-1)$, $c$ is a constant and $\u03b7$ denotes the ratio of channel strength) for the $G$-cells with more single-antenna users $K$ than base station antennas $M$ per cell. The main challenge lies in addressing the feasibility of a system of algebraic equations, which is difficult by itself in algebraic geometry. We tackle this problem in a probabilistic way, by exploiting the randomness of the involved coefficients and addressing the problem from the perspective of extreme value statistics and convex geometry. Moreover, numerical results confirm the tightness of our theoretical results.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06654",
        "abstract url": "https://arxiv.org/abs/2410.06654",
        "title": "Performance Evaluation in Multimedia Retrieval",
        "rating": "-10",
        "keywords": [],
        "abstract": "Performance evaluation in multimedia retrieval, as in the information retrieval domain at large, relies heavily on retrieval experiments, employing a broad range of techniques and metrics. These can involve human-in-the-loop and machine-only settings for the retrieval process itself and the subsequent verification of results. Such experiments can be elaborate and use-case-specific, which can make them difficult to compare or replicate. In this paper, we present a formal model to express all relevant aspects of such retrieval experiments, as well as a flexible open-source evaluation infrastructure that implements the model. These contributions intend to make a step towards lowering the hurdles for conducting retrieval experiments and improving their reproducibility.",
        "subjects": [
            "cs.IR",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06662",
        "abstract url": "https://arxiv.org/abs/2410.06662",
        "title": "A data-driven approach for safety quantification of non-linear stochastic systems with unknown additive noise distribution",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we present a novel data-driven approach to quantify safety for non-linear, discrete-time stochastic systems with unknown noise distribution. We define safety as the probability that the system remains in a given region of the state space for a given time horizon and, to quantify it, we present an approach based on Stochastic Barrier Functions (SBFs). In particular, we introduce an inner approximation of the stochastic program to design a SBF in terms of a chance-constrained optimisation problem, which allows us to leverage the scenario approach theory to design a SBF from samples of the system with Probably Approximately Correct (PAC) guarantees. Our approach leads to tractable, robust linear programs, which enable us to assert safety for non-linear models that were otherwise deemed infeasible with existing methods. To further mitigate the computational complexity of our approach, we exploit the structure of the system dynamics and rely on spatial data structures to accelerate the construction and solution of the underlying optimisation problem. We show the efficacy and validity of our framework in several benchmarks, showing that our approach can obtain substantially tighter certificates compared to state-of-the-art with a confidence that is several orders of magnitude higher.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06683",
        "abstract url": "https://arxiv.org/abs/2410.06683",
        "title": "Barter Exchange with Bounded Trading Cycles",
        "rating": "-10",
        "keywords": [],
        "abstract": "Consider a barter exchange problem over a finite set of agents, where each agent owns an item and is also associated with a (privately known) wish list of items belonging to the other agents. An outcome of the problem is a (re)allocation of the items to the agents such that each agent either keeps her own item or receives an item from her (reported) wish list, subject to the constraint that the length of the trading cycles induced by the allocation is up-bounded by a prespecified length bound k. The utility of an agent from an allocation is 1 if she receives an item from her (true) wish list and 0 if she keeps her own item (the agent incurs a large dis-utility if she receives an item that is neither hers nor belongs to her wish list). In this paper, we investigate the aforementioned barter exchange problem from the perspective of mechanism design without money, aiming for truthful (and individually rational) mechanisms whose objective is to maximize the social welfare. As the construction of a social welfare maximizing allocation is computationally intractable for length bounds k \\geq 3, this paper focuses on (computationally efficient) truthful mechanisms that approximate the (combinatorially) optimal social welfare.We also study a more general version of the barter exchange problem, where the utility of an agent from participating in a trading cycle of length 2 \\leq \\ell \\leq k is lambda(\\ell), where \u03bbis a general (monotonically non-increasing) length function. Our results include upper and lower bounds on the guaranteed approximation ratio, expressed in terms of the length bound k and the length function \u03bb. On the technical side, our main contribution is an algorithmic tool that can be viewed as a truthful version of the local search paradigm. As it turns out, this tool can be applied to more general (bounded size) coalition formation problems.",
        "subjects": [
            "cs.GT"
        ],
        "comment": "This is a full version of an extended abstract accepted to appear in the proceedings of WINE 2024"
    },
    {
        "paper id": "2410.06688",
        "abstract url": "https://arxiv.org/abs/2410.06688",
        "title": "Non-overshooting output shaping for switched linear systems under arbitrary switching using eigenstructure assignment",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider the analytical control design for a pair of switched linear multiple-input multiple-output (MIMO) systems that are subject to arbitrary switching signals. A state feedback controller design method is proposed to obtain an eigenstructure assignment that ensures that the closed-loop switched system is globally asymptotically stable, and the outputs achieve the non-overshooting tracking of a step reference. Our analysis indicates whether non-overshooting or even monotonic tracking is achievable for the given system and considered outputs and provides a choice of possible eigenstructures to be assigned to the constituent subsystems. We derive a structural condition that verifies the feasibility of the chosen assignment. A constructive algorithm to obtain suitable feedback matrices is provided, and the method is illustrated with numerical examples.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06693",
        "abstract url": "https://arxiv.org/abs/2410.06693",
        "title": "Autonomous localization of multiple ionizing radiation sources using miniature single-layer Compton cameras onboard a group of micro aerial vehicles",
        "rating": "-10",
        "keywords": [],
        "abstract": "A novel method for autonomous localization of multiple sources of gamma radiation using a group of Micro Aerial Vehicles (MAVs) is presented in this paper. The method utilizes an extremely lightweight (44 g) Compton camera MiniPIX TPX3. The compact size of the detector allows for deployment onboard safe and agile small-scale Unmanned Aerial Vehicles (UAVs). The proposed radiation mapping approach fuses measurements from multiple distributed Compton camera sensors to accurately estimate the positions of multiple radioactive sources in real time. Unlike commonly used intensity-based detectors, the Compton camera reconstructs the set of possible directions towards a radiation source from just a single ionizing particle. Therefore, the proposed approach can localize radiation sources without having to estimate the gradient of a radiation field or contour lines, which require longer measurements. The instant estimation is able to fully exploit the potential of highly mobile MAVs. The radiation mapping method is combined with an active search strategy, which coordinates the future actions of the MAVs in order to improve the quality of the estimate of the sources' positions, as well as to explore the area of interest faster. The proposed solution is evaluated in simulation and real world experiments with multiple Cesium-137 radiation sources.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "International Conference on Intelligent Robots and Systems (IROS) 2024"
    },
    {
        "paper id": "2410.06695",
        "abstract url": "https://arxiv.org/abs/2410.06695",
        "title": "Energy Efficient Scheduling for Serverless Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Serverless computing, also referred to as Function-as-a-Service (FaaS), is a cloud computing model that has attracted significant attention and has been widely adopted in recent years. The serverless computing model offers an intuitive, event-based interface that makes the development and deployment of scalable cloud-based applications easier and cost-effective. An important aspect that has not been examined in these systems is their energy consumption during the application execution. One way to deal with this issue is to schedule the function invocations in an energy-efficient way. However, efficient scheduling of applications in a multi-tenant environment, like FaaS systems, poses significant challenges. The trade-off between the server's energy usage and the hosted functions' performance requirements needs to be taken into consideration. In this work, we propose an Energy Efficient Scheduler for orchestrating the execution of serverless functions so that it minimizes energy consumption while it satisfies the applications' performance demands. Our approach considers real-time performance measurements and historical data and applies a novel DVFS technique to minimize energy consumption. Our detailed experimental evaluation using realistic workloads on our local cluster illustrates the working and benefits of our approach.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06705",
        "abstract url": "https://arxiv.org/abs/2410.06705",
        "title": "MERGE: Matching Electronic Results with Genuine Evidence for verifiable voting in person at remote locations",
        "rating": "-10",
        "keywords": [],
        "abstract": "Overseas military personnel often face significant challenges in participating in elections due to the slow pace of traditional mail systems, which can result in ballots missing crucial deadlines. While internet-based voting offers a faster alternative, it introduces serious risks to the integrity and privacy of the voting process. We introduce the MERGE protocol to address these issues by combining the speed of electronic ballot delivery with the reliability of paper returns. This protocol allows voters to submit an electronic record of their vote quickly while simultaneously mailing a paper ballot for verification. The electronic record can be used for preliminary results, but the paper ballot is used in a Risk Limiting Audit (RLA) if received in time, ensuring the integrity of the election. This approach extends the time window for ballot arrival without undermining the security and accuracy of the vote count.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06708",
        "abstract url": "https://arxiv.org/abs/2410.06708",
        "title": "Do Developers Adopt Green Architectural Tactics for ML-Enabled Systems? A Mining Software Repository Study",
        "rating": "-10",
        "keywords": [],
        "abstract": "As machine learning (ML) and artificial intelligence (AI) technologies become increasingly prevalent in society, concerns about their environmental sustainability have grown. Developing and deploying ML-enabled systems, especially during training and inference, are resource-intensive, raising sustainability issues. Green AI has emerged as a response, advocating for reducing the computational demands of AI while maintaining accuracy. While recent research has identified various green tactics for developing sustainable ML-enabled systems, there is a gap in understanding the extent to which these tactics are adopted in real-world projects and whether additional, undocumented practices exist. This paper addresses these gaps by presenting a mining software repository study that evaluates the adoption of green tactics in 168 open-source ML projects on GitHub. In doing so, we introduce a novel mining mechanism based on large language models to identify and analyze green tactics within software repositories. Our results provide insights into the adoption of green tactics found in the literature and expand previous catalogs by providing 12 new tactics, with code examples to support wider implementation. This study contributes to the development of more sustainable ML systems by identifying adopted green tactics that offer substantial environmental benefits with minimal implementation effort. It provides practical insights for developers to green their systems and offers a path for future research to automate the integration of these tactics.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06715",
        "abstract url": "https://arxiv.org/abs/2410.06715",
        "title": "FRESCO: Fast and Reliable Edge Offloading with Reputation-based Hybrid Smart Contracts",
        "rating": "-10",
        "keywords": [],
        "abstract": "Mobile devices offload latency-sensitive application tasks to edge servers to satisfy applications' Quality of Service (QoS) deadlines. Consequently, ensuring reliable offloading without QoS violations is challenging in distributed and unreliable edge environments. However, current edge offloading solutions are either centralized or do not adequately address challenges in distributed environments. We propose FRESCO, a fast and reliable edge offloading framework that utilizes a blockchain-based reputation system, which enhances the reliability of offloading in the distributed edge. The distributed reputation system tracks the historical performance of edge servers, while blockchain through a consensus mechanism ensures that sensitive reputation information is secured against tampering. However, blockchain consensus typically has high latency, and therefore we employ a Hybrid Smart Contract (HSC) that automatically computes and stores reputation securely on-chain (i.e., on the blockchain) while allowing fast offloading decisions off-chain (i.e., outside of blockchain). The offloading decision engine uses a reputation score to derive fast offloading decisions, which are based on Satisfiability Modulo Theory (SMT). The SMT models edge resource constraints, and QoS deadlines, and can formally guarantee a feasible solution that is valuable for latency-sensitive applications that require high reliability. With a combination of on-chain HSC reputation state management and an off-chain SMT decision engine, FRESCO offloads tasks to reliable servers without being hindered by blockchain consensus. We evaluate FRESCO against real availability traces and simulated applications. FRESCO reduces response time by up to 7.86 times and saves energy by up to 5.4% compared to all baselines while minimizing QoS violations to 0.4% and achieving an average decision time of 5.05 milliseconds.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "14 pages, 12 figures"
    },
    {
        "paper id": "2410.06721",
        "abstract url": "https://arxiv.org/abs/2410.06721",
        "title": "Orchestrating the Execution of Serverless Functions in Hybrid Clouds",
        "rating": "-10",
        "keywords": [],
        "abstract": "In recent years, serverless computing, especially Function as a Service (FaaS), is rapidly growing in popularity as a cloud programming model. The serverless computing model provides an intuitive interface for developing cloud-based applications, where the development and deployment of scalable microservices has become easier and cost-effective. An increasing number of batch-processing applications are deployed as pipelines that comprise a sequence of functions that must meet their deadline targets to be practical. In this paper, we present our Hybrid Cloud Scheduler (HCS) for orchestrating the execution of serverless batch-processing pipelines deployed over heterogeneous infrastructures. Our framework enables developers to (i) automatically schedule and execute batch-processing applications in heterogeneous environments such as the private edge and public cloud serverless infrastructures, (ii) benefit from cost reduction through the utilization of their own resources in a private cluster, and (iii) significantly improves the probability of meeting the deadline requirements of their applications. Our experimental evaluation demonstrates the efficiency and benefits of our approach.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06728",
        "abstract url": "https://arxiv.org/abs/2410.06728",
        "title": "Mitigation of Polarization-Induced Fading in Optical Vector Network Analyzer for the Characterization of km-scale Space-Division Multiplexing Fibers",
        "rating": "-10",
        "keywords": [],
        "abstract": "We propose an optimized optical vector network analyzer with automatic polarization control to stabilize the reference arm polarization throughout the sweep range. We demonstrate this technique, successfully removing the polarization-induced fading and measurement distortions in insertion loss by characterizing a 10 km multi-core fiber.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Oral presentation at ECOC 2024 (Tu4A.4)- version with correction on the last sentence of paragraph 3 in the introduction"
    },
    {
        "paper id": "2410.06759",
        "abstract url": "https://arxiv.org/abs/2410.06759",
        "title": "RIS-Assisted D2D Communication in the Presence of Interference: Outage Performance Analysis and DNN-Based Prediction",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper analyses the performance of reconfigurable intelligent surface (RIS)-assisted device-to-device (D2D) communication systems, focusing on addressing co-channel interference, a prevalent issue due to the frequency reuse of sidelink in the underlay in-band D2D communications. In contrast to previous studies that either neglect interference or consider it only at the user, our research investigates a performance analysis in terms of outage probability (OP) for RIS-assisted D2D communication systems considering the presence of interference at both the user and the RIS. More specifically, we introduce a novel integral-form expression for an exact analysis of OP. Additionally, we present a new accurate approximation expression for OP, using the gamma distributions to approximate the fading of both desired and interference links, thereby yielding a closed-form expression. Nevertheless, both derived expressions, i.e., the exact integral-form and the approximate closed-form, contain special functions, such as Meijer's G-function and the parabolic cylinder function, which complicate real-time OP analysis. To circumvent this, we employ a deep neural network (DNN) for real-time OP prediction, trained with data generated by the exact expression. Moreover, we present a tight upper bound that quantifies the impact of interference on achievable diversity order and coding gain. We validate the derived expressions through Monte Carlo simulations. Our analysis reveals that while interference does not affect the system's diversity order, it significantly degrades the performance by reducing the coding gain. The results further demonstrate that increasing the number of RIS's reflecting elements is an effective strategy to mitigate the adverse effects of the interference on the system performance.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "12 pages"
    },
    {
        "paper id": "2410.06767",
        "abstract url": "https://arxiv.org/abs/2410.06767",
        "title": "On the Performance of Pilot-Aided Simultaneous Communication and Tracking",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, the symbol error rate performance analysis is provided for a pilot-aided simultaneous communication and tracking (PASCAT) system. In specific, we employ multiple drones to actively transmit signals to a BS, which is responsible for continuously monitoring the location of drones over time and decoding the symbols transmitted from the drones. It is found that the estimated location parameters at a given moment during tracking follow Gaussian distributions with means equal to actual values and variances equal to root mean square error (RMSE). Afterwards, the obtained location information is employed for informing the channel information, which is then used to preprocess the received signal before decoding by using the maximum ratio combining (MRC) technique. The average symbol error rate (SER) is also evaluated over the distribution of the estimated location parameters and an approximate value for the average SER is obtained by using a Taylor approximation with fast convergence. The result indicates that there is a cooperation relationship between the RMSE of the estimated location parameters and the average SER. In addition, the effect of the number of pilot signals is analysed as well. By employing more pilots, it is found that both communication and sensing functionalities are enhanced. Furthermore, the SER performance of our PASCAT system is similar to that of maximum likelihood detection (MLD) when a number of pilot signals are employed, which demonstrates the efficiency of the PASCAT system. In the end, all results are validated by using Monte Carlo simulations.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "13 pages, 9 figures"
    },
    {
        "paper id": "2410.06769",
        "abstract url": "https://arxiv.org/abs/2410.06769",
        "title": "LR Parsing of Permutation Phrases",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents an efficient method for LR parsing of permutation phrases. In practical cases, the proposed algorithm constructs an LR(0) automaton that requires significantly fewer states to process a permutation phrase compared to the standard construction. For most real-world grammars, the number of states is typically reduced from $\u03a9(n!)$ to $O(2^{n})$, resulting in a much more compact parsing table. The state reduction increases with longer permutation phrases and a higher number of permutation phrases within the right-hand side of a rule. We demonstrate the effectiveness of this method through its application to parsing a JSON document.",
        "subjects": [
            "cs.FL",
            "cs.PL"
        ],
        "comment": "13 pages, 2 figures"
    },
    {
        "paper id": "2410.06770",
        "abstract url": "https://arxiv.org/abs/2410.06770",
        "title": "BLAS-like Interface for Binary Tensor Contractions",
        "rating": "-10",
        "keywords": [],
        "abstract": "In the world of linear algebra computation, a well-established standard exists called BLAS(Basic Linear Algebra Subprograms). This standard has been crucial for the development of software using linear algebra operations. Its benefits include portability with efficiency and mitigation of suboptimal re-implementations of linear algebra operations. Multilinear algebra is an extension of linear algebra in which the central objects are tensors, which are generalizations of vectors and matrices. Though tensor operations are becoming more common, they do not have a standard like BLAS. Such standardization would be beneficial and decrease the now-visible replication of work, as many libraries nowadays use their own implementations. This master thesis aims to work towards such a standard by discovering whether or not a BLAS-like interface is possible for the operation binary tensor contraction. To answer this, an interface has been developed in the programming language C together with an implementation and tested to see if it would be sufficient. The interface developed is: xGETT(RANKA, EXTA, INCA, A, RANKB, EXTB, INCB, B, CONTS, CONTA, CONTB, PERM, INCC, C) with the implementation and tests, it has been deemed sufficient as a BLAS-like interface for binary tensor contractions and possible to use in a BLAS-like standardization for tensor operations.",
        "subjects": [
            "cs.MS"
        ],
        "comment": "master thesis report, 21 pages, 6 figures"
    },
    {
        "paper id": "2410.06773",
        "abstract url": "https://arxiv.org/abs/2410.06773",
        "title": "A Hybrid Renewable-Battery-Electrolyzer Facility under the Single Imbalance Pricing Scheme",
        "rating": "-10",
        "keywords": [],
        "abstract": "European energy markets are decentralized and entail balance responsibility of each market player. This stresses the importance of imbalance management of renewable energy sources (RES), as the imbalance payments can strongly reduce their profitability. According to the EU Electricity Balancing Guideline, each European transmission system operator should use the single imbalance pricing method which treats both deviation directions the same, no matter if a deviation helps the system or pushes it away from the balance. This paper aims to investigate the behavior of a hybrid facility consisting of an uncontrollable RES, a battery and an electrolyzer under such market setting. The formulated mathematical model of the hybrid facility seeks to maximize profit in the day-ahead energy market, while minimizing the imbalance costs. Uncertainty of the RES output is captured using stochastic scenarios, while the direction of the power system deviation, relevant for the imbalance pricing, is modeled using a newly proposed robust approach. Results of the case study indicate that the single imbalance pricing scheme might bring flexible assets to temptation of intentional deviations should they anticipate favorable imbalance prices.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06775",
        "abstract url": "https://arxiv.org/abs/2410.06775",
        "title": "Participatory Budget Allocation Method for Approval Ballots",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we study the problem of Participatory Budgeting (PB) with approval ballots, inspired by Multi-Winner Voting schemes. We present generalized preference aggregation methods for participatory budgeting, especially for finding seemingly fair budget allocations. To achieve this, we generalize such preference aggregation methods from the well-known methods, namely the Sequential Chamberlin Courant rule and the Sequential Monroe Rule in the realm of social choice theory. Further, we provide an experimental evaluation of the preference aggregation methods using an impartial culture method of preference generation and study the extent to which such polynomial time algorithms satisfy one of the most popular notions of fairness called proportional representation.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "8 pages, 3 figures"
    },
    {
        "paper id": "2410.06792",
        "abstract url": "https://arxiv.org/abs/2410.06792",
        "title": "An Internal Logic of Virtual Double Categories",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present a type theory called fibrational virtual double type theory (FVDblTT) designed specifically for formal category theory, which is a succinct reformulation of New and Licata's Virtual Equipment Type Theory (VETT). FVDblTT formalizes reasoning on isomorphisms that are commonly employed in category theory. Virtual double categories are one of the most successful frameworks for developing formal category theory, and FVDblTT has them as a theoretical foundation. We validate its worth as an internal language of virtual double categories by providing a syntax-semantics duality between virtual double categories and specifications in FVDblTT as a biadjunction.",
        "subjects": [
            "math.CT",
            "cs.LO"
        ],
        "comment": "36 pages"
    },
    {
        "paper id": "2410.06797",
        "abstract url": "https://arxiv.org/abs/2410.06797",
        "title": "Cooperate or Compete: Coalition Formation in Congestion Games",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper investigates the potential benefits of cooperation in scenarios where finitely many agents compete for shared resources, leading to congestion and thereby reduced rewards. By appropriate coordination the members of the cooperating group (a.k.a., coalition) can minimize the congestion losses due to inmates, while efficiently facing the competition from outsiders (coalitions indulge in a non-cooperative congestion game). The quest in this paper is to identify the stable partition of coalitions that are not challenged by a new coalition. In contrast to the traditional cooperative games, the worth of a coalition in our game also depends upon the arrangement of the opponents. Every arrangement leads to a partition and a corresponding congestion game; the resultant Nash equilibria (NEs) dictate the `worth'. The analysis is further complicated due to the presence of multiple NEs for each such game.",
        "subjects": [
            "cs.GT"
        ],
        "comment": "8 pages, 2 figures, Accepted in CDC 2024"
    },
    {
        "paper id": "2410.06822",
        "abstract url": "https://arxiv.org/abs/2410.06822",
        "title": "Unary counting quantifiers do not increase the expressive power of Presburger aritmetic: an alternative shorter proof",
        "rating": "-10",
        "keywords": [],
        "abstract": "This work was presented in June 5-7, 2017 at the conference \"Journ\u00e9es sur les Arithm\u00e9tiques Faibles -- Weak Arithmetics Days\" held in Saint-Pertersburg of which no proceeding was ever published. It was not a new result but showed that a different approach is possible. The paper presented at ICALP 2024 addresses, among other problems, the complexity issues which were ignored in my 2017 talk.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06823",
        "abstract url": "https://arxiv.org/abs/2410.06823",
        "title": "Stabilization of Predator-Prey Age-Structured Hyperbolic PDE when Harvesting both Species is Inevitable",
        "rating": "-10",
        "keywords": [],
        "abstract": "Populations do not only interact over time but also age over time. It is therefore common to model them as age-structured PDEs, where age is the space variable. Since the models also involve integrals over age, both in the birth process and in the interaction among species, they are in fact integro-partial differential equations (IPDEs) with positive states. To regulate the population densities to desired profiles, harvesting is used as input. But non-discriminating harvesting, where wanting to repress one species will inevitably repress the other species as well, the positivity restriction on the input (no insertion of population), and the multiplicative nature of harvesting, makes control challenging even for ODE versions of such dynamics, let alone for their IPDE versions on an infinite-dimensional nonnegative state space. We introduce a design for a benchmark version of such a problem: a two-population predator-prey setup. The model is equivalent to two coupled ordinary differential equations (ODEs), actuated by harvesting which must not drop below zero, and strongly disturbed by two autonomous but exponentially stable integral delay equations (IDEs). We develop two control designs. With a modified Volterra-like control Lyapunov function, we design a simple feedback which employs possibly negative harvesting for global stabilization of the ODE model, while guaranteeing regional regulation with positive harvesting. With a more sophisticated, restrained controller we achieve regulation for the ODE model globally, with positive harvesting. For the full IPDE model, with the IDE dynamics acting as large disturbances, for both the simple and saturated feedback laws we provide explicit estimates of the regions of attraction. The paper charts a new pathway for control designs for infinite-dimensional multi-species dynamics and for nonlinear positive systems with positive controls.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "submitted to IEEE Transactions on Automatic Control"
    },
    {
        "paper id": "2410.06837",
        "abstract url": "https://arxiv.org/abs/2410.06837",
        "title": "Faster and Simpler Online Computation of String Net Frequency",
        "rating": "-10",
        "keywords": [],
        "abstract": "An occurrence of a repeated substring $u$ in a string $S$ is called a net occurrence if extending the occurrence to the left or to the right decreases the number of occurrences to 1. The net frequency (NF) of a repeated substring $u$ in a string $S$ is the number of net occurrences of $u$ in $S$. Very recently, Guo et al. [SPIRE 2024] proposed an online $O(n \\log \u03c3)$-time algorithm that maintains a data structure of $O(n)$ space which answers Single-NF queries in $O(m\\log \u03c3+ \u03c3^2)$ time and reports all answers of the All-NF problem in $O(n\u03c3^2)$ time. Here, $n$ is the length of the input string $S$, $m$ is the query pattern length, and $\u03c3$ is the alphabet size. The $\u03c3^2$ term is a major drawback of their method since computing string net frequencies is originally motivated for Chinese language processing where $\u03c3$ can be thousands large. This paper presents an improved online $O(n \\log \u03c3)$-time algorithm, which answers Single-NF queries in $O(m \\log \u03c3)$ time and reports all answers to the All-NF problem in output-optimal $O(|\\mathsf{NF}^+(S)|)$ time, where $\\mathsf{NF}^+(S)$ is the set of substrings of $S$ paired with their positive NF values. We note that $|\\mathsf{NF}^+(S)| = O(n)$ always holds. In contract to Guo et al.'s algorithm that is based on Ukkonen's suffix tree construction, our algorithm is based on Weiner's suffix tree construction.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06843",
        "abstract url": "https://arxiv.org/abs/2410.06843",
        "title": "Point-to-Point MIMO Channel Estimation by Exploiting Array Geometry and Clustered Multipath Propagation",
        "rating": "-10",
        "keywords": [],
        "abstract": "A large-scale MIMO (multiple-input multiple-output) system offers significant advantages in wireless communication, including potential spatial multiplexing and beamforming capabilities. However, channel estimation becomes challenging with multiple antennas at both the transmitter and receiver ends. The minimum mean-squared error (MMSE) estimator, for instance, requires a spatial correlation matrix whose dimensions scale with the square of the product of the number of antennas on the transmitter and receiver sides. This scaling presents a substantial challenge, particularly as antenna counts increase in line with current technological trends. Traditional MIMO literature offers alternative channel estimators that mitigate the need to fully acquire the spatial correlation matrix. In this paper, we revisit point-to-point MIMO channel estimation and introduce a reduced-subspace least squares (RS-LS) channel estimator designed to eliminate physically impossible channel dimensions inherent in uniform planar arrays. Additionally, we propose a cluster-aware RS-LS estimator that leverages both reduced and cluster-specific subspace properties, significantly enhancing performance over the conventional RS-LS approach. Notably, both proposed methods obviate the need for fully/partial knowledge of the spatial correlation matrix.",
        "subjects": [
            "eess.SP",
            "cs.IT"
        ],
        "comment": "6 pages, 2 figures, accepted to be presented at IEEE GLOBECOM 2024"
    },
    {
        "paper id": "2410.06849",
        "abstract url": "https://arxiv.org/abs/2410.06849",
        "title": "On the Security and Design of Cryptosystems Using Gabidulin-Kronecker Product Codes",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper is a preliminary study on the security and design of cryptosystems using Gabidulin-Kronecker Product Codes. In particular, we point out the design impracticality of the system, and propose ways to improve it.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06856",
        "abstract url": "https://arxiv.org/abs/2410.06856",
        "title": "On Wagner's k-Tree Algorithm Over Integers",
        "rating": "-10",
        "keywords": [],
        "abstract": "The k-Tree algorithm [Wagner 02] is a non-trivial algorithm for the average-case k-SUM problem that has found widespread use in cryptanalysis. Its input consists of k lists, each containing n integers from a range of size m. Wagner's original heuristic analysis suggested that this algorithm succeeds with constant probability if n = m^{1/(\\log{k}+1)}, and that in this case it runs in time O(kn). Subsequent rigorous analysis of the algorithm [Lyubashevsky 05, Shallue 08, Joux-Kippen-Loss 24] has shown that it succeeds with high probability if the input list sizes are significantly larger than this. We present a broader rigorous analysis of the k-Tree algorithm, showing upper and lower bounds on its success probability and complexity for any size of the input lists. Our results confirm Wagner's heuristic conclusions, and also give meaningful bounds for a wide range of list sizes that are not covered by existing analyses. We present analytical bounds that are asymptotically tight, as well as an efficient algorithm that computes (provably correct) bounds for a wide range of concrete parameter settings. We also do the same for the k-Tree algorithm over Z_m. Finally, we present experimental evaluation of the tightness of our results.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06857",
        "abstract url": "https://arxiv.org/abs/2410.06857",
        "title": "Digital Dotted Lines: Design and Evaluation of a Prototype for Digitally Signing Documents Using Identity Wallets",
        "rating": "-10",
        "keywords": [],
        "abstract": "Documents are largely stored and shared digitally. Yet, digital documents are still commonly signed using (copies of) handwritten signatures, which are sensitive to fraud. Though secure, cryptography-based signature solutions exist, they are hardly used due to usability issues. This paper proposes to use digital identity wallets for securely and intuitively signing digital documents with verified personal data. Using expert feedback, we implemented this vision in an interactive prototype. The prototype was assessed in a moderated usability test (N = 15) and a subsequent unmoderated remote usability test (N = 99). While participants generally expressed satisfaction with the system, they also misunderstood how to interpret the signature information displayed by the prototype. Specifically, signed documents were also trusted when the document was signed with irrelevant personal data of the signer. We conclude that such unwarranted trust forms a threat to usable digital signatures and requires attention by the usable security community.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06868",
        "abstract url": "https://arxiv.org/abs/2410.06868",
        "title": "Online Matching Meets Sampling Without Replacement",
        "rating": "-10",
        "keywords": [],
        "abstract": "Sampling without replacement is a natural online rounding strategy for converting fractional bipartite matching into an integral one. In Online Bipartite Matching, we can use the Balance algorithm to fractionally match each online vertex, and then sample an unmatched offline neighbor with probability proportional to the fractional matching. In Online Stochastic Matching, we can take the solution to a linear program relaxation as a reference, and then match each online vertex to an unmatched offline neighbor with probability proportional to the fractional matching of the online vertex's type. On the one hand, we find empirical evidence that online matching algorithms based on sampling without replacement outperform existing algorithms. On the other hand, the literature offers little theoretical understanding of the power of sampling without replacement in online matching problems. This paper fills the gap in the literature by giving the first non-trivial competitive analyses of sampling without replacement for online matching problems. In Online Stochastic Matching, we develop a potential function analysis framework to show that sampling without replacement is at least $0.707$-competitive. The new analysis framework further allows us to derandomize the algorithm to obtain the first polynomial-time deterministic algorithm that breaks the $1-\\frac{1}{e}$ barrier. In Online Bipartite Matching, we show that sampling without replacement provides provable online correlated selection guarantees when the selection probabilities correspond to the fractional matching chosen by the Balance algorithm. As a result, we prove that sampling without replacement is at least $0.513$-competitive for Online Bipartite Matching.",
        "subjects": [
            "cs.DS",
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06877",
        "abstract url": "https://arxiv.org/abs/2410.06877",
        "title": "Best-of-Both-Worlds Fair Allocation of Indivisible and Mixed Goods",
        "rating": "-10",
        "keywords": [],
        "abstract": "We study the problem of fairly allocating either a set of indivisible goods or a set of mixed divisible and indivisible goods (i.e., mixed goods) to agents with additive utilities, taking the best-of-both-worlds perspective of guaranteeing fairness properties both ex ante and ex post. The ex-post fairness notions considered in this paper are relaxations of envy-freeness, specifically, EFX for indivisible-goods allocation, and EFM for mixed-goods allocation. For two agents, we show that there is a polynomial-time randomized algorithm that achieves ex-ante envy-freeness and ex-post EFX / EFM simultaneously. For $n$ agents with bi-valued utilities, we show there exist randomized allocations that are (i) ex-ante proportional and ex-post EFM, and (ii) ex-ante envy-free, ex-post EFX, and ex-post fractionally Pareto optimal.",
        "subjects": [
            "cs.GT"
        ],
        "comment": "Appears in the 20th Conference on Web and Internet Economics (WINE), 2024"
    },
    {
        "paper id": "2410.06881",
        "abstract url": "https://arxiv.org/abs/2410.06881",
        "title": "Privately Counting Partially Ordered Data",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider differentially private counting when each data point consists of $d$ bits satisfying a partial order. Our main technical contribution is a problem-specific $K$-norm mechanism that runs in time $O(d^2)$. Experiments show that, depending on the partial order in question, our solution dominates existing pure differentially private mechanisms, and can reduce their error by an order of magnitude or more.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06941",
        "abstract url": "https://arxiv.org/abs/2410.06941",
        "title": "WorkflowHub: a registry for computational workflows",
        "rating": "-10",
        "keywords": [],
        "abstract": "The rising popularity of computational workflows is driven by the need for repetitive and scalable data processing, sharing of processing know-how, and transparent methods. As both combined records of analysis and descriptions of processing steps, workflows should be reproducible, reusable, adaptable, and available. Workflow sharing presents opportunities to reduce unnecessary reinvention, promote reuse, increase access to best practice analyses for non-experts, and increase productivity. In reality, workflows are scattered and difficult to find, in part due to the diversity of available workflow engines and ecosystems, and because workflow sharing is not yet part of research practice. WorkflowHub provides a unified registry for all computational workflows that links to community repositories, and supports both the workflow lifecycle and making workflows findable, accessible, interoperable, and reusable (FAIR). By interoperating with diverse platforms, services, and external registries, WorkflowHub adds value by supporting workflow sharing, explicitly assigning credit, enhancing FAIRness, and promoting workflows as scholarly artefacts. The registry has a global reach, with hundreds of research organisations involved, and more than 700 workflows registered.",
        "subjects": [
            "cs.DL",
            "cs.SE"
        ],
        "comment": "30 pages, 4 figures"
    },
    {
        "paper id": "2410.06948",
        "abstract url": "https://arxiv.org/abs/2410.06948",
        "title": "An Overview of zbMATH Open Digital Library",
        "rating": "-10",
        "keywords": [],
        "abstract": "Mathematical research thrives on the effective dissemination and discovery of knowledge. zbMATH Open has emerged as a pivotal platform in this landscape, offering a comprehensive repository of mathematical literature. Beyond indexing and abstracting, it serves as a unified quality-assured infrastructure for finding, evaluating, and connecting mathematical information that advances mathematical research as well as interdisciplinary exploration. zbMATH Open enables scientific quality control by post-publication reviews and promotes connections between researchers, institutions, and research outputs. This paper represents the functionalities of the most significant features of this open-access service, highlighting its role in shaping the future of mathematical information retrieval.",
        "subjects": [
            "cs.DL",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06953",
        "abstract url": "https://arxiv.org/abs/2410.06953",
        "title": "Control System Design and Experiments for Autonomous Underwater Helicopter Docking Procedure Based on Acoustic-inertial-optical Guidance",
        "rating": "-10",
        "keywords": [],
        "abstract": "A control system structure for the underwater docking procedure of an Autonomous Underwater Helicopter (AUH) is proposed in this paper, which utilizes acoustic-inertial-optical guidance. Unlike conventional Autonomous Underwater Vehicles (AUVs), the maneuverability requirements for AUHs are more stringent during the docking procedure, requiring it to remain stationary or have minimal horizontal movement while moving vertically. The docking procedure is divided into two stages: Homing and Landing, each stage utilizing different guidance methods. Additionally, a segmented aligning strategy operating at various altitudes and a linear velocity decision are both adopted in Landing stage. Due to the unique structure of the Subsea Docking System (SDS), the AUH is required to dock onto the SDS in a fixed orientation with specific attitude and altitude. Therefore, a particular criterion is proposed to determine whether the AUH has successfully docked onto the SDS. Furthermore, the effectiveness and robustness of the proposed control method in AUH's docking procedure are demonstrated through pool experiments and sea trials.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06972",
        "abstract url": "https://arxiv.org/abs/2410.06972",
        "title": "Diamond of Thought: A Design Thinking-Based Framework for LLMs in Wearable Design",
        "rating": "-10",
        "keywords": [],
        "abstract": "Wearable design is an interdisciplinary field that balances technological innovation, human factors, and human-computer interactions. Despite contributions from various disciplines, many projects lack stable interdisciplinary teams, which often leads to design failures. Large language models (LLMs) integrate diverse information and generate innovative solutions, making them a valuable tool for enhancing design processes. Thus, we have explored the use of LLMs in wearable design by combining design-thinking principles with LLM capabilities. We have developed the \"Diamond of Thought\" framework and analysed 1,603 prototypes and 1,129 products from a body-centric perspective to create a comprehensive database. We employed retrieval-augmented generation to input database details into the LLMs, ensuring applicability to wearable design challenges and integration of embodied cognition into the process. Our LLM-based methodology for wearables has been experimentally validated, demonstrating the potential of LLMs for the advancement of design practices. This study offers new tools and methods for future wearable designs.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06984",
        "abstract url": "https://arxiv.org/abs/2410.06984",
        "title": "Observability rank conditions for analysing practical identifiability a priori",
        "rating": "-10",
        "keywords": [],
        "abstract": "The concept of identifiability describes the possibility of inferring the parameters of a dynamic model by observing its output. It is common and useful to distinguish between structural and practical identifiability. The former property is fully determined by the model equations, while the latter is also influenced by the characteristics of the available experimental data. Structural identifiability can be determined by means of symbolic computations, which may be performed before collecting experimental data, and are hence sometimes called a priori analyses. Practical identifiability is typically assessed numerically, with methods that require simulations - and often also optimization - and are applied a posteriori. An approach to study structural local identifiability is to consider it as a particular case of observability, which is the possibility of inferring the internal state of a system from its output. Thus, both properties can be analysed jointly, by building a generalized observability matrix and computing its rank. The aim of this paper is to investigate to which extent such observability-based methods can also inform about practical identifiability. To this end, we explore a number of possible extensions of the rank tests, and discuss the purposes for which they can be informative as well as others for which they cannot.",
        "subjects": [
            "q-bio.QM",
            "eess.SY"
        ],
        "comment": "10 pages, 2 figures"
    },
    {
        "paper id": "2410.06992",
        "abstract url": "https://arxiv.org/abs/2410.06992",
        "title": "SWE-Bench+: Enhanced Coding Benchmark for LLMs",
        "rating": "-10",
        "keywords": [],
        "abstract": "Large Language Models (LLMs) in Software Engineering (SE) can offer assistance for coding. To facilitate a rigorous evaluation of LLMs in practical coding contexts, Carlos et al. introduced the SWE-bench dataset, which comprises 2,294 real-world GitHub issues and their corresponding pull requests, collected from 12 widely used Python repositories. Several impressive LLM-based toolkits recently are developed and evaluated on this dataset. However, a systematic evaluation of the quality of SWE-bench remains missing. In this paper, we addressed this gap by presenting an empirical analysis of the SWE-bench dataset. We conducted a manual screening of instances where SWEAgent + GPT-4 successfully resolved issues by comparing the model-generated patches with the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench leaderboard during the time of our study. Our analysis reveals some critical issues with the SWE-bench dataset: 1) 32.67% of the successful patches involve cheating as the solutions were directly provided in the issue report or the comments. We refer to as solution leakage problem. 2) 31.08% of the passed patches are suspicious patches due to weak test cases, i.e., the tests were not adequate to verify the correctness of a patch. When we filtered out these problematic issues, the resolution rate of SWE-Agent+GPT-4 dropped from 12.47% to 3.97%. We also observed that the same data quality issues also exist in the two variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In addition, over 94% of the issues were created before LLM's knowledge cutoff dates, posing potential data leakage issues.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.06998",
        "abstract url": "https://arxiv.org/abs/2410.06998",
        "title": "An Improved ESO-Based Line-of-Sight Guidance Law for Path Following of Underactuated Autonomous Underwater Helicopter With Nonlinear Tracking Differentiator and Anti-saturation Controller",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents an Improved Extended-state-observer based Line-of-Sight (IELOS) guidance law for path following of underactuated Autonomous Underwater helicopter (AUH) utilizing a nonlinear tracking differentiator and anti-saturation controller. Due to the high mobility of the AUH, the classical reduced-order Extended-State-Observer (ESO) struggles to accurately track the sideslip angle, especially when rapid variation occurs. By incorporating the nonlinear tracking differentiator and anti-saturation controller, the IELOS guidance law can precisely track sideslip angle and mitigate propeller thrust buffet compared to the classical Extended-state-observer based Line-of-Sight (ELOS) guidance law. The performance of ESO is significantly influenced by the bandwidth, with the Improved Extended-State-Observer (IESO) proving effective at low bandwidths where the classical ESO falls short. The paper establishes the input-to-state stability of the closed-loop system. Subsequently, simulation and pool experimental results are showcased to validate the effectiveness of the IELOS guidance law, which outperforms both the Line-of-Sight (LOS) and Adaptive Line-of-Sight (ALOS) guidance laws in terms of performance.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07020",
        "abstract url": "https://arxiv.org/abs/2410.07020",
        "title": "What Makes Programmers Laugh? Exploring the Subreddit r/ProgrammerHumor",
        "rating": "-10",
        "keywords": [],
        "abstract": "Background: Humor is a fundamental part of human communication, with prior work linking positive humor in the workplace to positive outcomes, such as improved performance and job satisfaction. Aims: This study aims to investigate programming-related humor in a large social media community. Methodology: We collected 139,718 submissions from Reddit subreddit r/ProgrammerHumor. Both textual and image-based (memes) submissions were considered. The image data was processed with OCR to extract text from images for NLP analysis. Multiple regression models were built to investigate what makes submissions humorous. Additionally, a random sample of 800 submissions was labeled by human annotators regarding their relation to theories of humor, suitability for the workplace, the need for programming knowledge to understand the submission, and whether images in image-based submissions added context to the submission. Results: Our results indicate that predicting the humor of software developers is difficult. Our best regression model was able to explain only 10% of the variance. However, statistically significant differences were observed between topics, submission times, and associated humor theories. Our analysis reveals that the highest submission scores are achieved by imagebased submissions that are created during the winter months in the northern hemisphere, between 2-3pm UTC on weekends, which are distinctly related to superiority and incongruity theories of humor, and are about the topic of \"Learning\". Conclusions: Predicting humor with natural language processing methods is challenging. We discuss the benefits and inherent difficulties in assessing perceived humor of submissions, as well as possible avenues for future work. Additionally, our replication package should help future studies and can act as a joke repository for the software industry and education.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "11 pages. Accepted to International Symposium on Empirical Software Engineering and Measurement (ESEM) 2024 research track"
    },
    {
        "paper id": "2410.07023",
        "abstract url": "https://arxiv.org/abs/2410.07023",
        "title": "Mechanism Design for Exchange Markets",
        "rating": "-10",
        "keywords": [],
        "abstract": "Exchange markets are a significant type of market economy, in which each agent holds a budget and certain (divisible) resources available for trading. Most research on equilibrium in exchange economies is based on an environment of completely free competition. However, the orderly operation of markets also relies on effective economic regulatory mechanisms. This paper initiates the study of the mechanism design problem in exchange markets, exploring the potential to establish truthful market rules and mechanisms. This task poses a significant challenge as unlike auctioneers in auction design, the mechanism designer in exchange markets lacks centralized authority to fully control the allocation of resources. In this paper, the mechanism design problem is formalized as a two-stage game. In stage 1, agents submit their private information to the manager, who then formulates market trading rules based on the submitted information. In stage 2, agents are free to engage in transactions within these rules, ultimately reaching an equilibrium. We generalize the concept of liquid welfare from classical budget-feasible auctions and use market liquid welfare as a measure to evaluate the performance of the designed mechanism. Moreover, an extra concept called profitability is introduced to assess whether the market is money-making (profitable) or money-losing (unprofitable). Our goal is to design a truthful mechanism that achieves an (approximate) optimal welfare while minimizing unprofitability as much as possible. Two mechanisms for the problem are proposed. The first one guarantees truthfulness and profitability while approaching an approximation ratio of 1/2 in large markets. The second one is also truthful and achieves 1/2 approximation in general markets but incurs bounded unprofitability. Our aim is for both mechanisms to provide valuable insights into the truthful market design problem.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07027",
        "abstract url": "https://arxiv.org/abs/2410.07027",
        "title": "Evaluation of Run-Time Energy Efficiency using Controlled Approximation in a RISC-V Core",
        "rating": "-10",
        "keywords": [],
        "abstract": "The limited energy available in most embedded systems poses a significant challenge in enhancing the performance of embedded processors and microcontrollers. One promising approach to address this challenge is the use of approximate computing, which can be implemented in both hardware and software layers to balance the trade-off between performance and power consumption. In this study, the impact of dynamic hardware approximation methods on the run-time energy efficiency of a RISC-V embedded processor with specialized features for approximate computing is investigated. The results indicate that the platform achieves an average energy efficiency of 13.3 pJ/instruction at a 500MHz clock frequency adhering approximation in 45nm CMOS technology. Compared to accurate circuits and computation, the approximate computing techniques in the processing core resulted in a significant improvement of 9.21% in overall energy efficiency, 60.83% in multiplication instructions, 14.64% in execution stage, and 9.23% in overall power consumption.",
        "subjects": [
            "cs.AR"
        ],
        "comment": "6 pages, 11 figures, accepted at The 6th Iranian International Conference on Microelectronics (IICM2024)"
    },
    {
        "paper id": "2410.07040",
        "abstract url": "https://arxiv.org/abs/2410.07040",
        "title": "The Euler-Lagrange equation and optimal control: Preliminary results",
        "rating": "-10",
        "keywords": [],
        "abstract": "Algebraically speaking, linear time-invariant (LTI) systems can be considered as modules. In this framework, controllability is translated as the freeness of the system module. Optimal control mainly relies on quadratic Lagrangians and the consideration of any basis of the system module leads to an open-loop control strategy via a linear Euler-Lagrange equation. In this approach, the endpoint is easily assignable and time horizon can be chosen to minimize the criterion. The loop is closed via an intelligent controller derived from model-free control, which exhibits excellent performances concerning model mismatches and disturbances. The extension to nonlinear systems is briefly discussed.",
        "subjects": [
            "math.OC",
            "eess.SY"
        ],
        "comment": "12th International Conference on Systems and Control, Batna (Algeria), 3-5 November 2024"
    },
    {
        "paper id": "2410.07067",
        "abstract url": "https://arxiv.org/abs/2410.07067",
        "title": "Further remarks on the dual negation in team logics",
        "rating": "-10",
        "keywords": [],
        "abstract": "The dual or game-theoretical negation $\\lnot$ of independence-friendly logic (IF) and dependence logic (D) exhibits an extreme degree of semantic indeterminacy in that for any pair of sentences $\u03c6$ and $\u03c8$ of IF/D, if $\u03c6$ and $\u03c8$ are incompatible in the sense that they share no models, there is a sentence $\u03b8$ of IF/D such that $\u03c6\\equiv \u03b8$ and $\u03c8\\equiv \\lnot \u03b8$ (as shown originally by Burgess in the equivalent context of the prenex fragment of Henkin quantifier logic). We show that by adjusting the notion of incompatibility employed, analogues of this result can be established for a number of modal and propositional team logics, including Aloni's bilateral state-based modal logic, Hawke and Steinert-Threlkeld's semantic expressivist logic for epistemic modals, as well as propositional dependence logic with the dual negation. Together with its converse, a result of this type can be seen as an expressive completeness theorem with respect to the relevant incompatibility notion; we formulate a notion of expressive completeness for pairs of properties to make this precise.",
        "subjects": [
            "math.LO",
            "cs.LO"
        ],
        "comment": "48 pages, 2 figures"
    },
    {
        "paper id": "2410.07079",
        "abstract url": "https://arxiv.org/abs/2410.07079",
        "title": "Automated and Complete Generation of Traffic Scenarios at Road Junctions Using a Multi-level Danger Definition",
        "rating": "-10",
        "keywords": [],
        "abstract": "To ensure their safe use, autonomous vehicles (AVs) must meet rigorous certification criteria that involve executing maneuvers safely within (arbitrary) scenarios where other actors perform their intended maneuvers. For that purpose, existing scenario generation approaches optimize search to derive scenarios with high probability of dangerous situations. In this paper, we hypothesise that at road junctions, potential danger predominantly arises from overlapping paths of individual actors carrying out their designated high-level maneuvers. As a step towards AV certification, we propose an approach to derive a complete set of (potentially dangerous) abstract scenarios at any given road junction, i.e. all permutations of overlapping abstract paths assigned to actors (including the AV) for a given set of possible abstract paths. From these abstract scenarios, we derive exact paths that actors must follow to guide simulation-based testing towards potential collisions. We conduct extensive experiments to evaluate the behavior of a state-of-the-art learning based AV controller on scenarios generated over two realistic road junctions with increasing number of external actors. Results show that the AV-under-test is involved in increasing percentages of unsafe behaviors in simulation, which vary according to functional- and logical-level scenario properties.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07102",
        "abstract url": "https://arxiv.org/abs/2410.07102",
        "title": "Non-linear Control of the Power Injected Into a Weak Grid by a Self-Synchronized Inverter",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this work, a non-linear controller designed using non-linear transformation linearization and feedback is proposed for an inverter connected to a weak grid through a single-stage inductive filter. The proposed strategy is self-synchronized, so that it is not necessary to have a voltage sensor at the Point of Common Coupling (PCC). The strategy allows to robustify, in the presence of a weak grid, a strategy that has already been demonstrated to allow a significant reduction in the size of the DC-link capacitor of the converter. For this purpose, a state observer is designed that allows estimating the voltage at the PCC from the measurement of the output inductor current. A start-up controller is also included, which allows synchronization even in the case of system start-up. Simulation results are presented for different operating cases, including start-up, normal operation, and grid-voltage sags and swells. In all these cases, it is considered that the exact parameters of the grid to which the inverter is connected are unknown.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "8 pages, 5 figures"
    },
    {
        "paper id": "2410.07353",
        "abstract url": "https://arxiv.org/abs/2410.07353",
        "title": "Fabrication-Aware Inverse Design For Shape Optimization",
        "rating": "-10",
        "keywords": [],
        "abstract": "Inverse design (ID) is a computational method that systematically explores a design space to find optimal device geometries based on specific performance criteria. In silicon photonics, ID often leads to devices with design features that degrade significantly due to the fabrication process, limiting the applicability of these devices in scalable silicon photonic fabrication. We demonstrate a solution to this performance degradation through fabrication-aware inverse design (FAID), integrating lithography models for deep-ultraviolet (DUV) lithography and electron beam lithography (EBL) into the shape optimization approach of ID. A Y-branch and an SWG-to-strip converter were generated and fabricated with this new approach. Simulated and measured results verify that the FAID yields devices with up to 0.6 dB lower insertion loss per device. The modified workflow enables designers to use ID to generate devices that adjust for process bias predicted by lithography models.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "4 pages"
    },
    {
        "paper id": "2410.07356",
        "abstract url": "https://arxiv.org/abs/2410.07356",
        "title": "Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models",
        "rating": "-10",
        "keywords": [],
        "abstract": "High-level synthesis (HLS) allows hardware designers to create hardware designs with high-level programming languages like C/C++/OpenCL, which greatly improves hardware design productivity. However, existing HLS flows require programmers' hardware design expertise and rely on programmers' manual code transformations and directive annotations to guide compiler optimizations. Optimizing HLS designs requires non-trivial HLS expertise and tedious iterative process in HLS code optimization. Automating HLS code optimizations has become a burning need. Recently, large language models (LLMs) trained on massive code and programming tasks have demonstrated remarkable proficiency in comprehending code, showing the ability to handle domain-specific programming queries directly without labor-intensive fine-tuning. In this work, we propose a novel retrieval-augmented LLM-based approach to effectively optimize high-level synthesis (HLS) programs. Our proposed method leverages few-shot learning, enabling large language models to adopt domain-specific knowledge through natural language prompts. We propose a unique framework, Retrieve Augmented Large Language Model Aided Design (RALAD), designed to enhance LLMs' performance in HLS code optimization tasks. RALAD employs advanced embedding techniques and top-\\emph{k} search algorithms to dynamically source relevant knowledge from extensive databases, thereby providing contextually appropriate responses to complex programming queries. Our implementation of RALAD on two specialized domains, utilizing comparatively smaller language models, achieves an impressive 80\\% success rate in compilation tasks and outperforms general LLMs by 3.7 -- 19$\\times$ in latency improvement.",
        "subjects": [
            "cs.AR",
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07362",
        "abstract url": "https://arxiv.org/abs/2410.07362",
        "title": "Large Language Models in Qualitative Research: Can We Do the Data Justice?",
        "rating": "-10",
        "keywords": [],
        "abstract": "Qualitative researchers use tools to collect, sort, and analyze their data. Should qualitative researchers use large language models (LLMs) as part of their practice? LLMs could augment qualitative research, but it is unclear if their use is appropriate, ethical, or aligned with qualitative researchers' goals and values. We interviewed twenty qualitative researchers to investigate these tensions. Many participants see LLMs as promising interlocutors with attractive use cases across the stages of research, but wrestle with their performance and appropriateness. Participants surface concerns regarding the use of LLMs while protecting participant interests, and call attention to an urgent lack of norms and tooling to guide the ethical use of LLMs in research. Given the importance of qualitative methods to human-computer interaction, we use the tensions surfaced by our participants to outline guidelines for researchers considering using LLMs in qualitative research and design principles for LLM-assisted qualitative data analysis tools.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07370",
        "abstract url": "https://arxiv.org/abs/2410.07370",
        "title": "Recommending and Release Planning of User-Driven Functionality Deletion for Mobile Apps",
        "rating": "-10",
        "keywords": [],
        "abstract": "Evolving software with an increasing number of features poses challenges in terms of comprehensibility and usability. Traditional software release planning has predominantly focused on orchestrating the addition of features, contributing to the growing complexity and maintenance demands of larger software systems. In mobile apps, an excess of functionality can significantly impact usability, maintainability, and resource consumption, necessitating a nuanced understanding of the applicability of the law of continuous growth to mobile apps. Previous work showed that the deletion of functionality is common and sometimes driven by user reviews. For most users, the removal of features is associated with negative sentiments, prompts changes in usage patterns, and may even result in user churn. Motivated by these preliminary results, we propose Radiation to input user reviews and recommend if any functionality should be deleted from an app's User Interface (UI). We evaluate radiation using historical data and survey developers' opinions. From the analysis of 190,062 reviews from 115 randomly selected apps, we show that Radiation can recommend functionality deletion with an average F-Score of 74% and if sufficiently many negative user reviews suggest so. We conducted a survey involving 141 software developers to gain insights into the decision-making process and the level of planning for feature deletions. Our findings indicate that 77.3% of the participants often or always plan for such deletions. This underscores the importance of incorporating feature deletion planning into the overall release decision-making process.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Pre-print of REJ paper. arXiv admin note: substantial text overlap with arXiv:2305.19384"
    },
    {
        "paper id": "2410.07376",
        "abstract url": "https://arxiv.org/abs/2410.07376",
        "title": "Optimal Attitude Control of Large Flexible Space Structures with Distributed Momentum Actuators",
        "rating": "-10",
        "keywords": [],
        "abstract": "Recent spacecraft mission concepts propose larger payloads that have lighter, less rigid structures. For large lightweight structures, the natural frequencies of their vibration modes may fall within the attitude controller bandwidth, threatening the stability and settling time of the controller and compromising performance. This work tackles this issue by proposing an attitude control design paradigm of distributing momentum actuators throughout the structure to have more control authority over vibration modes. The issue of jitter disturbances introduced by these actuators is addressed by expanding the bandwidth of the attitude controller to suppress excess vibrations. Numerical simulation results show that, at the expense of more control action, a distributed configuration can achieve lower settling times and reduce structural deformation compared to a more standard centralized configuration.",
        "subjects": [
            "astro-ph.IM",
            "eess.SY"
        ],
        "comment": "10 pages, 9 figures"
    },
    {
        "paper id": "2410.07378",
        "abstract url": "https://arxiv.org/abs/2410.07378",
        "title": "Static Pricing for Online Selection Problem and its Variants",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper studies an online selection problem, where a seller seeks to sequentially sell multiple copies of an item to arriving buyers. We consider an adversarial setting, making no modeling assumptions about buyers' valuations for the items except acknowledging a finite support. In this paper, we focus on a class of static pricing algorithms that sample a price from a pre-determined distribution and sell items to buyers whose valuations exceed the sampled price. Such algorithms are of practical interests due to their advantageous properties, such as ease of implementation and non-discrimination over prices. Our work shows that the simple static pricing strategy can achieve strong guarantees comparable to the best known dynamic pricing algorithms. Particularly, we design the optimal static pricing algorithms for the adversarial online selection problem and its two important variants: the online assignment problem and the online selection with convex cost. The static pricing algorithms can even attain the optimal competitive ratios among all online algorithms for the online selection problem and the online assignment problem. To achieve these results, we propose an economics-based approach in the competitive analysis of static pricing algorithms, and develop a novel representative function-based approach to derive the lower bounds. We expect these approaches will be useful in related problems such as online matching.",
        "subjects": [
            "cs.GT",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07381",
        "abstract url": "https://arxiv.org/abs/2410.07381",
        "title": "Tally: Non-Intrusive Performance Isolation for Concurrent Deep Learning Workloads",
        "rating": "-10",
        "keywords": [],
        "abstract": "GPU underutilization is a significant concern in many production deep learning clusters, leading to prolonged job queues and increased operational expenses. A promising solution to this inefficiency is GPU sharing, which improves resource utilization by allowing multiple workloads to execute concurrently on a single GPU. However, the practical deployment of GPU sharing in production settings faces critical obstacles due to the limitations of existing mechanisms, such as high integration costs, inadequate performance isolation, and limited application compatibility. To address these issues, we introduce \\emph{Tally}, a non-intrusive GPU sharing mechanism that provides robust performance isolation and comprehensive workload compatibility. Tally operates as a virtualization layer between applications and GPUs, transparently orchestrating the device execution of concurrent workloads. The key to Tally's robust performance isolation capability lies in its fine-grained thread-block level GPU kernel scheduling strategy, which allows the system to effectively mitigate interference caused by workload co-execution. Our evaluation, conducted on a diverse set of workload combinations, demonstrates that Tally on average incurs a mere $7.2\\%$ overhead on the $99^{th}$-percentile latency of high-priority inference tasks when executed concurrently with best-effort training workloads compared to $188.9\\%$ overhead exhibited by the state-of-the-art GPU sharing systems like TGS, while achieving over $80\\%$ of TGS's system throughput.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07382",
        "abstract url": "https://arxiv.org/abs/2410.07382",
        "title": "Optimal-Length Labeling Schemes for Fast Deterministic Communication in Radio Networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider two fundamental communication tasks in arbitrary radio networks: broadcasting (information from one source has to reach all nodes) and gossiping (every node has a message and all messages have to reach all nodes). Nodes are assigned labels that are (not necessarily different) binary strings. Each node knows its own label and can use it as a parameter in the same deterministic algorithm. The length of a labeling scheme is the largest length of a label. The goal is to find labeling schemes of asymptotically optimal length for the above tasks, and to design fast deterministic distributed algorithms for each of them, using labels of optimal length. Our main result concerns broadcasting. We show the existence of a labeling scheme of constant length that supports broadcasting in time $O(D+\\log^2 n)$, where $D$ is the diameter of the network and $n$ is the number of nodes. This broadcasting time is an improvement over the best currently known $O(D\\log n + \\log^2 n)$ time of broadcasting with constant-length labels, due to Ellen and Gilbert (SPAA 2020). It also matches the optimal broadcasting time in radio networks of known topology. Hence, we show that appropriately chosen node labels of constant length permit to achieve, in a distributed way, the optimal centralized broadcasting time. This is, perhaps, the most surprising finding of this paper. We are able to obtain our result thanks to a novel methodological tool of propagating information in radio networks, that we call a 2-height respecting tree. Next, we apply our broadcasting algorithm to solve the gossiping problem. We get a gossiping algorithm working in time $O(D + \u0394\\log n + \\log^2 n)$, using a labeling scheme of optimal length $O(\\log \u0394)$, where $\u0394$ is the maximum degree. Our time is the same as the best known gossiping time in radio networks of known topology.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07403",
        "abstract url": "https://arxiv.org/abs/2410.07403",
        "title": "On the Feasibility of A Mixed-Method Approach for Solving Long Horizon Task-Oriented Dexterous Manipulation",
        "rating": "-10",
        "keywords": [],
        "abstract": "In-hand manipulation of tools using dexterous hands in real-world is an underexplored problem in the literature. In addition to more complex geometry and larger size of the tools compared to more commonly used objects like cubes or cylinders, task oriented in-hand tool manipulation involves many sub-tasks to be performed sequentially. This may involve reaching to the tool, picking it up, reorienting it in hand with or without regrasping to reach to a desired final grasp appropriate for the tool usage, and carrying the tool to the desired pose. Research on long-horizon manipulation using dexterous hands is rather limited and the existing work focus on learning the individual sub-tasks using a method like reinforcement learning (RL) and combine the policies for different subtasks to perform a long horizon task. However, in general a single method may not be the best for all the sub-tasks, and this can be more pronounced when dealing with multi-fingered hands manipulating objects with complex geometry like tools. In this paper, we investigate the use of a mixed-method approach to solve for the long-horizon task of tool usage and we use imitation learning, reinforcement learning and model based control. We also discuss a new RL-based teacher-student framework that combines real world data into offline training. We show that our proposed approach for each subtask outperforms the commonly adopted reinforcement learning approach across different subtasks and in performing the long horizon task in simulation. Finally we show the successful transferability to real world.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07407",
        "abstract url": "https://arxiv.org/abs/2410.07407",
        "title": "Optimized Spatial Architecture Mapping Flow for Transformer Accelerators",
        "rating": "-10",
        "keywords": [],
        "abstract": "Recent innovations in Transformer-based large language models have significantly advanced the field of general-purpose neural language understanding and generation. With billions of trainable parameters, deployment of these large models relies on high-performance hardware accelerators to efficiently deliver the required computation. Spatial architectures, such as TPUs, offer a promising solution to accelerating computation-intensive workloads. However, the design process for existing spatial architectures is predominantly manual, and it often involves time-consuming redesigns for new applications and new problem dimensions, which greatly limits the development of optimally designed accelerators for Transformer models. To address these challenges, we propose SAMT (Spatial Architecture Mapping for Transformers), a comprehensive framework designed to optimize the dataflow mapping of Transformer inference workloads onto spatial accelerators. We demonstrate the effectiveness of SAMT in improving the performance of spatial accelerators for Transformer models. We propose and leverage the dynamic operator fusion schemes for the Transformer models and co-search the optimal dataflow mapping strategies for spatial accelerators. SAMT significantly reduces inference latency by 12% to 91% and energy consumption by 3% to 23% for evaluated Transformer models compared to traditional spatial accelerator designs among edge, mobile and cloud settings.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07422",
        "abstract url": "https://arxiv.org/abs/2410.07422",
        "title": "Understanding User Needs for Injury Recovery with Augmented Reality",
        "rating": "-10",
        "keywords": [],
        "abstract": "Physical therapy (PT) plays a crucial role in muscle injury recovery, but people struggle to adhere to and perform PT exercises correctly from home. To support challenges faced with in-home PT, augmented reality (AR) holds promise in enhancing patient's engagement and accuracy through immersive interactive visualizations. However, effectively leveraging AR requires a better understanding of patient needs during injury recovery. Through interviews with six individuals undergoing physical therapy, this paper introduces user-centered design considerations integrating AR and body motion data to enhance in-home PT for injury recovery. Our findings identify key challenges and propose design variables for future body-based visualizations of body motion data for PT.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07457",
        "abstract url": "https://arxiv.org/abs/2410.07457",
        "title": "Responding to Promises: No-regret learning against followers with memory",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider a repeated Stackelberg game setup where the leader faces a sequence of followers of unknown types and must learn what commitments to make. While previous works have considered followers that best respond to the commitment announced by the leader in every round, we relax this setup in two ways. Motivated by natural scenarios where the leader's reputation factors into how the followers choose their response, we consider followers with memory. Specifically, we model followers that base their response on not just the leader's current commitment but on an aggregate of their past commitments. In developing learning strategies that the leader can employ against such followers, we make the second relaxation and assume boundedly rational followers. In particular, we focus on followers employing quantal responses. Interestingly, we observe that the smoothness property offered by the quantal response (QR) model helps in addressing the challenge posed by learning against followers with memory. Utilizing techniques from online learning, we develop algorithms that guarantee $O(\\sqrt{T})$ regret for quantal responding memory-less followers and $O(\\sqrt{BT})$ regret for followers with bounded memory of length $B$ with both scaling polynomially in game parameters.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07459",
        "abstract url": "https://arxiv.org/abs/2410.07459",
        "title": "User Feedback in Continuous Software Engineering: Revealing the State-of-Practice",
        "rating": "-10",
        "keywords": [],
        "abstract": "Context: Organizations opt for continuous delivery of incremental updates to deal with uncertainty and minimize waste. However, applying continuous engineering (CSE) practices requires a continuous feedback loop with input from customers and end-users. Challenges: It becomes increasingly challenging to apply traditional requirements elicitation and validation techniques with ever-shrinking software delivery cycles. At the same time, frequent deliveries generate an abundance of usage data and telemetry informing engineering teams of end-user behavior. The literature describing how practitioners work with user feedback in CSE, is limited. Objectives: We aim to explore the state of practice related to utilization of user feedback in CSE. Specifically, what practices are used, how, and the shortcomings of these practices. Method: We conduct a qualitative survey and report analysis from 21 interviews in 13 product development companies. We apply thematic and cross-case analysis to interpret the data. Results: Based on our earlier work we suggest a conceptual model of how user feedback is utilized in CSE. We further report the identified challenges with the continuous collection and analysis of user feedback and identify implications for practice. Conclusions: Companies use a combination of qualitative and quantitative methods to infer end-user preferences. At the same time, continuous collection, analysis, interpretation, and use of data in decisions are problematic. The challenges pertain to selecting the right metrics and analysis techniques, resource allocation, and difficulties in accessing vaguely defined user groups. Our advice to practitioners in CSE is to ensure sufficient resources and effort for interpretation of the feedback, which can be facilitated by telemetry dashboards.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "The manuscript is accepted for publication in Empirical Software Engineering on 30.09.2024"
    },
    {
        "paper id": "2410.07466",
        "abstract url": "https://arxiv.org/abs/2410.07466",
        "title": "Skip Hash: A Fast Ordered Map Via Software Transactional Memory",
        "rating": "-10",
        "keywords": [],
        "abstract": "Scalable ordered maps must ensure that range queries, which operate over many consecutive keys, provide intuitive semantics (e.g., linearizability) without degrading the performance of concurrent insertions and removals. These goals are difficult to achieve simultaneously when concurrent data structures are built using only locks and compare-and-swap objects. However, recent innovations in software transactional memory (STM) allow programmers to assume that multi-word atomic operations can be fast and simple. This paper introduces the skip hash, a new ordered map designed around that assumption. It combines a skip list and a hash map behind a single abstraction, resulting in $O(1)$ overheads for most operations. The skip hash makes use of a novel range query manager -- again leveraging STM -- to achieve fast, linearizable range queries that do not inhibit scalability. In performance evaluation, we show that the skip hash outperforms the state of the art in almost all cases. This places the skip hash in the uncommon position of being both exceedingly fast and exceedingly simple.",
        "subjects": [
            "cs.DC",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07486",
        "abstract url": "https://arxiv.org/abs/2410.07486",
        "title": "Visual Writing: Writing by Manipulating Visual Representations of Stories",
        "rating": "-10",
        "keywords": [],
        "abstract": "We introduce \"visual writing\", an approach to writing stories by manipulating visuals instead of words. Visual writing relies on editable visual representations of time, entities, events, and locations to offer representations more suited to specific editing tasks. We propose a taxonomy for these representations and implement a prototype software supporting the visual writing workflow. The system allows writers to edit the story by alternating between modifying the text and manipulating visual representations to edit entities, actions, locations, and order of events. We evaluate this workflow with eight creative writers and find visual writing can help find specific passages, keep track of story elements, specify edits, and explore story variations in a way that encourages creativity.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07497",
        "abstract url": "https://arxiv.org/abs/2410.07497",
        "title": "Strategic Facility Location via Predictions",
        "rating": "-10",
        "keywords": [],
        "abstract": "The facility location with strategic agents is a canonical problem in the literature on mechanism design without money. Recently, Agrawal et. al. considered this problem in the context of machine learning augmented algorithms, where the mechanism designer is also given a prediction of the optimal facility location. An ideal mechanism in this framework produces an outcome that is close to the social optimum when the prediction is accurate (consistency) and gracefully degrades as the prediction deviates from the truth, while retaining some of the worst-case approximation guarantees (robustness). The previous work only addressed this problem in the two-dimensional Euclidean space providing optimal trade-offs between robustness and consistency guarantees for deterministic mechanisms. We consider the problem for \\emph{general} metric spaces. Our only assumption is that the metric is continuous, meaning that any pair of points must be connected by a continuous shortest path. We introduce a novel mechanism that in addition to agents' reported locations takes a predicted optimal facility location $\\hat{o}$. We call this mechanism $\\texttt{Harmonic}$, as it selects one of the reported locations $\\tilde{\\ell}_i$ with probability inversely proportional to $d(\\hat{o},\\tilde{\\ell}_i)+ \u0394$ for a constant parameter $\u0394$. While \\harm \\ mechanism is not truthful, we can \\emph{characterize the set of undominated strategies} for each agent $i$ as solely consisting of the points on a shortest path from their true location $\\ell_i$ to the predicted location $\\hat{o}$. We further derive \\emph{consistency and robustness guarantees on the Price of Anarchy (PoA)} for the game induced by the mechanism.",
        "subjects": [
            "cs.GT",
            "cs.DS"
        ],
        "comment": "To appear in WINE 2024"
    },
    {
        "paper id": "2410.07516",
        "abstract url": "https://arxiv.org/abs/2410.07516",
        "title": "Exploring and Lifting the Robustness of LLM-powered Automated Program Repair with Metamorphic Testing",
        "rating": "-10",
        "keywords": [],
        "abstract": "In recent years, Large language model-powered Automated Program Repair (LAPR) techniques have achieved state-of-the-art bug-fixing performance and have been pervasively applied and studied in both industry and academia. Nonetheless, LLMs were proved to be highly sensitive to input prompts, with slight differences in the expressions of semantically equivalent programs potentially causing repair failures. Therefore, it is crucial to conduct robustness testing on LAPR techniques before their practical deployment. However, related research is scarce. To this end, we propose MT-LAPR, a Metamorphic Testing framework exclusively for LAPR techniques, which summarizes nine widely-recognized Metamorphic Relations (MRs) by developers across three perturbation levels: token, statement, and block. Afterward, our proposed MRs are applied to buggy codes to generate test cases, which are semantically equivalent yet to affect the inference of LAPR. Experiments are carried out on two extensively examined bug-fixing datasets, i.e., Defect4J and QuixBugs, and four bug-fixing abled LLMs released recently, demonstrating that 34.4% - 48.5% of the test cases expose the instability of LAPR techniques on average, showing the effectiveness of MT-LAPR and uncovering a positive correlation between code readability and the robustness of LAPR techniques. Inspired by the above findings, this paper uses the test cases generated by MT-LAPR as samples to train a CodeT5-based code editing model aiming at improving code readability and then embeds it into the LAPR workflow as a data preprocessing step. Extensive experiments demonstrate that this approach significantly enhances the robustness of LAPR by 49.32% at most.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07535",
        "abstract url": "https://arxiv.org/abs/2410.07535",
        "title": "Constraint representation towards precise data-driven storytelling",
        "rating": "-10",
        "keywords": [],
        "abstract": "Data-driven storytelling serves as a crucial bridge for communicating ideas in a persuasive way. However, the manual creation of data stories is a multifaceted, labor-intensive, and case-specific effort, limiting their broader application. As a result, automating the creation of data stories has emerged as a significant research thrust. Despite advances in Artificial Intelligence, the systematic generation of data stories remains challenging due to their hybrid nature: they must frame a perspective based on a seed idea in a top-down manner, similar to traditional storytelling, while coherently grounding insights of given evidence in a bottom-up fashion, akin to data analysis. These dual requirements necessitate precise constraints on the permissible space of a data story. In this viewpoint, we propose integrating constraints into the data story generation process. Defined upon the hierarchies of interpretation and articulation, constraints shape both narrations and illustrations to align with seed ideas and contextualized evidence. We identify the taxonomy and required functionalities of these constraints. Although constraints can be heterogeneous and latent, we explore the potential to represent them in a computation-friendly fashion via Domain-Specific Languages. We believe that leveraging constraints will facilitate both artistic and scientific aspects of data story generation.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "In 2024 IEEE Visualization and Visual Analytics Gen4DS (VIS-Gen4DS'24)"
    },
    {
        "paper id": "2410.07558",
        "abstract url": "https://arxiv.org/abs/2410.07558",
        "title": "Streamlined shape of cyborg cockroach promotes traversability in confined environments by gap negotiation",
        "rating": "-10",
        "keywords": [],
        "abstract": "The centimeter-scale cyborg insects have a potential advantage for application in narrow environments where humans cannot operate. To realize such tasks, researchers have developed a small printed-circuit-board (PCB) which an insect can carry and control it. The electronic components usually remain bare on the board and the whole board is mounted on platform animals, resulting in uneven morphology of whole cyborg with sharp edges. It is well known that streamlined body shape in artificial vehicles or robots contributes to effective locomotion by reducing drag force in media. However, little is known how the entire body shape impacts on locomotor performance of cyborg insect. Here, we developed a 10 mm by 10 mm board which provided electrical stimulation via Sub-GHz communication and investigated the impact of physical arrangement of the board using Madagascar hissing cockroach. We compared the success rate of gap negotiation between the cyborg with mounted board and implanted board and found the latter outperformed the former. We demonstrated our cyborg cockroach with implanted board could follow faithfully to the locomotion command via antennal or cercal stimulation and traverse a narrow gap like air vent cover. In contrast to the conventional arrangement, our cyborg insects are suitable for application in a concealed environment.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07566",
        "abstract url": "https://arxiv.org/abs/2410.07566",
        "title": "Revisiting the Primitives of Transaction Fee Mechanism Design",
        "rating": "-10",
        "keywords": [],
        "abstract": "Transaction Fee Mechanism Design studies auctions run by untrusted miners for transaction inclusion in a blockchain. Under previously-considered desiderata, an auction is considered `good' if, informally-speaking, each party (i.e., the miner, the users, and coalitions of both miners and users) has no incentive to deviate from the fixed and pre-determined protocol. In this paper, we propose a novel desideratum for transaction fee mechanisms. We say that a TFM is off-chain influence proof when the miner cannot achieve additional revenue by running a separate auction off-chain. While the previously-highlighted EIP-1559 is the gold-standard according to prior desiderata, we show that it does not satisfy off-chain influence proofness. Intuitively, this holds because a Bayesian revenue-maximizing miner can strictly increase profits by persuasively threatening to censor any bids that do not transfer a tip directly to the miner off-chain. On the other hand, we reconsider the Cryptographic (multi-party computation assisted) Second Price Auction mechanism, which is technically not `simple for miners' according to previous desiderata (since miners may wish to set a reserve by fabricating bids). We show that, in a slightly different model where the miner is allowed to set the reserve directly, this auction satisfies simplicity for users and miners, and off-chain influence proofness. Finally, we prove a strong impossibility result: no mechanism satisfies all previously-considered properties along with off-chain influence proofness, even with unlimited supply, and even after soliciting input from the miner.",
        "subjects": [
            "cs.GT",
            "econ.TH"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07572",
        "abstract url": "https://arxiv.org/abs/2410.07572",
        "title": "Edge-guided inverse design of digital metamaterials for ultra-high-capacity on-chip multi-dimensional interconnect",
        "rating": "-10",
        "keywords": [],
        "abstract": "The escalating demands of compute-intensive applications, including artificial intelligence, urgently necessitate the adoption of sophisticated optical on-chip interconnect technologies to overcome critical bottlenecks in scaling future computing systems. This transition requires leveraging the inherent parallelism of wavelength and mode dimensions of light, complemented by high-order modulation formats, to significantly enhance data throughput. Here we experimentally demonstrate a novel synergy of these three dimensions, achieving multi-tens-of-terabits-per-second on-chip interconnects using ultra-broadband, multi-mode digital metamaterials. Employing a highly efficient edge-guided analog-and-digital optimization method, we inversely design foundry-compatible, robust, and multi-port digital metamaterials with an 8xhigher computational efficiency. Using a packaged five-mode multiplexing chip, we demonstrate a single-wavelength interconnect capacity of 1.62 Tbit s-1 and a record-setting multi-dimensional interconnect capacity of 38.2 Tbit s-1 across 5 modes and 88 wavelength channels. A theoretical analysis suggests that further system optimization can enable on-chip interconnects to reach sub-petabit-per-second data transmission rates. This study highlights the transformative potential of optical interconnect technologies to surmount the constraints of electronic links, thus setting the stage for next-generation datacenter and optical compute interconnects.",
        "subjects": [
            "physics.optics",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07594",
        "abstract url": "https://arxiv.org/abs/2410.07594",
        "title": "Design and Characterization of High Efficiency Single-stage Electromagnetic Coil Guns",
        "rating": "-10",
        "keywords": [],
        "abstract": "This study presents several novel approaches to improve the efficiency of a single-stage coil gun. Conventional designs typically feature a uniformly wound solenoid and a ferrite projectile. For our research, we constructed a microcontroller-based prototype to test several new enhancements, including the use of a bipolar current pulse, a stepped multilayer coil with non-uniform winding densities, and the replacement of conventional ferrite projectiles with a neodymium permanent magnet. These modifications were designed to reduce energy loss and improve projectile acceleration by changing magnetic field strength and effectively controlling the magnetic flux. The experimental results show that the proposed methods resulted in significant efficiency improvements, with the varying current pulse and stepped coil design providing enhanced magnetic force at key points in the projectile's path, and the permanent magnet projectile contributing to higher velocities and efficiencies by leveraging the current pulses. Our findings suggest that combining these enhancements significantly improves coil gun performance, achieving higher velocities and efficiencies. These findings can be applied to future coil gun developments, such as multi-stage coil gun systems.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "10 pages, 23 figures"
    },
    {
        "paper id": "2410.07603",
        "abstract url": "https://arxiv.org/abs/2410.07603",
        "title": "An Analysis of XML Compression Efficiency",
        "rating": "-10",
        "keywords": [],
        "abstract": "XML simplifies data exchange among heterogeneous computers, but it is notoriously verbose and has spawned the development of many XML-specific compressors and binary formats. We present an XML test corpus and a combined efficiency metric integrating compression ratio and execution speed. We use this corpus and linear regression to assess 14 general-purpose and XML-specific compressors relative to the proposed metric. We also identify key factors when selecting a compressor. Our results show XMill or WBXML may be useful in some instances, but a general-purpose compressor is often the best choice.",
        "subjects": [
            "cs.DB",
            "cs.IT",
            "cs.PF"
        ],
        "comment": "1. test data at https://web.archive.org/web/20160805043420/http://chris-augeri.com/wp-content/uploads/docs/xml_compress.htm 2. one next step is testing newer compressors, e.g., Brotli, along with Zstandard, which leverages the asymmetric numeral system (ANS) 3. citations at https://scholar.google.com/scholar?cluster=8178011010886368797&hl=en&as_sdt=5,33&sciodt=0,33"
    },
    {
        "paper id": "2410.07606",
        "abstract url": "https://arxiv.org/abs/2410.07606",
        "title": "Stop-N-Go: Search-based Conflict Resolution for Motion Planning of Multiple Robotic Manipulators",
        "rating": "-10",
        "keywords": [],
        "abstract": "We address the motion planning problem for multiple robotic manipulators in packed environments where shared workspace can result in goal positions occupied or blocked by other robots unless those other robots move away to make the goal positions free. While planning in a coupled configuration space (C-space) is straightforward, it struggles to scale with the number of robots and often fails to find solutions. Decoupled planning is faster but frequently leads to conflicts between trajectories. We propose a conflict resolution approach that inserts pauses into individually planned trajectories using an A* search strategy to minimize the makespan--the total time until all robots complete their tasks. This method allows some robots to stop, enabling others to move without collisions, and maintains short distances in the C-space. It also effectively handles cases where goal positions are initially blocked by other robots. Experimental results show that our method successfully solves challenging instances where baseline methods fail to find feasible solutions.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.07608",
        "abstract url": "https://arxiv.org/abs/2410.07608",
        "title": "Assessing the impacts of convening experts: a bibliometric analysis of a research program spanning four decades",
        "rating": "-10",
        "keywords": [],
        "abstract": "Over the last few decades, research institutions and funders have begun policies and programs that incentivize large-scale collaboration across institutions on focal research areas. Yet, few studies have evaluated the impact of those programs on research, particularly on timelines longer than a few years. Using the Canadian Institute for Advanced Research (CIFAR) as a case study, we examined the impacts of supporting a research program that convened experts across intuitions and countries for 40+ years. In this study, we used the Scopus bibliometric database to analyse publishing and citation trends within this team since its formation in 1986 and used nearest neighbour matching to compare these trends against authors across the globe with similar career characteristics to measure how effectively the CIFAR program Gravity & the Extreme Universe (CIFAR-GEU) has catalyzed collaborations and produced high quality research outputs. We found a greater degree of co-authorship within the CIFAR-GEU group compared to the Control group. We also found that the outputs generated by the CIFAR-GEU group had, overall, higher values for citation-based impact indicators (e.g., stronger metrics around citations, impact from international collaborations and reach beyond academia).",
        "subjects": [
            "cs.DL"
        ],
        "comment": null
    }
]