[
    {
        "paper id": "2410.08584",
        "abstract url": "https://arxiv.org/abs/2410.08584",
        "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression",
        "rating": "3",
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "Vision-Language"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The efficiency of large vision-language models (LVLMs) is constrained by the computational bottleneck of the attention mechanism during the prefill phase and the memory bottleneck of fetching the key-value (KV) cache in the decoding phase, particularly in scenarios involving high-resolution images or videos. Visual content often exhibits substantial redundancy, resulting in highly sparse attention maps within LVLMs. This sparsity can be leveraged to accelerate attention computation or compress the KV cache through various approaches. However, most studies focus on addressing only one of these bottlenecks and do not adequately support dynamic adjustment of sparsity concerning distinct layers or tasks. In this paper, we present ZipVL, an efficient inference framework designed for LVLMs that resolves both computation and memory bottlenecks through a dynamic ratio allocation strategy of important tokens. This ratio is adaptively determined based on the layer-specific distribution of attention scores, rather than fixed hyper-parameters, thereby improving efficiency for less complex tasks while maintaining high performance for more challenging ones. Then we select important tokens based on their normalized attention scores and perform attention mechanism solely on those important tokens to accelerate the prefill phase. To mitigate the memory bottleneck in the decoding phase, we employ mixed-precision quantization to the KV cache, where high-bit quantization is used for caches of important tokens, while low-bit quantization is applied to those of less importance. Our experiments demonstrate that ZipVL can accelerate the prefill phase by 2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy reduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively enhancing the generation efficiency of LVLMs.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "15 pages"
    },
    {
        "paper id": "2410.08611",
        "abstract url": "https://arxiv.org/abs/2410.08611",
        "title": "Conjugated Semantic Pool Improves OOD Detection with Pre-trained Vision-Language Models",
        "rating": "2.5",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.AI",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "A straightforward pipeline for zero-shot out-of-distribution (OOD) detection involves selecting potential OOD labels from an extensive semantic pool and then leveraging a pre-trained vision-language model to perform classification on both in-distribution (ID) and OOD labels. In this paper, we theorize that enhancing performance requires expanding the semantic pool, while increasing the expected probability of selected OOD labels being activated by OOD samples, and ensuring low mutual dependence among the activations of these OOD labels. A natural expansion manner is to adopt a larger lexicon; however, the inevitable introduction of numerous synonyms and uncommon words fails to meet the above requirements, indicating that viable expansion manners move beyond merely selecting words from a lexicon. Since OOD detection aims to correctly classify input images into ID/OOD class groups, we can \"make up\" OOD label candidates which are not standard class names but beneficial for the process. Observing that the original semantic pool is comprised of unmodified specific class names, we correspondingly construct a conjugated semantic pool (CSP) consisting of modified superclass names, each serving as a cluster center for samples sharing similar properties across different categories. Consistent with our established theory, expanding OOD label candidates with the CSP satisfies the requirements and outperforms existing works by 7.89% in FPR95. Codes are available in https://github.com/MengyuanChen21/NeurIPS2024-CSP.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "28 pages, accepted by NeurIPS 2024"
    },
    {
        "paper id": "2410.08661",
        "abstract url": "https://arxiv.org/abs/2410.08661",
        "title": "QEFT: Quantization for Efficient Fine-Tuning of LLMs",
        "rating": "2.5",
        "keywords": [
            [
                "parameter-efficient",
                "Efficient Fine-Tuning"
            ],
            [
                "cs.LG",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "With the rapid growth in the use of fine-tuning for large language models (LLMs), optimizing fine-tuning while keeping inference efficient has become highly important. However, this is a challenging task as it requires improvements in all aspects, including inference speed, fine-tuning speed, memory consumption, and, most importantly, model quality. Previous studies have attempted to achieve this by combining quantization with fine-tuning, but they have failed to enhance all four aspects simultaneously. In this study, we propose a new lightweight technique called Quantization for Efficient Fine-Tuning (QEFT). QEFT accelerates both inference and fine-tuning, is supported by robust theoretical foundations, offers high flexibility, and maintains good hardware compatibility. Our extensive experiments demonstrate that QEFT matches the quality and versatility of full-precision parameter-efficient fine-tuning, while using fewer resources. Our code is available at https://github.com/xvyaward/qeft.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Accepted at Findings of EMNLP 2024"
    },
    {
        "paper id": "2410.08791",
        "abstract url": "https://arxiv.org/abs/2410.08791",
        "title": "Superpipeline: A Universal Approach for Reducing GPU Memory Usage in Large Models",
        "rating": "2.5",
        "keywords": [
            [
                "GPU Memory"
            ],
            [
                "vision-language",
                "VLMs"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The rapid growth in machine learning models, especially in natural language processing and computer vision, has led to challenges when running these models on hardware with limited resources. This paper introduces Superpipeline, a new framework designed to optimize the execution of large AI models on constrained hardware during both training and inference. Our approach involves dynamically managing model execution by dividing models into individual layers and efficiently transferring these layers between GPU and CPU memory. Superpipeline reduces GPU memory usage by up to 60% in our experiments while maintaining model accuracy and acceptable processing speeds. This allows models that would otherwise exceed available GPU memory to run effectively. Unlike existing solutions that focus mainly on inference or specific model types, Superpipeline can be applied to large language models (LLMs), vision-language models (VLMs), and vision-based models. We tested Superpipeline's performance across various models and hardware setups. The method includes two key parameters that allow fine-tuning the balance between GPU memory use and processing speed. Importantly, Superpipeline does not require retraining or changing model parameters, ensuring that the original model's output remains unchanged. Superpipeline's simplicity and flexibility make it useful for researchers and professionals working with advanced AI models on limited hardware. It enables the use of larger models or bigger batch sizes on existing hardware, potentially speeding up innovation across many machine learning applications. This work marks an important step toward making advanced AI models more accessible and optimizing their deployment in resource-limited environments. The code for Superpipeline is available at https://github.com/abbasiReza/super-pipeline.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09004",
        "abstract url": "https://arxiv.org/abs/2410.09004",
        "title": "DA-Ada: Learning Domain-Aware Adapter for Domain Adaptive Object Detection",
        "rating": "2.5",
        "keywords": [
            [
                "visual-language",
                "VLMs"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Domain adaptive object detection (DAOD) aims to generalize detectors trained on an annotated source domain to an unlabelled target domain. As the visual-language models (VLMs) can provide essential general knowledge on unseen images, freezing the visual encoder and inserting a domain-agnostic adapter can learn domain-invariant knowledge for DAOD. However, the domain-agnostic adapter is inevitably biased to the source domain. It discards some beneficial knowledge discriminative on the unlabelled domain, i.e., domain-specific knowledge of the target domain. To solve the issue, we propose a novel Domain-Aware Adapter (DA-Ada) tailored for the DAOD task. The key point is exploiting domain-specific knowledge between the essential general knowledge and domain-invariant knowledge. DA-Ada consists of the Domain-Invariant Adapter (DIA) for learning domain-invariant knowledge and the Domain-Specific Adapter (DSA) for injecting the domain-specific knowledge from the information discarded by the visual encoder. Comprehensive experiments over multiple DAOD tasks show that DA-Ada can efficiently infer a domain-aware visual encoder for boosting domain adaptive object detection. Our code is available at https://github.com/Therock90421/DA-Ada.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by NeurIPS 2024"
    },
    {
        "paper id": "2410.08598",
        "abstract url": "https://arxiv.org/abs/2410.08598",
        "title": "Parameter-Efficient Fine-Tuning of Large Language Models using Semantic Knowledge Tuning",
        "rating": "2",
        "keywords": [
            [
                "Parameter-Efficient",
                "Efficient Fine-Tuning"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) are gaining significant popularity in recent years for specialized tasks using prompts due to their low computational cost. Standard methods like prefix tuning utilize special, modifiable tokens that lack semantic meaning and require extensive training for best performance, often falling short. In this context, we propose a novel method called Semantic Knowledge Tuning (SK-Tuning) for prompt and prefix tuning that employs meaningful words instead of random tokens. This method involves using a fixed LLM to understand and process the semantic content of the prompt through zero-shot capabilities. Following this, it integrates the processed prompt with the input text to improve the model's performance on particular tasks. Our experimental results show that SK-Tuning exhibits faster training times, fewer parameters, and superior performance on tasks such as text classification and understanding compared to other tuning methods. This approach offers a promising method for optimizing the efficiency and effectiveness of LLMs in processing language tasks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted in Nature Scientific Reports"
    },
    {
        "paper id": "2410.08695",
        "abstract url": "https://arxiv.org/abs/2410.08695",
        "title": "Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across multimodal tasks such as visual perception and reasoning, leading to good performance on various multimodal evaluation benchmarks. However, these benchmarks keep a static nature and overlap with the pre-training data, resulting in fixed complexity constraints and data contamination issues. This raises the concern regarding the validity of the evaluation. To address these two challenges, we introduce a dynamic multimodal evaluation protocol called Vision-Language Bootstrapping (VLB). VLB provides a robust and comprehensive assessment for LVLMs with reduced data contamination and flexible complexity. To this end, VLB dynamically generates new visual question-answering samples through a multimodal bootstrapping module that modifies both images and language, while ensuring that newly generated samples remain consistent with the original ones by a judge module. By composing various bootstrapping strategies, VLB offers dynamic variants of existing benchmarks with diverse complexities, enabling the evaluation to co-evolve with the ever-evolving capabilities of LVLMs. Extensive experimental results across multiple benchmarks, including SEEDBench, MMBench, and MME, show that VLB significantly reduces data contamination and exposes performance limitations of LVLMs.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08860",
        "abstract url": "https://arxiv.org/abs/2410.08860",
        "title": "Audio Description Generation in the Era of LLMs and VLMs: A Review of Transferable Generative AI Technologies",
        "rating": "2",
        "keywords": [
            [
                "vision-language",
                "VLMs"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Audio descriptions (ADs) function as acoustic commentaries designed to assist blind persons and persons with visual impairments in accessing digital media content on television and in movies, among other settings. As an accessibility service typically provided by trained AD professionals, the generation of ADs demands significant human effort, making the process both time-consuming and costly. Recent advancements in natural language processing (NLP) and computer vision (CV), particularly in large language models (LLMs) and vision-language models (VLMs), have allowed for getting a step closer to automatic AD generation. This paper reviews the technologies pertinent to AD generation in the era of LLMs and VLMs: we discuss how state-of-the-art NLP and CV technologies can be applied to generate ADs and identify essential research directions for the future.",
        "subjects": [
            "cs.CL",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08876",
        "abstract url": "https://arxiv.org/abs/2410.08876",
        "title": "RoRA-VLM: Robust Retrieval-Augmented Vision Language Models",
        "rating": "2",
        "keywords": [
            [
                "Vision Language",
                "VLM"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Current vision-language models (VLMs) still exhibit inferior performance on knowledge-intensive tasks, primarily due to the challenge of accurately encoding all the associations between visual objects and scenes to their corresponding entities and background knowledge. While retrieval augmentation methods offer an efficient way to integrate external knowledge, extending them to vision-language domain presents unique challenges in (1) precisely retrieving relevant information from external sources due to the inherent discrepancy within the multimodal queries, and (2) being resilient to the irrelevant, extraneous and noisy information contained in the retrieved multimodal knowledge snippets. In this work, we introduce RORA-VLM, a novel and robust retrieval augmentation framework specifically tailored for VLMs, with two key innovations: (1) a 2-stage retrieval process with image-anchored textual-query expansion to synergistically combine the visual and textual information in the query and retrieve the most relevant multimodal knowledge snippets; and (2) a robust retrieval augmentation method that strengthens the resilience of VLMs against irrelevant information in the retrieved multimodal knowledge by injecting adversarial noises into the retrieval-augmented training process, and filters out extraneous visual information, such as unrelated entities presented in images, via a query-oriented visual token refinement strategy. We conduct extensive experiments to validate the effectiveness and robustness of our proposed methods on three widely adopted benchmark datasets. Our results demonstrate that with a minimal amount of training instance, RORA-VLM enables the base model to achieve significant performance improvement and constantly outperform state-of-the-art retrieval-augmented VLMs on all benchmarks while also exhibiting a novel zero-shot domain transfer capability.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08895",
        "abstract url": "https://arxiv.org/abs/2410.08895",
        "title": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Cache-based approaches stand out as both effective and efficient for adapting vision-language models (VLMs). Nonetheless, the existing cache model overlooks three crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text similarity, neglecting the importance of image-image similarity, leading to a gap between pre-training and adaptation. 2) The current cache model is based on the Nadaraya-Watson (N-W) estimator, which disregards the intricate relationships among training samples while constructing weight function. 3) Under the condition of limited samples, the logits generated by cache model are of high uncertainty, directly using these logits without accounting for the confidence could be problematic. This work presents three calibration modules aimed at addressing the above challenges. Similarity Calibration refines the image-image similarity by using unlabeled images. We add a learnable projection layer with residual connection on top of the pre-trained image encoder of CLIP and optimize the parameters by minimizing self-supervised contrastive loss. Weight Calibration introduces a precision matrix into the weight function to adequately model the relation between training samples, transforming the existing cache model to a Gaussian Process (GP) regressor, which could be more accurate than N-W estimator. Confidence Calibration leverages the predictive variances computed by GP Regression to dynamically re-scale the logits of cache model, ensuring that the cache model's outputs are appropriately adjusted based on their confidence levels. Besides, to reduce the high complexity of GPs, we further propose a group-based learning strategy. Integrating the above designs, we propose both training-free and training-required variants. Extensive experiments on 11 few-shot classification datasets validate that the proposed methods can achieve state-of-the-art performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "submitted to IJCV"
    },
    {
        "paper id": "2410.09013",
        "abstract url": "https://arxiv.org/abs/2410.09013",
        "title": "The Impact of Visual Information in Chinese Characters: Evaluating Large Models' Ability to Recognize and Utilize Radicals",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The glyphic writing system of Chinese incorporates information-rich visual features in each character, such as radicals that provide hints about meaning or pronunciation. However, there has been no investigation into whether contemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can harness these sub-character features in Chinese through prompting. In this study, we establish a benchmark to evaluate LLMs' and VLMs' understanding of visual elements in Chinese characters, including radicals, composition structures, strokes, and stroke counts. Our results reveal that models surprisingly exhibit some, but still limited, knowledge of the visual information, regardless of whether images of characters are provided. To incite models' ability to use radicals, we further experiment with incorporating radicals into the prompts for Chinese language understanding tasks. We observe consistent improvement in Part-Of-Speech tagging when providing additional information about radicals, suggesting the potential to enhance CLP by integrating sub-character information.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09016",
        "abstract url": "https://arxiv.org/abs/2410.09016",
        "title": "Parameter-Efficient Fine-Tuning of State Space Models",
        "rating": "2",
        "keywords": [
            [
                "Parameter-Efficient",
                "PEFT",
                "Efficient Fine-Tuning"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Deep State Space Models (SSMs), such as Mamba (Gu & Dao, 2024), have emerged as powerful tools for language modeling, offering high performance with efficient inference and linear scaling in sequence length. However, the application of parameter-efficient fine-tuning (PEFT) methods to SSM-based models remains largely unexplored. This paper aims to systematically study two key questions: (i) How do existing PEFT methods perform on SSM-based models? (ii) Which modules are most effective for fine-tuning? We conduct an empirical benchmark of four basic PEFT methods on SSM-based models. Our findings reveal that prompt-based methods (e.g., prefix-tuning) are no longer effective, an empirical result further supported by theoretical analysis. In contrast, LoRA remains effective for SSM-based models. We further investigate the optimal application of LoRA within these models, demonstrating both theoretically and experimentally that applying LoRA to linear projection matrices without modifying SSM modules yields the best results, as LoRA is not effective at tuning SSM modules. To further improve performance, we introduce LoRA with Selective Dimension tuning (SDLoRA), which selectively updates certain channels and states on SSM modules while applying LoRA to linear projection matrices. Extensive experimental results show that this approach outperforms standard LoRA.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": "Code is available at https://github.com/furiosa-ai/ssm-peft"
    },
    {
        "paper id": "2410.09047",
        "abstract url": "https://arxiv.org/abs/2410.09047",
        "title": "Unraveling and Mitigating Safety Alignment Degradation of Vision-Language Models",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The safety alignment ability of Vision-Language Models (VLMs) is prone to be degraded by the integration of the vision module compared to its LLM backbone. We investigate this phenomenon, dubbed as ''safety alignment degradation'' in this paper, and show that the challenge arises from the representation gap that emerges when introducing vision modality to VLMs. In particular, we show that the representations of multi-modal inputs shift away from that of text-only inputs which represent the distribution that the LLM backbone is optimized for. At the same time, the safety alignment capabilities, initially developed within the textual embedding space, do not successfully transfer to this new multi-modal representation space. To reduce safety alignment degradation, we introduce Cross-Modality Representation Manipulation (CMRM), an inference time representation intervention method for recovering the safety alignment ability that is inherent in the LLM backbone of VLMs, while simultaneously preserving the functional capabilities of VLMs. The empirical results show that our framework significantly recovers the alignment ability that is inherited from the LLM backbone with minimal impact on the fluency and linguistic capabilities of pre-trained VLMs even without additional training. Specifically, the unsafe rate of LLaVA-7B on multi-modal input can be reduced from 61.53% to as low as 3.15% with only inference-time intervention. WARNING: This paper contains examples of toxic or harmful language.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Preprint"
    },
    {
        "paper id": "2410.09344",
        "abstract url": "https://arxiv.org/abs/2410.09344",
        "title": "DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned Models",
        "rating": "2",
        "keywords": [
            [
                "parameter-efficient",
                "efficient fine-tuning"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Storing open-source fine-tuned models separately introduces redundancy and increases response times in applications utilizing multiple models. Delta-parameter pruning (DPP), particularly the random drop and rescale (DARE) method proposed by Yu et al., addresses this by pruning the majority of delta parameters--the differences between fine-tuned and pre-trained model weights--while typically maintaining minimal performance loss. However, DARE fails when either the pruning rate or the magnitude of the delta parameters is large. We highlight two key reasons for this failure: (1) an excessively large rescaling factor as pruning rates increase, and (2) high mean and variance in the delta parameters. To push DARE's limits, we introduce DAREx (DARE the eXtreme), which features two algorithmic improvements: (1) DAREx-q, a rescaling factor modification that significantly boosts performance at high pruning rates (e.g., >30 % on COLA and SST2 for encoder models, with even greater gains in decoder models), and (2) DAREx-L2, which combines DARE with AdamR, an in-training method that applies appropriate delta regularization before DPP. We also demonstrate that DAREx-q can be seamlessly combined with vanilla parameter-efficient fine-tuning techniques like LoRA and can facilitate structural DPP. Additionally, we revisit the application of importance-based pruning techniques within DPP, demonstrating that they outperform random-based methods when delta parameters are large. Through this comprehensive study, we develop a pipeline for selecting the most appropriate DPP method under various practical scenarios.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09365",
        "abstract url": "https://arxiv.org/abs/2410.09365",
        "title": "Debiasing Vison-Language Models with Text-Only Training",
        "rating": "2",
        "keywords": [
            [
                "vision-language",
                "VLMs"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Pre-trained vision-language models (VLMs), such as CLIP, have exhibited remarkable performance across various downstream tasks by aligning text and images in a unified embedding space. However, due to the imbalanced distribution of pre-trained datasets, CLIP suffers from the bias problem in real-world applications. Existing debiasing methods struggle to obtain sufficient image samples for minority groups and incur high costs for group labeling. To address the limitations, we propose a Text-Only Debiasing framework called TOD, leveraging a text-as-image training paradigm to mitigate visual biases. Specifically, this approach repurposes the text encoder to function as an image encoder, thereby eliminating the need for image data. Simultaneously, it utilizes a large language model (LLM) to generate a balanced text dataset, which is then used for prompt tuning. However, we observed that the model overfits to the text modality because label names, serving as supervision signals, appear explicitly in the texts. To address this issue, we further introduce a Multi-Target Prediction (MTP) task that motivates the model to focus on complex contexts and distinguish between target and biased information. Extensive experiments on the Waterbirds and CelebA datasets show that our method significantly improves group robustness, achieving state-of-the-art results among image-free methods and even competitive performance compared to image-supervised methods. Furthermore, the proposed method can be adapted to challenging scenarios with multiple or unknown bias attributes, demonstrating its strong generalization and robustness.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08592",
        "abstract url": "https://arxiv.org/abs/2410.08592",
        "title": "VIBES -- Vision Backbone Efficient Selection",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ],
            [
                "WACV"
            ]
        ],
        "abstract": "This work tackles the challenge of efficiently selecting high-performance pre-trained vision backbones for specific target tasks. Although exhaustive search within a finite set of backbones can solve this problem, it becomes impractical for large datasets and backbone pools. To address this, we introduce Vision Backbone Efficient Selection (VIBES), which aims to quickly find well-suited backbones, potentially trading off optimality for efficiency. We propose several simple yet effective heuristics to address VIBES and evaluate them across four diverse computer vision datasets. Our results show that these approaches can identify backbones that outperform those selected from generic benchmarks, even within a limited search budget of one hour on a single GPU. We reckon VIBES marks a paradigm shift from benchmarks to task-specific optimization.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "9 pages, 4 figures, under review at WACV 2025"
    },
    {
        "paper id": "2410.08593",
        "abstract url": "https://arxiv.org/abs/2410.08593",
        "title": "VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Existing Video Corpus Moment Retrieval (VCMR) is limited to coarse-grained understanding, which hinders precise video moment localization when given fine-grained queries. In this paper, we propose a more challenging fine-grained VCMR benchmark requiring methods to localize the best-matched moment from the corpus with other partially matched candidates. To improve the dataset construction efficiency and guarantee high-quality data annotations, we propose VERIFIED, an automatic \\underline{V}id\\underline{E}o-text annotation pipeline to generate captions with \\underline{R}el\\underline{I}able \\underline{FI}n\\underline{E}-grained statics and \\underline{D}ynamics. Specifically, we resort to large language models (LLM) and large multimodal models (LMM) with our proposed Statics and Dynamics Enhanced Captioning modules to generate diverse fine-grained captions for each video. To filter out the inaccurate annotations caused by the LLM hallucination, we propose a Fine-Granularity Aware Noise Evaluator where we fine-tune a video foundation model with disturbed hard-negatives augmented contrastive and matching losses. With VERIFIED, we construct a more challenging fine-grained VCMR benchmark containing Charades-FIG, DiDeMo-FIG, and ActivityNet-FIG which demonstrate a high level of annotation quality. We evaluate several state-of-the-art VCMR models on the proposed dataset, revealing that there is still significant scope for fine-grained video understanding in VCMR. Code and Datasets are in \\href{https://github.com/hlchen23/VERIFIED}{https://github.com/hlchen23/VERIFIED}.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Accepted by 38th NeurIPS Datasets & Benchmarks Track (NeurIPS 2024)"
    },
    {
        "paper id": "2410.08623",
        "abstract url": "https://arxiv.org/abs/2410.08623",
        "title": "Retrieving Contextual Information for Long-Form Question Answering using Weak Supervision",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Long-form question answering (LFQA) aims at generating in-depth answers to end-user questions, providing relevant information beyond the direct answer. However, existing retrievers are typically optimized towards information that directly targets the question, missing out on such contextual information. Furthermore, there is a lack of training data for relevant context. To this end, we propose and compare different weak supervision techniques to optimize retrieval for contextual information. Experiments demonstrate improvements on the end-to-end QA performance on ASQA, a dataset for long-form question answering. Importantly, as more contextual information is retrieved, we improve the relevant page recall for LFQA by 14.7% and the groundedness of generated long-form answers by 12.5%. Finally, we show that long-form answers often anticipate likely follow-up questions, via experiments on a conversational QA dataset.",
        "subjects": [
            "cs.CL",
            "cs.IR"
        ],
        "comment": "Accepted at EMNLP 2024 (Findings)"
    },
    {
        "paper id": "2410.08731",
        "abstract url": "https://arxiv.org/abs/2410.08731",
        "title": "Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding in Large Language Models",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance in the legal domain, with GPT-4 even passing the Uniform Bar Exam in the U.S. However their efficacy remains limited for non-standardized tasks and tasks in languages other than English. This underscores the need for careful evaluation of LLMs within each legal system before application. Here, we introduce KBL, a benchmark for assessing the Korean legal language understanding of LLMs, consisting of (1) 7 legal knowledge tasks (510 examples), (2) 4 legal reasoning tasks (288 examples), and (3) the Korean bar exam (4 domains, 53 tasks, 2,510 examples). First two datasets were developed in close collaboration with lawyers to evaluate LLMs in practical scenarios in a certified manner. Furthermore, considering legal practitioners' frequent use of extensive legal documents for research, we assess LLMs in both a closed book setting, where they rely solely on internal knowledge, and a retrieval-augmented generation (RAG) setting, using a corpus of Korean statutes and precedents. The results indicate substantial room and opportunities for improvement.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "EMNLP 2024 Findings"
    },
    {
        "paper id": "2410.08764",
        "abstract url": "https://arxiv.org/abs/2410.08764",
        "title": "Measuring the Groundedness of Legal Question-Answering Systems",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "In high-stakes domains like legal question-answering, the accuracy and trustworthiness of generative AI systems are of paramount importance. This work presents a comprehensive benchmark of various methods to assess the groundedness of AI-generated responses, aiming to significantly enhance their reliability. Our experiments include similarity-based metrics and natural language inference models to evaluate whether responses are well-founded in the given contexts. We also explore different prompting strategies for large language models to improve the detection of ungrounded responses. We validated the effectiveness of these methods using a newly created grounding classification corpus, designed specifically for legal queries and corresponding responses from retrieval-augmented prompting, focusing on their alignment with source material. Our results indicate potential in groundedness classification of generated responses, with the best method achieving a macro-F1 score of 0.8. Additionally, we evaluated the methods in terms of their latency to determine their suitability for real-world applications, as this step typically follows the generation process. This capability is essential for processes that may trigger additional manual verification or automated response regeneration. In summary, this study demonstrates the potential of various detection methods to improve the trustworthiness of generative AI in legal settings.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "to appear NLLP @ EMNLP 2024"
    },
    {
        "paper id": "2410.08893",
        "abstract url": "https://arxiv.org/abs/2410.08893",
        "title": "Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter Efficient",
        "rating": "1.5",
        "keywords": [
            [
                "Parameter Efficient"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Model-based reinforcement learning (RL) offers a solution to the data inefficiency that plagues most model-free RL algorithms. However, learning a robust world model often demands complex and deep architectures, which are expensive to compute and train. Within the world model, dynamics models are particularly crucial for accurate predictions, and various dynamics-model architectures have been explored, each with its own set of challenges. Currently, recurrent neural network (RNN) based world models face issues such as vanishing gradients and difficulty in capturing long-term dependencies effectively. In contrast, use of transformers suffers from the well-known issues of self-attention mechanisms, where both memory and computational complexity scale as $O(n^2)$, with $n$ representing the sequence length. To address these challenges we propose a state space model (SSM) based world model, specifically based on Mamba, that achieves $O(n)$ memory and computational complexity while effectively capturing long-term dependencies and facilitating the use of longer training sequences efficiently. We also introduce a novel sampling method to mitigate the suboptimality caused by an incorrect world model in the early stages of training, combining it with the aforementioned technique to achieve a normalised score comparable to other state-of-the-art model-based RL algorithms using only a 7 million trainable parameter world model. This model is accessible and can be trained on an off-the-shelf laptop. Our code is available at https://github.com/realwenlongwang/drama.git.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08905",
        "abstract url": "https://arxiv.org/abs/2410.08905",
        "title": "Lifelong Event Detection via Optimal Transport",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Continual Event Detection (CED) poses a formidable challenge due to the catastrophic forgetting phenomenon, where learning new tasks (with new coming event types) hampers performance on previous ones. In this paper, we introduce a novel approach, Lifelong Event Detection via Optimal Transport (LEDOT), that leverages optimal transport principles to align the optimization of our classification module with the intrinsic nature of each class, as defined by their pre-trained language modeling. Our method integrates replay sets, prototype latent representations, and an innovative Optimal Transport component. Extensive experiments on MAVEN and ACE datasets demonstrate LEDOT's superior performance, consistently outperforming state-of-the-art baselines. The results underscore LEDOT as a pioneering solution in continual event detection, offering a more effective and nuanced approach to addressing catastrophic forgetting in evolving environments.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to EMNLP 2024"
    },
    {
        "paper id": "2410.08956",
        "abstract url": "https://arxiv.org/abs/2410.08956",
        "title": "Rapid Grassmannian Averaging with Chebyshev Polynomials",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "We propose new algorithms to efficiently average a collection of points on a Grassmannian manifold in both the centralized and decentralized settings. Grassmannian points are used ubiquitously in machine learning, computer vision, and signal processing to represent data through (often low-dimensional) subspaces. While averaging these points is crucial to many tasks (especially in the decentralized setting), existing methods unfortunately remain computationally expensive due to the non-Euclidean geometry of the manifold. Our proposed algorithms, Rapid Grassmannian Averaging (RGrAv) and Decentralized Rapid Grassmannian Averaging (DRGrAv), overcome this challenge by leveraging the spectral structure of the problem to rapidly compute an average using only small matrix multiplications and QR factorizations. We provide a theoretical guarantee of optimality and present numerical experiments which demonstrate that our algorithms outperform state-of-the-art methods in providing high accuracy solutions in minimal time. Additional experiments showcase the versatility of our algorithms to tasks such as K-means clustering on video motion data, establishing RGrAv and DRGrAv as powerful tools for generic Grassmannian averaging.",
        "subjects": [
            "math.NA",
            "cs.CV",
            "cs.LG",
            "math.OC"
        ],
        "comment": "Submitted to ICLR 2025"
    },
    {
        "paper id": "2410.08989",
        "abstract url": "https://arxiv.org/abs/2410.08989",
        "title": "SubZero: Random Subspace Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning",
        "rating": "1.5",
        "keywords": [
            [
                "Memory-Efficient"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Fine-tuning Large Language Models (LLMs) has proven effective for a variety of downstream tasks. However, as LLMs grow in size, the memory demands for backpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization methods offer a memory-efficient alternative by using forward passes to estimate gradients, but the variance of gradient estimates typically scales linearly with the model's parameter dimension$\\unicode{x2013}$a significant issue for LLMs. In this paper, we propose the random Subspace Zeroth-order (SubZero) optimization to address the challenges posed by LLMs' high dimensionality. We introduce a low-rank perturbation tailored for LLMs that significantly reduces memory consumption while improving training performance. Additionally, we prove that our gradient estimation closely approximates the backpropagation gradient, exhibits lower variance than traditional ZO methods, and ensures convergence when combined with SGD. Experimental results show that SubZero enhances fine-tuning performance and achieves faster convergence compared to standard ZO approaches like MeZO across various language modeling tasks.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09037",
        "abstract url": "https://arxiv.org/abs/2410.09037",
        "title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have displayed remarkable performances across various complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently, studies have proposed a Knowledge Distillation (KD) approach, reasoning distillation, which transfers such reasoning ability of LLMs through fine-tuning language models of multi-step rationales generated by LLM teachers. However, they have inadequately considered two challenges regarding insufficient distillation sets from the LLM teacher model, in terms of 1) data quality and 2) soft label provision. In this paper, we propose Mentor-KD, which effectively distills the multi-step reasoning capability of LLMs to smaller LMs while addressing the aforementioned challenges. Specifically, we exploit a mentor, intermediate-sized task-specific fine-tuned model, to augment additional CoT annotations and provide soft labels for the student model during reasoning distillation. We conduct extensive experiments and confirm Mentor-KD's effectiveness across various models and complex reasoning tasks.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "EMNLP 2024"
    },
    {
        "paper id": "2410.09045",
        "abstract url": "https://arxiv.org/abs/2410.09045",
        "title": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "The proliferation of inflammatory or misleading \"fake\" news content has become increasingly common in recent years. Simultaneously, it has become easier than ever to use AI tools to generate photorealistic images depicting any scene imaginable. Combining these two -- AI-generated fake news content -- is particularly potent and dangerous. To combat the spread of AI-generated fake news, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real and AI-generated image-caption pairs from state-of-the-art generators. We find that our dataset poses a significant challenge to humans (60% F-1) and state-of-the-art multi-modal LLMs (< 24% F-1). Using our dataset we train a multi-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art baselines on image-caption pairs from out-of-domain image generators and news publishers. We release our code and data to aid future work on detecting AI-generated content.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": "EMNLP 2024 Findings"
    },
    {
        "paper id": "2410.09220",
        "abstract url": "https://arxiv.org/abs/2410.09220",
        "title": "M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CY",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "In recent years, there has been a significant rise in the phenomenon of hate against women on social media platforms, particularly through the use of misogynous memes. These memes often target women with subtle and obscure cues, making their detection a challenging task for automated systems. Recently, Large Language Models (LLMs) have shown promising results in reasoning using Chain-of-Thought (CoT) prompting to generate the intermediate reasoning chains as the rationale to facilitate multimodal tasks, but often neglect cultural diversity and key aspects like emotion and contextual knowledge hidden in the visual modalities. To address this gap, we introduce a Multimodal Multi-hop CoT (M3Hop-CoT) framework for Misogynous meme identification, combining a CLIP-based classifier and a multimodal CoT module with entity-object-relationship integration. M3Hop-CoT employs a three-step multimodal prompting principle to induce emotions, target awareness, and contextual knowledge for meme analysis. Our empirical evaluation, including both qualitative and quantitative analysis, validates the efficacy of the M3Hop-CoT framework on the SemEval-2022 Task 5 (MAMI task) dataset, highlighting its strong performance in the macro-F1 score. Furthermore, we evaluate the model's generalizability by evaluating it on various benchmark meme datasets, offering a thorough insight into the effectiveness of our approach across different datasets.",
        "subjects": [
            "cs.CL",
            "cs.CY",
            "cs.LG"
        ],
        "comment": "34 Pages. Accepted in The 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024). Main Conference"
    },
    {
        "paper id": "2410.09329",
        "abstract url": "https://arxiv.org/abs/2410.09329",
        "title": "Zero-shot Commonsense Reasoning over Machine Imagination",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Recent approaches to zero-shot commonsense reasoning have enabled Pre-trained Language Models (PLMs) to learn a broad range of commonsense knowledge without being tailored to specific situations. However, they often suffer from human reporting bias inherent in textual commonsense knowledge, leading to discrepancies in understanding between PLMs and humans. In this work, we aim to bridge this gap by introducing an additional information channel to PLMs. We propose Imagine (Machine Imagination-based Reasoning), a novel zero-shot commonsense reasoning framework designed to complement textual inputs with visual signals derived from machine-generated images. To achieve this, we enhance PLMs with imagination capabilities by incorporating an image generator into the reasoning process. To guide PLMs in effectively leveraging machine imagination, we create a synthetic pre-training dataset that simulates visual question-answering. Our extensive experiments on diverse reasoning benchmarks and analysis show that Imagine outperforms existing methods by a large margin, highlighting the strength of machine imagination in mitigating reporting bias and enhancing generalization capabilities.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": "21 pages, 9 figures, EMNLP 2024 (Findings)"
    },
    {
        "paper id": "2410.09349",
        "abstract url": "https://arxiv.org/abs/2410.09349",
        "title": "Inference and Verbalization Functions During In-Context Learning",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Large language models (LMs) are capable of in-context learning from a few demonstrations (example-label pairs) to solve new tasks during inference. Despite the intuitive importance of high-quality demonstrations, previous work has observed that, in some settings, ICL performance is minimally affected by irrelevant labels (Min et al., 2022). We hypothesize that LMs perform ICL with irrelevant labels via two sequential processes: an inference function that solves the task, followed by a verbalization function that maps the inferred answer to the label space. Importantly, we hypothesize that the inference function is invariant to remappings of the label space (e.g., \"true\"/\"false\" to \"cat\"/\"dog\"), enabling LMs to share the same inference function across settings with different label words. We empirically validate this hypothesis with controlled layer-wise interchange intervention experiments. Our findings confirm the hypotheses on multiple datasets and tasks (natural language inference, sentiment analysis, and topic classification) and further suggest that the two functions can be localized in specific layers across various open-sourced models, including GEMMA-7B, MISTRAL-7B-V0.3, GEMMA-2-27B, and LLAMA-3.1-70B.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "EMNLP 2024 Findings"
    },
    {
        "paper id": "2410.08526",
        "abstract url": "https://arxiv.org/abs/2410.08526",
        "title": "\"I Am the One and Only, Your Cyber BFF\": Understanding the Impact of GenAI Requires Understanding the Impact of Anthropomorphic AI",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "Many state-of-the-art generative AI (GenAI) systems are increasingly prone to anthropomorphic behaviors, i.e., to generating outputs that are perceived to be human-like. While this has led to scholars increasingly raising concerns about possible negative impacts such anthropomorphic AI systems can give rise to, anthropomorphism in AI development, deployment, and use remains vastly overlooked, understudied, and underspecified. In this perspective, we argue that we cannot thoroughly map the social impacts of generative AI without mapping the social impacts of anthropomorphic AI, and outline a call to action.",
        "subjects": [
            "cs.CY",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08529",
        "abstract url": "https://arxiv.org/abs/2410.08529",
        "title": "VOVTrack: Exploring the Potentiality in Videos for Open-Vocabulary Object Tracking",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Open-vocabulary multi-object tracking (OVMOT) represents a critical new challenge involving the detection and tracking of diverse object categories in videos, encompassing both seen categories (base classes) and unseen categories (novel classes). This issue amalgamates the complexities of open-vocabulary object detection (OVD) and multi-object tracking (MOT). Existing approaches to OVMOT often merge OVD and MOT methodologies as separate modules, predominantly focusing on the problem through an image-centric lens. In this paper, we propose VOVTrack, a novel method that integrates object states relevant to MOT and video-centric training to address this challenge from a video object tracking standpoint. First, we consider the tracking-related state of the objects during tracking and propose a new prompt-guided attention mechanism for more accurate localization and classification (detection) of the time-varying objects. Subsequently, we leverage raw video data without annotations for training by formulating a self-supervised object similarity learning technique to facilitate temporal object association (tracking). Experimental results underscore that VOVTrack outperforms existing methods, establishing itself as a state-of-the-art solution for open-vocabulary tracking task.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08540",
        "abstract url": "https://arxiv.org/abs/2410.08540",
        "title": "Kaleidoscope: Learnable Masks for Heterogeneous Multi-agent Reinforcement Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In multi-agent reinforcement learning (MARL), parameter sharing is commonly employed to enhance sample efficiency. However, the popular approach of full parameter sharing often leads to homogeneous policies among agents, potentially limiting the performance benefits that could be derived from policy diversity. To address this critical limitation, we introduce \\emph{Kaleidoscope}, a novel adaptive partial parameter sharing scheme that fosters policy heterogeneity while still maintaining high sample efficiency. Specifically, Kaleidoscope maintains one set of common parameters alongside multiple sets of distinct, learnable masks for different agents, dictating the sharing of parameters. It promotes diversity among policy networks by encouraging discrepancy among these masks, without sacrificing the efficiencies of parameter sharing. This design allows Kaleidoscope to dynamically balance high sample efficiency with a broad policy representational capacity, effectively bridging the gap between full parameter sharing and non-parameter sharing across various environments. We further extend Kaleidoscope to critic ensembles in the context of actor-critic algorithms, which could help improve value estimations.Our empirical evaluations across extensive environments, including multi-agent particle environment, multi-agent MuJoCo and StarCraft multi-agent challenge v2, demonstrate the superior performance of Kaleidoscope compared with existing parameter sharing approaches, showcasing its potential for performance enhancement in MARL. The code is publicly available at \\url{https://github.com/LXXXXR/Kaleidoscope}.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.MA"
        ],
        "comment": "Accepted by the Thirty-Eighth Annual Conference on Neural Information Processing Systems(NeurIPS 2024)"
    },
    {
        "paper id": "2410.08553",
        "abstract url": "https://arxiv.org/abs/2410.08553",
        "title": "Balancing Innovation and Privacy: Data Security Strategies in Natural Language Processing Applications",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "This research addresses privacy protection in Natural Language Processing (NLP) by introducing a novel algorithm based on differential privacy, aimed at safeguarding user data in common applications such as chatbots, sentiment analysis, and machine translation. With the widespread application of NLP technology, the security and privacy protection of user data have become important issues that need to be solved urgently. This paper proposes a new privacy protection algorithm designed to effectively prevent the leakage of user sensitive information. By introducing a differential privacy mechanism, our model ensures the accuracy and reliability of data analysis results while adding random noise. This method not only reduces the risk caused by data leakage but also achieves effective processing of data while protecting user privacy. Compared to traditional privacy methods like data anonymization and homomorphic encryption, our approach offers significant advantages in terms of computational efficiency and scalability while maintaining high accuracy in data analysis. The proposed algorithm's efficacy is demonstrated through performance metrics such as accuracy (0.89), precision (0.85), and recall (0.88), outperforming other methods in balancing privacy and utility. As privacy protection regulations become increasingly stringent, enterprises and developers must take effective measures to deal with privacy risks. Our research provides an important reference for the application of privacy protection technology in the field of NLP, emphasizing the need to achieve a balance between technological innovation and user privacy. In the future, with the continuous advancement of technology, privacy protection will become a core element of data-driven applications and promote the healthy development of the entire industry.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08564",
        "abstract url": "https://arxiv.org/abs/2410.08564",
        "title": "Similar Phrases for Cause of Actions of Civil Cases",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "In the Taiwanese judicial system, Cause of Actions (COAs) are essential for identifying relevant legal judgments. However, the lack of standardized COA labeling creates challenges in filtering cases using basic methods. This research addresses this issue by leveraging embedding and clustering techniques to analyze the similarity between COAs based on cited legal articles. The study implements various similarity measures, including Dice coefficient and Pearson's correlation coefficient. An ensemble model combines rankings, and social network analysis identifies clusters of related COAs. This approach enhances legal analysis by revealing inconspicuous connections between COAs, offering potential applications in legal research beyond civil law.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "10 pages, 4 figures, 3 tables(including appendix)"
    },
    {
        "paper id": "2410.08565",
        "abstract url": "https://arxiv.org/abs/2410.08565",
        "title": "Baichuan-Omni Technical Report",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "The salient multimodal capabilities and interactive experience of GPT-4o highlight its critical role in practical applications, yet it lacks a high-performing open-source counterpart. In this paper, we introduce Baichuan-Omni, the first open-source 7B Multimodal Large Language Model (MLLM) adept at concurrently processing and analyzing modalities of image, video, audio, and text, while delivering an advanced multimodal interactive experience and strong performance. We propose an effective multimodal training schema starting with 7B model and proceeding through two stages of multimodal alignment and multitask fine-tuning across audio, image, video, and text modal. This approach equips the language model with the ability to handle visual and audio data effectively. Demonstrating strong performance across various omni-modal and multimodal benchmarks, we aim for this contribution to serve as a competitive baseline for the open-source community in advancing multimodal understanding and real-time interaction.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08582",
        "abstract url": "https://arxiv.org/abs/2410.08582",
        "title": "DeBiFormer: Vision Transformer with Deformable Agent Bi-level Routing Attention",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Vision Transformers with various attention modules have demonstrated superior performance on vision tasks. While using sparsity-adaptive attention, such as in DAT, has yielded strong results in image classification, the key-value pairs selected by deformable points lack semantic relevance when fine-tuning for semantic segmentation tasks. The query-aware sparsity attention in BiFormer seeks to focus each query on top-k routed regions. However, during attention calculation, the selected key-value pairs are influenced by too many irrelevant queries, reducing attention on the more important ones. To address these issues, we propose the Deformable Bi-level Routing Attention (DBRA) module, which optimizes the selection of key-value pairs using agent queries and enhances the interpretability of queries in attention maps. Based on this, we introduce the Deformable Bi-level Routing Attention Transformer (DeBiFormer), a novel general-purpose vision transformer built with the DBRA module. DeBiFormer has been validated on various computer vision tasks, including image classification, object detection, and semantic segmentation, providing strong evidence of its effectiveness.Code is available at {https://github.com/maclong01/DeBiFormer}",
        "subjects": [
            "cs.CV"
        ],
        "comment": "20 pages, 7 figures. arXiv admin note: text overlap with arXiv:2303.08810 by other authors"
    },
    {
        "paper id": "2410.08632",
        "abstract url": "https://arxiv.org/abs/2410.08632",
        "title": "Words as Beacons: Guiding RL Agents with High-Level Language Prompts",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Sparse reward environments in reinforcement learning (RL) pose significant challenges for exploration, often leading to inefficient or incomplete learning processes. To tackle this issue, this work proposes a teacher-student RL framework that leverages Large Language Models (LLMs) as \"teachers\" to guide the agent's learning process by decomposing complex tasks into subgoals. Due to their inherent capability to understand RL environments based on a textual description of structure and purpose, LLMs can provide subgoals to accomplish the task defined for the environment in a similar fashion to how a human would do. In doing so, three types of subgoals are proposed: positional targets relative to the agent, object representations, and language-based instructions generated directly by the LLM. More importantly, we show that it is possible to query the LLM only during the training phase, enabling agents to operate within the environment without any LLM intervention. We assess the performance of this proposed framework by evaluating three state-of-the-art open-source LLMs (Llama, DeepSeek, Qwen) eliciting subgoals across various procedurally generated environment of the MiniGrid benchmark. Experimental results demonstrate that this curriculum-based approach accelerates learning and enhances exploration in complex tasks, achieving up to 30 to 200 times faster convergence in training steps compared to recent baselines designed for sparse reward environments.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08633",
        "abstract url": "https://arxiv.org/abs/2410.08633",
        "title": "Transformers Provably Solve Parity Efficiently with Chain of Thought",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "This work provides the first theoretical analysis of training transformers to solve complex problems by recursively generating intermediate states, analogous to fine-tuning for chain-of-thought (CoT) reasoning. We consider training a one-layer transformer to solve the fundamental $k$-parity problem, extending the work on RNNs by Wies et al. (2023). We establish three key results: (1) any finite-precision gradient-based algorithm, without intermediate supervision, requires substantial iterations to solve parity with finite samples. (2) In contrast, when intermediate parities are incorporated into the loss function, our model can learn parity in one gradient update when aided by \\emph{teacher forcing}, where ground-truth labels of the reasoning chain are provided at each generation step. (3) Even without teacher forcing, where the model must generate CoT chains end-to-end, parity can be learned efficiently if augmented data is employed to internally verify the soundness of intermediate steps. These results rigorously show that task decomposition and stepwise reasoning naturally arise from optimizing transformers with CoT; moreover, self-consistency checking can improve reasoning ability, aligning with empirical studies of CoT.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "NeurIPS 2024 M3L Workshop"
    },
    {
        "paper id": "2410.08642",
        "abstract url": "https://arxiv.org/abs/2410.08642",
        "title": "More than Memes: A Multimodal Topic Modeling Approach to Conspiracy Theories on Telegram",
        "rating": "1",
        "keywords": [
            [
                "cs.SI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Research on conspiracy theories and related content online has traditionally focused on textual data. To address the increasing prevalence of (audio-)visual data on social media, and to capture the evolving and dynamic nature of this communication, researchers have begun to explore the potential of unsupervised approaches for analyzing multimodal online content. Our research contributes to this field by exploring the potential of multimodal topic modeling for analyzing conspiracy theories in German-language Telegram channels. Our work uses the BERTopic topic modeling approach in combination with CLIP for the analysis of textual and visual data. We analyze a corpus of ~40, 000 Telegram messages posted in October 2023 in 571 German-language Telegram channels known for disseminating conspiracy theories and other deceptive content. We explore the potentials and challenges of this approach for studying a medium-sized corpus of user-generated, text-image online content. We offer insights into the dominant topics across modalities, different text and image genres discovered during the analysis, quantitative inter-modal topic analyses, and a qualitative case study of textual, visual, and multimodal narrative strategies in the communication of conspiracy theories.",
        "subjects": [
            "cs.SI",
            "cs.CL",
            "cs.CV",
            "cs.MM"
        ],
        "comment": "11 pages, 11 figures"
    },
    {
        "paper id": "2410.08645",
        "abstract url": "https://arxiv.org/abs/2410.08645",
        "title": "Boosting Open-Vocabulary Object Detection by Handling Background Samples",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Open-vocabulary object detection is the task of accurately detecting objects from a candidate vocabulary list that includes both base and novel categories. Currently, numerous open-vocabulary detectors have achieved success by leveraging the impressive zero-shot capabilities of CLIP. However, we observe that CLIP models struggle to effectively handle background images (i.e. images without corresponding labels) due to their language-image learning methodology. This limitation results in suboptimal performance for open-vocabulary detectors that rely on CLIP when processing background samples. In this paper, we propose Background Information Representation for open-vocabulary Detector (BIRDet), a novel approach to address the limitations of CLIP in handling background samples. Specifically, we design Background Information Modeling (BIM) to replace the single, fixed background embedding in mainstream open-vocabulary detectors with dynamic scene information, and prompt it into image-related background representations. This method effectively enhances the ability to classify oversized regions as background. Besides, we introduce Partial Object Suppression (POS), an algorithm that utilizes the ratio of overlap area to address the issue of misclassifying partial regions as foreground. Experiments on OV-COCO and OV-LVIS benchmarks demonstrate that our proposed model is capable of achieving performance enhancements across various open-vocabulary detectors.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "16 pages, 5 figures, Accepted to ICONIP 2024"
    },
    {
        "paper id": "2410.08673",
        "abstract url": "https://arxiv.org/abs/2410.08673",
        "title": "SpikeBottleNet: Energy Efficient Spike Neural Network Partitioning for Feature Compression in Device-Edge Co-Inference Systems",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The advent of intelligent mobile applications highlights the crucial demand for deploying powerful deep learning models on resource-constrained mobile devices. An effective solution in this context is the device-edge co-inference framework, which partitions a deep neural network between a mobile device and a nearby edge server. This approach requires balancing on-device computations and communication costs, often achieved through compressed intermediate feature transmission. Conventional deep neural network architectures require continuous data processing, leading to substantial energy consumption by edge devices. This motivates exploring binary, event-driven activations enabled by spiking neural networks (SNNs), known for their extremely energy efficiency. In this research, we propose a novel architecture named SpikeBottleNet, a significant improvement to the existing architecture by integrating SNNs. A key aspect of our investigation is the development of an intermediate feature compression technique specifically designed for SNNs. This technique leverages a split computing approach for SNNs to partition complex architectures, such as Spike ResNet50. By incorporating the power of SNNs within device-edge co-inference systems, experimental results demonstrate that our SpikeBottleNet achieves a significant bit compression ratio of up to 256x in the final convolutional layer while maintaining high classification accuracy with only a 2.5% reduction. Moreover, compared to the baseline BottleNet++ architecture, our framework reduces the transmitted feature size at earlier splitting points by 75%. Furthermore, in terms of the energy efficiency of edge devices, our methodology surpasses the baseline by a factor of up to 98, demonstrating significant enhancements in both efficiency and performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The paper consists of 7 pages and 3 figures. It was submitted to ECAI-2024, and the authors are currently working on improving it based on the review"
    },
    {
        "paper id": "2410.08674",
        "abstract url": "https://arxiv.org/abs/2410.08674",
        "title": "Guidelines for Fine-grained Sentence-level Arabic Readability Annotation",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper presents the foundational framework and initial findings of the Balanced Arabic Readability Evaluation Corpus (BAREC) project, designed to address the need for comprehensive Arabic language resources aligned with diverse readability levels. Inspired by the Taha/Arabi21 readability reference, BAREC aims to provide a standardized reference for assessing sentence-level Arabic text readability across 19 distinct levels, ranging in targets from kindergarten to postgraduate comprehension. Our ultimate goal with BAREC is to create a comprehensive and balanced corpus that represents a wide range of genres, topics, and regional variations through a multifaceted approach combining manual annotation with AI-driven tools. This paper focuses on our meticulous annotation guidelines, demonstrated through the analysis of 10,631 sentences/phrases (113,651 words). The average pairwise inter-annotator agreement, measured by Quadratic Weighted Kappa, is 79.9%, reflecting a high level of substantial agreement. We also report competitive results for benchmarking automatic readability assessment. We will make the BAREC corpus and guidelines openly accessible to support Arabic language research and education.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "16 pages, 3 figures"
    },
    {
        "paper id": "2410.08696",
        "abstract url": "https://arxiv.org/abs/2410.08696",
        "title": "AMPO: Automatic Multi-Branched Prompt Optimization",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Prompt engineering is very important to enhance the performance of large language models (LLMs). When dealing with complex issues, prompt engineers tend to distill multiple patterns from examples and inject relevant solutions to optimize the prompts, achieving satisfying results. However, existing automatic prompt optimization techniques are only limited to producing single flow instructions, struggling with handling diverse patterns. In this paper, we present AMPO, an automatic prompt optimization method that can iteratively develop a multi-branched prompt using failure cases as feedback. Our goal is to explore a novel way of structuring prompts with multi-branches to better handle multiple patterns in complex tasks, for which we introduce three modules: Pattern Recognition, Branch Adjustment, and Branch Pruning. In experiments across five tasks, AMPO consistently achieves the best results. Additionally, our approach demonstrates significant optimization efficiency due to our adoption of a minimal search strategy.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "13 pages, 7 figures, 6 tables"
    },
    {
        "paper id": "2410.08698",
        "abstract url": "https://arxiv.org/abs/2410.08698",
        "title": "SocialGaze: Improving the Integration of Human Social Norms in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "While much research has explored enhancing the reasoning capabilities of large language models (LLMs) in the last few years, there is a gap in understanding the alignment of these models with social values and norms. We introduce the task of judging social acceptance. Social acceptance requires models to judge and rationalize the acceptability of people's actions in social situations. For example, is it socially acceptable for a neighbor to ask others in the community to keep their pets indoors at night? We find that LLMs' understanding of social acceptance is often misaligned with human consensus. To alleviate this, we introduce SocialGaze, a multi-step prompting framework, in which a language model verbalizes a social situation from multiple perspectives before forming a judgment. Our experiments demonstrate that the SocialGaze approach improves the alignment with human judgments by up to 11 F1 points with the GPT-3.5 model. We also identify biases and correlations in LLMs in assigning blame that is related to features such as the gender (males are significantly more likely to be judged unfairly) and age (LLMs are more aligned with humans for older narrators).",
        "subjects": [
            "cs.CL",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08703",
        "abstract url": "https://arxiv.org/abs/2410.08703",
        "title": "On the token distance modeling ability of higher RoPE attention dimension",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Length extrapolation algorithms based on Rotary position embedding (RoPE) have shown promising results in extending the context length of language models. However, understanding how position embedding can capture longer-range contextual information remains elusive. Based on the intuition that different dimensions correspond to different frequency of changes in RoPE encoding, we conducted a dimension-level analysis to investigate the correlation between a hidden dimension of an attention head and its contribution to capturing long-distance dependencies. Using our correlation metric, we identified a particular type of attention heads, which we named Positional Heads, from various length-extrapolated models. These heads exhibit a strong focus on long-range information interaction and play a pivotal role in long input processing, as evidence by our ablation. We further demonstrate the correlation between the efficiency of length extrapolation and the extension of the high-dimensional attention allocation of these heads. The identification of Positional Heads provides insights for future research in long-text comprehension.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08728",
        "abstract url": "https://arxiv.org/abs/2410.08728",
        "title": "From N-grams to Pre-trained Multilingual Models For Language Identification",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we investigate the use of N-gram models and Large Pre-trained Multilingual models for Language Identification (LID) across 11 South African languages. For N-gram models, this study shows that effective data size selection remains crucial for establishing effective frequency distributions of the target languages, that efficiently model each language, thus, improving language ranking. For pre-trained multilingual models, we conduct extensive experiments covering a diverse set of massively pre-trained multilingual (PLM) models -- mBERT, RemBERT, XLM-r, and Afri-centric multilingual models -- AfriBERTa, Afro-XLMr, AfroLM, and Serengeti. We further compare these models with available large-scale Language Identification tools: Compact Language Detector v3 (CLD V3), AfroLID, GlotLID, and OpenLID to highlight the importance of focused-based LID. From these, we show that Serengeti is a superior model across models: N-grams to Transformers on average. Moreover, we propose a lightweight BERT-based LID model (za_BERT_lid) trained with NHCLT + Vukzenzele corpus, which performs on par with our best-performing Afri-centric models.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "The paper has been accepted at The 4th International Conference on Natural Language Processing for Digital Humanities (NLP4DH 2024)"
    },
    {
        "paper id": "2410.08769",
        "abstract url": "https://arxiv.org/abs/2410.08769",
        "title": "Efficient Multi-Object Tracking on Edge Devices via Reconstruction-Based Channel Pruning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The advancement of multi-object tracking (MOT) technologies presents the dual challenge of maintaining high performance while addressing critical security and privacy concerns. In applications such as pedestrian tracking, where sensitive personal data is involved, the potential for privacy violations and data misuse becomes a significant issue if data is transmitted to external servers. To mitigate these risks, processing data directly on an edge device, such as a smart camera, has emerged as a viable solution. Edge computing ensures that sensitive information remains local, thereby aligning with stringent privacy principles and significantly reducing network latency. However, the implementation of MOT on edge devices is not without its challenges. Edge devices typically possess limited computational resources, necessitating the development of highly optimized algorithms capable of delivering real-time performance under these constraints. The disparity between the computational requirements of state-of-the-art MOT algorithms and the capabilities of edge devices emphasizes a significant obstacle. To address these challenges, we propose a neural network pruning method specifically tailored to compress complex networks, such as those used in modern MOT systems. This approach optimizes MOT performance by ensuring high accuracy and efficiency within the constraints of limited edge devices, such as NVIDIA's Jetson Orin Nano. By applying our pruning method, we achieve model size reductions of up to 70% while maintaining a high level of accuracy and further improving performance on the Jetson Orin Nano, demonstrating the effectiveness of our approach for edge computing applications.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08779",
        "abstract url": "https://arxiv.org/abs/2410.08779",
        "title": "HpEIS: Learning Hand Pose Embeddings for Multimedia Interactive Systems",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present a novel Hand-pose Embedding Interactive System (HpEIS) as a virtual sensor, which maps users' flexible hand poses to a two-dimensional visual space using a Variational Autoencoder (VAE) trained on a variety of hand poses. HpEIS enables visually interpretable and guidable support for user explorations in multimedia collections, using only a camera as an external hand pose acquisition device. We identify general usability issues associated with system stability and smoothing requirements through pilot experiments with expert and inexperienced users. We then design stability and smoothing improvements, including hand-pose data augmentation, an anti-jitter regularisation term added to loss function, stabilising post-processing for movement turning points and smoothing post-processing based on One Euro Filters. In target selection experiments (n=12), we evaluate HpEIS by measures of task completion time and the final distance to target points, with and without the gesture guidance window condition. Experimental responses indicate that HpEIS provides users with a learnable, flexible, stable and smooth mid-air hand movement interaction experience.",
        "subjects": [
            "cs.CV",
            "cs.HC"
        ],
        "comment": "6 pages, 8 figures, 3 tables"
    },
    {
        "paper id": "2410.08792",
        "abstract url": "https://arxiv.org/abs/2410.08792",
        "title": "VLM See, Robot Do: Human Demo Video to Robot Action Plan via Vision Language Model",
        "rating": "1",
        "keywords": [
            [
                "Vision Language",
                "VLM"
            ],
            [
                "robotics",
                "Robot"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Vision Language Models (VLMs) have recently been adopted in robotics for their capability in common sense reasoning and generalizability. Existing work has applied VLMs to generate task and motion planning from natural language instructions and simulate training data for robot learning. In this work, we explore using VLM to interpret human demonstration videos and generate robot task planning. Our method integrates keyframe selection, visual perception, and VLM reasoning into a pipeline. We named it SeeDo because it enables the VLM to ''see'' human demonstrations and explain the corresponding plans to the robot for it to ''do''. To validate our approach, we collected a set of long-horizon human videos demonstrating pick-and-place tasks in three diverse categories and designed a set of metrics to comprehensively benchmark SeeDo against several baselines, including state-of-the-art video-input VLMs. The experiments demonstrate SeeDo's superior performance. We further deployed the generated task plans in both a simulation environment and on a real robot arm.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08800",
        "abstract url": "https://arxiv.org/abs/2410.08800",
        "title": "Data Processing for the OpenGPT-X Model Family",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper presents a comprehensive overview of the data preparation pipeline developed for the OpenGPT-X project, a large-scale initiative aimed at creating open and high-performance multilingual large language models (LLMs). The project goal is to deliver models that cover all major European languages, with a particular focus on real-world applications within the European Union. We explain all data processing steps, starting with the data selection and requirement definition to the preparation of the final datasets for model training. We distinguish between curated data and web data, as each of these categories is handled by distinct pipelines, with curated data undergoing minimal filtering and web data requiring extensive filtering and deduplication. This distinction guided the development of specialized algorithmic solutions for both pipelines. In addition to describing the processing methodologies, we provide an in-depth analysis of the datasets, increasing transparency and alignment with European data regulations. Finally, we share key insights and challenges faced during the project, offering recommendations for future endeavors in large-scale multilingual data preparation for LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08815",
        "abstract url": "https://arxiv.org/abs/2410.08815",
        "title": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Retrieval-augmented generation (RAG) is a key means to effectively enhance large language models (LLMs) in many knowledge-based tasks. However, existing RAG methods struggle with knowledge-intensive reasoning tasks, because useful information required to these tasks are badly scattered. This characteristic makes it difficult for existing RAG methods to accurately identify key information and perform global reasoning with such noisy augmentation. In this paper, motivated by the cognitive theories that humans convert raw information into various structured knowledge when tackling knowledge-intensive reasoning, we proposes a new framework, StructRAG, which can identify the optimal structure type for the task at hand, reconstruct original documents into this structured format, and infer answers based on the resulting structure. Extensive experiments across various knowledge-intensive tasks show that StructRAG achieves state-of-the-art performance, particularly excelling in challenging scenarios, demonstrating its potential as an effective solution for enhancing LLMs in complex real-world applications.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08820",
        "abstract url": "https://arxiv.org/abs/2410.08820",
        "title": "Which Demographics do LLMs Default to During Annotation?",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Demographics and cultural background of annotators influence the labels they assign in text annotation -- for instance, an elderly woman might find it offensive to read a message addressed to a \"bro\", but a male teenager might find it appropriate. It is therefore important to acknowledge label variations to not under-represent members of a society. Two research directions developed out of this observation in the context of using large language models (LLM) for data annotations, namely (1) studying biases and inherent knowledge of LLMs and (2) injecting diversity in the output by manipulating the prompt with demographic information. We combine these two strands of research and ask the question to which demographics an LLM resorts to when no demographics is given. To answer this question, we evaluate which attributes of human annotators LLMs inherently mimic. Furthermore, we compare non-demographic conditioned prompts and placebo-conditioned prompts (e.g., \"you are an annotator who lives in house number 5\") to demographics-conditioned prompts (\"You are a 45 year old man and an expert on politeness annotation. How do you rate {instance}\"). We study these questions for politeness and offensiveness annotations on the POPQUORN data set, a corpus created in a controlled manner to investigate human label variations based on demographics which has not been used for LLM-based analyses so far. We observe notable influences related to gender, race, and age in demographic prompting, which contrasts with previous studies that found no such effects.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08821",
        "abstract url": "https://arxiv.org/abs/2410.08821",
        "title": "Retriever-and-Memory: Towards Adaptive Note-Enhanced Retrieval-Augmented Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Retrieval-Augmented Generation (RAG) mitigates issues of the factual errors and hallucinated outputs generated by Large Language Models (LLMs) in open-domain question-answering tasks (OpenQA) via introducing external knowledge. For complex QA, however, existing RAG methods use LLMs to actively predict retrieval timing and directly use the retrieved information for generation, regardless of whether the retrieval timing accurately reflects the actual information needs, or sufficiently considers prior retrieved knowledge, which may result in insufficient information gathering and interaction, yielding low-quality answers. To address these, we propose a generic RAG approach called Adaptive Note-Enhanced RAG (Adaptive-Note) for complex QA tasks, which includes the iterative information collector, adaptive memory reviewer, and task-oriented generator, while following a new Retriever-and-Memory paradigm. Specifically, Adaptive-Note introduces an overarching view of knowledge growth, iteratively gathering new information in the form of notes and updating them into the existing optimal knowledge structure, enhancing high-quality knowledge interactions. In addition, we employ an adaptive, note-based stop-exploration strategy to decide \"what to retrieve and when to stop\" to encourage sufficient knowledge exploration. We conduct extensive experiments on five complex QA datasets, and the results demonstrate the superiority and effectiveness of our method and its components. The code and data are at https://github.com/thunlp/Adaptive-Note.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "15 pages, 2 figures"
    },
    {
        "paper id": "2410.08828",
        "abstract url": "https://arxiv.org/abs/2410.08828",
        "title": "Enhancing Indonesian Automatic Speech Recognition: Evaluating Multilingual Models with Diverse Speech Variabilities",
        "rating": "1",
        "keywords": [
            [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "An ideal speech recognition model has the capability to transcribe speech accurately under various characteristics of speech signals, such as speaking style (read and spontaneous), speech context (formal and informal), and background noise conditions (clean and moderate). Building such a model requires a significant amount of training data with diverse speech characteristics. Currently, Indonesian data is dominated by read, formal, and clean speech, leading to a scarcity of Indonesian data with other speech variabilities. To develop Indonesian automatic speech recognition (ASR), we present our research on state-of-the-art speech recognition models, namely Massively Multilingual Speech (MMS) and Whisper, as well as compiling a dataset comprising Indonesian speech with variabilities to facilitate our study. We further investigate the models' predictive ability to transcribe Indonesian speech data across different variability groups. The best results were achieved by the Whisper fine-tuned model across datasets with various characteristics, as indicated by the decrease in word error rate (WER) and character error rate (CER). Moreover, we found that speaking style variability affected model performance the most.",
        "subjects": [
            "cs.CL",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "Accepted at O-COCOSDA 2024"
    },
    {
        "paper id": "2410.08847",
        "abstract url": "https://arxiv.org/abs/2410.08847",
        "title": "Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Direct Preference Optimization (DPO) and its variants are increasingly used for aligning language models with human preferences. Although these methods are designed to teach a model to generate preferred responses more frequently relative to dispreferred responses, prior work has observed that the likelihood of preferred responses often decreases during training. The current work sheds light on the causes and implications of this counter-intuitive phenomenon, which we term likelihood displacement. We demonstrate that likelihood displacement can be catastrophic, shifting probability mass from preferred responses to responses with an opposite meaning. As a simple example, training a model to prefer $\\texttt{No}$ over $\\texttt{Never}$ can sharply increase the probability of $\\texttt{Yes}$. Moreover, when aligning the model to refuse unsafe prompts, we show that such displacement can unintentionally lead to unalignment, by shifting probability mass from preferred refusal responses to harmful responses (e.g., reducing the refusal rate of Llama-3-8B-Instruct from 74.4% to 33.4%). We theoretically characterize that likelihood displacement is driven by preferences that induce similar embeddings, as measured by a centered hidden embedding similarity (CHES) score. Empirically, the CHES score enables identifying which training samples contribute most to likelihood displacement in a given dataset. Filtering out these samples effectively mitigated unintentional unalignment in our experiments. More broadly, our results highlight the importance of curating data with sufficiently distinct preferences, for which we believe the CHES score may prove valuable.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "stat.ML"
        ],
        "comment": "Code available at https://github.com/princeton-nlp/unintentional-unalignment"
    },
    {
        "paper id": "2410.08851",
        "abstract url": "https://arxiv.org/abs/2410.08851",
        "title": "Measuring the Inconsistency of Large Language Models in Preferential Ranking",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Despite large language models' (LLMs) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios with dense decision space or lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLMs reveal their inability to meet these criteria, indicating a strong positional bias and poor transitivity, with preferences easily swayed by irrelevant alternatives. These findings highlight a significant inconsistency in LLM-generated preferential rankings, underscoring the need for further research to address these limitations.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "In Proceedings of the 1st Workshop on Towards Knowledgeable Language Models (KnowLLM 2024)"
    },
    {
        "paper id": "2410.08879",
        "abstract url": "https://arxiv.org/abs/2410.08879",
        "title": "Multi-modal Fusion based Q-distribution Prediction for Controlled Nuclear Fusion",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Q-distribution prediction is a crucial research direction in controlled nuclear fusion, with deep learning emerging as a key approach to solving prediction challenges. In this paper, we leverage deep learning techniques to tackle the complexities of Q-distribution prediction. Specifically, we explore multimodal fusion methods in computer vision, integrating 2D line image data with the original 1D data to form a bimodal input. Additionally, we employ the Transformer's attention mechanism for feature extraction and the interactive fusion of bimodal information. Extensive experiments validate the effectiveness of our approach, significantly reducing prediction errors in Q-distribution.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08885",
        "abstract url": "https://arxiv.org/abs/2410.08885",
        "title": "Can GPTs Evaluate Graphic Design Based on Design Principles?",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in foundation models show promising capability in graphic design generation. Several studies have started employing Large Multimodal Models (LMMs) to evaluate graphic designs, assuming that LMMs can properly assess their quality, but it is unclear if the evaluation is reliable. One way to evaluate the quality of graphic design is to assess whether the design adheres to fundamental graphic design principles, which are the designer's common practice. In this paper, we compare the behavior of GPT-based evaluation and heuristic evaluation based on design principles using human annotations collected from 60 subjects. Our experiments reveal that, while GPTs cannot distinguish small details, they have a reasonably good correlation with human annotation and exhibit a similar tendency to heuristic metrics based on design principles, suggesting that they are indeed capable of assessing the quality of graphic design. Our dataset is available at https://cyberagentailab.github.io/Graphic-design-evaluation .",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "Accepted to SIGGRAPH Asia 2024 (Technical Communications Track)"
    },
    {
        "paper id": "2410.08889",
        "abstract url": "https://arxiv.org/abs/2410.08889",
        "title": "Exploiting Memory-aware Q-distribution Prediction for Nuclear Fusion via Modern Hopfield Network",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This study addresses the critical challenge of predicting the Q-distribution in long-term stable nuclear fusion task, a key component for advancing clean energy solutions. We introduce an innovative deep learning framework that employs Modern Hopfield Networks to incorporate associative memory from historical shots. Utilizing a newly compiled dataset, we demonstrate the effectiveness of our approach in enhancing Q-distribution prediction. The proposed method represents a significant advancement by leveraging historical memory information for the first time in this context, showcasing improved prediction accuracy and contributing to the optimization of nuclear fusion research.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08900",
        "abstract url": "https://arxiv.org/abs/2410.08900",
        "title": "A Benchmark for Cross-Domain Argumentative Stance Classification on Social Media",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Argumentative stance classification plays a key role in identifying authors' viewpoints on specific topics. However, generating diverse pairs of argumentative sentences across various domains is challenging. Existing benchmarks often come from a single domain or focus on a limited set of topics. Additionally, manual annotation for accurate labeling is time-consuming and labor-intensive. To address these challenges, we propose leveraging platform rules, readily available expert-curated content, and large language models to bypass the need for human annotation. Our approach produces a multidomain benchmark comprising 4,498 topical claims and 30,961 arguments from three sources, spanning 21 domains. We benchmark the dataset in fully supervised, zero-shot, and few-shot settings, shedding light on the strengths and limitations of different methodologies. We release the dataset and code in this study at hidden for anonymity.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08917",
        "abstract url": "https://arxiv.org/abs/2410.08917",
        "title": "AutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We introduce AutoPersuade, a three-part framework for constructing persuasive messages. First, we curate a large dataset of arguments with human evaluations. Next, we develop a novel topic model to identify argument features that influence persuasiveness. Finally, we use this model to predict the effectiveness of new arguments and assess the causal impact of different components to provide explanations. We validate AutoPersuade through an experimental study on arguments for veganism, demonstrating its effectiveness with human studies and out-of-sample predictions.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08918",
        "abstract url": "https://arxiv.org/abs/2410.08918",
        "title": "Wikimedia data for AI: a review of Wikimedia datasets for NLP tasks and AI-assisted editing",
        "rating": "1",
        "keywords": [
            [
                "cs.CY"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Wikimedia content is used extensively by the AI community and within the language modeling community in particular. In this paper, we provide a review of the different ways in which Wikimedia data is curated to use in NLP tasks across pre-training, post-training, and model evaluations. We point to opportunities for greater use of Wikimedia content but also identify ways in which the language modeling community could better center the needs of Wikimedia editors. In particular, we call for incorporating additional sources of Wikimedia data, a greater focus on benchmarks for LLMs that encode Wikimedia principles, and greater multilingualism in Wikimedia-derived datasets.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "Accepted to NLP for Wikipedia Workshop at EMNLP '24"
    },
    {
        "paper id": "2410.08920",
        "abstract url": "https://arxiv.org/abs/2410.08920",
        "title": "Efficient Hyperparameter Importance Assessment for CNNs",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Hyperparameter selection is an essential aspect of the machine learning pipeline, profoundly impacting models' robustness, stability, and generalization capabilities. Given the complex hyperparameter spaces associated with Neural Networks and the constraints of computational resources and time, optimizing all hyperparameters becomes impractical. In this context, leveraging hyperparameter importance assessment (HIA) can provide valuable guidance by narrowing down the search space. This enables machine learning practitioners to focus their optimization efforts on the hyperparameters with the most significant impact on model performance while conserving time and resources. This paper aims to quantify the importance weights of some hyperparameters in Convolutional Neural Networks (CNNs) with an algorithm called N-RReliefF, laying the groundwork for applying HIA methodologies in the Deep Learning field. We conduct an extensive study by training over ten thousand CNN models across ten popular image classification datasets, thereby acquiring a comprehensive dataset containing hyperparameter configuration instances and their corresponding performance metrics. It is demonstrated that among the investigated hyperparameters, the top five important hyperparameters of the CNN model are the number of convolutional layers, learning rate, dropout rate, optimizer and epoch.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "15 pages"
    },
    {
        "paper id": "2410.08925",
        "abstract url": "https://arxiv.org/abs/2410.08925",
        "title": "HyperPg -- Prototypical Gaussians on the Hypersphere for Interpretable Deep Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Prototype Learning methods provide an interpretable alternative to black-box deep learning models. Approaches such as ProtoPNet learn, which part of a test image \"look like\" known prototypical parts from training images, combining predictive power with the inherent interpretability of case-based reasoning. However, existing approaches have two main drawbacks: A) They rely solely on deterministic similarity scores without statistical confidence. B) The prototypes are learned in a black-box manner without human input. This work introduces HyperPg, a new prototype representation leveraging Gaussian distributions on a hypersphere in latent space, with learnable mean and variance. HyperPg prototypes adapt to the spread of clusters in the latent space and output likelihood scores. The new architecture, HyperPgNet, leverages HyperPg to learn prototypes aligned with human concepts from pixel-level annotations. Consequently, each prototype represents a specific concept such as color, image texture, or part of the image subject. A concept extraction pipeline built on foundation models provides pixel-level annotations, significantly reducing human labeling effort. Experiments on CUB-200-2011 and Stanford Cars datasets demonstrate that HyperPgNet outperforms other prototype learning architectures while using fewer parameters and training steps. Additionally, the concept-aligned HyperPg prototypes are learned transparently, enhancing model interpretability.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08926",
        "abstract url": "https://arxiv.org/abs/2410.08926",
        "title": "Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "We explore the transformative potential of SAM 2, a vision foundation model, in advancing gaze estimation and eye tracking technologies. By significantly reducing annotation time, lowering technical barriers through its ease of deployment, and enhancing segmentation accuracy, SAM 2 addresses critical challenges faced by researchers and practitioners. Utilizing its zero-shot segmentation capabilities with minimal user input-a single click per video-we tested SAM 2 on over 14 million eye images from diverse datasets, including virtual reality setups and the world's largest unified dataset recorded using wearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches the performance of domain-specific models trained solely on eye images, achieving competitive mean Intersection over Union (mIoU) scores of up to 93% without fine-tuning. Additionally, we provide our code and segmentation masks for these widely used datasets to promote further research.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.HC"
        ],
        "comment": "Virmarie Maquiling and Sean Anthony Byrne contributed equally to this paper, 8 pages, 3 figures, CHI Case Study, pre-print"
    },
    {
        "paper id": "2410.08928",
        "abstract url": "https://arxiv.org/abs/2410.08928",
        "title": "Towards Cross-Lingual LLM Evaluation for European Languages",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The rise of Large Language Models (LLMs) has revolutionized natural language processing across numerous languages and tasks. However, evaluating LLM performance in a consistent and meaningful way across multiple European languages remains challenging, especially due to the scarcity of multilingual benchmarks. We introduce a cross-lingual evaluation approach tailored for European languages. We employ translated versions of five widely-used benchmarks to assess the capabilities of 40 LLMs across 21 European languages. Our contributions include examining the effectiveness of translated benchmarks, assessing the impact of different translation services, and offering a multilingual evaluation framework for LLMs that includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC, EU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly available to encourage further research in multilingual LLM evaluation.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08964",
        "abstract url": "https://arxiv.org/abs/2410.08964",
        "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have achieved state-of-the-art performance across numerous tasks. However, these advancements have predominantly benefited \"first-class\" languages such as English and Chinese, leaving many other languages underrepresented. This imbalance, while limiting broader applications, generates a natural preference ranking between languages, offering an opportunity to bootstrap the multilingual capabilities of LLM in a self-improving manner. Thus, we propose $\\textit{Language Imbalance Driven Rewarding}$, where the inherent imbalance between dominant and non-dominant languages within LLMs is leveraged as a reward signal. Iterative DPO training demonstrates that this approach not only enhances LLM performance in non-dominant languages but also improves the dominant language's capacity, thereby yielding an iterative reward signal. Fine-tuning Meta-Llama-3-8B-Instruct over two iterations of this approach results in continuous improvements in multilingual performance across instruction-following and arithmetic reasoning tasks, evidenced by an average improvement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9% accuracy on the MGSM benchmark. This work serves as an initial exploration, paving the way for multilingual self-improvement of LLMs.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2410.08968",
        "abstract url": "https://arxiv.org/abs/2410.08968",
        "title": "Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The current paradigm for safety alignment of large language models (LLMs) follows a one-size-fits-all approach: the model refuses to interact with any content deemed unsafe by the model provider. This approach lacks flexibility in the face of varying social norms across cultures and regions. In addition, users may have diverse safety needs, making a model with static safety standards too restrictive to be useful, as well as too costly to be re-aligned. We propose Controllable Safety Alignment (CoSA), a framework designed to adapt models to diverse safety requirements without re-training. Instead of aligning a fixed model, we align models to follow safety configs -- free-form natural language descriptions of the desired safety behaviors -- that are provided as part of the system prompt. To adjust model safety behavior, authorized users only need to modify such safety configs at inference time. To enable that, we propose CoSAlign, a data-centric method for aligning LLMs to easily adapt to diverse safety configs. Furthermore, we devise a novel controllability evaluation protocol that considers both helpfulness and configured safety, summarizing them into CoSA-Score, and construct CoSApien, a human-authored benchmark that consists of real-world LLM use cases with diverse safety requirements and corresponding evaluation prompts. We show that CoSAlign leads to substantial gains of controllability over strong baselines including in-context alignment. Our framework encourages better representation and adaptation to pluralistic human values in LLMs, and thereby increasing their practicality.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08970",
        "abstract url": "https://arxiv.org/abs/2410.08970",
        "title": "NoVo: Norm Voting off Hallucinations with Attention Heads in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Hallucinations in Large Language Models (LLMs) remain a major obstacle, particularly in high-stakes applications where factual accuracy is critical. While representation editing and reading methods have made strides in reducing hallucinations, their heavy reliance on specialised tools and training on in-domain samples, makes them difficult to scale and prone to overfitting. This limits their accuracy gains and generalizability to diverse datasets. This paper presents a lightweight method, Norm Voting (NoVo), which harnesses the untapped potential of attention head norms to dramatically enhance factual accuracy in zero-shot multiple-choice questions (MCQs). NoVo begins by automatically selecting truth-correlated head norms with an efficient, inference-only algorithm using only 30 random samples, allowing NoVo to effortlessly scale to diverse datasets. Afterwards, selected head norms are employed in a simple voting algorithm, which yields significant gains in prediction accuracy. On TruthfulQA MC1, NoVo surpasses the current state-of-the-art and all previous methods by an astounding margin -- at least 19 accuracy points. NoVo demonstrates exceptional generalization to 20 diverse datasets, with significant gains in over 90\\% of them, far exceeding all current representation editing and reading methods. NoVo also reveals promising gains to finetuning strategies and building textual adversarial defence. NoVo's effectiveness with head norms opens new frontiers in LLM interpretability, robustness and reliability.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08971",
        "abstract url": "https://arxiv.org/abs/2410.08971",
        "title": "Extra Global Attention Designation Using Keyword Detection in Sparse Transformer Architectures",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we propose an extension to Longformer Encoder-Decoder, a popular sparse transformer architecture. One common challenge with sparse transformers is that they can struggle with encoding of long range context, such as connections between topics discussed at a beginning and end of a document. A method to selectively increase global attention is proposed and demonstrated for abstractive summarization tasks on several benchmark data sets. By prefixing the transcript with additional keywords and encoding global attention on these keywords, improvement in zero-shot, few-shot, and fine-tuned cases is demonstrated for some benchmark data sets.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08972",
        "abstract url": "https://arxiv.org/abs/2410.08972",
        "title": "ALVIN: Active Learning Via INterpolation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Active Learning aims to minimize annotation effort by selecting the most useful instances from a pool of unlabeled data. However, typical active learning methods overlook the presence of distinct example groups within a class, whose prevalence may vary, e.g., in occupation classification datasets certain demographics are disproportionately represented in specific classes. This oversight causes models to rely on shortcuts for predictions, i.e., spurious correlations between input attributes and labels occurring in well-represented groups. To address this issue, we propose Active Learning Via INterpolation (ALVIN), which conducts intra-class interpolations between examples from under-represented and well-represented groups to create anchors, i.e., artificial points situated between the example groups in the representation space. By selecting instances close to the anchors for annotation, ALVIN identifies informative examples exposing the model to regions of the representation space that counteract the influence of shortcuts. Crucially, since the model considers these examples to be of high certainty, they are likely to be ignored by typical active learning methods. Experimental results on six datasets encompassing sentiment analysis, natural language inference, and paraphrase detection demonstrate that ALVIN outperforms state-of-the-art active learning methods in both in-distribution and out-of-distribution generalization.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Accepted to EMNLP 2024 (Main)"
    },
    {
        "paper id": "2410.08974",
        "abstract url": "https://arxiv.org/abs/2410.08974",
        "title": "UniGlyph: A Seven-Segment Script for Universal Language Representation",
        "rating": "1",
        "keywords": [
            [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "UniGlyph is a constructed language (conlang) designed to create a universal transliteration system using a script derived from seven-segment characters. The goal of UniGlyph is to facilitate cross-language communication by offering a flexible and consistent script that can represent a wide range of phonetic sounds. This paper explores the design of UniGlyph, detailing its script structure, phonetic mapping, and transliteration rules. The system addresses imperfections in the International Phonetic Alphabet (IPA) and traditional character sets by providing a compact, versatile method to represent phonetic diversity across languages. With pitch and length markers, UniGlyph ensures accurate phonetic representation while maintaining a small character set. Applications of UniGlyph include artificial intelligence integrations, such as natural language processing and multilingual speech recognition, enhancing communication across different languages. Future expansions are discussed, including the addition of animal phonetic sounds, where unique scripts are assigned to different species, broadening the scope of UniGlyph beyond human communication. This study presents the challenges and solutions in developing such a universal script, demonstrating the potential of UniGlyph to bridge linguistic gaps in cross-language communication, educational phonetics, and AI-driven applications.",
        "subjects": [
            "cs.CL",
            "cs.HC",
            "cs.SC",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "This submission includes 23 pages and tables. No external funding has been received for this research. Acknowledgments to Jeseentha V. for contributions to the phonetic study"
    },
    {
        "paper id": "2410.08996",
        "abstract url": "https://arxiv.org/abs/2410.08996",
        "title": "Hypothesis-only Biases in Large Language Model-Elicited Natural Language Inference",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We test whether replacing crowdsource workers with LLMs to write Natural Language Inference (NLI) hypotheses similarly results in annotation artifacts. We recreate a portion of the Stanford NLI corpus using GPT-4, Llama-2 and Mistral 7b, and train hypothesis-only classifiers to determine whether LLM-elicited hypotheses contain annotation artifacts. On our LLM-elicited NLI datasets, BERT-based hypothesis-only classifiers achieve between 86-96% accuracy, indicating these datasets contain hypothesis-only artifacts. We also find frequent \"give-aways\" in LLM-generated hypotheses, e.g. the phrase \"swimming in a pool\" appears in more than 10,000 contradictions generated by GPT-4. Our analysis provides empirical evidence that well-attested biases in NLI can persist in LLM-generated data.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09008",
        "abstract url": "https://arxiv.org/abs/2410.09008",
        "title": "SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown significant improvements in various reasoning tasks. However, smaller models such as Llama-3-8B and DeepSeekMath-Base still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning errors. Recent reflection-based methods aim to address these issues by enabling self-reflection and self-correction, but they still face challenges in independently detecting errors in their reasoning steps. To overcome these limitations, we propose SuperCorrect, a novel two-stage framework that uses a large teacher model to supervise and correct both the reasoning and reflection processes of a smaller student model. In the first stage, we extract hierarchical high-level and detailed thought templates from the teacher model to guide the student model in eliciting more fine-grained reasoning thoughts. In the second stage, we introduce cross-model collaborative direct preference optimization (DPO) to enhance the self-correction abilities of the student model by following the teacher's correction traces during training. This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems. Extensive experiments consistently demonstrate our superiority over previous methods. Notably, our SuperCorrect-7B model significantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models. Code: https://github.com/YangLing0818/SuperCorrect-llm",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Project: https://github.com/YangLing0818/SuperCorrect-llm"
    },
    {
        "paper id": "2410.09038",
        "abstract url": "https://arxiv.org/abs/2410.09038",
        "title": "SimpleStrat: Diversifying Language Model Generation with Stratification",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Generating diverse responses from large language models (LLMs) is crucial for applications such as planning/search and synthetic data generation, where diversity provides distinct answers across generations. Prior approaches rely on increasing temperature to increase diversity. However, contrary to popular belief, we show not only does this approach produce lower quality individual generations as temperature increases, but it depends on model's next-token probabilities being similar to the true distribution of answers. We propose SimpleStrat, an alternative approach that uses the language model itself to partition the space into strata. At inference, a random stratum is selected and a sample drawn from within the strata. To measure diversity, we introduce CoverageQA, a dataset of underspecified questions with multiple equally plausible answers, and assess diversity by measuring KL Divergence between the output distribution and uniform distribution over valid ground truth answers. As computing probability per response/solution for proprietary models is infeasible, we measure recall on ground truth solutions. Our evaluation show using SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36 average reduction in KL Divergence compared to Llama 3.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09130",
        "abstract url": "https://arxiv.org/abs/2410.09130",
        "title": "Energy-efficient SNN Architecture using 3nm FinFET Multiport SRAM-based CIM with Online Learning",
        "rating": "1",
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "Current Artificial Intelligence (AI) computation systems face challenges, primarily from the memory-wall issue, limiting overall system-level performance, especially for Edge devices with constrained battery budgets, such as smartphones, wearables, and Internet-of-Things sensor systems. In this paper, we propose a new SRAM-based Compute-In-Memory (CIM) accelerator optimized for Spiking Neural Networks (SNNs) Inference. Our proposed architecture employs a multiport SRAM design with multiple decoupled Read ports to enhance the throughput and Transposable Read-Write ports to facilitate online learning. Furthermore, we develop an Arbiter circuit for efficient data-processing and port allocations during the computation. Results for a 128$\\times$128 array in 3nm FinFET technology demonstrate a 3.1$\\times$ improvement in speed and a 2.2$\\times$ enhancement in energy efficiency with our proposed multiport SRAM design compared to the traditional single-port design. At system-level, a throughput of 44 MInf/s at 607 pJ/Inf and 29mW is achieved.",
        "subjects": [
            "cs.AR",
            "cs.NE",
            "eess.IV"
        ],
        "comment": "DAC 2024 Research Manuscript"
    },
    {
        "paper id": "2410.09135",
        "abstract url": "https://arxiv.org/abs/2410.09135",
        "title": "Enabling Advanced Land Cover Analytics: An Integrated Data Extraction Pipeline for Predictive Modeling with the Dynamic World Dataset",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Understanding land cover holds considerable potential for a myriad of practical applications, particularly as data accessibility transitions from being exclusive to governmental and commercial entities to now including the broader research community. Nevertheless, although the data is accessible to any community member interested in exploration, there exists a formidable learning curve and no standardized process for accessing, pre-processing, and leveraging the data for subsequent tasks. In this study, we democratize this data by presenting a flexible and efficient end to end pipeline for working with the Dynamic World dataset, a cutting-edge near-real-time land use/land cover (LULC) dataset. This includes a pre-processing and representation framework which tackles noise removal, efficient extraction of large amounts of data, and re-representation of LULC data in a format well suited for several downstream tasks. To demonstrate the power of our pipeline, we use it to extract data for an urbanization prediction problem and build a suite of machine learning models with excellent performance. This task is easily generalizable to the prediction of any type of land cover and our pipeline is also compatible with a series of other downstream tasks.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09141",
        "abstract url": "https://arxiv.org/abs/2410.09141",
        "title": "ACER: Automatic Language Model Context Extension via Retrieval",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Long-context modeling is one of the critical capabilities of language AI for digesting and reasoning over complex information pieces. In practice, long-context capabilities are typically built into a pre-trained language model~(LM) through a carefully designed context extension stage, with the goal of producing generalist long-context capabilities. In our preliminary experiments, however, we discovered that the current open-weight generalist long-context models are still lacking in practical long-context processing tasks. While this means perfectly effective long-context modeling demands task-specific data, the cost can be prohibitive. In this paper, we draw inspiration from how humans process a large body of information: a lossy \\textbf{retrieval} stage ranks a large set of documents while the reader ends up reading deeply only the top candidates. We build an \\textbf{automatic} data synthesis pipeline that mimics this process using short-context LMs. The short-context LMs are further tuned using these self-generated data to obtain task-specific long-context capabilities. Similar to how pre-training learns from imperfect data, we hypothesize and further demonstrate that the short-context model can bootstrap over the synthetic data, outperforming not only long-context generalist models but also the retrieval and read pipeline used to synthesize the training data in real-world tasks such as long-context retrieval augmented generation.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09168",
        "abstract url": "https://arxiv.org/abs/2410.09168",
        "title": "Hybrid Training Approaches for LLMs: Leveraging Real and Synthetic Data to Enhance Model Performance in Domain-Specific Applications",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This research explores a hybrid approach to fine-tuning large language models (LLMs) by integrating real-world and synthetic data to boost model performance, particularly in generating accurate and contextually relevant responses. By leveraging a dataset combining transcribed real interactions with high-quality synthetic sessions, we aimed to overcome the limitations of scarce, noisy, and domain-specific real data. Synthetic personas and scenarios were employed to enhance training diversity. The study evaluated three models: a base foundational model, a model fine-tuned with real data, and a hybrid fine-tuned model. Experimental results showed that the hybrid model consistently outperformed the others in specific vertical applications, achieving the highest scores across all metrics. Further testing confirmed the hybrid model's superior adaptability and contextual understanding across diverse scenarios. These findings suggest that combining real and synthetic data can significantly improve the robustness and contextual sensitivity of LLMs, particularly in domain-specific and vertical use cases.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "22 pages, 7 figures"
    },
    {
        "paper id": "2410.09176",
        "abstract url": "https://arxiv.org/abs/2410.09176",
        "title": "Cross-Domain Evaluation of Few-Shot Classification Models: Natural Images vs. Histopathological Images",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this study, we investigate the performance of few-shot classification models across different domains, specifically natural images and histopathological images. We first train several few-shot classification models on natural images and evaluate their performance on histopathological images. Subsequently, we train the same models on histopathological images and compare their performance. We incorporated four histopathology datasets and one natural images dataset and assessed performance across 5-way 1-shot, 5-way 5-shot, and 5-way 10-shot scenarios using a selection of state-of-the-art classification techniques. Our experimental results reveal insights into the transferability and generalization capabilities of few-shot classification models between diverse image domains. We analyze the strengths and limitations of these models in adapting to new domains and provide recommendations for optimizing their performance in cross-domain scenarios. This research contributes to advancing our understanding of few-shot learning in the context of image classification across diverse domains.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09184",
        "abstract url": "https://arxiv.org/abs/2410.09184",
        "title": "L3Cube-MahaSum: A Comprehensive Dataset and BART Models for Abstractive Text Summarization in Marathi",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We present the MahaSUM dataset, a large-scale collection of diverse news articles in Marathi, designed to facilitate the training and evaluation of models for abstractive summarization tasks in Indic languages. The dataset, containing 25k samples, was created by scraping articles from a wide range of online news sources and manually verifying the abstract summaries. Additionally, we train an IndicBART model, a variant of the BART model tailored for Indic languages, using the MahaSUM dataset. We evaluate the performance of our trained models on the task of abstractive summarization and demonstrate their effectiveness in producing high-quality summaries in Marathi. Our work contributes to the advancement of natural language processing research in Indic languages and provides a valuable resource for future research in this area using state-of-the-art models. The dataset and models are shared publicly at https://github.com/l3cube-pune/MarathiNLP",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09187",
        "abstract url": "https://arxiv.org/abs/2410.09187",
        "title": "Automated Rewards via LLM-Generated Progress Functions",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have the potential to automate reward engineering by leveraging their broad domain knowledge across various tasks. However, they often need many iterations of trial-and-error to generate effective reward functions. This process is costly because evaluating every sampled reward function requires completing the full policy optimization process for each function. In this paper, we introduce an LLM-driven reward generation framework that is able to produce state-of-the-art policies on the challenging Bi-DexHands benchmark \\textbf{with 20$\\times$ fewer reward function samples} than the prior state-of-the-art work. Our key insight is that we reduce the problem of generating task-specific rewards to the problem of coarsely estimating \\emph{task progress}. Our two-step solution leverages the task domain knowledge and the code synthesis abilities of LLMs to author \\emph{progress functions} that estimate task progress from a given state. Then, we use this notion of progress to discretize states, and generate count-based intrinsic rewards using the low-dimensional state space. We show that the combination of LLM-generated progress functions and count-based intrinsic rewards is essential for our performance gains, while alternatives such as generic hash-based counts or using progress directly as a reward function fall short.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "26 pages, 5 figures"
    },
    {
        "paper id": "2410.09207",
        "abstract url": "https://arxiv.org/abs/2410.09207",
        "title": "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Existing methods on understanding the capabilities of LLMs in logical reasoning rely on binary entailment classification or synthetically derived rationales, which are not sufficient for proper investigation of model's capabilities. We present P-FOLIO, a human-annotated dataset consisting of diverse and complex reasoning chains for a set of realistic logical reasoning stories also written by humans. P-FOLIO is collected with an annotation protocol that facilitates humans to annotate well-structured natural language proofs for first-order logic reasoning problems in a step-by-step manner. The number of reasoning steps in P-FOLIO span from 0 to 20. We further use P-FOLIO to evaluate and improve large-language-model (LLM) reasoning capabilities. We evaluate LLM reasoning capabilities at a fine granularity via single-step inference rule classification, with more diverse inference rules of more diverse and higher levels of complexities than previous works. Given that a single model-generated reasoning chain could take a completely different path than the human-annotated one, we sample multiple reasoning chains from a model and use pass@k metrics for evaluating the quality of model-generated reasoning chains. We show that human-written reasoning chains significantly boost the logical reasoning capabilities of LLMs via many-shot prompting and fine-tuning. Furthermore, fine-tuning Llama3-7B on P-FOLIO improves the model performance by 10% or more on three other out-of-domain logical reasoning datasets. We also conduct detailed analysis to show where most powerful LLMs fall short in reasoning. We will release the dataset and code publicly.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09223",
        "abstract url": "https://arxiv.org/abs/2410.09223",
        "title": "The Same But Different: Structural Similarities and Differences in Multilingual Language Modeling",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "We employ new tools from mechanistic interpretability in order to ask whether the internal structure of large language models (LLMs) shows correspondence to the linguistic structures which underlie the languages on which they are trained. In particular, we ask (1) when two languages employ the same morphosyntactic processes, do LLMs handle them using shared internal circuitry? and (2) when two languages require different morphosyntactic processes, do LLMs handle them using different internal circuitry? Using English and Chinese multilingual and monolingual models, we analyze the internal circuitry involved in two tasks. We find evidence that models employ the same circuit to handle the same syntactic process independently of the language in which it occurs, and that this is the case even for monolingual models trained completely independently. Moreover, we show that multilingual models employ language-specific components (attention heads and feed-forward networks) when needed to handle linguistic processes (e.g., morphological marking) that only exist in some languages. Together, our results provide new insights into how LLMs trade off between exploiting common structures and preserving linguistic differences when tasked with modeling multiple languages simultaneously.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09239",
        "abstract url": "https://arxiv.org/abs/2410.09239",
        "title": "Scaling Gaussian Processes for Learning Curve Prediction via Latent Kronecker Structure",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "A key task in AutoML is to model learning curves of machine learning models jointly as a function of model hyper-parameters and training progression. While Gaussian processes (GPs) are suitable for this task, na\u00efve GPs require $\\mathcal{O}(n^3m^3)$ time and $\\mathcal{O}(n^2 m^2)$ space for $n$ hyper-parameter configurations and $\\mathcal{O}(m)$ learning curve observations per hyper-parameter. Efficient inference via Kronecker structure is typically incompatible with early-stopping due to missing learning curve values. We impose $\\textit{latent Kronecker structure}$ to leverage efficient product kernels while handling missing values. In particular, we interpret the joint covariance matrix of observed values as the projection of a latent Kronecker product. Combined with iterative linear solvers and structured matrix-vector multiplication, our method only requires $\\mathcal{O}(n^3 + m^3)$ time and $\\mathcal{O}(n^2 + m^2)$ space. We show that our GP model can match the performance of a Transformer on a learning curve prediction task.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "Bayesian Decision-making and Uncertainty Workshop at NeurIPS 2024"
    },
    {
        "paper id": "2410.09243",
        "abstract url": "https://arxiv.org/abs/2410.09243",
        "title": "Enhanced Kalman with Adaptive Appearance Motion SORT for Grounded Generic Multiple Object Tracking",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Despite recent progress, Multi-Object Tracking (MOT) continues to face significant challenges, particularly its dependence on prior knowledge and predefined categories, complicating the tracking of unfamiliar objects. Generic Multiple Object Tracking (GMOT) emerges as a promising solution, requiring less prior information. Nevertheless, existing GMOT methods, primarily designed as OneShot-GMOT, rely heavily on initial bounding boxes and often struggle with variations in viewpoint, lighting, occlusion, and scale. To overcome the limitations inherent in both MOT and GMOT when it comes to tracking objects with specific generic attributes, we introduce Grounded-GMOT, an innovative tracking paradigm that enables users to track multiple generic objects in videos through natural language descriptors. Our contributions begin with the introduction of the G2MOT dataset, which includes a collection of videos featuring a wide variety of generic objects, each accompanied by detailed textual descriptions of their attributes. Following this, we propose a novel tracking method, KAM-SORT, which not only effectively integrates visual appearance with motion cues but also enhances the Kalman filter. KAM-SORT proves particularly advantageous when dealing with objects of high visual similarity from the same generic category in GMOT scenarios. Through comprehensive experiments, we demonstrate that Grounded-GMOT outperforms existing OneShot-GMOT approaches. Additionally, our extensive comparisons between various trackers highlight KAM-SORT's efficacy in GMOT, further establishing its significance in the field. Project page: https://UARK-AICV.github.io/G2MOT. The source code and dataset will be made publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "ACCV 2024, main track, oral presentation"
    },
    {
        "paper id": "2410.09245",
        "abstract url": "https://arxiv.org/abs/2410.09245",
        "title": "Sui Generis: Large Language Models for Authorship Attribution and Verification in Latin",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper evaluates the performance of Large Language Models (LLMs) in authorship attribution and authorship verification tasks for Latin texts of the Patristic Era. The study showcases that LLMs can be robust in zero-shot authorship verification even on short texts without sophisticated feature engineering. Yet, the models can also be easily \"mislead\" by semantics. The experiments also demonstrate that steering the model's authorship analysis and decision-making is challenging, unlike what is reported in the studies dealing with high-resource modern languages. Although LLMs prove to be able to beat, under certain circumstances, the traditional baselines, obtaining a nuanced and truly explainable decision requires at best a lot of experimentation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "9 pages, NLP4DH 2024"
    },
    {
        "paper id": "2410.09247",
        "abstract url": "https://arxiv.org/abs/2410.09247",
        "title": "Benchmark Inflation: Revealing LLM Performance Gaps Using Retro-Holdouts",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The training data for many Large Language Models (LLMs) is contaminated with test data. This means that public benchmarks used to assess LLMs are compromised, suggesting a performance gap between benchmark scores and actual capabilities. Ideally, a private holdout set could be used to accurately verify scores. Unfortunately, such datasets do not exist for most benchmarks, and post-hoc construction of sufficiently similar datasets is non-trivial. To address these issues, we introduce a systematic methodology for (i) retrospectively constructing a holdout dataset for a target dataset, (ii) demonstrating the statistical indistinguishability of this retro-holdout dataset, and (iii) comparing LLMs on the two datasets to quantify the performance gap due to the dataset's public availability. Applying these methods to TruthfulQA, we construct and release Retro-Misconceptions, on which we evaluate twenty LLMs and find that some have inflated scores by as much as 16 percentage points. Our results demonstrate that public benchmark scores do not always accurately assess model properties, and underscore the importance of improved data practices in the field.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09275",
        "abstract url": "https://arxiv.org/abs/2410.09275",
        "title": "Articulated Animal AI: An Environment for Animal-like Cognition in a Limbed Agent",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "This paper presents the Articulated Animal AI Environment for Animal Cognition, an enhanced version of the previous AnimalAI Environment. Key improvements include the addition of agent limbs, enabling more complex behaviors and interactions with the environment that closely resemble real animal movements. The testbench features an integrated curriculum training sequence and evaluation tools, eliminating the need for users to develop their own training programs. Additionally, the tests and training procedures are randomized, which will improve the agent's generalization capabilities. These advancements significantly expand upon the original AnimalAI framework and will be used to evaluate agents on various aspects of animal cognition.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
        ],
        "comment": "8 pages, accepted to Workshop on Open-World Agents (OWA-2024) at NeurIPS 2024 in Vancouver, Canada"
    },
    {
        "paper id": "2410.09283",
        "abstract url": "https://arxiv.org/abs/2410.09283",
        "title": "Comparative Analysis of Static and Contextual Embeddings for Analyzing Semantic Changes in Medieval Latin Charters",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The Norman Conquest of 1066 C.E. brought profound transformations to England's administrative, societal, and linguistic practices. The DEEDS (Documents of Early England Data Set) database offers a unique opportunity to explore these changes by examining shifts in word meanings within a vast collection of Medieval Latin charters. While computational linguistics typically relies on vector representations of words like static and contextual embeddings to analyze semantic changes, existing embeddings for scarce and historical Medieval Latin are limited and may not be well-suited for this task. This paper presents the first computational analysis of semantic change pre- and post-Norman Conquest and the first systematic comparison of static and contextual embeddings in a scarce historical data set. Our findings confirm that, consistent with existing studies, contextual embeddings outperform static word embeddings in capturing semantic change within a scarce historical corpus.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "11 pages, 6 figures"
    },
    {
        "paper id": "2410.09300",
        "abstract url": "https://arxiv.org/abs/2410.09300",
        "title": "Nudging: Inference-time Alignment via Model Collaboration",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) require alignment, such as instruction-tuning or reinforcement learning from human feedback, to effectively and safely follow user instructions. This process necessitates training aligned versions for every model size in each model family, resulting in significant computational overhead. In this work, we propose nudging, a simple, plug-and-play, and training-free algorithm that aligns any base model at inference time using a small aligned model. Nudging is motivated by recent findings that alignment primarily alters the model's behavior on a small subset of stylistic tokens, such as \"Sure\" or \"Thank\". We find that base models are significantly more uncertain when generating these tokens. Leveraging this observation, nudging employs a small aligned model to generate nudging tokens to steer the large base model's output toward desired directions when the base model's uncertainty is high. We evaluate the effectiveness of nudging across 3 model families and 13 tasks, covering reasoning, general knowledge, instruction following, and safety benchmarks. Without any additional training, nudging a large base model with a 7x - 14x smaller aligned model achieves zero-shot performance comparable to, and sometimes surpassing, that of large aligned models. For example, nudging OLMo-7b with OLMo-1b-instruct, affecting less than 9% of tokens, achieves a 10% absolute improvement on GSM8K over OLMo-7b-instruct. Unlike prior inference-time tuning methods, nudging enables off-the-shelf collaboration between model families. For instance, nudging Gemma-2-27b with Llama-2-7b-chat outperforms Llama-2-70b-chat on various tasks. Overall, this work introduces a simple yet powerful approach to token-level model collaboration, offering a modular solution to LLM alignment. Our project website: https://fywalter.github.io/nudging/ .",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09302",
        "abstract url": "https://arxiv.org/abs/2410.09302",
        "title": "Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Reinforcement Learning (RL) plays a crucial role in aligning large language models (LLMs) with human preferences and improving their ability to perform complex tasks. However, current approaches either require significant computational resources due to the use of multiple models and extensive online sampling for training (e.g., PPO) or are framed as bandit problems (e.g., DPO, DRO), which often struggle with multi-step reasoning tasks, such as math problem-solving and complex reasoning that involve long chains of thought. To overcome these limitations, we introduce Direct Q-function Optimization (DQO), which formulates the response generation process as a Markov Decision Process (MDP) and utilizes the soft actor-critic (SAC) framework to optimize a Q-function directly parameterized by the language model. The MDP formulation of DQO offers structural advantages over bandit-based methods, enabling more effective process supervision. Experimental results on two math problem-solving datasets, GSM8K and MATH, demonstrate that DQO outperforms previous methods, establishing it as a promising offline reinforcement learning approach for aligning language models.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09303",
        "abstract url": "https://arxiv.org/abs/2410.09303",
        "title": "Exact Byte-Level Probabilities from Tokenized Language Models for FIM-Tasks and Model Ensembles",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Tokenization is associated with many poorly understood shortcomings in language models (LMs), yet remains an important component for long sequence scaling purposes. This work studies how tokenization impacts model performance by analyzing and comparing the stochastic behavior of tokenized models with their byte-level, or token-free, counterparts. We discover that, even when the two models are statistically equivalent, their predictive distributions over the next byte can be substantially different, a phenomenon we term as \"tokenization bias''. To fully characterize this phenomenon, we introduce the Byte-Token Representation Lemma, a framework that establishes a mapping between the learned token distribution and its equivalent byte-level distribution. From this result, we develop a next-byte sampling algorithm that eliminates tokenization bias without requiring further training or optimization. In other words, this enables zero-shot conversion of tokenized LMs into statistically equivalent token-free ones. We demonstrate its broad applicability with two use cases: fill-in-the-middle (FIM) tasks and model ensembles. In FIM tasks where input prompts may terminate mid-token, leading to out-of-distribution tokenization, our method mitigates performance degradation and achieves an approximately 18% improvement in FIM coding benchmarks, consistently outperforming the standard token healing fix. For model ensembles where each model employs a distinct vocabulary, our approach enables seamless integration, resulting in improved performance (up to 3.7%) over individual models across various standard baselines in reasoning, knowledge, and coding.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09314",
        "abstract url": "https://arxiv.org/abs/2410.09314",
        "title": "\\llinstruct: An Instruction-tuned model for English Language Proficiency Assessments",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "We present \\llinstruct: An 8B instruction-tuned model that is designed to generate content for English Language Proficiency Assessments (ELPA) and related applications. Our work involves creating a new dataset of 70K instructions and explanations in the ELPA domain and using these to fine-tune Llama-3 8B models (SFT) of different sizes (e.g., SFT-17K, SFT-50K and SFT-70K). Human evaluations are conducted over unseen instructions to compare these SFT models against SOTA models (e.g., Dolly-2, Mistral, Llama-3 base version, and GPT-3.5). The findings show although all three SFT models perform comparably, the model trained on largest instruction dataset -- SFT-70K - leads to the most valid outputs ready for assessments. However, although the SFT models perform better than larger model, e.g., GPT 3.5 on the aspect of explanations of outputs, many outputs still need human interventions to make them actual ready for real world assessments.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09324",
        "abstract url": "https://arxiv.org/abs/2410.09324",
        "title": "Token Pruning using a Lightweight Background Aware Vision Transformer",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "High runtime memory and high latency puts significant constraint on Vision Transformer training and inference, especially on edge devices. Token pruning reduces the number of input tokens to the ViT based on importance criteria of each token. We present a Background Aware Vision Transformer (BAViT) model, a pre-processing block to object detection models like DETR/YOLOS aimed to reduce runtime memory and increase throughput by using a novel approach to identify background tokens in the image. The background tokens can be pruned completely or partially before feeding to a ViT based object detector. We use the semantic information provided by segmentation map and/or bounding box annotation to train a few layers of ViT to classify tokens to either foreground or background. Using 2 layers and 10 layers of BAViT, background and foreground tokens can be separated with 75% and 88% accuracy on VOC dataset and 71% and 80% accuracy on COCO dataset respectively. We show a 2 layer BAViT-small model as pre-processor to YOLOS can increase the throughput by 30% - 40% with a mAP drop of 3% without any sparse fine-tuning and 2% with sparse fine-tuning. Our approach is specifically targeted for Edge AI use cases.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "7 pages, 2 tables, 4 figures, FITML workshop@NeuRIPS 2024"
    },
    {
        "paper id": "2410.09335",
        "abstract url": "https://arxiv.org/abs/2410.09335",
        "title": "Rethinking Data Selection at Scale: Random Selection is Almost All You Need",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Supervised fine-tuning (SFT) is crucial for aligning Large Language Models (LLMs) with human instructions. The primary goal during SFT is to select a small yet representative subset of training data from the larger pool, such that fine-tuning with this subset achieves results comparable to or even exceeding those obtained using the entire dataset. However, most existing data selection techniques are designed for small-scale data pools, which fail to meet the demands of real-world SFT scenarios. In this paper, we replicated several self-scoring methods those that do not rely on external model assistance on two million scale datasets, and found that nearly all methods struggled to significantly outperform random selection when dealing with such large-scale data pools. Moreover, our comparisons suggest that, during SFT, diversity in data selection is more critical than simply focusing on high quality data. We also analyzed the limitations of several current approaches, explaining why they perform poorly on large-scale datasets and why they are unsuitable for such contexts. Finally, we found that filtering data by token length offers a stable and efficient method for improving results. This approach, particularly when training on long text data, proves highly beneficial for relatively weaker base models, such as Llama3.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09342",
        "abstract url": "https://arxiv.org/abs/2410.09342",
        "title": "LLM$\\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Enlarging the context window of large language models (LLMs) has become a crucial research area, particularly for applications involving extremely long texts. In this work, we propose a novel training-free framework for processing long texts, utilizing a divide-and-conquer strategy to achieve comprehensive document understanding. The proposed LLM$\\times$MapReduce framework splits the entire document into several chunks for LLMs to read and then aggregates the intermediate answers to produce the final output. The main challenge for divide-and-conquer long text processing frameworks lies in the risk of losing essential long-range information when splitting the document, which can lead the model to produce incomplete or incorrect answers based on the segmented texts. Disrupted long-range information can be classified into two categories: inter-chunk dependency and inter-chunk conflict. We design a structured information protocol to better cope with inter-chunk dependency and an in-context confidence calibration mechanism to resolve inter-chunk conflicts. Experimental results demonstrate that LLM$\\times$MapReduce can outperform representative open-source and commercial long-context LLMs, and is applicable to several different models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in Progress. Code: https://github.com/thunlp/LLMxMapReduce"
    },
    {
        "paper id": "2410.09343",
        "abstract url": "https://arxiv.org/abs/2410.09343",
        "title": "ELICIT: LLM Augmentation via External In-Context Capability",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Enhancing the adaptive capabilities of large language models is a critical pursuit in both research and application. Traditional fine-tuning methods require substantial data and computational resources, especially for enhancing specific capabilities, while in-context learning is limited by the need for appropriate demonstrations and efficient token usage. Inspired by the expression of in-context learned capabilities through task vectors and the concept of modularization, we propose \\alg, a framework consisting of two modules designed to effectively store and reuse task vectors to elicit the diverse capabilities of models without additional training or inference tokens. Our comprehensive experiments and analysis demonstrate that our pipeline is highly transferable across different input formats, tasks, and model architectures. ELICIT serves as a plug-and-play performance booster to enable adaptive elicitation of model capabilities. By externally storing and reusing vectors that represent in-context learned capabilities, \\alg not only demonstrates the potential to operate modular capabilities but also significantly enhances the performance, versatility, adaptability, and scalability of large language models. Our code will be publicly available at https://github.com/LINs-lab/ELICIT.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2410.09347",
        "abstract url": "https://arxiv.org/abs/2410.09347",
        "title": "Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Classifier-Free Guidance (CFG) is a critical technique for enhancing the sample quality of visual generative models. However, in autoregressive (AR) multi-modal generation, CFG introduces design inconsistencies between language and visual content, contradicting the design philosophy of unifying different modalities for visual AR. Motivated by language model alignment methods, we propose \\textit{Condition Contrastive Alignment} (CCA) to facilitate guidance-free AR visual generation with high performance and analyze its theoretical connection with guided sampling methods. Unlike guidance methods that alter the sampling process to achieve the ideal sampling distribution, CCA directly fine-tunes pretrained models to fit the same distribution target. Experimental results show that CCA can significantly enhance the guidance-free performance of all tested models with just one epoch of fine-tuning ($\\sim$ 1\\% of pretraining epochs) on the pretraining dataset, on par with guided sampling methods. This largely removes the need for guided sampling in AR visual generation and cuts the sampling cost by half. Moreover, by adjusting training parameters, CCA can achieve trade-offs between sample diversity and fidelity similar to CFG. This experimentally confirms the strong theoretical connection between language-targeted alignment and visual-targeted guidance methods, unifying two previously independent research fields. Code and model weights: https://github.com/thu-ml/CCA.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09352",
        "abstract url": "https://arxiv.org/abs/2410.09352",
        "title": "LogLM: From Task-based to Instruction-based Automated Log Analysis",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Automatic log analysis is essential for the efficient Operation and Maintenance (O&M) of software systems, providing critical insights into system behaviors. However, existing approaches mostly treat log analysis as training a model to perform an isolated task, using task-specific log-label pairs. These task-based approaches are inflexible in generalizing to complex scenarios, depend on task-specific training data, and cost significantly when deploying multiple models. In this paper, we propose an instruction-based training approach that transforms log-label pairs from multiple tasks and domains into a unified format of instruction-response pairs. Our trained model, LogLM, can follow complex user instructions and generalize better across different tasks, thereby increasing flexibility and reducing the dependence on task-specific training data. By integrating major log analysis tasks into a single model, our approach also relieves model deployment burden. Experimentally, LogLM outperforms existing approaches across five log analysis capabilities, and exhibits strong generalization abilities on complex instructions and unseen tasks.",
        "subjects": [
            "cs.SE",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09355",
        "abstract url": "https://arxiv.org/abs/2410.09355",
        "title": "On Divergence Measures for Training GFlowNets",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Generative Flow Networks (GFlowNets) are amortized inference models designed to sample from unnormalized distributions over composable objects, with applications in generative modeling for tasks in fields such as causal discovery, NLP, and drug discovery. Traditionally, the training procedure for GFlowNets seeks to minimize the expected log-squared difference between a proposal (forward policy) and a target (backward policy) distribution, which enforces certain flow-matching conditions. While this training procedure is closely related to variational inference (VI), directly attempting standard Kullback-Leibler (KL) divergence minimization can lead to proven biased and potentially high-variance estimators. Therefore, we first review four divergence measures, namely, Renyi-$\u03b1$'s, Tsallis-$\u03b1$'s, reverse and forward KL's, and design statistically efficient estimators for their stochastic gradients in the context of training GFlowNets. Then, we verify that properly minimizing these divergences yields a provably correct and empirically effective training scheme, often leading to significantly faster convergence than previously proposed optimization. To achieve this, we design control variates based on the REINFORCE leave-one-out and score-matching estimators to reduce the variance of the learning objectives' gradients. Our work contributes by narrowing the gap between GFlowNets training and generalized variational approximations, paving the way for algorithmic ideas informed by the divergence minimization viewpoint.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "Accepted at NeurIPS 2024, https://openreview.net/forum?id=N5H4z0Pzvn"
    },
    {
        "paper id": "2410.09360",
        "abstract url": "https://arxiv.org/abs/2410.09360",
        "title": "Towards the Synthesis of Non-speech Vocalizations",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "In this report, we focus on the unconditional generation of infant cry sounds using the DiffWave framework, which has shown great promise in generating high-quality audio from noise. We use two distinct datasets of infant cries: the Baby Chillanto and the deBarbaro cry dataset. These datasets are used to train the DiffWave model to generate new cry sounds that maintain high fidelity and diversity. The focus here is on DiffWave's capability to handle the unconditional generation task.",
        "subjects": [
            "cs.SD",
            "cs.LG",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.10892",
        "abstract url": "https://arxiv.org/abs/2410.10892",
        "title": "Replicable Uniformity Testing",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Uniformity testing is arguably one of the most fundamental distribution testing problems. Given sample access to an unknown distribution $\\mathbf{p}$ on $[n]$, one must decide if $\\mathbf{p}$ is uniform or $\\varepsilon$-far from uniform (in total variation distance). A long line of work established that uniformity testing has sample complexity $\u0398(\\sqrt{n}\\varepsilon^{-2})$. However, when the input distribution is neither uniform nor far from uniform, known algorithms may have highly non-replicable behavior. Consequently, if these algorithms are applied in scientific studies, they may lead to contradictory results that erode public trust in science. In this work, we revisit uniformity testing under the framework of algorithmic replicability [STOC '22], requiring the algorithm to be replicable under arbitrary distributions. While replicability typically incurs a $\u03c1^{-2}$ factor overhead in sample complexity, we obtain a replicable uniformity tester using only $\\tilde{O}(\\sqrt{n} \\varepsilon^{-2} \u03c1^{-1})$ samples. To our knowledge, this is the first replicable learning algorithm with (nearly) linear dependence on $\u03c1$. Lastly, we consider a class of ``symmetric\" algorithms [FOCS '00] whose outputs are invariant under relabeling of the domain $[n]$, which includes all existing uniformity testers (including ours). For this natural class of algorithms, we prove a nearly matching sample complexity lower bound for replicable uniformity testing.",
        "subjects": [
            "stat.ML",
            "cs.DS",
            "cs.LG"
        ],
        "comment": "To appear in NeurIPS 2024"
    },
    {
        "paper id": "2410.11883",
        "abstract url": "https://arxiv.org/abs/2410.11883",
        "title": "Simulation-based inference with scattering representations: scattering is all you need",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We demonstrate the first successful use of scattering representations without further compression for simulation-based inference (SBI) with images (i.e. field-level), illustrated with a cosmological case study. Scattering representations provide a highly effective representational space for subsequent learning tasks, although the higher dimensional compressed space introduces challenges. We overcome these through spatial averaging, coupled with more expressive density estimators. Compared to alternative methods, such an approach does not require additional simulations for either training or computing derivatives, is interpretable, and resilient to covariate shift. As expected, we show that a scattering only approach extracts more information than traditional second order summary statistics.",
        "subjects": [
            "cs.LG",
            "astro-ph.CO",
            "astro-ph.IM",
            "stat.ML"
        ],
        "comment": "9 pages, 2 figures, accepted by NeurIPS workshop on Machine Learning and the Physical Sciences"
    },
    {
        "paper id": "2410.08537",
        "abstract url": "https://arxiv.org/abs/2410.08537",
        "title": "Robust Offline Policy Learning with Observational Data from Multiple Sources",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider the problem of using observational bandit feedback data from multiple heterogeneous data sources to learn a personalized decision policy that robustly generalizes across diverse target settings. To achieve this, we propose a minimax regret optimization objective to ensure uniformly low regret under general mixtures of the source distributions. We develop a policy learning algorithm tailored to this objective, combining doubly robust offline policy evaluation techniques and no-regret learning algorithms for minimax optimization. Our regret analysis shows that this approach achieves the minimal worst-case mixture regret up to a moderated vanishing rate of the total data across all sources. Our analysis, extensions, and experimental results demonstrate the benefits of this approach for learning robust decision policies from multiple data sources.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2305.12407"
    },
    {
        "paper id": "2410.08549",
        "abstract url": "https://arxiv.org/abs/2410.08549",
        "title": "Score Neural Operator: A Generative Model for Learning and Generalizing Across Multiple Probability Distributions",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Most existing generative models are limited to learning a single probability distribution from the training data and cannot generalize to novel distributions for unseen data. An architecture that can generate samples from both trained datasets and unseen probability distributions would mark a significant breakthrough. Recently, score-based generative models have gained considerable attention for their comprehensive mode coverage and high-quality image synthesis, as they effectively learn an operator that maps a probability distribution to its corresponding score function. In this work, we introduce the $\\emph{Score Neural Operator}$, which learns the mapping from multiple probability distributions to their score functions within a unified framework. We employ latent space techniques to facilitate the training of score matching, which tends to over-fit in the original image pixel space, thereby enhancing sample generation quality. Our trained Score Neural Operator demonstrates the ability to predict score functions of probability measures beyond the training space and exhibits strong generalization performance in both 2-dimensional Gaussian Mixture Models and 1024-dimensional MNIST double-digit datasets. Importantly, our approach offers significant potential for few-shot learning applications, where a single image from a new distribution can be leveraged to generate multiple distinct images from that distribution.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08555",
        "abstract url": "https://arxiv.org/abs/2410.08555",
        "title": "Design of Secure, Privacy-focused, and Accessible E-Payment Applications for Older Adults",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "E-payments are essential for transactional convenience in today's digital economy and are becoming increasingly important for older adults, emphasizing the need for enhanced security, privacy, and usability. To address this, we conducted a survey-based study with 400 older adults aged 65 and above to evaluate a high-fidelity prototype of an e-payment mobile application, which included features such as multi-factor authentication (MFA) and QR code-based recipient addition. Based on our findings, we developed a tailored \\b{eta} version of the application to meet the specific needs of this demographic. Notably, approximately 91% of participants preferred traditional knowledge-based and single-mode authentication compared to expert-recommended MFA. We concluded by providing recommendations aimed at developing inclusive e-payment solutions that address the security, privacy, and usability requirements of older adults.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08559",
        "abstract url": "https://arxiv.org/abs/2410.08559",
        "title": "Learning General Representation of 12-Lead Electrocardiogram with a Joint-Embedding Predictive architecture",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We propose a self-supervised learning method for 12-lead Electrocardiogram (ECG) analysis, named ECG Joint Embedding Predictive Architecture (ECG-JEPA). ECG-JEPA employs a masking strategy to learn semantic representations of ECG data. Unlike existing methods, ECG-JEPA predicts at the hidden representation level rather than reconstructing raw data. This approach offers several advantages in the ECG domain: (1) it avoids producing unnecessary details, such as noise, which is common in standard ECG; and (2) it addresses the limitations of na\u00efve L2 loss between raw signals. Another key contribution is the introduction of a special masked attention tailored for 12-lead ECG data, Cross-Pattern Attention (CroPA). CroPA enables the model to effectively capture inter-patch relationships. Additionally, ECG-JEPA is highly scalable, allowing efficient training on large datasets. Our code is openly available https://github.com/sehunfromdaegu/ECG_JEPA.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08562",
        "abstract url": "https://arxiv.org/abs/2410.08562",
        "title": "Adaptive Constraint Integration for Simultaneously Optimizing Crystal Structures with Multiple Targeted Properties",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In materials science, finding crystal structures that have targeted properties is crucial. While recent methodologies such as Bayesian optimization and deep generative models have made some advances on this issue, these methods often face difficulties in adaptively incorporating various constraints, such as electrical neutrality and targeted properties optimization, while keeping the desired specific crystal structure. To address these challenges, we have developed the Simultaneous Multi-property Optimization using Adaptive Crystal Synthesizer (SMOACS), which utilizes state-of-the-art property prediction models and their gradients to directly optimize input crystal structures for targeted properties simultaneously. SMOACS enables the integration of adaptive constraints into the optimization process without necessitating model retraining. Thanks to this feature, SMOACS has succeeded in simultaneously optimizing targeted properties while maintaining perovskite structures, even with models trained on diverse crystal types. We have demonstrated the band gap optimization while meeting a challenging constraint, that is, maintaining electrical neutrality in large atomic configurations up to 135 atom sites, where the verification of the electrical neutrality is challenging. The properties of the most promising materials have been confirmed by density functional theory calculations.",
        "subjects": [
            "cond-mat.mtrl-sci",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08568",
        "abstract url": "https://arxiv.org/abs/2410.08568",
        "title": "GPR Full-Waveform Inversion through Adaptive Filtering of Model Parameters and Gradients Using CNN",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "GPR full-waveform inversion optimizes the subsurface property model iteratively to match the entire waveform information. However, the model gradients derived from wavefield continuation often contain errors, such as ghost values and excessively large values at transmitter and receiver points. Furthermore, models updated based on these gradients frequently exhibit unclear characterization of anomalous bodies or false anomalies, making it challenging to obtain accurate inversion results. To address these issues, we introduced a novel full-waveform inversion (FWI) framework that incorporates an embedded convolutional neural network (CNN) to adaptively filter model parameters and gradients. Specifically, we embedded the CNN module before the forward modeling process and ensured the entire FWI process remains differentiable. This design leverages the auto-grad tool of the deep learning library, allowing model values to pass through the CNN module during forward computation and model gradients to pass through the CNN module during backpropagation. Experiments have shown that filtering the model parameters during forward computation and the model gradients during backpropagation can ultimately yield high-quality inversion results.",
        "subjects": [
            "physics.geo-ph",
            "cs.LG"
        ],
        "comment": "16 pages, 6 figures"
    },
    {
        "paper id": "2410.08578",
        "abstract url": "https://arxiv.org/abs/2410.08578",
        "title": "Logarithmic Regret for Unconstrained Submodular Maximization Stochastic Bandit",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We address the online unconstrained submodular maximization problem (Online USM), in a setting with stochastic bandit feedback. In this framework, a decision-maker receives noisy rewards from a nonmonotone submodular function, taking values in a known bounded interval. This paper proposes Double-Greedy - Explore-then-Commit (DG-ETC), adapting the Double-Greedy approach from the offline and online full-information settings. DG-ETC satisfies a O(d log(dT)) problemdependent upper bound for the 1/2-approximate pseudo-regret, as well as a O(dT^{2/3}log(dT)^{1/3}) problem-free one at the same time, outperforming existing approaches. To that end, we introduce a notion of hardness for submodular functions, characterizing how difficult it is to maximize them with this type of strategy.",
        "subjects": [
            "cs.LG",
            "math.CO",
            "math.OC",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08581",
        "abstract url": "https://arxiv.org/abs/2410.08581",
        "title": "Integrating AI for Enhanced Feedback in Translation Revision- A Mixed-Methods Investigation of Student Engagement",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Despite the well-established importance of feedback in education, the application of Artificial Intelligence (AI)-generated feedback, particularly from language models like ChatGPT, remains understudied in translation education. This study investigates the engagement of master's students in translation with ChatGPT-generated feedback during their revision process. A mixed-methods approach, combining a translation-and-revision experiment with quantitative and qualitative analyses, was employed to examine the feedback, translations pre-and post-revision, the revision process, and student reflections. The results reveal complex interrelations among cognitive, affective, and behavioural dimensions influencing students' engagement with AI feedback and their subsequent revisions. Specifically, the findings indicate that students invested considerable cognitive effort in the revision process, despite finding the feedback comprehensible. Additionally, they exhibited moderate affective satisfaction with the feedback model. Behaviourally, their actions were largely influenced by cognitive and affective factors, although some inconsistencies were observed. This research provides novel insights into the potential applications of AI-generated feedback in translation teachingand opens avenues for further investigation into the integration of AI tools in language teaching settings.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08589",
        "abstract url": "https://arxiv.org/abs/2410.08589",
        "title": "Retraining-Free Merging of Sparse Mixture-of-Experts via Hierarchical Clustering",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Sparse Mixture-of-Experts (SMoE) models represent a significant breakthrough in large language model development. These models enable performance improvements without a proportional increase in inference costs. By selectively activating a small set of parameters during task execution, SMoEs enhance model capacity. However, their deployment remains challenging due to the substantial memory footprint required to accommodate the growing number of experts. This constraint renders them less feasible in environments with limited hardware resources. To address this challenge, we propose Hierarchical Clustering for Sparsely activated Mixture of Experts (HC-SMoE), a task-agnostic expert merging framework that reduces SMoE model parameters without retraining. Unlike previous methods, HC-SMoE employs hierarchical clustering based on expert outputs. This approach ensures that the merging process remains unaffected by routing decisions. The output-based clustering strategy captures functional similarities between experts, offering an adaptable solution for models with numerous experts. We validate our approach through extensive experiments on eight zero-shot language tasks and demonstrate its effectiveness in large-scale SMoE models such as Qwen and Mixtral. Our comprehensive results demonstrate that HC-SMoE consistently achieves strong performance, which highlights its potential for real-world deployment.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Code: https://github.com/wazenmai/HC-SMoE"
    },
    {
        "paper id": "2410.08597",
        "abstract url": "https://arxiv.org/abs/2410.08597",
        "title": "What killed the cat? Towards a logical formalization of curiosity (and suspense, and surprise) in narratives",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "We provide a unified framework in which the three emotions at the heart of narrative tension (curiosity, suspense and surprise) are formalized. This framework is built on nonmonotonic reasoning which allows us to compactly represent the default behavior of the world and to simulate the affective evolution of an agent receiving a story. After formalizing the notions of awareness, curiosity, surprise and suspense, we explore the properties induced by our definitions and study the computational complexity of detecting them. We finally propose means to evaluate these emotions' intensity for a given agent listening to a story.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08604",
        "abstract url": "https://arxiv.org/abs/2410.08604",
        "title": "MergePrint: Robust Fingerprinting against Merging Large Language Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "As the cost of training large language models (LLMs) rises, protecting their intellectual property has become increasingly critical. Model merging, which integrates multiple expert models into a single model capable of performing multiple tasks, presents a growing risk of unauthorized and malicious usage. While fingerprinting techniques have been studied for asserting model ownership, existing methods have primarily focused on fine-tuning, leaving model merging underexplored. To address this gap, we propose a novel fingerprinting method MergePrint that embeds robust fingerprints designed to preserve ownership claims even after model merging. By optimizing against a pseudo-merged model, which simulates post-merged model weights, MergePrint generates fingerprints that remain detectable after merging. Additionally, we optimize the fingerprint inputs to minimize performance degradation, enabling verification through specific outputs from targeted inputs. This approach provides a practical fingerprinting strategy for asserting ownership in cases of misappropriation through model merging.",
        "subjects": [
            "cs.CR",
            "cs.LG"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2410.08635",
        "abstract url": "https://arxiv.org/abs/2410.08635",
        "title": "Efficient line search for optimizing Area Under the ROC Curve in gradient descent",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Receiver Operating Characteristic (ROC) curves are useful for evaluation in binary classification and changepoint detection, but difficult to use for learning since the Area Under the Curve (AUC) is piecewise constant (gradient zero almost everywhere). Recently the Area Under Min (AUM) of false positive and false negative rates has been proposed as a differentiable surrogate for AUC. In this paper we study the piecewise linear/constant nature of the AUM/AUC, and propose new efficient path-following algorithms for choosing the learning rate which is optimal for each step of gradient descent (line search), when optimizing a linear model. Remarkably, our proposed line search algorithm has the same log-linear asymptotic time complexity as gradient descent with constant step size, but it computes a complete representation of the AUM/AUC as a function of step size. In our empirical study of binary classification problems, we verify that our proposed algorithm is fast and exact; in changepoint detection problems we show that the proposed algorithm is just as accurate as grid search, but faster.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08643",
        "abstract url": "https://arxiv.org/abs/2410.08643",
        "title": "SOAK: Same/Other/All K-fold cross-validation for estimating similarity of patterns in data subsets",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In many real-world applications of machine learning, we are interested to know if it is possible to train on the data that we have gathered so far, and obtain accurate predictions on a new test data subset that is qualitatively different in some respect (time period, geographic region, etc). Another question is whether data subsets are similar enough so that it is beneficial to combine subsets during model training. We propose SOAK, Same/Other/All K-fold cross-validation, a new method which can be used to answer both questions. SOAK systematically compares models which are trained on different subsets of data, and then used for prediction on a fixed test subset, to estimate the similarity of learnable/predictable patterns in data subsets. We show results of using SOAK on six new real data sets (with geographic/temporal subsets, to check if predictions are accurate on new subsets), 3 image pair data sets (subsets are different image types, to check that we get smaller prediction error on similar images), and 11 benchmark data sets with predefined train/test splits (to check similarity of predefined splits).",
        "subjects": [
            "stat.ML",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08654",
        "abstract url": "https://arxiv.org/abs/2410.08654",
        "title": "Finite Sample Complexity Analysis of Binary Segmentation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Binary segmentation is the classic greedy algorithm which recursively splits a sequential data set by optimizing some loss or likelihood function. Binary segmentation is widely used for changepoint detection in data sets measured over space or time, and as a sub-routine for decision tree learning. In theory it should be extremely fast for $N$ data and $K$ splits, $O(N K)$ in the worst case, and $O(N \\log K)$ in the best case. In this paper we describe new methods for analyzing the time and space complexity of binary segmentation for a given finite $N$, $K$, and minimum segment length parameter. First, we describe algorithms that can be used to compute the best and worst case number of splits the algorithm must consider. Second, we describe synthetic data that achieve the best and worst case and which can be used to test for correct implementation of the algorithm. Finally, we provide an empirical analysis of real data which suggests that binary segmentation is often close to optimal speed in practice.",
        "subjects": [
            "cs.LG",
            "stat.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08659",
        "abstract url": "https://arxiv.org/abs/2410.08659",
        "title": "Carefully Structured Compression: Efficiently Managing StarCraft II Data",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Creation and storage of datasets are often overlooked input costs in machine learning, as many datasets are simple image label pairs or plain text. However, datasets with more complex structures, such as those from the real time strategy game StarCraft II, require more deliberate thought and strategy to reduce cost of ownership. We introduce a serialization framework for StarCraft II that reduces the cost of dataset creation and storage, as well as improving usage ergonomics. We benchmark against the most comparable existing dataset from \\textit{AlphaStar-Unplugged} and highlight the benefit of our framework in terms of both the cost of creation and storage. We use our dataset to train deep learning models that exceed the performance of comparable models trained on other datasets. The dataset conversion and usage framework introduced is open source and can be used as a framework for datasets with similar characteristics such as digital twin simulations. Pre-converted StarCraft II tournament data is also available online.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "14 pages, 7 figures"
    },
    {
        "paper id": "2410.08666",
        "abstract url": "https://arxiv.org/abs/2410.08666",
        "title": "DeltaDQ: Ultra-High Delta Compression for Fine-Tuned LLMs via Group-wise Dropout and Separate Quantization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Large language models achieve exceptional performance on various downstream tasks through supervised fine-tuning. However, the diversity of downstream tasks and practical requirements makes deploying multiple full-parameter fine-tuned models challenging. Current methods that compress the delta weight struggle to achieve ultra-high compression, failing to minimize the deployment overhead. To address the above issue, we propose a novel distribution-driven delta compression framework DeltaDQ, which utilizes Group-wise Dropout and Separate Quantization to achieve ultra-high compression for the delta weight. We have observed that the matrix-computed intermediate results for the delta weight exhibit extremely small variance and min-max range characteristics, referred to as Balanced Intermediate Results. Exploiting this phenomenon, we introduce Group-wise Dropout to perform dropout on the delta weight using an optimal group size. Furthermore, using Separate Quantization, sparse weights are quantized and decomposed to achieve a lower bit. Experimental results show that DeltaDQ achieves 16x compression with improved accuracy compared to baselines for WizardMath and WizardCoder models across different parameter scales. Moreover, DeltaDQ demonstrates the ability for ultra-high compression ratio, achieving 128x compression for the WizardMath-7B model and 512x compression for the WizardMath-70B model.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08672",
        "abstract url": "https://arxiv.org/abs/2410.08672",
        "title": "Explorative pedestrian mobility GPS data from a citizen science experiment in a neighbourhood",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Pedestrian GPS data are key to a better understanding of micro-mobility and micro-behaviour within a neighbourhood. These data can bring new insights into walkability and livability in the context of urban sustainability. However, pedestrian open data are scarce and often lack a context for their transformation into actionable knowledge in a neighbourhood. Citizen science and public involvement practices are powerful instruments for obtaining these data and take a community-centred placemaking approach. The study shares some 3000 GPS recordings corresponding to 19 unique trajectories made and recorded by groups of participants from three distinct communities in a relatively small neighbourhood. The groups explored the neighbourhood through a number of tasks and chose different places to stop and perform various social and festive activities. The study shares not only raw data but also processed records with specific filtering and processing to facilitate and accelerate data usage. Citizen science practices and the data-collection protocols involved are reported in order to offer a complete perspective of the research undertaken jointly with an assessment of how community-centred placemaking and operative mapping are incorporated into local urban transformation actions.",
        "subjects": [
            "physics.soc-ph",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08681",
        "abstract url": "https://arxiv.org/abs/2410.08681",
        "title": "Efficiently Scanning and Resampling Spatio-Temporal Tasks with Irregular Observations",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Various works have aimed at combining the inference efficiency of recurrent models and training parallelism of multi-head attention for sequence modeling. However, most of these works focus on tasks with fixed-dimension observation spaces, such as individual tokens in language modeling or pixels in image completion. To handle an observation space of varying size, we propose a novel algorithm that alternates between cross-attention between a 2D latent state and observation, and a discounted cumulative sum over the sequence dimension to efficiently accumulate historical information. We find this resampling cycle is critical for performance. To evaluate efficient sequence modeling in this domain, we introduce two multi-agent intention tasks: simulated agents chasing bouncing particles and micromanagement analysis in professional StarCraft II games. Our algorithm achieves comparable accuracy with a lower parameter count, faster training and inference compared to existing methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "11 pages, 10 figures"
    },
    {
        "paper id": "2410.08710",
        "abstract url": "https://arxiv.org/abs/2410.08710",
        "title": "Preferential Normalizing Flows",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Eliciting a high-dimensional probability distribution from an expert via noisy judgments is notoriously challenging, yet useful for many applications, such as prior elicitation and reward modeling. We introduce a method for eliciting the expert's belief density as a normalizing flow based solely on preferential questions such as comparing or ranking alternatives. This allows eliciting in principle arbitrarily flexible densities, but flow estimation is susceptible to the challenge of collapsing or diverging probability mass that makes it difficult in practice. We tackle this problem by introducing a novel functional prior for the flow, motivated by a decision-theoretic argument, and show empirically that the belief density can be inferred as the function-space maximum a posteriori estimate. We demonstrate our method by eliciting multivariate belief densities of simulated experts, including the prior belief of a general-purpose large language model over a real-world dataset.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "29 pages"
    },
    {
        "paper id": "2410.08751",
        "abstract url": "https://arxiv.org/abs/2410.08751",
        "title": "Zero-Shot Offline Imitation Learning via Optimal Transport",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Zero-shot imitation learning algorithms hold the promise of reproducing unseen behavior from as little as a single demonstration at test time. Existing practical approaches view the expert demonstration as a sequence of goals, enabling imitation with a high-level goal selector, and a low-level goal-conditioned policy. However, this framework can suffer from myopic behavior: the agent's immediate actions towards achieving individual goals may undermine long-term objectives. We introduce a novel method that mitigates this issue by directly optimizing the occupancy matching objective that is intrinsic to imitation learning. We propose to lift a goal-conditioned value function to a distance between occupancies, which are in turn approximated via a learned world model. The resulting method can learn from offline, suboptimal data, and is capable of non-myopic, zero-shot imitation, as we demonstrate in complex, continuous benchmarks.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08783",
        "abstract url": "https://arxiv.org/abs/2410.08783",
        "title": "Integrating Expert Judgment and Algorithmic Decision Making: An Indistinguishability Framework",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "We introduce a novel framework for human-AI collaboration in prediction and decision tasks. Our approach leverages human judgment to distinguish inputs which are algorithmically indistinguishable, or \"look the same\" to any feasible predictive algorithm. We argue that this framing clarifies the problem of human-AI collaboration in prediction and decision tasks, as experts often form judgments by drawing on information which is not encoded in an algorithm's training data. Algorithmic indistinguishability yields a natural test for assessing whether experts incorporate this kind of \"side information\", and further provides a simple but principled method for selectively incorporating human feedback into algorithmic predictions. We show that this method provably improves the performance of any feasible algorithmic predictor and precisely quantify this improvement. We demonstrate the utility of our framework in a case study of emergency room triage decisions, where we find that although algorithmic risk scores are highly competitive with physicians, there is strong evidence that physician judgments provide signal which could not be replicated by any predictive algorithm. This insight yields a range of natural decision rules which leverage the complementary strengths of human experts and predictive algorithms.",
        "subjects": [
            "cs.LG",
            "cs.CY",
            "cs.HC",
            "stat.ML"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2402.00793"
    },
    {
        "paper id": "2410.08796",
        "abstract url": "https://arxiv.org/abs/2410.08796",
        "title": "Calibrated Computation-Aware Gaussian Processes",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Gaussian processes are notorious for scaling cubically with the size of the training set, preventing application to very large regression problems. Computation-aware Gaussian processes (CAGPs) tackle this scaling issue by exploiting probabilistic linear solvers to reduce complexity, widening the posterior with additional computational uncertainty due to reduced computation. However, the most commonly used CAGP framework results in (sometimes dramatically) conservative uncertainty quantification, making the posterior unrealistic in practice. In this work, we prove that if the utilised probabilistic linear solver is calibrated, in a rigorous statistical sense, then so too is the induced CAGP. We thus propose a new CAGP framework, CAGP-GS, based on using Gauss-Seidel iterations for the underlying probabilistic linear solver. CAGP-GS performs favourably compared to existing approaches when the test set is low-dimensional and few iterations are performed. We test the calibratedness on a synthetic problem, and compare the performance to existing approaches on a large-scale global temperature regression problem.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08806",
        "abstract url": "https://arxiv.org/abs/2410.08806",
        "title": "Don't Transform the Code, Code the Transforms: Towards Precise Code Rewriting using LLMs",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Tools for rewriting, refactoring and optimizing code should be fast and correct. Large language models (LLMs), by their nature, possess neither of these qualities. Yet, there remains tremendous opportunity in using LLMs to improve code. We explore the use of LLMs not to transform code, but to code transforms. We propose a chain-of-thought approach to synthesizing code transformations from a small number of input/output code examples that incorporates execution and feedback. Unlike the direct rewrite approach, LLM-generated transformations are easy to inspect, debug, and validate. The logic of the rewrite is explicitly coded and easy to adapt. The compute required to run code transformations is minute compared to that of LLM rewriting. We test our approach on 16 Python code transformations and find that LLM- generated transforms are perfectly precise for 7 of them and less imprecise than direct LLM rewriting on the others. We hope to encourage further research to improving the precision of LLM code rewriting.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08829",
        "abstract url": "https://arxiv.org/abs/2410.08829",
        "title": "Unveiling Molecular Secrets: An LLM-Augmented Linear Model for Explainable and Calibratable Molecular Property Prediction",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Explainable molecular property prediction is essential for various scientific fields, such as drug discovery and material science. Despite delivering intrinsic explainability, linear models struggle with capturing complex, non-linear patterns. Large language models (LLMs), on the other hand, yield accurate predictions through powerful inference capabilities yet fail to provide chemically meaningful explanations for their predictions. This work proposes a novel framework, called MoleX, which leverages LLM knowledge to build a simple yet powerful linear model for accurate molecular property prediction with faithful explanations. The core of MoleX is to model complicated molecular structure-property relationships using a simple linear model, augmented by LLM knowledge and a crafted calibration strategy. Specifically, to extract the maximum amount of task-relevant knowledge from LLM embeddings, we employ information bottleneck-inspired fine-tuning and sparsity-inducing dimensionality reduction. These informative embeddings are then used to fit a linear model for explainable inference. Moreover, we introduce residual calibration to address prediction errors stemming from linear models' insufficient expressiveness of complex LLM embeddings, thus recovering the LLM's predictive power and boosting overall accuracy. Theoretically, we provide a mathematical foundation to justify MoleX's explainability. Extensive experiments demonstrate that MoleX outperforms existing methods in molecular property prediction, establishing a new milestone in predictive performance, explainability, and efficiency. In particular, MoleX enables CPU inference and accelerates large-scale dataset processing, achieving comparable performance 300x faster with 100,000 fewer parameters than LLMs. Additionally, the calibration improves model performance by up to 12.7% without compromising explainability.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08835",
        "abstract url": "https://arxiv.org/abs/2410.08835",
        "title": "A Block-Based Testing Framework for Scratch",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Block-based programming environments like Scratch are widely used in introductory programming courses. They facilitate learning pivotal programming concepts by eliminating syntactical errors, but logical errors that break the desired program behaviour are nevertheless possible. Finding such errors requires testing, i.e., running the program and checking its behaviour. In many programming environments, this step can be automated by providing executable tests as code; in Scratch, testing can only be done manually by invoking events through user input and observing the rendered stage. While this is arguably sufficient for learners, the lack of automated testing may be inhibitive for teachers wishing to provide feedback on their students' solutions. In order to address this issue, we introduce a new category of blocks in Scratch that enables the creation of automated tests. With these blocks, students and teachers alike can create tests and receive feedback directly within the Scratch environment using familiar block-based programming logic. To facilitate the creation and to enable batch processing sets of student solutions, we extend the Scratch user interface with an accompanying test interface. We evaluated this testing framework with 28 teachers who created tests for a popular Scratch game and subsequently used these tests to assess and provide feedback on student implementations. An overall accuracy of 0.93 of teachers' tests compared to manually evaluating the functionality of 21 student solutions demonstrates that teachers are able to create and effectively use tests. A subsequent survey confirms that teachers consider the block-based test approach useful.",
        "subjects": [
            "cs.SE",
            "cs.CY"
        ],
        "comment": "To appear at the 24th Koli Calling International Conference on Computing Education Research"
    },
    {
        "paper id": "2410.08840",
        "abstract url": "https://arxiv.org/abs/2410.08840",
        "title": "Learning Interaction-aware 3D Gaussian Splatting for One-shot Hand Avatars",
        "rating": "0.5",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In this paper, we propose to create animatable avatars for interacting hands with 3D Gaussian Splatting (GS) and single-image inputs. Existing GS-based methods designed for single subjects often yield unsatisfactory results due to limited input views, various hand poses, and occlusions. To address these challenges, we introduce a novel two-stage interaction-aware GS framework that exploits cross-subject hand priors and refines 3D Gaussians in interacting areas. Particularly, to handle hand variations, we disentangle the 3D presentation of hands into optimization-based identity maps and learning-based latent geometric features and neural texture maps. Learning-based features are captured by trained networks to provide reliable priors for poses, shapes, and textures, while optimization-based identity maps enable efficient one-shot fitting of out-of-distribution hands. Furthermore, we devise an interaction-aware attention module and a self-adaptive Gaussian refinement module. These modules enhance image rendering quality in areas with intra- and inter-hand interactions, overcoming the limitations of existing GS-based methods. Our proposed method is validated via extensive experiments on the large-scale InterHand2.6M dataset, and it significantly improves the state-of-the-art performance in image quality. Project Page: \\url{https://github.com/XuanHuang0/GuassianHand}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to NeurIPS 2024"
    },
    {
        "paper id": "2410.08841",
        "abstract url": "https://arxiv.org/abs/2410.08841",
        "title": "Public Transport Network Design for Equality of Accessibility via Message Passing Neural Networks and Reinforcement Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Designing Public Transport (PT) networks able to satisfy mobility needs of people is essential to reduce the number of individual vehicles on the road, and thus pollution and congestion. Urban sustainability is thus tightly coupled to an efficient PT. Current approaches on Transport Network Design (TND) generally aim to optimize generalized cost, i.e., a unique number including operator and users' costs. Since we intend quality of PT as the capability of satisfying mobility needs, we focus instead on PT accessibility, i.e., the ease of reaching surrounding points of interest via PT. PT accessibility is generally unequally distributed in urban regions: suburbs generally suffer from poor PT accessibility, which condemns residents therein to be dependent on their private cars. We thus tackle the problem of designing bus lines so as to minimize the inequality in the geographical distribution of accessibility. We combine state-of-the-art Message Passing Neural Networks (MPNN) and Reinforcement Learning. We show the efficacy of our method against metaheuristics (classically used in TND) in a use case representing in simplified terms the city of Montreal.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "14 pages"
    },
    {
        "paper id": "2410.08850",
        "abstract url": "https://arxiv.org/abs/2410.08850",
        "title": "Deep Learning Algorithms for Mean Field Optimal Stopping in Finite Space and Discrete Time",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Optimal stopping is a fundamental problem in optimization that has found applications in risk management, finance, economics, and recently in the fields of computer science. We extend the standard framework to a multi-agent setting, named multi-agent optimal stopping (MAOS), where a group of agents cooperatively solves finite-space, discrete-time optimal stopping problems. Solving the finite-agent case is computationally prohibitive when the number of agents is very large, so this work studies the mean field optimal stopping (MFOS) problem, obtained as the number of agents approaches infinity. We prove that MFOS provides a good approximate solution to MAOS. We also prove a dynamic programming principle (DPP), based on the theory of mean field control. We then propose two deep learning methods: one simulates full trajectories to learn optimal decisions, whereas the other leverages DPP with backward induction; both methods train neural networks for the optimal stopping decisions. We demonstrate the effectiveness of these approaches through numerical experiments on 6 different problems in spatial dimension up to 300. To the best of our knowledge, this is the first work to study MFOS in finite space and discrete time, and to propose efficient and scalable computational methods for this type of problem.",
        "subjects": [
            "math.OC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08855",
        "abstract url": "https://arxiv.org/abs/2410.08855",
        "title": "MATCH: Model-Aware TVM-based Compilation for Heterogeneous Edge Devices",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Streamlining the deployment of Deep Neural Networks (DNNs) on heterogeneous edge platforms, coupling within the same micro-controller unit (MCU) instruction processors and hardware accelerators for tensor computations, is becoming one of the crucial challenges of the TinyML field. The best-performing DNN compilation toolchains are usually deeply customized for a single MCU family, and porting to a different heterogeneous MCU family implies labor-intensive re-development of almost the entire compiler. On the opposite side, retargetable toolchains, such as TVM, fail to exploit the capabilities of custom accelerators, resulting in the generation of general but unoptimized code. To overcome this duality, we introduce MATCH, a novel TVM-based DNN deployment framework designed for easy agile retargeting across different MCU processors and accelerators, thanks to a customizable model-based hardware abstraction. We show that a general and retargetable mapping framework enhanced with hardware cost models can compete with and even outperform custom toolchains on diverse targets while only needing the definition of an abstract hardware model and a SoC-specific API. We tested MATCH on two state-of-the-art heterogeneous MCUs, GAP9 and DIANA. On the four DNN models of the MLPerf Tiny suite MATCH reduces inference latency by up to 60.88 times on DIANA, compared to using the plain TVM, thanks to the exploitation of the on-board HW accelerator. Compared to HTVM, a fully customized toolchain for DIANA, we still reduce the latency by 16.94%. On GAP9, using the same benchmarks, we improve the latency by 2.15 times compared to the dedicated DORY compiler, thanks to our heterogeneous DNN mapping approach that synergically exploits the DNN accelerator and the eight-cores cluster available on board.",
        "subjects": [
            "cs.DC",
            "cs.AI"
        ],
        "comment": "13 pages, 11 figures, 4 tables"
    },
    {
        "paper id": "2410.08868",
        "abstract url": "https://arxiv.org/abs/2410.08868",
        "title": "Improved Sample Complexity for Global Convergence of Actor-Critic Algorithms",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we establish the global convergence of the actor-critic algorithm with a significantly improved sample complexity of $O(\u03b5^{-3})$, advancing beyond the existing local convergence results. Previous works provide local convergence guarantees with a sample complexity of $O(\u03b5^{-2})$ for bounding the squared gradient of the return, which translates to a global sample complexity of $O(\u03b5^{-4})$ using the gradient domination lemma. In contrast to traditional methods that employ decreasing step sizes for both the actor and critic, we demonstrate that a constant step size for the critic is sufficient to ensure convergence in expectation. This key insight reveals that using a decreasing step size for the actor alone is sufficient to handle the noise for both the actor and critic. Our findings provide theoretical support for the practical success of many algorithms that rely on constant step sizes.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08870",
        "abstract url": "https://arxiv.org/abs/2410.08870",
        "title": "Can we hop in general? A discussion of benchmark selection and design using the Hopper environment",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Empirical, benchmark-driven testing is a fundamental paradigm in the current RL community. While using off-the-shelf benchmarks in reinforcement learning (RL) research is a common practice, this choice is rarely discussed. Benchmark choices are often done based on intuitive ideas like \"legged robots\" or \"visual observations\". In this paper, we argue that benchmarking in RL needs to be treated as a scientific discipline itself. To illustrate our point, we present a case study on different variants of the Hopper environment to show that the selection of standard benchmarking suites can drastically change how we judge performance of algorithms. The field does not have a cohesive notion of what the different Hopper environments are representative - they do not even seem to be representative of each other. Our experimental results suggests a larger issue in the deep RL literature: benchmark choices are neither commonly justified, nor does there exist a language that could be used to justify the selection of certain environments. This paper concludes with a discussion of the requirements for proper discussion and evaluations of benchmarks and recommends steps to start a dialogue towards this goal.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08874",
        "abstract url": "https://arxiv.org/abs/2410.08874",
        "title": "Experiments with Choice in Dependently-Typed Higher-Order Logic",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Recently an extension to higher-order logic -- called DHOL -- was introduced, enriching the language with dependent types, and creating a powerful extensional type theory. In this paper we propose two ways how choice can be added to DHOL. We extend the DHOL term structure by Hilbert's indefinite choice operator $\u03b5$, define a translation of the choice terms to HOL choice that extends the existing translation from DHOL to HOL and show that the extension of the translation is complete and give an argument for soundness. We finally evaluate the extended translation on a set of dependent HOL problems that require choice.",
        "subjects": [
            "cs.LO",
            "cs.AI"
        ],
        "comment": "10 pages incl. references; published in the proceedings of LPAR25"
    },
    {
        "paper id": "2410.08896",
        "abstract url": "https://arxiv.org/abs/2410.08896",
        "title": "MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Building deep reinforcement learning (RL) agents that find a good policy with few samples has proven notoriously challenging. To achieve sample efficiency, recent work has explored updating neural networks with large numbers of gradient steps for every new sample. While such high update-to-data (UTD) ratios have shown strong empirical performance, they also introduce instability to the training process. Previous approaches need to rely on periodic neural network parameter resets to address this instability, but restarting the training process is infeasible in many real-world applications and requires tuning the resetting interval. In this paper, we focus on one of the core difficulties of stable training with limited samples: the inability of learned value functions to generalize to unobserved on-policy actions. We mitigate this issue directly by augmenting the off-policy RL training process with a small amount of data generated from a learned world model. Our method, Model-Augmented Data for Temporal Difference learning (MAD-TD) uses small amounts of generated data to stabilize high UTD training and achieve competitive performance on the most challenging tasks in the DeepMind control suite. Our experiments further highlight the importance of employing a good model to generate data, MAD-TD's ability to combat value overestimation, and its practical stability gains for continued learning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08898",
        "abstract url": "https://arxiv.org/abs/2410.08898",
        "title": "Low-Dimension-to-High-Dimension Generalization And Its Implications for Length Generalization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Low-Dimension-to-High-Dimension (LDHD) generalization is a special case of Out-of-Distribution (OOD) generalization, where the training data are restricted to a low-dimensional subspace of the high-dimensional testing space. Assuming that each instance is generated from a latent variable and the dimension of the latent variable reflects the problem scale, the inherent scaling challenge in length generalization can be captured by the LDHD generalization in the latent space. We theoretically demonstrate that LDHD generalization is generally unattainable without exploiting prior knowledge to provide appropriate inductive bias. Specifically, we explore LDHD generalization in Boolean functions. We verify that different architectures trained with (S)GD converge to \\emph{min-degree interpolators w.r.t. different independent sets}. LDHD generalization is achievable if and only if the target function coincides with this inductive bias. Applying the insights from LDHD generalization to length generalization, we explain the effectiveness of CoT as changing the structure latent space to enable better LDHD generalization. We also propose a principle for position embedding design to handle both the inherent LDHD generalization and the nuisances such as the data format. Following the principle, we propose a novel position embedding called RPE-Square that remedies the RPE for dealing with the data format nuisance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08899",
        "abstract url": "https://arxiv.org/abs/2410.08899",
        "title": "Utilizing ChatGPT in a Data Structures and Algorithms Course: A Teaching Assistant's Perspective",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Integrating large language models (LLMs) like ChatGPT is revolutionizing the field of computer science education. These models offer new possibilities for enriching student learning and supporting teaching assistants (TAs) in providing prompt feedback and supplementary learning resources. This research delves into the use of ChatGPT in a data structures and algorithms (DSA) course, particularly when combined with TA supervision. The findings demonstrate that incorporating ChatGPT with structured prompts and active TA guidance enhances students' understanding of intricate algorithmic concepts, boosts engagement, and elevates academic performance. However, challenges exist in addressing academic integrity and the limitations of LLMs in tackling complex problems. The study underscores the importance of active TA involvement in reducing students' reliance on AI-generated content and amplifying the overall educational impact. The results suggest that while LLMs can be advantageous for education, their successful integration demands continuous oversight and a thoughtful balance between AI and human guidance.",
        "subjects": [
            "cs.HC",
            "cs.AI",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08911",
        "abstract url": "https://arxiv.org/abs/2410.08911",
        "title": "Test-driven Software Experimentation with LASSO: an LLM Benchmarking Example",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Empirical software engineering faces a critical gap: the lack of standardized tools for rapid development and execution of Test-Driven Software Experiments (TDSEs) - that is, experiments that involve the execution of software subjects and the observation and analysis of their \"de facto\" run-time behavior. In this paper we present a general-purpose analysis platform called LASSO that provides a minimal set of domain-specific languages and data structures to conduct TDSEs. By empowering users with an executable scripting language to design and execute TDSEs, LASSO enables efficient evaluation of run-time semantics and execution characteristics in addition to statically determined properties. We present an example TDSE that demonstrates the practical benefits of LASSO's scripting capabilities for assessing the reliability of LLMs for code generation by means of a self-contained, reusable and extensible study script. The LASSO platform is freely available at: https://softwareobservatorium.github.io/, and a demo video is available on YouTube: https://youtu.be/tzY9oNTWXzw",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08914",
        "abstract url": "https://arxiv.org/abs/2410.08914",
        "title": "An End-to-End Deep Learning Method for Solving Nonlocal Allen-Cahn and Cahn-Hilliard Phase-Field Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose an efficient end-to-end deep learning method for solving nonlocal Allen-Cahn (AC) and Cahn-Hilliard (CH) phase-field models. One motivation for this effort emanates from the fact that discretized partial differential equation-based AC or CH phase-field models result in diffuse interfaces between phases, with the only recourse for remediation is to severely refine the spatial grids in the vicinity of the true moving sharp interface whose width is determined by a grid-independent parameter that is substantially larger than the local grid size. In this work, we introduce non-mass conserving nonlocal AC or CH phase-field models with regular, logarithmic, or obstacle double-well potentials. Because of non-locality, some of these models feature totally sharp interfaces separating phases. The discretization of such models can lead to a transition between phases whose width is only a single grid cell wide. Another motivation is to use deep learning approaches to ameliorate the otherwise high cost of solving discretized nonlocal phase-field models. To this end, loss functions of the customized neural networks are defined using the residual of the fully discrete approximations of the AC or CH models, which results from applying a Fourier collocation method and a temporal semi-implicit approximation. To address the long-range interactions in the models, we tailor the architecture of the neural network by incorporating a nonlocal kernel as an input channel to the neural network model. We then provide the results of extensive computational experiments to illustrate the accuracy, structure-preserving properties, predictive capabilities, and cost reductions of the proposed method.",
        "subjects": [
            "cs.LG",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08922",
        "abstract url": "https://arxiv.org/abs/2410.08922",
        "title": "Exploring the Design Space of Cognitive Engagement Techniques with AI-Generated Code for Enhanced Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Novice programmers are increasingly relying on Large Language Models (LLMs) to generate code for learning programming concepts. However, this interaction can lead to superficial engagement, giving learners an illusion of learning and hindering skill development. To address this issue, we conducted a systematic design exploration to develop seven cognitive engagement techniques aimed at promoting deeper engagement with AI-generated code. In this paper, we describe our design process, the initial seven techniques and results from a between-subjects study (N=82). We then iteratively refined the top techniques and further evaluated them through a within-subjects study (N=42). We evaluate the friction each technique introduces, their effectiveness in helping learners apply concepts to isomorphic tasks without AI assistance, and their success in aligning learners' perceived and actual coding abilities. Ultimately, our results highlight the most effective technique: guiding learners through the step-by-step problem-solving process, where they engage in an interactive dialog with the AI, prompting what needs to be done at each stage before the corresponding code is revealed.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": "19 pages, 6 figures"
    },
    {
        "paper id": "2410.08923",
        "abstract url": "https://arxiv.org/abs/2410.08923",
        "title": "Path-minimizing Latent ODEs for improved extrapolation and inference",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Latent ODE models provide flexible descriptions of dynamic systems, but they can struggle with extrapolation and predicting complicated non-linear dynamics. The latent ODE approach implicitly relies on encoders to identify unknown system parameters and initial conditions, whereas the evaluation times are known and directly provided to the ODE solver. This dichotomy can be exploited by encouraging time-independent latent representations. By replacing the common variational penalty in latent space with an $\\ell_2$ penalty on the path length of each system, the models learn data representations that can easily be distinguished from those of systems with different configurations. This results in faster training, smaller models, more accurate interpolation and long-time extrapolation compared to the baseline ODE models with GRU, RNN, and LSTM encoder/decoders on tests with damped harmonic oscillator, self-gravitating fluid, and predator-prey systems. We also demonstrate superior results for simulation-based inference of the Lotka-Volterra parameters and initial conditions by using the latents as data summaries for a conditional normalizing flow. Our change to the training loss is agnostic to the specific recognition network used by the decoder and can therefore easily be adopted by other latent ODE models.",
        "subjects": [
            "cs.LG",
            "astro-ph.IM"
        ],
        "comment": "20 pages 11 figures"
    },
    {
        "paper id": "2410.08931",
        "abstract url": "https://arxiv.org/abs/2410.08931",
        "title": "Enhancing Motion Variation in Text-to-Motion Models via Pose and Video Conditioned Editing",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Text-to-motion models that generate sequences of human poses from textual descriptions are garnering significant attention. However, due to data scarcity, the range of motions these models can produce is still limited. For instance, current text-to-motion models cannot generate a motion of kicking a football with the instep of the foot, since the training data only includes martial arts kicks. We propose a novel method that uses short video clips or images as conditions to modify existing basic motions. In this approach, the model's understanding of a kick serves as the prior, while the video or image of a football kick acts as the posterior, enabling the generation of the desired motion. By incorporating these additional modalities as conditions, our method can create motions not present in the training set, overcoming the limitations of text-motion datasets. A user study with 26 participants demonstrated that our approach produces unseen motions with realism comparable to commonly represented motions in text-motion datasets (e.g., HumanML3D), such as walking, running, squatting, and kicking.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08942",
        "abstract url": "https://arxiv.org/abs/2410.08942",
        "title": "Maximizing the Potential of Synthetic Data: Insights from Random Matrix Theory",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Synthetic data has gained attention for training large language models, but poor-quality data can harm performance (see, e.g., Shumailov et al. (2023); Seddik et al. (2024)). A potential solution is data pruning, which retains only high-quality data based on a score function (human or machine feedback). Previous work Feng et al. (2024) analyzed models trained on synthetic data as sample size increases. We extend this by using random matrix theory to derive the performance of a binary classifier trained on a mix of real and pruned synthetic data in a high dimensional setting. Our findings identify conditions where synthetic data could improve performance, focusing on the quality of the generative model and verification strategy. We also show a smooth phase transition in synthetic label noise, contrasting with prior sharp behavior in infinite sample limits. Experiments with toy models and large language models validate our theoretical results.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08948",
        "abstract url": "https://arxiv.org/abs/2410.08948",
        "title": "The Dynamics of Social Conventions in LLM populations: Spontaneous Emergence, Collective Biases and Tipping Points",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "Social conventions are the foundation for social and economic life. As legions of AI agents increasingly interact with each other and with humans, their ability to form shared conventions will determine how effectively they will coordinate behaviors, integrate into society and influence it. Here, we investigate the dynamics of conventions within populations of Large Language Model (LLM) agents using simulated interactions. First, we show that globally accepted social conventions can spontaneously arise from local interactions between communicating LLMs. Second, we demonstrate how strong collective biases can emerge during this process, even when individual agents appear to be unbiased. Third, we examine how minority groups of committed LLMs can drive social change by establishing new social conventions. We show that once these minority groups reach a critical size, they can consistently overturn established behaviors. In all cases, contrasting the experimental results with predictions from a minimal multi-agent model allows us to isolate the specific role of LLM agents. Our results clarify how AI systems can autonomously develop norms without explicit programming and have implications for designing AI systems that align with human values and societal goals.",
        "subjects": [
            "cs.MA",
            "cs.AI",
            "cs.CY",
            "physics.soc-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08958",
        "abstract url": "https://arxiv.org/abs/2410.08958",
        "title": "Lifted Coefficient of Determination: Fast model-free prediction intervals and likelihood-free model comparison",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose the $\\textit{lifted linear model}$, and derive model-free prediction intervals that become tighter as the correlation between predictions and observations increases. These intervals motivate the $\\textit{Lifted Coefficient of Determination}$, a model comparison criterion for arbitrary loss functions in prediction-based settings, e.g., regression, classification or counts. We extend the prediction intervals to more general error distributions, and propose a fast model-free outlier detection algorithm for regression. Finally, we illustrate the framework via numerical experiments.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "14 pages, 5 figures"
    },
    {
        "paper id": "2410.08976",
        "abstract url": "https://arxiv.org/abs/2410.08976",
        "title": "Learning Representations of Instruments for Partial Identification of Treatment Effects",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Reliable estimation of treatment effects from observational data is important in many disciplines such as medicine. However, estimation is challenging when unconfoundedness as a standard assumption in the causal inference literature is violated. In this work, we leverage arbitrary (potentially high-dimensional) instruments to estimate bounds on the conditional average treatment effect (CATE). Our contributions are three-fold: (1) We propose a novel approach for partial identification through a mapping of instruments to a discrete representation space so that we yield valid bounds on the CATE. This is crucial for reliable decision-making in real-world applications. (2) We derive a two-step procedure that learns tight bounds using a tailored neural partitioning of the latent instrument space. As a result, we avoid instability issues due to numerical approximations or adversarial training. Furthermore, our procedure aims to reduce the estimation variance in finite-sample settings to yield more reliable estimates. (3) We show theoretically that our procedure obtains valid bounds while reducing estimation variance. We further perform extensive experiments to demonstrate the effectiveness across various settings. Overall, our procedure offers a novel path for practitioners to make use of potentially high-dimensional instruments (e.g., as in Mendelian randomization).",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08979",
        "abstract url": "https://arxiv.org/abs/2410.08979",
        "title": "Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Reinforcement learning (RL) is rapidly reaching and surpassing human-level control capabilities. However, state-of-the-art RL algorithms often require timesteps and reaction times significantly faster than human capabilities, which is impractical in real-world settings and typically necessitates specialized hardware. Such speeds are difficult to achieve in the real world and often requires specialized hardware. We introduce Sequence Reinforcement Learning (SRL), an RL algorithm designed to produce a sequence of actions for a given input state, enabling effective control at lower decision frequencies. SRL addresses the challenges of learning action sequences by employing both a model and an actor-critic architecture operating at different temporal scales. We propose a \"temporal recall\" mechanism, where the critic uses the model to estimate intermediate states between primitive actions, providing a learning signal for each individual action within the sequence. Once training is complete, the actor can generate action sequences independently of the model, achieving model-free control at a slower frequency. We evaluate SRL on a suite of continuous control tasks, demonstrating that it achieves performance comparable to state-of-the-art algorithms while significantly reducing actor sample complexity. To better assess performance across varying decision frequencies, we introduce the Frequency-Averaged Score (FAS) metric. Our results show that SRL significantly outperforms traditional RL algorithms in terms of FAS, making it particularly suitable for applications requiring variable decision frequencies. Additionally, we compare SRL with model-based online planning, showing that SRL achieves superior FAS while leveraging the same model during training that online planners use for planning.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08993",
        "abstract url": "https://arxiv.org/abs/2410.08993",
        "title": "The structure of the token space for large language models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large language models encode the correlational structure present in natural language by fitting segments of utterances (tokens) into a high dimensional ambient latent space upon which the models then operate. We assert that in order to develop a foundational, first-principles understanding of the behavior and limitations of large language models, it is crucial to understand the topological and geometric structure of this token subspace. In this article, we present estimators for the dimension and Ricci scalar curvature of the token subspace, and apply it to three open source large language models of moderate size: GPT2, LLEMMA7B, and MISTRAL7B. In all three models, using these measurements, we find that the token subspace is not a manifold, but is instead a stratified manifold, where on each of the individual strata, the Ricci curvature is significantly negative. We additionally find that the dimension and curvature correlate with generative fluency of the models, which suggest that these findings have implications for model behavior.",
        "subjects": [
            "math.DG",
            "cs.AI"
        ],
        "comment": "33 pages, 22 figures"
    },
    {
        "paper id": "2410.08994",
        "abstract url": "https://arxiv.org/abs/2410.08994",
        "title": "Optimal Downsampling for Imbalanced Classification with Generalized Linear Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Downsampling or under-sampling is a technique that is utilized in the context of large and highly imbalanced classification models. We study optimal downsampling for imbalanced classification using generalized linear models (GLMs). We propose a pseudo maximum likelihood estimator and study its asymptotic normality in the context of increasingly imbalanced populations relative to an increasingly large sample size. We provide theoretical guarantees for the introduced estimator. Additionally, we compute the optimal downsampling rate using a criterion that balances statistical accuracy and computational efficiency. Our numerical experiments, conducted on both synthetic and empirical data, further validate our theoretical results, and demonstrate that the introduced estimator outperforms commonly available alternatives.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08997",
        "abstract url": "https://arxiv.org/abs/2410.08997",
        "title": "Hierarchical Universal Value Function Approximators",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "There have been key advancements to building universal approximators for multi-goal collections of reinforcement learning value functions -- key elements in estimating long-term returns of states in a parameterized manner. We extend this to hierarchical reinforcement learning, using the options framework, by introducing hierarchical universal value function approximators (H-UVFAs). This allows us to leverage the added benefits of scaling, planning, and generalization expected in temporal abstraction settings. We develop supervised and reinforcement learning methods for learning embeddings of the states, goals, options, and actions in the two hierarchical value functions: $Q(s, g, o; \u03b8)$ and $Q(s, g, o, a; \u03b8)$. Finally we demonstrate generalization of the HUVFAs and show they outperform corresponding UVFAs.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "12 pages, 10 figures, 3 appendices. Currently under review"
    },
    {
        "paper id": "2410.09005",
        "abstract url": "https://arxiv.org/abs/2410.09005",
        "title": "Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Neural scaling laws describe how the performance of deep neural networks scales with key factors such as training data size, model complexity, and training time, often following power-law behaviors over multiple orders of magnitude. Despite their empirical observation, the theoretical understanding of these scaling laws remains limited. In this work, we employ techniques from statistical mechanics to analyze one-pass stochastic gradient descent within a student-teacher framework, where both the student and teacher are two-layer neural networks. Our study primarily focuses on the generalization error and its behavior in response to data covariance matrices that exhibit power-law spectra. For linear activation functions, we derive analytical expressions for the generalization error, exploring different learning regimes and identifying conditions under which power-law scaling emerges. Additionally, we extend our analysis to non-linear activation functions in the feature learning regime, investigating how power-law spectra in the data covariance matrix impact learning dynamics. Importantly, we find that the length of the symmetric plateau depends on the number of distinct eigenvalues of the data covariance matrix and the number of hidden units, demonstrating how these plateaus behave under various configurations. In addition, our results reveal a transition from exponential to power-law convergence in the specialized phase when the data covariance matrix possesses a power-law spectrum. This work contributes to the theoretical understanding of neural scaling laws and provides insights into optimizing learning performance in practical scenarios involving complex data structures.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09027",
        "abstract url": "https://arxiv.org/abs/2410.09027",
        "title": "Variance reduction combining pre-experiment and in-experiment data",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Online controlled experiments (A/B testing) are essential in data-driven decision-making for many companies. Increasing the sensitivity of these experiments, particularly with a fixed sample size, relies on reducing the variance of the estimator for the average treatment effect (ATE). Existing methods like CUPED and CUPAC use pre-experiment data to reduce variance, but their effectiveness depends on the correlation between the pre-experiment data and the outcome. In contrast, in-experiment data is often more strongly correlated with the outcome and thus more informative. In this paper, we introduce a novel method that combines both pre-experiment and in-experiment data to achieve greater variance reduction than CUPED and CUPAC, without introducing bias or additional computation complexity. We also establish asymptotic theory and provide consistent variance estimators for our method. Applying this method to multiple online experiments at Etsy, we reach substantial variance reduction over CUPAC with the inclusion of only a few in-experiment covariates. These results highlight the potential of our approach to significantly improve experiment sensitivity and accelerate decision-making.",
        "subjects": [
            "stat.ME",
            "cs.LG",
            "econ.EM",
            "stat.AP"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2410.09123",
        "abstract url": "https://arxiv.org/abs/2410.09123",
        "title": "Context-Aware Adapter Tuning for Few-Shot Relation Learning in Knowledge Graphs",
        "rating": "0.5",
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "Graphs"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Knowledge graphs (KGs) are instrumental in various real-world applications, yet they often suffer from incompleteness due to missing relations. To predict instances for novel relations with limited training examples, few-shot relation learning approaches have emerged, utilizing techniques such as meta-learning. However, the assumption is that novel relations in meta-testing and base relations in meta-training are independently and identically distributed, which may not hold in practice. To address the limitation, we propose RelAdapter, a context-aware adapter for few-shot relation learning in KGs designed to enhance the adaptation process in meta-learning. First, RelAdapter is equipped with a lightweight adapter module that facilitates relation-specific, tunable adaptation of meta-knowledge in a parameter-efficient manner. Second, RelAdapter is enriched with contextual information about the target relation, enabling enhanced adaptation to each distinct relation. Extensive experiments on three benchmark KGs validate the superiority of RelAdapter over state-of-the-art methods.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "9 pages"
    },
    {
        "paper id": "2410.09156",
        "abstract url": "https://arxiv.org/abs/2410.09156",
        "title": "On Discriminative Probabilistic Modeling for Self-Supervised Representation Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study the discriminative probabilistic modeling problem on a continuous domain for (multimodal) self-supervised representation learning. To address the challenge of computing the integral in the partition function for each anchor data, we leverage the multiple importance sampling (MIS) technique for robust Monte Carlo integration, which can recover InfoNCE-based contrastive loss as a special case. Within this probabilistic modeling framework, we conduct generalization error analysis to reveal the limitation of current InfoNCE-based contrastive loss for self-supervised representation learning and derive insights for developing better approaches by reducing the error of Monte Carlo integration. To this end, we propose a novel non-parametric method for approximating the sum of conditional densities required by MIS through convex optimization, yielding a new contrastive objective for self-supervised representation learning. Moreover, we design an efficient algorithm for solving the proposed objective. We empirically compare our algorithm to representative baselines on the contrastive image-language pretraining task. Experimental results on the CC3M and CC12M datasets demonstrate the superior overall performance of our algorithm.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09193",
        "abstract url": "https://arxiv.org/abs/2410.09193",
        "title": "Synthetic Students: A Comparative Study of Bug Distribution Between Large Language Models and Computing Students",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "Large language models (LLMs) present an exciting opportunity for generating synthetic classroom data. Such data could include code containing a typical distribution of errors, simulated student behaviour to address the cold start problem when developing education tools, and synthetic user data when access to authentic data is restricted due to privacy reasons. In this research paper, we conduct a comparative study examining the distribution of bugs generated by LLMs in contrast to those produced by computing students. Leveraging data from two previous large-scale analyses of student-generated bugs, we investigate whether LLMs can be coaxed to exhibit bug patterns that are similar to authentic student bugs when prompted to inject errors into code. The results suggest that unguided, LLMs do not generate plausible error distributions, and many of the generated errors are unlikely to be generated by real students. However, with guidance including descriptions of common errors and typical frequencies, LLMs can be shepherded to generate realistic distributions of errors in synthetic code.",
        "subjects": [
            "cs.CY",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09196",
        "abstract url": "https://arxiv.org/abs/2410.09196",
        "title": "Scalable Signature-Based Distribution Regression via Reference Sets",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Distribution Regression (DR) on stochastic processes describes the learning task of regression on collections of time series. Path signatures, a technique prevalent in stochastic analysis, have been used to solve the DR problem. Recent works have demonstrated the ability of such solutions to leverage the information encoded in paths via signature-based features. However, current state of the art DR solutions are memory intensive and incur a high computation cost. This leads to a trade-off between path length and the number of paths considered. This computational bottleneck limits the application to small sample sizes which consequently introduces estimation uncertainty. In this paper, we present a methodology for addressing the above issues; resolving estimation uncertainties whilst also proposing a pipeline that enables us to use DR for a wide variety of learning tasks. Integral to our approach is our novel distance approximator. This allows us to seamlessly apply our methodology across different application domains, sampling rates, and stochastic process dimensions. We show that our model performs well in applications related to estimation theory, quantitative finance, and physical sciences. We demonstrate that our model generalises well, not only to unseen data within a given distribution, but also under unseen regimes (unseen classes of stochastic models).",
        "subjects": [
            "cs.LG",
            "q-fin.MF",
            "stat.ML"
        ],
        "comment": "24 pages, 4 figures"
    },
    {
        "paper id": "2410.09218",
        "abstract url": "https://arxiv.org/abs/2410.09218",
        "title": "Continual Learning with Neuromorphic Computing: Theories, Methods, and Applications",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "To adapt to real-world dynamics, intelligent systems need to assimilate new knowledge without catastrophic forgetting, where learning new tasks leads to a degradation in performance on old tasks. To address this, continual learning concept is proposed for enabling autonomous systems to acquire new knowledge and dynamically adapt to changing environments. Specifically, energy-efficient continual learning is needed to ensure the functionality of autonomous systems under tight compute and memory resource budgets (i.e., so-called autonomous embedded systems). Neuromorphic computing, with brain-inspired Spiking Neural Networks (SNNs), offers inherent advantages for enabling low-power/energy continual learning in autonomous embedded systems. In this paper, we comprehensively discuss the foundations and methods for enabling continual learning in neural networks, then analyze the state-of-the-art works considering SNNs. Afterward, comparative analyses of existing methods are conducted while considering crucial design factors, such as network complexity, memory, latency, and power/energy efficiency. We also explore the practical applications that can benefit from SNN-based continual learning and open challenges in real-world scenarios. In this manner, our survey provides valuable insights into the recent advancements of SNN-based continual learning for real-world application use-cases.",
        "subjects": [
            "cs.NE",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "71 pages, 31 figures, 6 tables"
    },
    {
        "paper id": "2410.09268",
        "abstract url": "https://arxiv.org/abs/2410.09268",
        "title": "One Step at a Time: Combining LLMs and Static Analysis to Generate Next-Step Hints for Programming Tasks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "Students often struggle with solving programming problems when learning to code, especially when they have to do it online, with one of the most common disadvantages of working online being the lack of personalized help. This help can be provided as next-step hint generation, i.e., showing a student what specific small step they need to do next to get to the correct solution. There are many ways to generate such hints, with large language models (LLMs) being among the most actively studied right now. While LLMs constitute a promising technology for providing personalized help, combining them with other techniques, such as static analysis, can significantly improve the output quality. In this work, we utilize this idea and propose a novel system to provide both textual and code hints for programming tasks. The pipeline of the proposed approach uses a chain-of-thought prompting technique and consists of three distinct steps: (1) generating subgoals - a list of actions to proceed with the task from the current student's solution, (2) generating the code to achieve the next subgoal, and (3) generating the text to describe this needed action. During the second step, we apply static analysis to the generated code to control its size and quality. The tool is implemented as a modification to the open-source JetBrains Academy plugin, supporting students in their in-IDE courses. To evaluate our approach, we propose a list of criteria for all steps in our pipeline and conduct two rounds of expert validation. Finally, we evaluate the next-step hints in a classroom with 14 students from two universities. Our results show that both forms of the hints - textual and code - were helpful for the students, and the proposed system helped them to proceed with the coding tasks.",
        "subjects": [
            "cs.SE",
            "cs.AI",
            "cs.CY",
            "cs.HC"
        ],
        "comment": "12 pages, 5 figures"
    },
    {
        "paper id": "2410.09297",
        "abstract url": "https://arxiv.org/abs/2410.09297",
        "title": "Refinements on the Complementary PDB Construction Mechanism",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Pattern database (PDB) is one of the most popular automated heuristic generation techniques. A PDB maps states in a planning task to abstract states by considering a subset of variables and stores their optimal costs to the abstract goal in a look up table. As the result of the progress made on symbolic search over recent years, symbolic-PDB-based planners achieved impressive results in the International Planning Competition (IPC) 2018. Among them, Complementary 1 (CPC1) tied as the second best planners and the best non-portfolio planners in the cost optimal track, only 2 tasks behind the winner. It uses a combination of different pattern generation algorithms to construct PDBs that are complementary to existing ones. As shown in the post contest experiments, there is room for improvement. In this paper, we would like to present our work on refining the PDB construction mechanism of CPC1. By testing on IPC 2018 benchmarks, the results show that a significant improvement is made on our modified planner over the original version.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09298",
        "abstract url": "https://arxiv.org/abs/2410.09298",
        "title": "DeepOSets: Non-Autoregressive In-Context Learning of Supervised Learning Operators",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We introduce DeepSets Operator Networks (DeepOSets), an efficient, non-autoregressive neural network architecture for in-context operator learning. In-context learning allows a trained machine learning model to learn from a user prompt without further training. DeepOSets adds in-context learning capabilities to Deep Operator Networks (DeepONets) by combining it with the DeepSets architecture. As the first non-autoregressive model for in-context operator learning, DeepOSets allow the user prompt to be processed in parallel, leading to significant computational savings. Here, we present the application of DeepOSets in the problem of learning supervised learning algorithms, which are operators mapping a finite-dimensional space of labeled data into an infinite-dimensional hypothesis space of prediction functions. In an empirical comparison with a popular autoregressive (transformer-based) model for in-context learning of the least-squares linear regression algorithm, DeepOSets reduced the number of model weights by several orders of magnitude and required a fraction of training and inference time. Furthermore, DeepOSets proved to be less sensitive to noise, outperforming the transformer model in noisy settings.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09311",
        "abstract url": "https://arxiv.org/abs/2410.09311",
        "title": "Data Deletion for Linear Regression with Noisy SGD",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the current era of big data and machine learning, it's essential to find ways to shrink the size of training dataset while preserving the training performance to improve efficiency. However, the challenge behind it includes providing practical ways to find points that can be deleted without significantly harming the training result and suffering from problems like underfitting. We therefore present the perfect deleted point problem for 1-step noisy SGD in the classical linear regression task, which aims to find the perfect deleted point in the training dataset such that the model resulted from the deleted dataset will be identical to the one trained without deleting it. We apply the so-called signal-to-noise ratio and suggest that its value is closely related to the selection of the perfect deleted point. We also implement an algorithm based on this and empirically show the effectiveness of it in a synthetic dataset. Finally we analyze the consequences of the perfect deleted point, specifically how it affects the training performance and privacy budget, therefore highlighting its potential. This research underscores the importance of data deletion and calls for urgent need for more studies in this field.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09316",
        "abstract url": "https://arxiv.org/abs/2410.09316",
        "title": "Combinatorial optimization of the coefficient of determination",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Robust correlation analysis is among the most critical challenges in statistics. Herein, we develop an efficient algorithm for selecting the $k$- subset of $n$ points in the plane with the highest coefficient of determination $\\left( R^2 \\right)$. Drawing from combinatorial geometry, we propose a method called the \\textit{quadratic sweep} that consists of two steps: (i) projectively lifting the data points into $\\mathbb R^5$ and then (ii) iterating over each linearly separable $k$-subset. Its basis is that the optimal set of outliers is separable from its complement in $\\mathbb R^2$ by a conic section, which, in $\\mathbb R^5$, can be found by a topological sweep in $\u0398\\left( n^5 \\log n \\right)$ time. Although key proofs of quadratic separability remain underway, we develop strong mathematical intuitions for our conjectures, then experimentally demonstrate our method's optimality over several million trials up to $n=30$ without error. Implementations in Julia and fully seeded, reproducible experiments are available at https://github.com/marc-harary/QuadraticSweep.",
        "subjects": [
            "stat.ML",
            "cs.DS",
            "cs.LG",
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09345",
        "abstract url": "https://arxiv.org/abs/2410.09345",
        "title": "Contrastive Learning for Implicit Social Factors in Social Media Popularity Prediction",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.SI"
            ]
        ],
        "abstract": "On social media sharing platforms, some posts are inherently destined for popularity. Therefore, understanding the reasons behind this phenomenon and predicting popularity before post publication holds significant practical value. The previous work predominantly focuses on enhancing post content extraction for better prediction results. However, certain factors introduced by social platforms also impact post popularity, which has not been extensively studied. For instance, users are more likely to engage with posts from individuals they follow, potentially influencing the popularity of these posts. We term these factors, unrelated to the explicit attractiveness of content, as implicit social factors. Through the analysis of users' post browsing behavior (also validated in public datasets), we propose three implicit social factors related to popularity, including content relevance, user influence similarity, and user identity. To model the proposed social factors, we introduce three supervised contrastive learning tasks. For different task objectives and data types, we assign them to different encoders and control their gradient flows to achieve joint optimization. We also design corresponding sampling and augmentation algorithms to improve the effectiveness of contrastive learning. Extensive experiments on the Social Media Popularity Dataset validate the superiority of our proposed method and also confirm the important role of implicit social factors in popularity prediction. We open source the code at https://github.com/Daisy-zzz/PPCL.git.",
        "subjects": [
            "cs.SI",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09350",
        "abstract url": "https://arxiv.org/abs/2410.09350",
        "title": "Generative Subgraph Retrieval for Knowledge Graph-Grounded Dialog Generation",
        "rating": "0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Knowledge graph-grounded dialog generation requires retrieving a dialog-relevant subgraph from the given knowledge base graph and integrating it with the dialog history. Previous works typically represent the graph using an external encoder, such as graph neural networks, and retrieve relevant triplets based on the similarity between single-vector representations of triplets and the dialog history. However, these external encoders fail to leverage the rich knowledge of pretrained language models, and the retrieval process is also suboptimal due to the information bottleneck caused by the single-vector abstraction of the dialog history. In this work, we propose Dialog generation with Generative Subgraph Retrieval (DialogGSR), which retrieves relevant knowledge subgraphs by directly generating their token sequences on top of language models. For effective generative subgraph retrieval, we introduce two key methods: (i) structure-aware knowledge graph linearization with self-supervised graph-specific tokens and (ii) graph-constrained decoding utilizing graph structural proximity-based entity informativeness scores for valid and relevant generative retrieval. DialogGSR achieves state-of-the-art performance in knowledge graph-grounded dialog generation, as demonstrated on OpenDialKG and KOMODIS datasets.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "EMNLP (main)"
    },
    {
        "paper id": "2410.09361",
        "abstract url": "https://arxiv.org/abs/2410.09361",
        "title": "Decision-Point Guided Safe Policy Improvement",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Within batch reinforcement learning, safe policy improvement (SPI) seeks to ensure that the learnt policy performs at least as well as the behavior policy that generated the dataset. The core challenge in SPI is seeking improvements while balancing risk when many state-action pairs may be infrequently visited. In this work, we introduce Decision Points RL (DPRL), an algorithm that restricts the set of state-action pairs (or regions for continuous states) considered for improvement. DPRL ensures high-confidence improvement in densely visited states (i.e. decision points) while still utilizing data from sparsely visited states. By appropriately limiting where and how we may deviate from the behavior policy, we achieve tighter bounds than prior work; specifically, our data-dependent bounds do not scale with the size of the state and action spaces. In addition to the analysis, we demonstrate that DPRL is both safe and performant on synthetic and real datasets.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09362",
        "abstract url": "https://arxiv.org/abs/2410.09362",
        "title": "SeRA: Self-Reviewing and Alignment of Large Language Models using Implicit Reward Margins",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Direct alignment algorithms (DAAs), such as direct preference optimization (DPO), have become popular alternatives for Reinforcement Learning from Human Feedback (RLHF) due to their simplicity, efficiency, and stability. However, the preferences used in DAAs are usually collected before the alignment training begins and remain unchanged (off-policy). This can lead to two problems where the policy model (1) picks up on spurious correlations in the dataset (as opposed to learning the intended alignment expressed in the human preference labels), and (2) overfits to feedback on off-policy trajectories that have less likelihood of being generated by an updated policy model. To address these issues, we introduce Self-Reviewing and Alignment (SeRA), a cost-efficient and effective method that can be readily combined with existing DAAs. SeRA comprises of two components: (1) sample selection using implicit reward margins, which helps alleviate over-fitting to some undesired features, and (2) preference bootstrapping using implicit rewards to augment preference data with updated policy models in a cost-efficient manner. Extensive experimentation, including some on instruction-following tasks, demonstrate the effectiveness and generality of SeRA in training LLMs on offline preference datasets with DAAs.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08530",
        "abstract url": "https://arxiv.org/abs/2410.08530",
        "title": "Ego3DT: Tracking Every 3D Object in Ego-centric Videos",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The growing interest in embodied intelligence has brought ego-centric perspectives to contemporary research. One significant challenge within this realm is the accurate localization and tracking of objects in ego-centric videos, primarily due to the substantial variability in viewing angles. Addressing this issue, this paper introduces a novel zero-shot approach for the 3D reconstruction and tracking of all objects from the ego-centric video. We present Ego3DT, a novel framework that initially identifies and extracts detection and segmentation information of objects within the ego environment. Utilizing information from adjacent video frames, Ego3DT dynamically constructs a 3D scene of the ego view using a pre-trained 3D scene reconstruction model. Additionally, we have innovated a dynamic hierarchical association mechanism for creating stable 3D tracking trajectories of objects in ego-centric videos. Moreover, the efficacy of our approach is corroborated by extensive experiments on two newly compiled datasets, with 1.04x - 2.90x in HOTA, showcasing the robustness and accuracy of our method in diverse ego-centric scenarios.",
        "subjects": [
            "cs.CV",
            "cs.MM"
        ],
        "comment": "Accepted by ACM Multimedia 2024"
    },
    {
        "paper id": "2410.08531",
        "abstract url": "https://arxiv.org/abs/2410.08531",
        "title": "Diffusion Models Need Visual Priors for Image Generation",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Conventional class-guided diffusion models generally succeed in generating images with correct semantic content, but often struggle with texture details. This limitation stems from the usage of class priors, which only provide coarse and limited conditional information. To address this issue, we propose Diffusion on Diffusion (DoD), an innovative multi-stage generation framework that first extracts visual priors from previously generated samples, then provides rich guidance for the diffusion model leveraging visual priors from the early stages of diffusion sampling. Specifically, we introduce a latent embedding module that employs a compression-reconstruction approach to discard redundant detail information from the conditional samples in each stage, retaining only the semantic information for guidance. We evaluate DoD on the popular ImageNet-$256 \\times 256$ dataset, reducing 7$\\times$ training cost compared to SiT and DiT with even better performance in terms of the FID-50K score. Our largest model DoD-XL achieves an FID-50K score of 1.83 with only 1 million training steps, which surpasses other state-of-the-art methods without bells and whistles during inference.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Preprint"
    },
    {
        "paper id": "2410.08608",
        "abstract url": "https://arxiv.org/abs/2410.08608",
        "title": "Text-To-Image with Generative Adversarial Networks",
        "rating": "0",
        "keywords": [
            [
                "GAN",
                "Text-To-Image"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Generating realistic images from human texts is one of the most challenging problems in the field of computer vision (CV). The meaning of descriptions given can be roughly reflected by existing text-to-image approaches. In this paper, our main purpose is to propose a brief comparison between five different methods base on the Generative Adversarial Networks (GAN) to make image from the text. In addition, each model architectures synthesis images with different resolution. Furthermore, the best and worst obtained resolutions is 64*64, 256*256 respectively. However, we checked and compared some metrics that introduce the accuracy of each model. Also, by doing this study, we found out the best model for this problem by comparing these different approaches essential metrics.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08709",
        "abstract url": "https://arxiv.org/abs/2410.08709",
        "title": "Distillation of Discrete Diffusion through Dimensional Correlations",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Diffusion models have demonstrated exceptional performances in various fields of generative modeling. While they often outperform competitors including VAEs and GANs in sample quality and diversity, they suffer from slow sampling speed due to their iterative nature. Recently, distillation techniques and consistency models are mitigating this issue in continuous domains, but discrete diffusion models have some specific challenges towards faster generation. Most notably, in the current literature, correlations between different dimensions (pixels, locations) are ignored, both by its modeling and loss functions, due to computational limitations. In this paper, we propose \"mixture\" models in discrete diffusion that are capable of treating dimensional correlations while remaining scalable, and we provide a set of loss functions for distilling the iterations of existing models. Two primary theoretical insights underpin our approach: first, that dimensionally independent models can well approximate the data distribution if they are allowed to conduct many sampling steps, and second, that our loss functions enables mixture models to distill such many-step conventional models into just a few steps by learning the dimensional correlations. We empirically demonstrate that our proposed method for discrete diffusions work in practice, by distilling a continuous-time discrete diffusion model pretrained on the CIFAR-10 dataset.",
        "subjects": [
            "cs.LG",
            "math.NA",
            "stat.ML"
        ],
        "comment": "To be presented at Machine Learning and Compression Workshop @ NeurIPS 2024"
    },
    {
        "paper id": "2410.08713",
        "abstract url": "https://arxiv.org/abs/2410.08713",
        "title": "Impact of Surface Reflections in Maritime Obstacle Detection",
        "rating": "0",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Maritime obstacle detection aims to detect possible obstacles for autonomous driving of unmanned surface vehicles. In the context of maritime obstacle detection, the water surface can act like a mirror on certain circumstances, causing reflections on imagery. Previous works have indicated surface reflections as a source of false positives for object detectors in maritime obstacle detection tasks. In this work, we show that surface reflections indeed adversely affect detector performance. We measure the effect of reflections by testing on two custom datasets, which we make publicly available. The first one contains imagery with reflections, while in the second reflections are inpainted. We show that the reflections reduce mAP by 1.2 to 9.6 points across various detectors. To remove false positives on reflections, we propose a novel filtering approach named Heatmap Based Sliding Filter. We show that the proposed method reduces the total number of false positives by 34.64% while minimally affecting true positives. We also conduct qualitative analysis and show that the proposed method indeed removes false positives on the reflections. The datasets can be found on https://github.com/SamedYalcin/MRAD.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at RROW2024 Workshop @ British Machine Vision Conference (BMVC) 2024"
    },
    {
        "paper id": "2410.08734",
        "abstract url": "https://arxiv.org/abs/2410.08734",
        "title": "Gradients Stand-in for Defending Deep Leakage in Federated Learning",
        "rating": "0",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Federated Learning (FL) has become a cornerstone of privacy protection, shifting the paradigm towards localizing sensitive data while only sending model gradients to a central server. This strategy is designed to reinforce privacy protections and minimize the vulnerabilities inherent in centralized data storage systems. Despite its innovative approach, recent empirical studies have highlighted potential weaknesses in FL, notably regarding the exchange of gradients. In response, this study introduces a novel, efficacious method aimed at safeguarding against gradient leakage, namely, ``AdaDefense\". Following the idea that model convergence can be achieved by using different types of optimization methods, we suggest using a local stand-in rather than the actual local gradient for global gradient aggregation on the central server. This proposed approach not only effectively prevents gradient leakage, but also ensures that the overall performance of the model remains largely unaffected. Delving into the theoretical dimensions, we explore how gradients may inadvertently leak private information and present a theoretical framework supporting the efficacy of our proposed method. Extensive empirical tests, supported by popular benchmark experiments, validate that our approach maintains model integrity and is robust against gradient leakage, marking an important step in our pursuit of safe and efficient FL.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08739",
        "abstract url": "https://arxiv.org/abs/2410.08739",
        "title": "MMLF: Multi-modal Multi-class Late Fusion for Object Detection with Uncertainty Estimation",
        "rating": "0",
        "keywords": [
            [
                "Autonomous driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Autonomous driving necessitates advanced object detection techniques that integrate information from multiple modalities to overcome the limitations associated with single-modal approaches. The challenges of aligning diverse data in early fusion and the complexities, along with overfitting issues introduced by deep fusion, underscore the efficacy of late fusion at the decision level. Late fusion ensures seamless integration without altering the original detector's network structure. This paper introduces a pioneering Multi-modal Multi-class Late Fusion method, designed for late fusion to enable multi-class detection. Fusion experiments conducted on the KITTI validation and official test datasets illustrate substantial performance improvements, presenting our model as a versatile solution for multi-modal object detection in autonomous driving. Moreover, our approach incorporates uncertainty analysis into the classification fusion process, rendering our model more transparent and trustworthy and providing more reliable insights into category predictions.",
        "subjects": [
            "cs.CV",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08743",
        "abstract url": "https://arxiv.org/abs/2410.08743",
        "title": "Look Gauss, No Pose: Novel View Synthesis using Gaussian Splatting without Accurate Pose Initialization",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "6-DoF"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D Gaussian Splatting has recently emerged as a powerful tool for fast and accurate novel-view synthesis from a set of posed input images. However, like most novel-view synthesis approaches, it relies on accurate camera pose information, limiting its applicability in real-world scenarios where acquiring accurate camera poses can be challenging or even impossible. We propose an extension to the 3D Gaussian Splatting framework by optimizing the extrinsic camera parameters with respect to photometric residuals. We derive the analytical gradients and integrate their computation with the existing high-performance CUDA implementation. This enables downstream tasks such as 6-DoF camera pose estimation as well as joint reconstruction and camera refinement. In particular, we achieve rapid convergence and high accuracy for pose estimation on real-world scenes. Our method enables fast reconstruction of 3D scenes without requiring accurate pose information by jointly optimizing geometry and camera poses, while achieving state-of-the-art results in novel-view synthesis. Our approach is considerably faster to optimize than most competing methods, and several times faster in rendering. We show results on real-world scenes and complex trajectories through simulated environments, achieving state-of-the-art results on LLFF while reducing runtime by two to four times compared to the most efficient competing method. Source code will be available at https://github.com/Schmiddo/noposegs .",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted in IROS 2024"
    },
    {
        "paper id": "2410.08811",
        "abstract url": "https://arxiv.org/abs/2410.08811",
        "title": "PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Preference learning is a central component for aligning current LLMs, but this process can be vulnerable to data poisoning attacks. To address this concern, we introduce PoisonBench, a benchmark for evaluating large language models' susceptibility to data poisoning during preference learning. Data poisoning attacks can manipulate large language model responses to include hidden malicious content or biases, potentially causing the model to generate harmful or unintended outputs while appearing to function normally. We deploy two distinct attack types across eight realistic scenarios, assessing 21 widely-used models. Our findings reveal concerning trends: (1) Scaling up parameter size does not inherently enhance resilience against poisoning attacks; (2) There exists a log-linear relationship between the effects of the attack and the data poison ratio; (3) The effect of data poisoning can generalize to extrapolated triggers that are not included in the poisoned data. These results expose weaknesses in current preference learning techniques, highlighting the urgent need for more robust defenses against malicious models and data manipulation.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "Tingchen Fu and Fazl Barez are core research contributors"
    },
    {
        "paper id": "2410.08814",
        "abstract url": "https://arxiv.org/abs/2410.08814",
        "title": "A Social Context-aware Graph-based Multimodal Attentive Learning Framework for Disaster Content Classification during Emergencies",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "In times of crisis, the prompt and precise classification of disaster-related information shared on social media platforms is crucial for effective disaster response and public safety. During such critical events, individuals use social media to communicate, sharing multimodal textual and visual content. However, due to the significant influx of unfiltered and diverse data, humanitarian organizations face challenges in leveraging this information efficiently. Existing methods for classifying disaster-related content often fail to model users' credibility, emotional context, and social interaction information, which are essential for accurate classification. To address this gap, we propose CrisisSpot, a method that utilizes a Graph-based Neural Network to capture complex relationships between textual and visual modalities, as well as Social Context Features to incorporate user-centric and content-centric information. We also introduce Inverted Dual Embedded Attention (IDEA), which captures both harmonious and contrasting patterns within the data to enhance multimodal interactions and provide richer insights. Additionally, we present TSEqD (Turkey-Syria Earthquake Dataset), a large annotated dataset for a single disaster event, containing 10,352 samples. Through extensive experiments, CrisisSpot demonstrated significant improvements, achieving an average F1-score gain of 9.45% and 5.01% compared to state-of-the-art methods on the publicly available CrisisMMD dataset and the TSEqD dataset, respectively.",
        "subjects": [
            "cs.CY",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08824",
        "abstract url": "https://arxiv.org/abs/2410.08824",
        "title": "One-shot Generative Domain Adaptation in 3D GANs",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "3D-aware image generation necessitates extensive training data to ensure stable training and mitigate the risk of overfitting. This paper first considers a novel task known as One-shot 3D Generative Domain Adaptation (GDA), aimed at transferring a pre-trained 3D generator from one domain to a new one, relying solely on a single reference image. One-shot 3D GDA is characterized by the pursuit of specific attributes, namely, high fidelity, large diversity, cross-domain consistency, and multi-view consistency. Within this paper, we introduce 3D-Adapter, the first one-shot 3D GDA method, for diverse and faithful generation. Our approach begins by judiciously selecting a restricted weight set for fine-tuning, and subsequently leverages four advanced loss functions to facilitate adaptation. An efficient progressive fine-tuning strategy is also implemented to enhance the adaptation process. The synergy of these three technological components empowers 3D-Adapter to achieve remarkable performance, substantiated both quantitatively and qualitatively, across all desired properties of 3D GDA. Furthermore, 3D-Adapter seamlessly extends its capabilities to zero-shot scenarios, and preserves the potential for crucial tasks such as interpolation, reconstruction, and editing within the latent space of the pre-trained generator. Code will be available at https://github.com/iceli1007/3D-Adapter.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "IJCV"
    },
    {
        "paper id": "2410.08901",
        "abstract url": "https://arxiv.org/abs/2410.08901",
        "title": "SegGrasp: Zero-Shot Task-Oriented Grasping via Semantic and Geometric Guided Segmentation",
        "rating": "0",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "Task-oriented grasping, which involves grasping specific parts of objects based on their functions, is crucial for developing advanced robotic systems capable of performing complex tasks in dynamic environments. In this paper, we propose a training-free framework that incorporates both semantic and geometric priors for zero-shot task-oriented grasp generation. The proposed framework, SegGrasp, first leverages the vision-language models like GLIP for coarse segmentation. It then uses detailed geometric information from convex decomposition to improve segmentation quality through a fusion policy named GeoFusion. An effective grasp pose can be generated by a grasping network with improved segmentation. We conducted the experiments on both segmentation benchmark and real-world robot grasping. The experimental results show that SegGrasp surpasses the baseline by more than 15\\% in grasp and segmentation performance.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7pages,6 figures"
    },
    {
        "paper id": "2410.08919",
        "abstract url": "https://arxiv.org/abs/2410.08919",
        "title": "Low-complexity Attention-based Unsupervised Anomalous Sound Detection exploiting Separable Convolutions and Angular Loss",
        "rating": "0",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "eess.AS"
            ]
        ],
        "abstract": "In this work, a novel deep neural network, designed to enhance the efficiency and effectiveness of unsupervised sound anomaly detection, is presented. The proposed model exploits an attention module and separable convolutions to identify salient time-frequency patterns in audio data to discriminate between normal and anomalous sounds with reduced computational complexity. The approach is validated through extensive experiments using the Task 2 dataset of the DCASE 2020 challenge. Results demonstrate superior performance in terms of anomaly detection accuracy while having fewer parameters than state-of-the-art methods. Implementation details, code, and pre-trained models are available in https://github.com/michaelneri/unsupervised-audio-anomaly-detection.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "Accepted for publication in IEEE Sensors Letters. 4 pages, 4 figures"
    },
    {
        "paper id": "2410.08941",
        "abstract url": "https://arxiv.org/abs/2410.08941",
        "title": "MeshGS: Adaptive Mesh-Aligned Gaussian Splatting for High-Quality Rendering",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "NeRF"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently, 3D Gaussian splatting has gained attention for its capability to generate high-fidelity rendering results. At the same time, most applications such as games, animation, and AR/VR use mesh-based representations to represent and render 3D scenes. We propose a novel approach that integrates mesh representation with 3D Gaussian splats to perform high-quality rendering of reconstructed real-world scenes. In particular, we introduce a distance-based Gaussian splatting technique to align the Gaussian splats with the mesh surface and remove redundant Gaussian splats that do not contribute to the rendering. We consider the distance between each Gaussian splat and the mesh surface to distinguish between tightly-bound and loosely-bound Gaussian splats. The tightly-bound splats are flattened and aligned well with the mesh geometry. The loosely-bound Gaussian splats are used to account for the artifacts in reconstructed 3D meshes in terms of rendering. We present a training strategy of binding Gaussian splats to the mesh geometry, and take into account both types of splats. In this context, we introduce several regularization techniques aimed at precisely aligning tightly-bound Gaussian splats with the mesh surface during the training process. We validate the effectiveness of our method on large and unbounded scene from mip-NeRF 360 and Deep Blending datasets. Our method surpasses recent mesh-based neural rendering techniques by achieving a 2dB higher PSNR, and outperforms mesh-based Gaussian splatting methods by 1.3 dB PSNR, particularly on the outdoor mip-NeRF 360 dataset, demonstrating better rendering quality. We provide analyses for each type of Gaussian splat and achieve a reduction in the number of Gaussian splats by 30% compared to the original 3D Gaussian splatting.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "ACCV (Asian Conference on Computer Vision) 2024"
    },
    {
        "paper id": "2410.08946",
        "abstract url": "https://arxiv.org/abs/2410.08946",
        "title": "Parallel Watershed Partitioning: GPU-Based Hierarchical Image Segmentation",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Many image processing applications rely on partitioning an image into disjoint regions whose pixels are 'similar.' The watershed and waterfall transforms are established mathematical morphology pixel clustering techniques. They are both relevant to modern applications where groups of pixels are to be decided upon in one go, or where adjacency information is relevant. We introduce three new parallel partitioning algorithms for GPUs. By repeatedly applying watershed algorithms, we produce waterfall results which form a hierarchy of partition regions over an input image. Our watershed algorithms attain competitive execution times in both 2D and 3D, processing an 800 megavoxel image in less than 1.4 sec. We also show how to use this fully deterministic image partitioning as a pre-processing step to machine learning based semantic segmentation. This replaces the role of superpixel algorithms, and results in comparable accuracy and faster training times.",
        "subjects": [
            "cs.CV",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08985",
        "abstract url": "https://arxiv.org/abs/2410.08985",
        "title": "Towards Trustworthy Knowledge Graph Reasoning: An Uncertainty Aware Perspective",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Recently, Knowledge Graphs (KGs) have been successfully coupled with Large Language Models (LLMs) to mitigate their hallucinations and enhance their reasoning capability, such as in KG-based retrieval-augmented frameworks. However, current KG-LLM frameworks lack rigorous uncertainty estimation, limiting their reliable deployment in high-stakes applications. Directly incorporating uncertainty quantification into KG-LLM frameworks presents challenges due to their complex architectures and the intricate interactions between the knowledge graph and language model components. To address this gap, we propose a new trustworthy KG-LLM framework, Uncertainty Aware Knowledge-Graph Reasoning (UAG), which incorporates uncertainty quantification into the KG-LLM framework. We design an uncertainty-aware multi-step reasoning framework that leverages conformal prediction to provide a theoretical guarantee on the prediction set. To manage the error rate of the multi-step process, we additionally introduce an error rate control module to adjust the error rate within the individual components. Extensive experiments show that our proposed UAG can achieve any pre-defined coverage rate while reducing the prediction set/interval size by 40% on average over the baselines.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08991",
        "abstract url": "https://arxiv.org/abs/2410.08991",
        "title": "Science is Exploration: Computational Frontiers for Conceptual Metaphor Theory",
        "rating": "0",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Metaphors are everywhere. They appear extensively across all domains of natural language, from the most sophisticated poetry to seemingly dry academic prose. A significant body of research in the cognitive science of language argues for the existence of conceptual metaphors, the systematic structuring of one domain of experience in the language of another. Conceptual metaphors are not simply rhetorical flourishes but are crucial evidence of the role of analogical reasoning in human cognition. In this paper, we ask whether Large Language Models (LLMs) can accurately identify and explain the presence of such conceptual metaphors in natural language data. Using a novel prompting technique based on metaphor annotation guidelines, we demonstrate that LLMs are a promising tool for large-scale computational research on conceptual metaphors. Further, we show that LLMs are able to apply procedural guidelines designed for human annotators, displaying a surprising depth of linguistic knowledge.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Accepted to the 2024 Computational Humanities Research Conference (CHR)"
    },
    {
        "paper id": "2410.09010",
        "abstract url": "https://arxiv.org/abs/2410.09010",
        "title": "CVAM-Pose: Conditional Variational Autoencoder for Multi-Object Monocular Pose Estimation",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Estimating rigid objects' poses is one of the fundamental problems in computer vision, with a range of applications across automation and augmented reality. Most existing approaches adopt one network per object class strategy, depend heavily on objects' 3D models, depth data, and employ a time-consuming iterative refinement, which could be impractical for some applications. This paper presents a novel approach, CVAM-Pose, for multi-object monocular pose estimation that addresses these limitations. The CVAM-Pose method employs a label-embedded conditional variational autoencoder network, to implicitly abstract regularised representations of multiple objects in a single low-dimensional latent space. This autoencoding process uses only images captured by a projective camera and is robust to objects' occlusion and scene clutter. The classes of objects are one-hot encoded and embedded throughout the network. The proposed label-embedded pose regression strategy interprets the learnt latent space representations utilising continuous pose representations. Ablation tests and systematic evaluations demonstrate the scalability and efficiency of the CVAM-Pose method for multi-object scenarios. The proposed CVAM-Pose outperforms competing latent space approaches. For example, it is respectively 25% and 20% better than AAE and Multi-Path methods, when evaluated using the $\\mathrm{AR_{VSD}}$ metric on the Linemod-Occluded dataset. It also achieves results somewhat comparable to methods reliant on 3D models reported in BOP challenges. Code available: https://github.com/JZhao12/CVAM-Pose",
        "subjects": [
            "cs.CV"
        ],
        "comment": "BMVC 2024, oral presentation, the main paper and supplementary materials are included"
    },
    {
        "paper id": "2410.09024",
        "abstract url": "https://arxiv.org/abs/2410.09024",
        "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which use external tools and can execute multi-stage tasks -- may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09040",
        "abstract url": "https://arxiv.org/abs/2410.09040",
        "title": "AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation",
        "rating": "0",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper studies the vulnerabilities of transformer-based Large Language Models (LLMs) to jailbreaking attacks, focusing specifically on the optimization-based Greedy Coordinate Gradient (GCG) strategy. We first observe a positive correlation between the effectiveness of attacks and the internal behaviors of the models. For instance, attacks tend to be less effective when models pay more attention to system prompts designed to ensure LLM safety alignment. Building on this discovery, we introduce an enhanced method that manipulates models' attention scores to facilitate LLM jailbreaking, which we term AttnGCG. Empirically, AttnGCG shows consistent improvements in attack efficacy across diverse LLMs, achieving an average increase of ~7% in the Llama-2 series and ~10% in the Gemma series. Our strategy also demonstrates robust attack transferability against both unseen harmful goals and black-box LLMs like GPT-3.5 and GPT-4. Moreover, we note our attention-score visualization is more interpretable, allowing us to gain better insights into how our targeted attention manipulation facilitates more effective jailbreaking. We release the code at https://github.com/UCSC-VLAA/AttnGCG-attack.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09129",
        "abstract url": "https://arxiv.org/abs/2410.09129",
        "title": "nextlocllm: next location prediction using LLMs",
        "rating": "0",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Next location prediction is a critical task in human mobility analysis and serves as a foundation for various downstream applications. Existing methods typically rely on discrete IDs to represent locations, which inherently overlook spatial relationships and cannot generalize across cities. In this paper, we propose NextLocLLM, which leverages the advantages of large language models (LLMs) in processing natural language descriptions and their strong generalization capabilities for next location prediction. Specifically, instead of using IDs, NextLocLLM encodes locations based on continuous spatial coordinates to better model spatial relationships. These coordinates are further normalized to enable robust cross-city generalization. Another highlight of NextlocLLM is its LLM-enhanced POI embeddings. It utilizes LLMs' ability to encode each POI category's natural language description into embeddings. These embeddings are then integrated via nonlinear projections to form this LLM-enhanced POI embeddings, effectively capturing locations' functional attributes. Furthermore, task and data prompt prefix, together with trajectory embeddings, are incorporated as input for partly-frozen LLM backbone. NextLocLLM further introduces prediction retrieval module to ensure structural consistency in prediction. Experiments show that NextLocLLM outperforms existing models in next location prediction, excelling in both supervised and zero-shot settings.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "19 pages"
    },
    {
        "paper id": "2410.09132",
        "abstract url": "https://arxiv.org/abs/2410.09132",
        "title": "When Graph meets Multimodal: Benchmarking on Multimodal Attributed Graphs Learning",
        "rating": "0",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Multimodal attributed graphs (MAGs) are prevalent in various real-world scenarios and generally contain two kinds of knowledge: (a) Attribute knowledge is mainly supported by the attributes of different modalities contained in nodes (entities) themselves, such as texts and images. (b) Topology knowledge, on the other hand, is provided by the complex interactions posed between nodes. The cornerstone of MAG representation learning lies in the seamless integration of multimodal attributes and topology. Recent advancements in Pre-trained Language/Vision models (PLMs/PVMs) and Graph neural networks (GNNs) have facilitated effective learning on MAGs, garnering increased research interest. However, the absence of meaningful benchmark datasets and standardized evaluation procedures for MAG representation learning has impeded progress in this field. In this paper, we propose Multimodal Attribute Graph Benchmark (MAGB)}, a comprehensive and diverse collection of challenging benchmark datasets for MAGs. The MAGB datasets are notably large in scale and encompass a wide range of domains, spanning from e-commerce networks to social networks. In addition to the brand-new datasets, we conduct extensive benchmark experiments over MAGB with various learning paradigms, ranging from GNN-based and PLM-based methods, to explore the necessity and feasibility of integrating multimodal attributes and graph topology. In a nutshell, we provide an overview of the MAG datasets, standardized evaluation procedures, and present baseline experiments. The entire MAGB project is publicly accessible at https://github.com/sktsherlock/ATG.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09140",
        "abstract url": "https://arxiv.org/abs/2410.09140",
        "title": "RealEra: Semantic-level Concept Erasure via Neighbor-Concept Mining",
        "rating": "0",
        "keywords": [
            [
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The remarkable development of text-to-image generation models has raised notable security concerns, such as the infringement of portrait rights and the generation of inappropriate content. Concept erasure has been proposed to remove the model's knowledge about protected and inappropriate concepts. Although many methods have tried to balance the efficacy (erasing target concepts) and specificity (retaining irrelevant concepts), they can still generate abundant erasure concepts under the steering of semantically related inputs. In this work, we propose RealEra to address this \"concept residue\" issue. Specifically, we first introduce the mechanism of neighbor-concept mining, digging out the associated concepts by adding random perturbation into the embedding of erasure concept, thus expanding the erasing range and eliminating the generations even through associated concept inputs. Furthermore, to mitigate the negative impact on the generation of irrelevant concepts caused by the expansion of erasure scope, RealEra preserves the specificity through the beyond-concept regularization. This makes irrelevant concepts maintain their corresponding spatial position, thereby preserving their normal generation performance. We also employ the closed-form solution to optimize weights of U-Net for the cross-attention alignment, as well as the prediction noise alignment with the LoRA module. Extensive experiments on multiple benchmarks demonstrate that RealEra outperforms previous concept erasing methods in terms of superior erasing efficacy, specificity, and generality. More details are available on our project page https://realerasing.github.io/RealEra/ .",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09204",
        "abstract url": "https://arxiv.org/abs/2410.09204",
        "title": "Encoding Agent Trajectories as Representations with Sequence Transformers",
        "rating": "0",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Spatiotemporal data faces many analogous challenges to natural language text including the ordering of locations (words) in a sequence, long range dependencies between locations, and locations having multiple meanings. In this work, we propose a novel model for representing high dimensional spatiotemporal trajectories as sequences of discrete locations and encoding them with a Transformer-based neural network architecture. Similar to language models, our Sequence Transformer for Agent Representation Encodings (STARE) model can learn representations and structure in trajectory data through both supervisory tasks (e.g., classification), and self-supervisory tasks (e.g., masked modelling). We present experimental results on various synthetic and real trajectory datasets and show that our proposed model can learn meaningful encodings that are useful for many downstream tasks including discriminating between labels and indicating similarity between locations. Using these encodings, we also learn relationships between agents and locations present in spatiotemporal data.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "12 pages, to be presented at GeoAI workshop at ACM SigSpatial 2024"
    },
    {
        "paper id": "2410.09237",
        "abstract url": "https://arxiv.org/abs/2410.09237",
        "title": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via Training-free Adaptor",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advances in deep learning for processing point clouds hold increased interest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision. This paper introduces a new method to tackle the Few-Shot Continual Incremental Learning (FSCIL) problem in 3D point cloud environments. We leverage a foundational 3D model trained extensively on point cloud data. Drawing from recent improvements in foundation models, known for their ability to work well across different tasks, we propose a novel strategy that does not require additional training to adapt to new tasks. Our approach uses a dual cache system: first, it uses previous test samples based on how confident the model was in its predictions to prevent forgetting, and second, it includes a small number of new task samples to prevent overfitting. This dynamic adaptation ensures strong performance across different learning tasks without needing lots of fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet, ScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and demonstrating its effectiveness and versatility. The code is available at \\url{https://github.com/ahmadisahar/ACCV_FCIL3D}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "ACCV 2024"
    },
    {
        "paper id": "2410.09246",
        "abstract url": "https://arxiv.org/abs/2410.09246",
        "title": "DFM: Interpolant-free Dual Flow Matching",
        "rating": "0",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Continuous normalizing flows (CNFs) can model data distributions with expressive infinite-length architectures. But this modeling involves computationally expensive process of solving an ordinary differential equation (ODE) during maximum likelihood training. Recently proposed flow matching (FM) framework allows to substantially simplify the training phase using a regression objective with the interpolated forward vector field. In this paper, we propose an interpolant-free dual flow matching (DFM) approach without explicit assumptions about the modeled vector field. DFM optimizes the forward and, additionally, a reverse vector field model using a novel objective that facilitates bijectivity of the forward and reverse transformations. Our experiments with the SMAP unsupervised anomaly detection show advantages of DFM when compared to the CNF trained with either maximum likelihood or FM objectives with the state-of-the-art performance metrics.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "Extended Abstract Track at the Unifying Representations in Neural Models Workshop (NeurIPS 2024)"
    },
    {
        "paper id": "2410.09295",
        "abstract url": "https://arxiv.org/abs/2410.09295",
        "title": "Natural Language Counterfactual Explanations for Graphs Using Large Language Models",
        "rating": "0",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Explainable Artificial Intelligence (XAI) has emerged as a critical area of research to unravel the opaque inner logic of (deep) machine learning models. Among the various XAI techniques proposed in the literature, counterfactual explanations stand out as one of the most promising approaches. However, these ``what-if'' explanations are frequently complex and technical, making them difficult for non-experts to understand and, more broadly, challenging for humans to interpret. To bridge this gap, in this work, we exploit the power of open-source Large Language Models to generate natural language explanations when prompted with valid counterfactual instances produced by state-of-the-art explainers for graph-based models. Experiments across several graph datasets and counterfactual explainers show that our approach effectively produces accurate natural language representations of counterfactual instances, as demonstrated by key performance metrics.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09338",
        "abstract url": "https://arxiv.org/abs/2410.09338",
        "title": "Keys to Robust Edits: from Theoretical Insights to Practical Advances",
        "rating": "0",
        "keywords": [
            [
                "Knowledge editing"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have revolutionized knowledge storage and retrieval, but face challenges with conflicting and outdated information. Knowledge editing techniques have been proposed to address these issues, yet they struggle with robustness tests involving long contexts, paraphrased subjects, and continuous edits. This work investigates the cause of these failures in locate-and-edit methods, offering theoretical insights into their key-value modeling and deriving mathematical bounds for robust and specific edits, leading to a novel 'group discussion' conceptual model for locate-and-edit methods. Empirical analysis reveals that keys used by current methods fail to meet robustness and specificity requirements. To address this, we propose a Robust Edit Pathway (REP) that disentangles editing keys from LLMs' inner representations. Evaluations on LLaMA2-7B and Mistral-7B using the CounterFact dataset show that REP significantly improves robustness across various metrics, both in-domain and out-of-domain, with minimal trade-offs in success rate and locality. Our findings advance the development of reliable and flexible knowledge updating in LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2410.08557",
        "abstract url": "https://arxiv.org/abs/2410.08557",
        "title": "MUSO: Achieving Exact Machine Unlearning in Over-Parameterized Regimes",
        "rating": "-0.5",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine unlearning (MU) is to make a well-trained model behave as if it had never been trained on specific data. In today's over-parameterized models, dominated by neural networks, a common approach is to manually relabel data and fine-tune the well-trained model. It can approximate the MU model in the output space, but the question remains whether it can achieve exact MU, i.e., in the parameter space. We answer this question by employing random feature techniques to construct an analytical framework. Under the premise of model optimization via stochastic gradient descent, we theoretically demonstrated that over-parameterized linear models can achieve exact MU through relabeling specific data. We also extend this work to real-world nonlinear networks and propose an alternating optimization algorithm that unifies the tasks of unlearning and relabeling. The algorithm's effectiveness, confirmed through numerical experiments, highlights its superior performance in unlearning across various scenarios compared to current state-of-the-art methods, particularly excelling over similar relabeling-based MU approaches.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08634",
        "abstract url": "https://arxiv.org/abs/2410.08634",
        "title": "GAI-Enabled Explainable Personalized Federated Semi-Supervised Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning (FL) is a commonly distributed algorithm for mobile users (MUs) training artificial intelligence (AI) models, however, several challenges arise when applying FL to real-world scenarios, such as label scarcity, non-IID data, and unexplainability. As a result, we propose an explainable personalized FL framework, called XPFL. First, we introduce a generative AI (GAI) assisted personalized federated semi-supervised learning, called GFed. Particularly, in local training, we utilize a GAI model to learn from large unlabeled data and apply knowledge distillation-based semi-supervised learning to train the local FL model using the knowledge acquired from the GAI model. In global aggregation, we obtain the new local FL model by fusing the local and global FL models in specific proportions, allowing each local model to incorporate knowledge from others while preserving its personalized characteristics. Second, we propose an explainable AI mechanism for FL, named XFed. Specifically, in local training, we apply a decision tree to match the input and output of the local FL model. In global aggregation, we utilize t-distributed stochastic neighbor embedding (t-SNE) to visualize the local models before and after aggregation. Finally, simulation results validate the effectiveness of the proposed XPFL framework.",
        "subjects": [
            "cs.LG",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08660",
        "abstract url": "https://arxiv.org/abs/2410.08660",
        "title": "RePD: Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition Process",
        "rating": "-0.5",
        "keywords": [
            [
                "Attack"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In this study, we introduce RePD, an innovative attack Retrieval-based Prompt Decomposition framework designed to mitigate the risk of jailbreak attacks on large language models (LLMs). Despite rigorous pretraining and finetuning focused on ethical alignment, LLMs are still susceptible to jailbreak exploits. RePD operates on a one-shot learning model, wherein it accesses a database of pre-collected jailbreak prompt templates to identify and decompose harmful inquiries embedded within user prompts. This process involves integrating the decomposition of the jailbreak prompt into the user's original query into a one-shot learning example to effectively teach the LLM to discern and separate malicious components. Consequently, the LLM is equipped to first neutralize any potentially harmful elements before addressing the user's prompt in a manner that aligns with its ethical guidelines. RePD is versatile and compatible with a variety of open-source LLMs acting as agents. Through comprehensive experimentation with both harmful and benign prompts, we have demonstrated the efficacy of our proposed RePD in enhancing the resilience of LLMs against jailbreak attacks, without compromising their performance in responding to typical user requests.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2403.04783 by other authors"
    },
    {
        "paper id": "2410.08687",
        "abstract url": "https://arxiv.org/abs/2410.08687",
        "title": "Uncertainty Estimation and Out-of-Distribution Detection for LiDAR Scene Semantic Segmentation",
        "rating": "-0.5",
        "keywords": [
            [
                "LiDAR"
            ],
            [
                "navigation"
            ],
            [
                "cs.LG",
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Safe navigation in new environments requires autonomous vehicles and robots to accurately interpret their surroundings, relying on LiDAR scene segmentation, out-of-distribution (OOD) obstacle detection, and uncertainty computation. We propose a method to distinguish in-distribution (ID) from OOD samples and quantify both epistemic and aleatoric uncertainties using the feature space of a single deterministic model. After training a semantic segmentation network, a Gaussian Mixture Model (GMM) is fitted to its feature space. OOD samples are detected by checking if their squared Mahalanobis distances to each Gaussian component conform to a chi-squared distribution, eliminating the need for an additional OOD training set. Given that the estimated mean and covariance matrix of a multivariate Gaussian distribution follow Gaussian and Inverse-Wishart distributions, multiple GMMs are generated by sampling from these distributions to assess epistemic uncertainty through classification variability. Aleatoric uncertainty is derived from the entropy of responsibility values within Gaussian components. Comparing our method with deep ensembles and logit-sampling for uncertainty computation demonstrates its superior performance in real-world applications for quantifying epistemic and aleatoric uncertainty, as well as detecting OOD samples. While deep ensembles miss some highly uncertain samples, our method successfully detects them and assigns high epistemic uncertainty.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": "Accepted for publication in the Proceedings of the European Conference on Computer Vision (ECCV) 2024"
    },
    {
        "paper id": "2410.08753",
        "abstract url": "https://arxiv.org/abs/2410.08753",
        "title": "Who should fight the spread of fake news?",
        "rating": "-0.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "This study investigates who should bear the responsibility of combating the spread of misinformation in social networks. Should that be the online platforms or their users? Should that be done by debunking the \"fake news\" already in circulation or by investing in preemptive efforts to prevent their diffusion altogether? We seek to answer such questions in a stylized opinion dynamics framework, where agents in a network aggregate the information they receive from peers and/or from influential external sources, with the aim of learning a ground truth among a set of competing hypotheses. In most cases, we find centralized sources to be more effective at combating misinformation than distributed ones, suggesting that online platforms should play an active role in the fight against fake news. In line with literature on the \"backfire effect\", we find that debunking in certain circumstances can be a counterproductive strategy, whereas some targeted strategies (akin to \"deplatforming\") and/or preemptive campaigns turn out to be quite effective. Despite its simplicity, our model provides useful guidelines that could inform the ongoing debate on online disinformation and the best ways to limit its damaging effects.",
        "subjects": [
            "physics.soc-ph",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08759",
        "abstract url": "https://arxiv.org/abs/2410.08759",
        "title": "Enhancing GNNs with Architecture-Agnostic Graph Transformations: A Systematic Analysis",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In recent years, a wide variety of graph neural network (GNN) architectures have emerged, each with its own strengths, weaknesses, and complexities. Various techniques, including rewiring, lifting, and node annotation with centrality values, have been employed as pre-processing steps to enhance GNN performance. However, there are no universally accepted best practices, and the impact of architecture and pre-processing on performance often remains opaque. This study systematically explores the impact of various graph transformations as pre-processing steps on the performance of common GNN architectures across standard datasets. The models are evaluated based on their ability to distinguish non-isomorphic graphs, referred to as expressivity. Our findings reveal that certain transformations, particularly those augmenting node features with centrality measures, consistently improve expressivity. However, these gains come with trade-offs, as methods like graph encoding, while enhancing expressivity, introduce numerical inaccuracies widely-used python packages. Additionally, we observe that these pre-processing techniques are limited when addressing complex tasks involving 3-WL and 4-WL indistinguishable graphs.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08760",
        "abstract url": "https://arxiv.org/abs/2410.08760",
        "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL) is an emerging paradigm that enables intelligent agents to collaboratively train Machine Learning (ML) models in a distributed manner, eliminating the need for sharing their local data. The recent work (arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL) algorithms, marking a significant step towards applying second-order methods to FL and large-scale optimization. However, the reference FedNL prototype exhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch a single experiment in a sever-grade workstation; (ii) The prototype only simulates multi-node setting; (iii) Prototype integration into resource-constrained applications is challenging. To bridge the gap between theory and practice, we present a self-contained implementation of FedNL, FedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves the aforementioned issues and reduces the wall clock time by x1000. With this FedNL outperforms alternatives for training logistic regression in a single-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark (arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose two practical-orientated compressors for FedNL - adaptive TopLEK and cache-aware RandSeqK, which fulfill the theory of FedNL.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.MS",
            "cs.PF",
            "math.OC"
        ],
        "comment": "55 pages, 12 figures, 12 tables"
    },
    {
        "paper id": "2410.08776",
        "abstract url": "https://arxiv.org/abs/2410.08776",
        "title": "F2A: An Innovative Approach for Prompt Injection by Utilizing Feign Security Detection Agents",
        "rating": "-0.5",
        "keywords": [
            [
                "attack"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "With the rapid development of Large Language Models (LLMs), numerous mature applications of LLMs have emerged in the field of content safety detection. However, we have found that LLMs exhibit blind trust in safety detection agents. The general LLMs can be compromised by hackers with this vulnerability. Hence, this paper proposed an attack named Feign Agent Attack (F2A).Through such malicious forgery methods, adding fake safety detection results into the prompt, the defense mechanism of LLMs can be bypassed, thereby obtaining harmful content and hijacking the normal conversation. Continually, a series of experiments were conducted. In these experiments, the hijacking capability of F2A on LLMs was analyzed and demonstrated, exploring the fundamental reasons why LLMs blindly trust safety detection results. The experiments involved various scenarios where fake safety detection results were injected into prompts, and the responses were closely monitored to understand the extent of the vulnerability. Also, this paper provided a reasonable solution to this attack, emphasizing that it is important for LLMs to critically evaluate the results of augmented agents to prevent the generating harmful content. By doing so, the reliability and security can be significantly improved, protecting the LLMs from F2A.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": "1. Fixed typo in abstract 2. Provisionally completed the article update to facilitate future version revisions"
    },
    {
        "paper id": "2410.08787",
        "abstract url": "https://arxiv.org/abs/2410.08787",
        "title": "Efficient Differentiable Discovery of Causal Order",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the algorithm Intersort, Chevalley et al. (2024) proposed a score-based method to discover the causal order of variables in a Directed Acyclic Graph (DAG) model, leveraging interventional data to outperform existing methods. However, as a score-based method over the permutahedron, Intersort is computationally expensive and non-differentiable, limiting its ability to be utilised in problems involving large-scale datasets, such as those in genomics and climate models, or to be integrated into end-to-end gradient-based learning frameworks. We address this limitation by reformulating Intersort using differentiable sorting and ranking techniques. Our approach enables scalable and differentiable optimization of causal orderings, allowing the continuous score function to be incorporated as a regularizer in downstream tasks. Empirical results demonstrate that causal discovery algorithms benefit significantly from regularizing on the causal order, underscoring the effectiveness of our method. Our work opens the door to efficiently incorporating regularization for causal order into the training of differentiable models and thereby addresses a long-standing limitation of purely associational supervised learning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08794",
        "abstract url": "https://arxiv.org/abs/2410.08794",
        "title": "M$^3$-Impute: Mask-guided Representation Learning for Missing Value Imputation",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Missing values are a common problem that poses significant challenges to data analysis and machine learning. This problem necessitates the development of an effective imputation method to fill in the missing values accurately, thereby enhancing the overall quality and utility of the datasets. Existing imputation methods, however, fall short of explicitly considering the `missingness' information in the data during the embedding initialization stage and modeling the entangled feature and sample correlations during the learning process, thus leading to inferior performance. We propose M$^3$-Impute, which aims to explicitly leverage the missingness information and such correlations with novel masking schemes. M$^3$-Impute first models the data as a bipartite graph and uses a graph neural network to learn node embeddings, where the refined embedding initialization process directly incorporates the missingness information. They are then optimized through M$^3$-Impute's novel feature correlation unit (FRU) and sample correlation unit (SRU) that effectively captures feature and sample correlations for imputation. Experiment results on 25 benchmark datasets under three different missingness settings show the effectiveness of M$^3$-Impute by achieving 20 best and 4 second-best MAE scores on average.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08852",
        "abstract url": "https://arxiv.org/abs/2410.08852",
        "title": "Conformalized Interactive Imitation Learning: Handling Expert Shift and Intermittent Feedback",
        "rating": "-0.5",
        "keywords": [
            [
                "robot"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In interactive imitation learning (IL), uncertainty quantification offers a way for the learner (i.e. robot) to contend with distribution shifts encountered during deployment by actively seeking additional feedback from an expert (i.e. human) online. Prior works use mechanisms like ensemble disagreement or Monte Carlo dropout to quantify when black-box IL policies are uncertain; however, these approaches can lead to overconfident estimates when faced with deployment-time distribution shifts. Instead, we contend that we need uncertainty quantification algorithms that can leverage the expert human feedback received during deployment time to adapt the robot's uncertainty online. To tackle this, we draw upon online conformal prediction, a distribution-free method for constructing prediction intervals online given a stream of ground-truth labels. Human labels, however, are intermittent in the interactive IL setting. Thus, from the conformal prediction side, we introduce a novel uncertainty quantification algorithm called intermittent quantile tracking (IQT) that leverages a probabilistic model of intermittent labels, maintains asymptotic coverage guarantees, and empirically achieves desired coverage levels. From the interactive IL side, we develop ConformalDAgger, a new approach wherein the robot uses prediction intervals calibrated by IQT as a reliable measure of deployment-time uncertainty to actively query for more expert feedback. We compare ConformalDAgger to prior uncertainty-aware DAgger methods in scenarios where the distribution shift is (and isn't) present because of changes in the expert's policy. We find that in simulated and hardware deployments on a 7DOF robotic manipulator, ConformalDAgger detects high uncertainty when the expert shifts and increases the number of interventions compared to baselines, allowing the robot to more quickly learn the new behavior.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.HC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08854",
        "abstract url": "https://arxiv.org/abs/2410.08854",
        "title": "Hybrid LLM-DDQN based Joint Optimization of V2I Communication and Autonomous Driving",
        "rating": "-0.5",
        "keywords": [
            [
                "Autonomous Driving",
                "vehicle"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Large language models (LLMs) have received considerable interest recently due to their outstanding reasoning and comprehension capabilities. This work explores applying LLMs to vehicular networks, aiming to jointly optimize vehicle-to-infrastructure (V2I) communications and autonomous driving (AD) policies. We deploy LLMs for AD decision-making to maximize traffic flow and avoid collisions for road safety, and a double deep Q-learning algorithm (DDQN) is used for V2I optimization to maximize the received data rate and reduce frequent handovers. In particular, for LLM-enabled AD, we employ the Euclidean distance to identify previously explored AD experiences, and then LLMs can learn from past good and bad decisions for further improvement. Then, LLM-based AD decisions will become part of states in V2I problems, and DDQN will optimize the V2I decisions accordingly. After that, the AD and V2I decisions are iteratively optimized until convergence. Such an iterative optimization approach can better explore the interactions between LLMs and conventional reinforcement learning techniques, revealing the potential of using LLMs for network optimization and management. Finally, the simulations demonstrate that our proposed hybrid LLM-DDQN approach outperforms the conventional DDQN algorithm, showing faster convergence and higher average rewards.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.NI",
            "eess.SY"
        ],
        "comment": "Submission for possible publication"
    },
    {
        "paper id": "2410.08869",
        "abstract url": "https://arxiv.org/abs/2410.08869",
        "title": "Evolution of SAE Features Across Layers in LLMs",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Sparse Autoencoders for transformer-based language models are typically defined independently per layer. In this work we analyze statistical relationships between features in adjacent layers to understand how features evolve through a forward pass. We provide a graph visualization interface for features and their most similar next-layer neighbors, and build communities of related features across layers. We find that a considerable amount of features are passed through from a previous layer, some features can be expressed as quasi-boolean combinations of previous features, and some features become more specialized in later layers.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08872",
        "abstract url": "https://arxiv.org/abs/2410.08872",
        "title": "Fragile Giants: Understanding the Susceptibility of Models to Subpopulation Attacks",
        "rating": "-0.5",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "As machine learning models become increasingly complex, concerns about their robustness and trustworthiness have become more pressing. A critical vulnerability of these models is data poisoning attacks, where adversaries deliberately alter training data to degrade model performance. One particularly stealthy form of these attacks is subpopulation poisoning, which targets distinct subgroups within a dataset while leaving overall performance largely intact. The ability of these attacks to generalize within subpopulations poses a significant risk in real-world settings, as they can be exploited to harm marginalized or underrepresented groups within the dataset. In this work, we investigate how model complexity influences susceptibility to subpopulation poisoning attacks. We introduce a theoretical framework that explains how overparameterized models, due to their large capacity, can inadvertently memorize and misclassify targeted subpopulations. To validate our theory, we conduct extensive experiments on large-scale image and text datasets using popular model architectures. Our results show a clear trend: models with more parameters are significantly more vulnerable to subpopulation poisoning. Moreover, we find that attacks on smaller, human-interpretable subgroups often go undetected by these models. These results highlight the need to develop defenses that specifically address subpopulation vulnerabilities.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08875",
        "abstract url": "https://arxiv.org/abs/2410.08875",
        "title": "Online design of dynamic networks",
        "rating": "-0.5",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "cs.AI",
                "cs.SI"
            ]
        ],
        "abstract": "Designing a network (e.g., a telecommunication or transport network) is mainly done offline, in a planning phase, prior to the operation of the network. On the other hand, a massive effort has been devoted to characterizing dynamic networks, i.e., those that evolve over time. The novelty of this paper is that we introduce a method for the online design of dynamic networks. The need to do so emerges when a network needs to operate in a dynamic and stochastic environment. In this case, one may wish to build a network over time, on the fly, in order to react to the changes of the environment and to keep certain performance targets. We tackle this online design problem with a rolling horizon optimization based on Monte Carlo Tree Search. The potential of online network design is showcased for the design of a futuristic dynamic public transport network, where bus lines are constructed on the fly to better adapt to a stochastic user demand. In such a scenario, we compare our results with state-of-the-art dynamic vehicle routing problem (VRP) resolution methods, simulating requests from a New York City taxi dataset. Differently from classic VRP methods, that extend vehicle trajectories in isolation, our method enables us to build a structured network of line buses, where complex user journeys are possible, thus increasing system performance.",
        "subjects": [
            "cs.AI",
            "cs.SI",
            "physics.soc-ph"
        ],
        "comment": "14 pages"
    },
    {
        "paper id": "2410.08886",
        "abstract url": "https://arxiv.org/abs/2410.08886",
        "title": "Bank Loan Prediction Using Machine Learning Techniques",
        "rating": "-0.5",
        "keywords": [
            [
                "SVM"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Banks are important for the development of economies in any financial ecosystem through consumer and business loans. Lending, however, presents risks; thus, banks have to determine the applicant's financial position to reduce the probabilities of default. A number of banks have currently, therefore, adopted data analytics and state-of-the-art technology to arrive at better decisions in the process. The probability of payback is prescribed by a predictive modeling technique in which machine learning algorithms are applied. In this research project, we will apply several machine learning methods to further improve the accuracy and efficiency of loan approval processes. Our work focuses on the prediction of bank loan approval; we have worked on a dataset of 148,670 instances and 37 attributes using machine learning methods. The target property segregates the loan applications into \"Approved\" and \"Denied\" groups. various machine learning techniques have been used, namely, Decision Tree Categorization, AdaBoosting, Random Forest Classifier, SVM, and GaussianNB. Following that, the models were trained and evaluated. Among these, the best-performing algorithm was AdaBoosting, which achieved an incredible accuracy of 99.99%. The results therefore show how ensemble learning works effectively to improve the prediction skills of loan approval decisions. The presented work points to the possibility of achieving extremely accurate and efficient loan prediction models that provide useful insights for applying machine learning to financial domains.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "10 pages, 18 figures, 6 tables"
    },
    {
        "paper id": "2410.08892",
        "abstract url": "https://arxiv.org/abs/2410.08892",
        "title": "Federated Learning in Practice: Reflections and Projections",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL) is a machine learning technique that enables multiple entities to collaboratively learn a shared model without exchanging their local data. Over the past decade, FL systems have achieved substantial progress, scaling to millions of devices across various learning domains while offering meaningful differential privacy (DP) guarantees. Production systems from organizations like Google, Apple, and Meta demonstrate the real-world applicability of FL. However, key challenges remain, including verifying server-side DP guarantees and coordinating training across heterogeneous devices, limiting broader adoption. Additionally, emerging trends such as large (multi-modal) models and blurred lines between training, inference, and personalization challenge traditional FL frameworks. In response, we propose a redefined FL framework that prioritizes privacy principles rather than rigid definitions. We also chart a path forward by leveraging trusted execution environments and open-source ecosystems to address these challenges and facilitate future advancements in FL.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08924",
        "abstract url": "https://arxiv.org/abs/2410.08924",
        "title": "DiffPO: A causal diffusion model for learning distributions of potential outcomes",
        "rating": "-0.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Predicting potential outcomes of interventions from observational data is crucial for decision-making in medicine, but the task is challenging due to the fundamental problem of causal inference. Existing methods are largely limited to point estimates of potential outcomes with no uncertain quantification; thus, the full information about the distributions of potential outcomes is typically ignored. In this paper, we propose a novel causal diffusion model called DiffPO, which is carefully designed for reliable inferences in medicine by learning the distribution of potential outcomes. In our DiffPO, we leverage a tailored conditional denoising diffusion model to learn complex distributions, where we address the selection bias through a novel orthogonal diffusion loss. Another strength of our DiffPO method is that it is highly flexible (e.g., it can also be used to estimate different causal quantities such as CATE). Across a wide range of experiments, we show that our method achieves state-of-the-art performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08934",
        "abstract url": "https://arxiv.org/abs/2410.08934",
        "title": "The Effect of Personalization in FedProx: A Fine-grained Analysis on Statistical Accuracy and Communication Efficiency",
        "rating": "-0.5",
        "keywords": [
            [
                "federated learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "FedProx is a simple yet effective federated learning method that enables model personalization via regularization. Despite remarkable success in practice, a rigorous analysis of how such a regularization provably improves the statistical accuracy of each client's local model hasn't been fully established. Setting the regularization strength heuristically presents a risk, as an inappropriate choice may even degrade accuracy. This work fills in the gap by analyzing the effect of regularization on statistical accuracy, thereby providing a theoretical guideline for setting the regularization strength for achieving personalization. We prove that by adaptively choosing the regularization strength under different statistical heterogeneity, FedProx can consistently outperform pure local training and achieve a nearly minimax-optimal statistical rate. In addition, to shed light on resource allocation, we design an algorithm, provably showing that stronger personalization reduces communication complexity without increasing the computation cost overhead. Finally, our theory is validated on both synthetic and real-world datasets and its generalizability is verified in a non-convex setting.",
        "subjects": [
            "stat.ML",
            "cs.DC",
            "cs.LG",
            "math.ST",
            "stat.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08947",
        "abstract url": "https://arxiv.org/abs/2410.08947",
        "title": "Meta-Transfer Learning Empowered Temporal Graph Networks for Cross-City Real Estate Appraisal",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Real estate appraisal is important for a variety of endeavors such as real estate deals, investment analysis, and real property taxation. Recently, deep learning has shown great promise for real estate appraisal by harnessing substantial online transaction data from web platforms. Nonetheless, deep learning is data-hungry, and thus it may not be trivially applicable to enormous small cities with limited data. To this end, we propose Meta-Transfer Learning Empowered Temporal Graph Networks (MetaTransfer) to transfer valuable knowledge from multiple data-rich metropolises to the data-scarce city to improve valuation performance. Specifically, by modeling the ever-growing real estate transactions with associated residential communities as a temporal event heterogeneous graph, we first design an Event-Triggered Temporal Graph Network to model the irregular spatiotemporal correlations between evolving real estate transactions. Besides, we formulate the city-wide real estate appraisal as a multi-task dynamic graph link label prediction problem, where the valuation of each community in a city is regarded as an individual task. A Hypernetwork-Based Multi-Task Learning module is proposed to simultaneously facilitate intra-city knowledge sharing between multiple communities and task-specific parameters generation to accommodate the community-wise real estate price distribution. Furthermore, we propose a Tri-Level Optimization Based Meta- Learning framework to adaptively re-weight training transaction instances from multiple source cities to mitigate negative transfer, and thus improve the cross-city knowledge transfer effectiveness. Finally, extensive experiments based on five real-world datasets demonstrate the significant superiority of MetaTransfer compared with eleven baseline algorithms.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "12 pages"
    },
    {
        "paper id": "2410.08961",
        "abstract url": "https://arxiv.org/abs/2410.08961",
        "title": "Evaluating Federated Kolmogorov-Arnold Networks on Non-IID Data",
        "rating": "-0.5",
        "keywords": [
            [
                "federated learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Federated Kolmogorov-Arnold Networks (F-KANs) have already been proposed, but their assessment is at an initial stage. We present a comparison between KANs (using B-splines and Radial Basis Functions as activation functions) and Multi- Layer Perceptrons (MLPs) with a similar number of parameters for 100 rounds of federated learning in the MNIST classification task using non-IID partitions with 100 clients. After 15 trials for each model, we show that the best accuracies achieved by MLPs can be achieved by Spline-KANs in half of the time (in rounds), with just a moderate increase in computing time.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "10 pages, 5 figures, for associated code see https://github.com/artsasse/fedkan"
    },
    {
        "paper id": "2410.08977",
        "abstract url": "https://arxiv.org/abs/2410.08977",
        "title": "Online-to-PAC generalization bounds under graph-mixing dependencies",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Traditional generalization results in statistical learning require a training data set made of independently drawn examples. Most of the recent efforts to relax this independence assumption have considered either purely temporal (mixing) dependencies, or graph-dependencies, where non-adjacent vertices correspond to independent random variables. Both approaches have their own limitations, the former requiring a temporal ordered structure, and the latter lacking a way to quantify the strength of inter-dependencies. In this work, we bridge these two lines of work by proposing a framework where dependencies decay with graph distance. We derive generalization bounds leveraging the online-to-PAC framework, by deriving a concentration result and introducing an online learning framework incorporating the graph structure. The resulting high-probability generalization guarantees depend on both the mixing rate and the graph's chromatic number.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "13 pages (10 main + 3 supplementary material). All authors contributed equally"
    },
    {
        "paper id": "2410.09046",
        "abstract url": "https://arxiv.org/abs/2410.09046",
        "title": "Linear Convergence of Diffusion Models Under the Manifold Hypothesis",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Score-matching generative models have proven successful at sampling from complex high-dimensional data distributions. In many applications, this distribution is believed to concentrate on a much lower $d$-dimensional manifold embedded into $D$-dimensional space; this is known as the manifold hypothesis. The current best-known convergence guarantees are either linear in $D$ or polynomial (superlinear) in $d$. The latter exploits a novel integration scheme for the backward SDE. We take the best of both worlds and show that the number of steps diffusion models require in order to converge in Kullback-Leibler~(KL) divergence is linear (up to logarithmic terms) in the intrinsic dimension $d$. Moreover, we show that this linear dependency is sharp.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09049",
        "abstract url": "https://arxiv.org/abs/2410.09049",
        "title": "SceneCraft: Layout-Guided 3D Scene Generation",
        "rating": "-0.5",
        "keywords": [
            [
                "3D",
                "depth",
                "NeRF"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "The creation of complex 3D scenes tailored to user specifications has been a tedious and challenging task with traditional 3D modeling tools. Although some pioneering methods have achieved automatic text-to-3D generation, they are generally limited to small-scale scenes with restricted control over the shape and texture. We introduce SceneCraft, a novel method for generating detailed indoor scenes that adhere to textual descriptions and spatial layout preferences provided by users. Central to our method is a rendering-based technique, which converts 3D semantic layouts into multi-view 2D proxy maps. Furthermore, we design a semantic and depth conditioned diffusion model to generate multi-view images, which are used to learn a neural radiance field (NeRF) as the final scene representation. Without the constraints of panorama image generation, we surpass previous methods in supporting complicated indoor space generation beyond a single room, even as complicated as a whole multi-bedroom apartment with irregular shapes and layouts. Through experimental analysis, we demonstrate that our method significantly outperforms existing approaches in complex indoor scene generation with diverse textures, consistent geometry, and realistic visual quality. Code and more results are available at: https://orangesodahub.github.io/SceneCraft",
        "subjects": [
            "cs.CV"
        ],
        "comment": "NeurIPS 2024. Code: https://github.com/OrangeSodahub/SceneCraft Project Page: https://orangesodahub.github.io/SceneCraft"
    },
    {
        "paper id": "2410.09127",
        "abstract url": "https://arxiv.org/abs/2410.09127",
        "title": "CYCLE: Cross-Year Contrastive Learning in Entity-Linking",
        "rating": "-0.5",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Knowledge graphs constantly evolve with new entities emerging, existing definitions being revised, and entity relationships changing. These changes lead to temporal degradation in entity linking models, characterized as a decline in model performance over time. To address this issue, we propose leveraging graph relationships to aggregate information from neighboring entities across different time periods. This approach enhances the ability to distinguish similar entities over time, thereby minimizing the impact of temporal degradation. We introduce \\textbf{CYCLE}: \\textbf{C}ross-\\textbf{Y}ear \\textbf{C}ontrastive \\textbf{L}earning for \\textbf{E}ntity-Linking. This model employs a novel graph contrastive learning method to tackle temporal performance degradation in entity linking tasks. Our contrastive learning method treats newly added graph relationships as \\textit{positive} samples and newly removed ones as \\textit{negative} samples. This approach helps our model effectively prevent temporal degradation, achieving a 13.90\\% performance improvement over the state-of-the-art from 2023 when the time gap is one year, and a 17.79\\% improvement as the gap expands to three years. Further analysis shows that CYCLE is particularly robust for low-degree entities, which are less resistant to temporal degradation due to their sparse connectivity, making them particularly suitable for our method. The code and data are made available at \\url{https://github.com/pengyu-zhang/CYCLE-Cross-Year-Contrastive-Learning-in-Entity-Linking}.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09134",
        "abstract url": "https://arxiv.org/abs/2410.09134",
        "title": "Multi-Agent Actor-Critics in Autonomous Cyber Defense",
        "rating": "-0.5",
        "keywords": [
            [
                "attack"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The need for autonomous and adaptive defense mechanisms has become paramount in the rapidly evolving landscape of cyber threats. Multi-Agent Deep Reinforcement Learning (MADRL) presents a promising approach to enhancing the efficacy and resilience of autonomous cyber operations. This paper explores the application of Multi-Agent Actor-Critic algorithms which provides a general form in Multi-Agent learning to cyber defense, leveraging the collaborative interactions among multiple agents to detect, mitigate, and respond to cyber threats. We demonstrate each agent is able to learn quickly and counter act on the threats autonomously using MADRL in simulated cyber-attack scenarios. The results indicate that MADRL can significantly enhance the capability of autonomous cyber defense systems, paving the way for more intelligent cybersecurity strategies. This study contributes to the growing body of knowledge on leveraging artificial intelligence for cybersecurity and sheds light for future research and development in autonomous cyber operations.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.MA"
        ],
        "comment": "6 pages. 2 figures"
    },
    {
        "paper id": "2410.09173",
        "abstract url": "https://arxiv.org/abs/2410.09173",
        "title": "Resource-Constrained Heuristic for Max-SAT",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "We propose a resource-constrained heuristic for instances of Max-SAT that iteratively decomposes a larger problem into smaller subcomponents that can be solved by optimized solvers and hardware. The unconstrained outer loop maintains the state space of a given problem and selects a subset of the SAT variables for optimization independent of previous calls. The resource-constrained inner loop maximizes the number of satisfiable clauses in the \"sub-SAT\" problem. Our outer loop is agnostic to the mechanisms of the inner loop, allowing for the use of traditional solvers for the optimization step. However, we can also transform the selected \"sub-SAT\" problem into a quadratic unconstrained binary optimization (QUBO) one and use specialized hardware for optimization. In contrast to existing solutions that convert a SAT instance into a QUBO one before decomposition, we choose a subset of the SAT variables before QUBO optimization. We analyze a set of variable selection methods, including a novel graph-based method that exploits the structure of a given SAT instance. The number of QUBO variables needed to encode a (sub-)SAT problem varies, so we additionally learn a model that predicts the size of sub-SAT problems that will fit a fixed-size QUBO solver. We empirically demonstrate our results on a set of randomly generated Max-SAT instances as well as real world examples from the Max-SAT evaluation benchmarks and outperform existing QUBO decomposer solutions.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09225",
        "abstract url": "https://arxiv.org/abs/2410.09225",
        "title": "Harmonizing vs Polarizing Platform Influence Functions",
        "rating": "-0.5",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "We investigate the dynamics of opinion formation on social networking platforms, focusing on how individual opinions, influenced by both social connections and platform algorithms, evolve. We model this process using a differential equation, accounting for both peer influence and the platform's content curation based on user preferences. Our primary aim is to analyze how these factors contribute to opinion polarization and identify potential strategies for its mitigation. We explore the conditions under which opinions converge to a consensus or remain polarized, emphasizing the role of the platform's influence function. Our findings in two-agent, complete graphs, and stochastic block model provide insights into the impact of social media algorithms on public discourse and offer a framework for understanding how polarization can be avoided.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09230",
        "abstract url": "https://arxiv.org/abs/2410.09230",
        "title": "Improving semantic understanding in speech language models via brain-tuning",
        "rating": "-0.5",
        "keywords": [
            [
                "fMRI"
            ],
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Speech language models align with human brain responses to natural language to an impressive degree. However, current models rely heavily on low-level speech features, indicating they lack brain-relevant semantics which limits their utility as model organisms of semantic processing in the brain. In this work, we address this limitation by inducing brain-relevant bias directly into the models via fine-tuning with fMRI recordings of people listening to natural stories, a process we name brain-tuning. After testing it on 3 different pretrained model families, we show that brain-tuning not only improves overall alignment with new brain recordings in semantic language regions, but also reduces the reliance on low-level speech features for this alignment. Excitingly, we further show that brain-tuning leads to 1) consistent improvements in performance on a range of downstream tasks and 2) a representational space with increased semantic preference. Our results provide converging evidence, for the first time, that incorporating brain signals into the training of language models improves the models' semantic understanding.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Under Review at ICLR 2025"
    },
    {
        "paper id": "2410.09280",
        "abstract url": "https://arxiv.org/abs/2410.09280",
        "title": "Predicting Drug Effects from High-Dimensional, Asymmetric Drug Datasets by Using Graph Neural Networks: A Comprehensive Analysis of Multitarget Drug Effect Prediction",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph neural networks (GNNs) have emerged as one of the most effective ML techniques for drug effect prediction from drug molecular graphs. Despite having immense potential, GNN models lack performance when using datasets that contain high-dimensional, asymmetrically co-occurrent drug effects as targets with complex correlations between them. Training individual learning models for each drug effect and incorporating every prediction result for a wide spectrum of drug effects are impractical. Therefore, an opportunity exists to address this challenge as multitarget prediction problems and predict all drug effects at a time. We developed standard and hybrid GNNs to perform two separate tasks: multiregression for continuous values and multilabel classification for categorical values contained in our datasets. Because multilabel classification makes the target data even more sparse and introduces asymmetric label co-occurrence, learning these models becomes difficult and heavily impacts the GNN's performance. To address these challenges, we propose a new data oversampling technique to improve multilabel classification performances on all the given imbalanced molecular graph datasets. Using the technique, we improve the data imbalance ratio of the drug effects while protecting the datasets' integrity. Finally, we evaluate the multilabel classification performance of the best-performing hybrid GNN model on all the oversampled datasets obtained from the proposed oversampling technique. In all the evaluation metrics (i.e., precision, recall, and F1 score), this model significantly outperforms other ML models, including GNN models when they are trained on the original datasets or oversampled datasets with MLSMOTE, which is a well-known oversampling technique.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "8 pages, 4 figures, 14 sub-figures, 4 tables"
    },
    {
        "paper id": "2410.09286",
        "abstract url": "https://arxiv.org/abs/2410.09286",
        "title": "Language-Model-Assisted Bi-Level Programming for Reward Learning from Internet Videos",
        "rating": "-0.5",
        "keywords": [
            [
                "vision-language",
                "VLM"
            ],
            [
                "biological"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Learning from Demonstrations, particularly from biological experts like humans and animals, often encounters significant data acquisition challenges. While recent approaches leverage internet videos for learning, they require complex, task-specific pipelines to extract and retarget motion data for the agent. In this work, we introduce a language-model-assisted bi-level programming framework that enables a reinforcement learning agent to directly learn its reward from internet videos, bypassing dedicated data preparation. The framework includes two levels: an upper level where a vision-language model (VLM) provides feedback by comparing the learner's behavior with expert videos, and a lower level where a large language model (LLM) translates this feedback into reward updates. The VLM and LLM collaborate within this bi-level framework, using a \"chain rule\" approach to derive a valid search direction for reward learning. We validate the method for reward learning from YouTube videos, and the results have shown that the proposed method enables efficient reward design from expert videos of biological agents for complex behavior synthesis.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09307",
        "abstract url": "https://arxiv.org/abs/2410.09307",
        "title": "Graph Neural Alchemist: An innovative fully modular architecture for time series-to-graph classification",
        "rating": "-0.5",
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "This paper introduces a novel Graph Neural Network (GNN) architecture for time series classification, based on visibility graph representations. Traditional time series classification methods often struggle with high computational complexity and inadequate capture of spatio-temporal dynamics. By representing time series as visibility graphs, it is possible to encode both spatial and temporal dependencies inherent to time series data, while being computationally efficient. Our architecture is fully modular, enabling flexible experimentation with different models and representations. We employ directed visibility graphs encoded with in-degree and PageRank features to improve the representation of time series, ensuring efficient computation while enhancing the model's ability to capture long-range dependencies in the data. We show the robustness and generalization capability of the proposed architecture across a diverse set of classification tasks and against a traditional model. Our work represents a significant advancement in the application of GNNs for time series analysis, offering a powerful and flexible framework for future research and practical implementations.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09319",
        "abstract url": "https://arxiv.org/abs/2410.09319",
        "title": "Hey AI Can You Grade My Essay?: Automatic Essay Grading",
        "rating": "-0.5",
        "keywords": [
            [
                "grammatical"
            ],
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Automatic essay grading (AEG) has attracted the the attention of the NLP community because of its applications to several educational applications, such as scoring essays, short answers, etc. AEG systems can save significant time and money when grading essays. In the existing works, the essays are graded where a single network is responsible for the whole process, which may be ineffective because a single network may not be able to learn all the features of a human-written essay. In this work, we have introduced a new model that outperforms the state-of-the-art models in the field of AEG. We have used the concept of collaborative and transfer learning, where one network will be responsible for checking the grammatical and structural features of the sentences of an essay while another network is responsible for scoring the overall idea present in the essay. These learnings are transferred to another network to score the essay. We also compared the performances of the different models mentioned in our work, and our proposed model has shown the highest accuracy of 85.50%.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Accepted in ICAAAIML (4th International Conference on Advances and Applications of Artificial Intelligence and Machine Learning) 2023"
    },
    {
        "paper id": "2410.09348",
        "abstract url": "https://arxiv.org/abs/2410.09348",
        "title": "BANGS: Game-Theoretic Node Selection for Graph Self-Training",
        "rating": "-0.5",
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Graph self-training is a semi-supervised learning method that iteratively selects a set of unlabeled data to retrain the underlying graph neural network (GNN) model and improve its prediction performance. While selecting highly confident nodes has proven effective for self-training, this pseudo-labeling strategy ignores the combinatorial dependencies between nodes and suffers from a local view of the distribution. To overcome these issues, we propose BANGS, a novel framework that unifies the labeling strategy with conditional mutual information as the objective of node selection. Our approach -- grounded in game theory -- selects nodes in a combinatorial fashion and provides theoretical guarantees for robustness under noisy objective. More specifically, unlike traditional methods that rank and select nodes independently, BANGS considers nodes as a collective set in the self-training process. Our method demonstrates superior performance and robustness across various datasets, base models, and hyperparameter settings, outperforming existing techniques. The codebase is available on https://github.com/fangxin-wang/BANGS .",
        "subjects": [
            "cs.LG",
            "cs.SI"
        ],
        "comment": "Preprint"
    },
    {
        "paper id": "2410.10887",
        "abstract url": "https://arxiv.org/abs/2410.10887",
        "title": "ActNAS : Generating Efficient YOLO Models using Activation NAS",
        "rating": "-0.5",
        "keywords": [
            [
                "Architecture Search",
                "NAS"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Activation functions introduce non-linearity into Neural Networks, enabling them to learn complex patterns. Different activation functions vary in speed and accuracy, ranging from faster but less accurate options like ReLU to slower but more accurate functions like SiLU or SELU. Typically, same activation function is used throughout an entire model architecture. In this paper, we conduct a comprehensive study on the effects of using mixed activation functions in YOLO-based models, evaluating their impact on latency, memory usage, and accuracy across CPU, NPU, and GPU edge devices. We also propose a novel approach that leverages Neural Architecture Search (NAS) to design YOLO models with optimized mixed activation functions.The best model generated through this method demonstrates a slight improvement in mean Average Precision (mAP) compared to baseline model (SiLU), while it is 22.28% faster and consumes 64.15% less memory on the reference NPU device.",
        "subjects": [
            "cs.LG",
            "cs.NE"
        ],
        "comment": "7 pages, 4 figures, FITML workshop, NeuRIPS 2024"
    },
    {
        "paper id": "2410.08527",
        "abstract url": "https://arxiv.org/abs/2410.08527",
        "title": "Scaling Laws for Predicting Downstream Performance in LLMs",
        "rating": "-1",
        "keywords": [
            [
                "forecast"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Precise estimation of downstream performance in large language models (LLMs) prior to training is essential for guiding their development process. Scaling laws analysis utilizes the statistics of a series of significantly smaller sampling language models (LMs) to predict the performance of the target LLM. For downstream performance prediction, the critical challenge lies in the emergent abilities in LLMs that occur beyond task-specific computational thresholds. In this work, we focus on the pre-training loss as a more computation-efficient metric for performance estimation. Our two-stage approach consists of first estimating a function that maps computational resources (e.g., FLOPs) to the pre-training Loss using a series of sampling models, followed by mapping the pre-training loss to downstream task Performance after the critical \"emergent phase\". In preliminary experiments, this FLP solution accurately predicts the performance of LLMs with 7B and 13B parameters using a series of sampling LMs up to 3B, achieving error margins of 5% and 10%, respectively, and significantly outperforming the FLOPs-to-Performance approach. This motivates FLP-M, a fundamental approach for performance prediction that addresses the practical need to integrate datasets from multiple sources during pre-training, specifically blending general corpora with code data to accurately represent the common necessity. FLP-M extends the power law analytical function to predict domain-specific pre-training loss based on FLOPs across data sources, and employs a two-layer neural network to model the non-linear relationship between multiple domain-specific loss and downstream performance. By utilizing a 3B LLM trained on a specific ratio and a series of smaller sampling LMs, FLP-M can effectively forecast the performance of 3B and 7B LLMs across various data mixtures for most benchmarks within 10% error margins.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08545",
        "abstract url": "https://arxiv.org/abs/2410.08545",
        "title": "Humanity in AI: Detecting the Personality of Large Language Models",
        "rating": "-1",
        "keywords": [
            [
                "psychological"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Questionnaires are a common method for detecting the personality of Large Language Models (LLMs). However, their reliability is often compromised by two main issues: hallucinations (where LLMs produce inaccurate or irrelevant responses) and the sensitivity of responses to the order of the presented options. To address these issues, we propose combining text mining with questionnaires method. Text mining can extract psychological features from the LLMs' responses without being affected by the order of options. Furthermore, because this method does not rely on specific answers, it reduces the influence of hallucinations. By normalizing the scores from both methods and calculating the root mean square error, our experiment results confirm the effectiveness of this approach. To further investigate the origins of personality traits in LLMs, we conduct experiments on both pre-trained language models (PLMs), such as BERT and GPT, as well as conversational models (ChatLLMs), such as ChatGPT. The results show that LLMs do contain certain personalities, for example, ChatGPT and ChatGLM exhibit the personality traits of 'Conscientiousness'. Additionally, we find that the personalities of LLMs are derived from their pre-trained data. The instruction data used to train ChatLLMs can enhance the generation of data containing personalities and expose their hidden personality. We compare the results with the human average personality score, and we find that the personality of FLAN-T5 in PLMs and ChatGPT in ChatLLMs is more similar to that of a human, with score differences of 0.34 and 0.22, respectively.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08567",
        "abstract url": "https://arxiv.org/abs/2410.08567",
        "title": "Diffusion-Based Depth Inpainting for Transparent and Reflective Objects",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "RGB-D",
                "Depth"
            ],
            [
                "Diffusion",
                "Inpainting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Transparent and reflective objects, which are common in our everyday lives, present a significant challenge to 3D imaging techniques due to their unique visual and optical properties. Faced with these types of objects, RGB-D cameras fail to capture the real depth value with their accurate spatial information. To address this issue, we propose DITR, a diffusion-based Depth Inpainting framework specifically designed for Transparent and Reflective objects. This network consists of two stages, including a Region Proposal stage and a Depth Inpainting stage. DITR dynamically analyzes the optical and geometric depth loss and inpaints them automatically. Furthermore, comprehensive experimental results demonstrate that DITR is highly effective in depth inpainting tasks of transparent and reflective objects with robust adaptability.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08601",
        "abstract url": "https://arxiv.org/abs/2410.08601",
        "title": "StraGo: Harnessing Strategic Guidance for Prompt Optimization",
        "rating": "-1",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Prompt engineering is pivotal for harnessing the capabilities of large language models (LLMs) across diverse applications. While existing prompt optimization methods improve prompt effectiveness, they often lead to prompt drifting, where newly generated prompts can adversely impact previously successful cases while addressing failures. Furthermore, these methods tend to rely heavily on LLMs' intrinsic capabilities for prompt optimization tasks. In this paper, we introduce StraGo (Strategic-Guided Optimization), a novel approach designed to mitigate prompt drifting by leveraging insights from both successful and failed cases to identify critical factors for achieving optimization objectives. StraGo employs a how-to-do methodology, integrating in-context learning to formulate specific, actionable strategies that provide detailed, step-by-step guidance for prompt optimization. Extensive experiments conducted across a range of tasks, including reasoning, natural language understanding, domain-specific knowledge, and industrial applications, demonstrate StraGo's superior performance. It establishes a new state-of-the-art in prompt optimization, showcasing its ability to deliver stable and effective prompt improvements.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "19 pages, 3 figures, 20 tables"
    },
    {
        "paper id": "2410.08607",
        "abstract url": "https://arxiv.org/abs/2410.08607",
        "title": "Riemannian Gradient Descent Method to Joint Blind Super-Resolution and Demixing in ISAC",
        "rating": "-1",
        "keywords": [
            [
                "Super-Resolution"
            ]
        ],
        "abstract": "Integrated Sensing and Communication (ISAC) has emerged as a promising technology for next-generation wireless networks. In this work, we tackle an ill-posed parameter estimation problem within ISAC, formulating it as a joint blind super-resolution and demixing problem. Leveraging the low-rank structures of the vectorized Hankel matrices associated with the unknown parameters, we propose a Riemannian gradient descent (RGD) method. Our theoretical analysis demonstrates that the proposed method achieves linear convergence to the target matrices under standard assumptions. Additionally, extensive numerical experiments validate the effectiveness of the proposed approach.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08612",
        "abstract url": "https://arxiv.org/abs/2410.08612",
        "title": "Synth-SONAR: Sonar Image Synthesis with Enhanced Diversity and Realism via Dual Diffusion Models and GPT Prompting",
        "rating": "-1",
        "keywords": [
            [
                "visual language",
                "VLMs"
            ],
            [
                "Diffusion"
            ],
            [
                "biology"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Sonar image synthesis is crucial for advancing applications in underwater exploration, marine biology, and defence. Traditional methods often rely on extensive and costly data collection using sonar sensors, jeopardizing data quality and diversity. To overcome these limitations, this study proposes a new sonar image synthesis framework, Synth-SONAR leveraging diffusion models and GPT prompting. The key novelties of Synth-SONAR are threefold: First, by integrating Generative AI-based style injection techniques along with publicly available real/simulated data, thereby producing one of the largest sonar data corpus for sonar research. Second, a dual text-conditioning sonar diffusion model hierarchy synthesizes coarse and fine-grained sonar images with enhanced quality and diversity. Third, high-level (coarse) and low-level (detailed) text-based sonar generation methods leverage advanced semantic information available in visual language models (VLMs) and GPT-prompting. During inference, the method generates diverse and realistic sonar images from textual prompts, bridging the gap between textual descriptions and sonar image generation. This marks the application of GPT-prompting in sonar imagery for the first time, to the best of our knowledge. Synth-SONAR achieves state-of-the-art results in producing high-quality synthetic sonar datasets, significantly enhancing their diversity and realism.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "12 pages, 5 tables and 9 figures"
    },
    {
        "paper id": "2410.08613",
        "abstract url": "https://arxiv.org/abs/2410.08613",
        "title": "Cross-Modal Bidirectional Interaction Model for Referring Remote Sensing Image Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "Remote Sensing"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Given a natural language expression and a remote sensing image, the goal of referring remote sensing image segmentation (RRSIS) is to generate a pixel-level mask of the target object identified by the referring expression. In contrast to natural scenarios, expressions in RRSIS often involve complex geospatial relationships, with target objects of interest that vary significantly in scale and lack visual saliency, thereby increasing the difficulty of achieving precise segmentation. To address the aforementioned challenges, a novel RRSIS framework is proposed, termed the cross-modal bidirectional interaction model (CroBIM). Specifically, a context-aware prompt modulation (CAPM) module is designed to integrate spatial positional relationships and task-specific knowledge into the linguistic features, thereby enhancing the ability to capture the target object. Additionally, a language-guided feature aggregation (LGFA) module is introduced to integrate linguistic information into multi-scale visual features, incorporating an attention deficit compensation mechanism to enhance feature aggregation. Finally, a mutual-interaction decoder (MID) is designed to enhance cross-modal feature alignment through cascaded bidirectional cross-attention, thereby enabling precise segmentation mask prediction. To further forster the research of RRSIS, we also construct RISBench, a new large-scale benchmark dataset comprising 52,472 image-language-label triplets. Extensive benchmarking on RISBench and two other prevalent datasets demonstrates the superior performance of the proposed CroBIM over existing state-of-the-art (SOTA) methods. The source code for CroBIM and the RISBench dataset will be publicly available at https://github.com/HIT-SIRS/CroBIM",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08620",
        "abstract url": "https://arxiv.org/abs/2410.08620",
        "title": "Natural Language Induced Adversarial Images",
        "rating": "-1",
        "keywords": [
            [
                "image editing",
                "text-to-image"
            ],
            [
                "attacks"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Research of adversarial attacks is important for AI security because it shows the vulnerability of deep learning models and helps to build more robust models. Adversarial attacks on images are most widely studied, which include noise-based attacks, image editing-based attacks, and latent space-based attacks. However, the adversarial examples crafted by these methods often lack sufficient semantic information, making it challenging for humans to understand the failure modes of deep learning models under natural conditions. To address this limitation, we propose a natural language induced adversarial image attack method. The core idea is to leverage a text-to-image model to generate adversarial images given input prompts, which are maliciously constructed to lead to misclassification for a target model. To adopt commercial text-to-image models for synthesizing more natural adversarial images, we propose an adaptive genetic algorithm (GA) for optimizing discrete adversarial prompts without requiring gradients and an adaptive word space reduction method for improving query efficiency. We further used CLIP to maintain the semantic consistency of the generated images. In our experiments, we found that some high-frequency semantic information such as \"foggy\", \"humid\", \"stretching\", etc. can easily cause classifier errors. This adversarial semantic information exists not only in generated images but also in photos captured in the real world. We also found that some adversarial semantic information can be transferred to unknown classification tasks. Furthermore, our attack method can transfer to different text-to-image models (e.g., Midjourney, DALL-E 3, etc.) and image classifiers. Our code is available at: https://github.com/zxp555/Natural-Language-Induced-Adversarial-Images.",
        "subjects": [
            "cs.CR",
            "cs.CV",
            "cs.MM"
        ],
        "comment": "Carmera-ready version. To appear in ACM MM 2024"
    },
    {
        "paper id": "2410.08625",
        "abstract url": "https://arxiv.org/abs/2410.08625",
        "title": "Data-driven Feedback Control of Lattice Structures with Localized Actuation and Sensing",
        "rating": "-1",
        "keywords": [
            [
                "voxel"
            ]
        ],
        "abstract": "Assembling lattices from discrete building blocks enables the composition of large, heterogeneous, and easily reconfigurable objects with desirable mass-to-stiffness ratios. This type of building system may also be referred to as a digital material, as it is constituted from discrete, error-correcting components. Researchers have demonstrated various active structures and even robotic systems that take advantage of the reconfigurable, mass-efficient properties of discrete lattice structures. However, the existing literature has predominantly used open-loop control strategies, limiting the performance of the presented systems. In this paper, we present a novel approach to feedback control of digital lattice structures, leveraging real-time measurements of the system dynamics. We introduce an actuated voxel which constitutes a novel means for actuation of lattice structures. Our control method is based on the Extended Dynamical Mode Decomposition algorithm in conjunction with the Linear Quadratic Regulator and the Koopman Model Predictive Control. The key advantage of our approach lies in its purely data-driven nature, without the need for any prior knowledge of a system's structure. We illustrate the developed method via real experiments with custom-built flexible lattice beam, showing its ability to accomplish various tasks even with minimal sensing and actuation resources. In particular, we address two problems: stabilization together with disturbance attenuation, and reference tracking.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08627",
        "abstract url": "https://arxiv.org/abs/2410.08627",
        "title": "Making a Complete Mess and Getting Away with it: Traveling Salesperson Problems with Circle Placement Variants",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "This paper explores a variation of the Traveling Salesperson Problem, where the agent places a circular obstacle next to each node once it visits it. Referred to as the Traveling Salesperson Problem with Circle Placement (TSP-CP), the aim is to maximize the obstacle radius for which a valid closed tour exists and then minimize the tour cost. The TSP-CP finds relevance in various real-world applications, such as harvesting, quarrying, and open-pit mining. We propose several novel solvers to address the TSP-CP, its variant tailored for Dubins vehicles, and a crucial subproblem known as the Traveling Salesperson Problem on self-deleting graphs (TSP-SD). Our extensive experimental results show that the proposed solvers outperform the current state-of-the-art on related problems in solution quality.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 7 figures, accepted to IEEE Robotics and Automation Letters in August 2024"
    },
    {
        "paper id": "2410.08641",
        "abstract url": "https://arxiv.org/abs/2410.08641",
        "title": "Multi-Source Temporal Attention Network for Precipitation Nowcasting",
        "rating": "-1",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Precipitation nowcasting is crucial across various industries and plays a significant role in mitigating and adapting to climate change. We introduce an efficient deep learning model for precipitation nowcasting, capable of predicting rainfall up to 8 hours in advance with greater accuracy than existing operational physics-based and extrapolation-based models. Our model leverages multi-source meteorological data and physics-based forecasts to deliver high-resolution predictions in both time and space. It captures complex spatio-temporal dynamics through temporal attention networks and is optimized using data quality maps and dynamic thresholds. Experiments demonstrate that our model outperforms state-of-the-art, and highlight its potential for fast reliable responses to evolving weather conditions.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08646",
        "abstract url": "https://arxiv.org/abs/2410.08646",
        "title": "Fully Unsupervised Dynamic MRI Reconstruction via Diffeo-Temporal Equivariance",
        "rating": "-1",
        "keywords": [
            [
                "MRI",
                "cardiac"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Reconstructing dynamic MRI image sequences from undersampled accelerated measurements is crucial for faster and higher spatiotemporal resolution real-time imaging of cardiac motion, free breathing motion and many other applications. Classical paradigms, such as gated cine MRI, assume periodicity, disallowing imaging of true motion. Supervised deep learning methods are fundamentally flawed as, in dynamic imaging, ground truth fully-sampled videos are impossible to truly obtain. We propose an unsupervised framework to learn to reconstruct dynamic MRI sequences from undersampled measurements alone by leveraging natural geometric spatiotemporal equivariances of MRI. Dynamic Diffeomorphic Equivariant Imaging (DDEI) significantly outperforms state-of-the-art unsupervised methods such as SSDU on highly accelerated dynamic cardiac imaging. Our method is agnostic to the underlying neural network architecture and can be used to adapt the latest models and post-processing approaches. Our code and video demos are at https://github.com/Andrewwango/ddei.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "Pre-print"
    },
    {
        "paper id": "2410.08655",
        "abstract url": "https://arxiv.org/abs/2410.08655",
        "title": "FRASA: An End-to-End Reinforcement Learning Agent for Fall Recovery and Stand Up of Humanoid Robots",
        "rating": "-1",
        "keywords": [
            [
                "robotics"
            ]
        ],
        "abstract": "Humanoid robotics faces significant challenges in achieving stable locomotion and recovering from falls in dynamic environments. Traditional methods, such as Model Predictive Control (MPC) and Key Frame Based (KFB) routines, either require extensive fine-tuning or lack real-time adaptability. This paper introduces FRASA, a Deep Reinforcement Learning (DRL) agent that integrates fall recovery and stand up strategies into a unified framework. Leveraging the Cross-Q algorithm, FRASA significantly reduces training time and offers a versatile recovery strategy that adapts to unpredictable disturbances. Comparative tests on Sigmaban humanoid robots demonstrate FRASA superior performance against the KFB method deployed in the RoboCup 2023 by the Rhoban Team, world champion of the KidSize League.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08670",
        "abstract url": "https://arxiv.org/abs/2410.08670",
        "title": "Mahi-Mahi: Low-Latency Asynchronous BFT DAG-Based Consensus",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "We present Mahi-Mahi, the first asynchronous BFT consensus protocol that achieves sub-second latency in the WAN while processing over 100,000 transactions per second. We accomplish this remarkable performance by building Mahi-Mahi on an uncertified structured Directed Acyclic Graph (DAG). By forgoing explicit certification, we significantly reduce the number of messages required to commit and minimize CPU overhead associated with certificate verification. Mahi-Mahi introduces a novel commit rule that allows committing multiple blocks in each DAG round, while ensuring liveness in the presence of an asynchronous adversary. Mahi-Mahi can be parametrized to either attempt to commit within 5 message delays, maximizing the probability of commitment under a continuously active asynchronous adversary, or within 4 message delays, which reduces latency under a more moderate and realistic asynchronous adversary. We demonstrate the safety and liveness of Mahi-Mahi in a Byzantine context. Subsequently, we evaluate Mahi-Mahi in a geo-replicated setting and compare its performance against state-of-the-art asynchronous consensus protocols, showcasing Mahi-Mahi's significantly lower latency.",
        "subjects": [
            "cs.DC",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08675",
        "abstract url": "https://arxiv.org/abs/2410.08675",
        "title": "Bukva: Russian Sign Language Alphabet",
        "rating": "-1",
        "keywords": [
            [
                "Sign Language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper investigates the recognition of the Russian fingerspelling alphabet, also known as the Russian Sign Language (RSL) dactyl. Dactyl is a component of sign languages where distinct hand movements represent individual letters of a written language. This method is used to spell words without specific signs, such as proper nouns or technical terms. The alphabet learning simulator is an essential isolated dactyl recognition application. There is a notable issue of data shortage in isolated dactyl recognition: existing Russian dactyl datasets lack subject heterogeneity, contain insufficient samples, or cover only static signs. We provide Bukva, the first full-fledged open-source video dataset for RSL dactyl recognition. It contains 3,757 videos with more than 101 samples for each RSL alphabet sign, including dynamic ones. We utilized crowdsourcing platforms to increase the subject's heterogeneity, resulting in the participation of 155 deaf and hard-of-hearing experts in the dataset creation. We use a TSM (Temporal Shift Module) block to handle static and dynamic signs effectively, achieving 83.6% top-1 accuracy with a real-time inference with CPU only. The dataset, demo code, and pre-trained models are publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Preptrint. Title: \"Bukva: Russian Sign Language Alphabet\". 9 pages"
    },
    {
        "paper id": "2410.08677",
        "abstract url": "https://arxiv.org/abs/2410.08677",
        "title": "On the impact of key design aspects in simulated Hybrid Quantum Neural Networks for Earth Observation",
        "rating": "-1",
        "keywords": [
            [
                "Quantum"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Quantum computing has introduced novel perspectives for tackling and improving machine learning tasks. Moreover, the integration of quantum technologies together with well-known deep learning (DL) architectures has emerged as a potential research trend gaining attraction across various domains, such as Earth Observation (EO) and many other research fields. However, prior related works in EO literature have mainly focused on convolutional architectural advancements, leaving several essential topics unexplored. Consequently, this research investigates through three cases of study fundamental aspects of hybrid quantum machine models for EO tasks aiming to provide a solid groundwork for future research studies towards more adequate simulations and looking at the post-NISQ era. More in detail, we firstly (1) investigate how different quantum libraries behave when training hybrid quantum models, assessing their computational efficiency and effectiveness. Secondly, (2) we analyze the stability/sensitivity to initialization values (i.e., seed values) in both traditional model and quantum-enhanced counterparts. Finally, (3) we explore the benefits of hybrid quantum attention-based models in EO applications, examining how integrating quantum circuits into ViTs can improve model performance.",
        "subjects": [
            "quant-ph",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08688",
        "abstract url": "https://arxiv.org/abs/2410.08688",
        "title": "Chain-of-Restoration: Multi-Task Image Restoration Models are Zero-Shot Step-by-Step Universal Image Restorers",
        "rating": "-1",
        "keywords": [
            [
                "Image Restoration"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Despite previous works typically targeting isolated degradation types, recent research has increasingly focused on addressing composite degradations which involve a complex interplay of multiple different isolated degradations. Recognizing the challenges posed by the exponential number of possible degradation combinations, we propose Universal Image Restoration (UIR), a new task setting that requires models to be trained on a set of degradation bases and then remove any degradation that these bases can potentially compose in a zero-shot manner. Inspired by the Chain-of-Thought which prompts LLMs to address problems step-by-step, we propose the Chain-of-Restoration (CoR), which instructs models to step-by-step remove unknown composite degradations. By integrating a simple Degradation Discriminator into pre-trained multi-task models, CoR facilitates the process where models remove one degradation basis per step, continuing this process until the image is fully restored from the unknown composite degradation. Extensive experiments show that CoR significantly improves model performance in removing composite degradations, achieving results comparable to or surpassing those of State-of-The-Art (SoTA) methods trained on all degradations. The code will be released at https://github.com/toummHus/Chain-of-Restoration.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "11 pages, 9 figures"
    },
    {
        "paper id": "2410.08700",
        "abstract url": "https://arxiv.org/abs/2410.08700",
        "title": "Progressive Pruning: Estimating Anonymity of Stream-Based Communication",
        "rating": "-1",
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "Streams of data have become the ubiquitous communication model on today's Internet. For strong anonymous communication, this conflicts with the traditional notion of single, independent messages, as assumed e.g. by many mixnet designs. In this work, we investigate the anonymity factors that are inherent to stream communication. We introduce Progressive Pruning}, a methodology suitable for estimating the anonymity level of streams. By mimicking an intersection attack, it captures the susceptibility of streams against traffic analysis attacks. We apply it to simulations of tailored examples of stream communication as well as to large-scale simulations of Tor using our novel TorFS simulator, finding that the stream length, the number of users, and how streams are distributed over the network have interdependent impacts on anonymity. Our work draws attention to challenges that need to be solved in order to provide strong anonymity for stream-based communication in the future.",
        "subjects": [
            "cs.CR",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08715",
        "abstract url": "https://arxiv.org/abs/2410.08715",
        "title": "Multi-Functional RIS for a Multi-Functional System: Integrating Sensing, Communication, and Wireless Power Transfer",
        "rating": "-1",
        "keywords": [
            [
                "radar"
            ]
        ],
        "abstract": "Communication networks are evolving from solely emphasizing communication to facilitating multiple functionalities. In this regard, integrated sensing, communication, and powering (ISCAP) provides an efficient way of enabling data transmission, radar sensing, and wireless power transfer simultaneously. Such a multi-functional network requires a multi-functional architectural solution. Toward this end, sensor-aided zero-energy reconfigurable intelligent surfaces (SAZE-RISs) offer an energy-efficient solution for ISCAP by meeting the requirements of the end users as well as supplying power for the RIS. This paper explores the use of SAZE-RIS within the ISCAP framework. First, we present the general system architecture, operational protocols, and main application scenarios for employing SAZE-RIS in ISCAP. Next, we discuss methods for managing the conflicting requirements of communication, sensing, and powering within ISCAP and the role of SAZE-RIS in this process. We then provide a detailed case study complete with simulation results, offering valuable insights into the design choices and tradeoffs that come into play when adopting this technology. Furthermore, we discuss the related challenges and open research avenues, highlighting areas that require further exploration to fully realize the potential of SAZE-RIS within this ISCAP framework.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Accepted for publication in IEEE Network"
    },
    {
        "paper id": "2410.08740",
        "abstract url": "https://arxiv.org/abs/2410.08740",
        "title": "Hespi: A pipeline for automatically detecting information from hebarium specimen sheets",
        "rating": "-1",
        "keywords": [
            [
                "biodiversity"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Specimen associated biodiversity data are sought after for biological, environmental, climate, and conservation sciences. A rate shift is required for the extraction of data from specimen images to eliminate the bottleneck that the reliance on human-mediated transcription of these data represents. We applied advanced computer vision techniques to develop the `Hespi' (HErbarium Specimen sheet PIpeline), which extracts a pre-catalogue subset of collection data on the institutional labels on herbarium specimens from their digital images. The pipeline integrates two object detection models; the first detects bounding boxes around text-based labels and the second detects bounding boxes around text-based data fields on the primary institutional label. The pipeline classifies text-based institutional labels as printed, typed, handwritten, or a combination and applies Optical Character Recognition (OCR) and Handwritten Text Recognition (HTR) for data extraction. The recognized text is then corrected against authoritative databases of taxon names. The extracted text is also corrected with the aide of a multimodal Large Language Model (LLM). Hespi accurately detects and extracts text for test datasets including specimen sheet images from international herbaria. The components of the pipeline are modular and users can train their own models with their own data and use them in place of the models provided.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08752",
        "abstract url": "https://arxiv.org/abs/2410.08752",
        "title": "T\u0159iVis: Versatile, Reliable, and High-Performance Tool for Computing Visibility in Polygonal Environments",
        "rating": "-1",
        "keywords": [
            [
                "robotics"
            ]
        ],
        "abstract": "Visibility is a fundamental concept in computational geometry, with numerous applications in robotics, surveillance systems, video games, and other fields. This software paper presents T\u0159iVis, a C++ library developed by the authors for computing numerous visibility-related queries in highly complex polygonal environments. Adapting the triangular expansion algorithm (TEA), T\u0159iVis stands out as a versatile, high-performance, more reliable and easy-to-use alternative to current solutions that is also free of heavy dependencies. Through evaluation on a challenging dataset, T\u0159iVis has been benchmarked against existing visibility libraries. The results demonstrate that T\u0159iVis outperforms the competing solutions by at least an order of magnitude in query times, while exhibiting more reliable runtime behavior. T\u0159iVis is freely available for private, research, and institutional use at https://github.com/janmikulacz/trivis.",
        "subjects": [
            "cs.RO",
            "cs.CG"
        ],
        "comment": "8 pages, 12 figures (including subfigures); submitted to the 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS); associated repository: https://github.com/janmikulacz/trivis"
    },
    {
        "paper id": "2410.08766",
        "abstract url": "https://arxiv.org/abs/2410.08766",
        "title": "Integrating Supertag Features into Neural Discontinuous Constituent Parsing",
        "rating": "-1",
        "keywords": [
            [
                "grammar"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Syntactic parsing is essential in natural-language processing, with constituent structure being one widely used description of syntax. Traditional views of constituency demand that constituents consist of adjacent words, but this poses challenges in analysing syntax with non-local dependencies, common in languages like German. Therefore, in a number of treebanks like NeGra and TIGER for German and DPTB for English, long-range dependencies are represented by crossing edges. Various grammar formalisms have been used to describe discontinuous trees - often with high time complexities for parsing. Transition-based parsing aims at reducing this factor by eliminating the need for an explicit grammar. Instead, neural networks are trained to produce trees given raw text input using supervised learning on large annotated corpora. An elegant proposal for a stack-free transition-based parser developed by Coavoux and Cohen (2019) successfully allows for the derivation of any discontinuous constituent tree over a sentence in worst-case quadratic time. The purpose of this work is to explore the introduction of supertag information into transition-based discontinuous constituent parsing. In lexicalised grammar formalisms like CCG (Steedman, 1989) informative categories are assigned to the words in a sentence and act as the building blocks for composing the sentence's syntax. These supertags indicate a word's structural role and syntactic relationship with surrounding items. The study examines incorporating supertag information by using a dedicated supertagger as additional input for a neural parser (pipeline) and by jointly training a neural model for both parsing and supertagging (multi-task). In addition to CCG, several other frameworks (LTAG-spinal, LCFRS) and sequence labelling tasks (chunking, dependency parsing) will be compared in terms of their suitability as auxiliary tasks for parsing.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.FL"
        ],
        "comment": "Bachelor's Thesis. Supervised by Dr. Kilian Evang and Univ.-Prof. Dr. Laura Kallmeyer"
    },
    {
        "paper id": "2410.08781",
        "abstract url": "https://arxiv.org/abs/2410.08781",
        "title": "VideoSAM: Open-World Video Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "robotics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video segmentation is essential for advancing robotics and autonomous driving, particularly in open-world settings where continuous perception and object association across video frames are critical. While the Segment Anything Model (SAM) has excelled in static image segmentation, extending its capabilities to video segmentation poses significant challenges. We tackle two major hurdles: a) SAM's embedding limitations in associating objects across frames, and b) granularity inconsistencies in object segmentation. To this end, we introduce VideoSAM, an end-to-end framework designed to address these challenges by improving object tracking and segmentation consistency in dynamic environments. VideoSAM integrates an agglomerated backbone, RADIO, enabling object association through similarity metrics and introduces Cycle-ack-Pairs Propagation with a memory mechanism for stable object tracking. Additionally, we incorporate an autoregressive object-token mechanism within the SAM decoder to maintain consistent granularity across frames. Our method is extensively evaluated on the UVO and BURST benchmarks, and robotic videos from RoboTAP, demonstrating its effectiveness and robustness in real-world scenarios. All codes will be available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08784",
        "abstract url": "https://arxiv.org/abs/2410.08784",
        "title": "Hybrid Filtering Heuristic for the Sensor-Placement Problem to Discretize 2D Continuous Environments",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "This paper addresses the sensor-placement problem (SPP) within the context of discretizing large, complex continuous 2D environments into graphs for efficient task-oriented route planning. The SPP aims to minimize the number of sensors required to achieve a user-defined coverage ratio while considering a general visibility model. We propose the hybrid filtering heuristic (HFH) framework, which enhances or combines outputs of existing sensor-placement methods, incorporating a filtering step. This step eliminates redundant sensors or those contributing marginally to the coverage, ensuring the coverage ratio remains within the desired interval. We implement two versions of HFH: the basic version and a variant, HFHB, incorporating a preprocessing technique known as bucketing to accelerate region clipping. We evaluate HFH and HFHB on a dataset of large, complex polygonal environments, comparing them to several baseline methods under both unlimited and limited-range omnidirectional visibility models. The results demonstrate that HFH and HFHB outperform baselines in terms of the number of sensors required to achieve the desired coverage ratio. Additionally, HFHB significantly reduces the runtime of more competitive baseline methods. We also adapt HFHB to a visibility model with localization uncertainty, demonstrating its effectiveness up to a certain level of uncertainty.",
        "subjects": [
            "cs.RO",
            "cs.CG"
        ],
        "comment": "16 pages, 33 figures (including subfigures); submitted to the IEEE Transactions on Robotics (T-RO); associated repository: https://github.com/janmikulacz/spp"
    },
    {
        "paper id": "2410.08793",
        "abstract url": "https://arxiv.org/abs/2410.08793",
        "title": "On the State of NLP Approaches to Modeling Depression in Social Media: A Post-COVID-19 Outlook",
        "rating": "-1",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Computational approaches to predicting mental health conditions in social media have been substantially explored in the past years. Multiple surveys have been published on this topic, providing the community with comprehensive accounts of the research in this area. Among all mental health conditions, depression is the most widely studied due to its worldwide prevalence. The COVID-19 global pandemic, starting in early 2020, has had a great impact on mental health worldwide. Harsh measures employed by governments to slow the spread of the virus (e.g., lockdowns) and the subsequent economic downturn experienced in many countries have significantly impacted people's lives and mental health. Studies have shown a substantial increase of above 50% in the rate of depression in the population. In this context, we present a survey on natural language processing (NLP) approaches to modeling depression in social media, providing the reader with a post-COVID-19 outlook. This survey contributes to the understanding of the impacts of the pandemic on modeling depression in social media. We outline how state-of-the-art approaches and new datasets have been used in the context of the COVID-19 pandemic. Finally, we also discuss ethical issues in collecting and processing mental health data, considering fairness, accountability, and ethics.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08804",
        "abstract url": "https://arxiv.org/abs/2410.08804",
        "title": "Batched Energy-Entropy acquisition for Bayesian Optimization",
        "rating": "-1",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Bayesian optimization (BO) is an attractive machine learning framework for performing sample-efficient global optimization of black-box functions. The optimization process is guided by an acquisition function that selects points to acquire in each round of BO. In batched BO, when multiple points are acquired in parallel, commonly used acquisition functions are often high-dimensional and intractable, leading to the use of sampling-based alternatives. We propose a statistical physics inspired acquisition function for BO with Gaussian processes that can natively handle batches. Batched Energy-Entropy acquisition for BO (BEEBO) enables tight control of the explore-exploit trade-off of the optimization process and generalizes to heteroskedastic black-box problems. We demonstrate the applicability of BEEBO on a range of problems, showing competitive performance to existing methods.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "14 pages (+31 appendix), 21 figures. Accepted at NeurIPS 2024"
    },
    {
        "paper id": "2410.08810",
        "abstract url": "https://arxiv.org/abs/2410.08810",
        "title": "LIME-Eval: Rethinking Low-light Image Enhancement Evaluation via Object Detection",
        "rating": "-1",
        "keywords": [
            [
                "Image Enhancement"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Due to the nature of enhancement--the absence of paired ground-truth information, high-level vision tasks have been recently employed to evaluate the performance of low-light image enhancement. A widely-used manner is to see how accurately an object detector trained on enhanced low-light images by different candidates can perform with respect to annotated semantic labels. In this paper, we first demonstrate that the mentioned approach is generally prone to overfitting, and thus diminishes its measurement reliability. In search of a proper evaluation metric, we propose LIME-Bench, the first online benchmark platform designed to collect human preferences for low-light enhancement, providing a valuable dataset for validating the correlation between human perception and automated evaluation metrics. We then customize LIME-Eval, a novel evaluation framework that utilizes detectors pre-trained on standard-lighting datasets without object annotations, to judge the quality of enhanced images. By adopting an energy-based strategy to assess the accuracy of output confidence maps, our LIME-Eval can simultaneously bypass biases associated with retraining detectors and circumvent the reliance on annotations for dim images. Comprehensive experiments are provided to reveal the effectiveness of our LIME-Eval. Our benchmark platform (https://huggingface.co/spaces/lime-j/eval) and code (https://github.com/lime-j/lime-eval) are available online.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08816",
        "abstract url": "https://arxiv.org/abs/2410.08816",
        "title": "Uncertainty-Aware Optimal Treatment Selection for Clinical Time Series",
        "rating": "-1",
        "keywords": [
            [
                "healthcare",
                "Clinical"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In personalized medicine, the ability to predict and optimize treatment outcomes across various time frames is essential. Additionally, the ability to select cost-effective treatments within specific budget constraints is critical. Despite recent advancements in estimating counterfactual trajectories, a direct link to optimal treatment selection based on these estimates is missing. This paper introduces a novel method integrating counterfactual estimation techniques and uncertainty quantification to recommend personalized treatment plans adhering to predefined cost constraints. Our approach is distinctive in its handling of continuous treatment variables and its incorporation of uncertainty quantification to improve prediction reliability. We validate our method using two simulated datasets, one focused on the cardiovascular system and the other on COVID-19. Our findings indicate that our method has robust performance across different counterfactual estimation baselines, showing that introducing uncertainty quantification in these settings helps the current baselines in finding more reliable and accurate treatment selection. The robustness of our method across various settings highlights its potential for broad applicability in personalized healthcare solutions.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "appeared at the workshop on Causal Representation Learning at NeurIPS 2024 (oral)"
    },
    {
        "paper id": "2410.08825",
        "abstract url": "https://arxiv.org/abs/2410.08825",
        "title": "Grand-children weight-balanced binary search trees",
        "rating": "-1",
        "keywords": [
            [
                "depth"
            ]
        ],
        "abstract": "We revisit weight-balanced trees, also known as trees of bounded balance. This class of binary search trees was invented by Nievergelt and Reingold in 1972. Such trees are obtained by assigning a weight to each node and requesting that the weight of each node should be quite larger than the weights of its children, the precise meaning of ``quite larger'' depending on a real-valued parameter~$\u03b3$. Blum and Mehlhorn then showed how to maintain these trees in a recursive (bottom-up) fashion when~$2/11 \\leqslant \u03b3\\leqslant 1-1/\\sqrt{2}$, their algorithm requiring only an amortised constant number of tree rebalancing operations per update (insertion or deletion). Later, in 1993, Lai and Wood proposed a top-down procedure for updating these trees when~$2/11 \\leqslant \u03b3\\leqslant 1/4$. Our contribution is two-fold. First, we strengthen the requirements of Nievergelt and Reingold, by also requesting that each node should have a substantially larger weight than its grand-children, thereby obtaining what we call grand-children balanced trees. Grand-children balanced trees are not harder to maintain than weight-balanced trees, but enjoy a smaller node depth, both in the worst case (with a 6~\\% decrease) and on average (with a 1.6~\\% decrease). In particular, unlike standard weight-balanced trees, all grand-children balanced trees with $n$ nodes are of height less than $2 \\log_2(n)$. Second, we adapt the algorithm of Lai and Wood to all weight-balanced trees, i.e., to all parameter values~$\u03b3$ such that~$2/11 \\leqslant \u03b3\\leqslant 1-1/\\sqrt{2}$. More precisely, we adapt it to all grand-children balanced trees for which~$1/4 < \u03b3\\leqslant 1 - 1/\\sqrt{2}$. Finally, we show that, except in critical cases, all these algorithms result in making a constant amortised number of tree rebalancing operations per tree update.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08826",
        "abstract url": "https://arxiv.org/abs/2410.08826",
        "title": "Towards virtual painting recolouring using Vision Transformer on X-Ray Fluorescence datacubes",
        "rating": "-1",
        "keywords": [
            [
                "X-Ray"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "In this contribution, we define (and test) a pipeline to perform virtual painting recolouring using raw data of X-Ray Fluorescence (XRF) analysis on pictorial artworks. To circumvent the small dataset size, we generate a synthetic dataset, starting from a database of XRF spectra; furthermore, to ensure a better generalisation capacity (and to tackle the issue of in-memory size and inference time), we define a Deep Variational Embedding network to embed the XRF spectra into a lower dimensional, K-Means friendly, metric space. We thus train a set of models to assign coloured images to embedded XRF images. We report here the devised pipeline performances in terms of visual quality metrics, and we close on a discussion on the results.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "physics.app-ph"
        ],
        "comment": "v1: 20 pages, 10 figures; link to code repository"
    },
    {
        "paper id": "2410.08848",
        "abstract url": "https://arxiv.org/abs/2410.08848",
        "title": "Learning Spatial Bimanual Action Models Based on Affordance Regions and Human Demonstrations",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "In this paper, we present a novel approach for learning bimanual manipulation actions from human demonstration by extracting spatial constraints between affordance regions, termed affordance constraints, of the objects involved. Affordance regions are defined as object parts that provide interaction possibilities to an agent. For example, the bottom of a bottle affords the object to be placed on a surface, while its spout affords the contained liquid to be poured. We propose a novel approach to learn changes of affordance constraints in human demonstration to construct spatial bimanual action models representing object interactions. To exploit the information encoded in these spatial bimanual action models, we formulate an optimization problem to determine optimal object configurations across multiple execution keypoints while taking into account the initial scene, the learned affordance constraints, and the robot's kinematics. We evaluate the approach in simulation with two example tasks (pouring drinks and rolling dough) and compare three different definitions of affordance constraints: (i) component-wise distances between affordance regions in Cartesian space, (ii) component-wise distances between affordance regions in cylindrical space, and (iii) degrees of satisfaction of manually defined symbolic spatial affordance constraints.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, accepted for publication at Humanoids 2024 - This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible"
    },
    {
        "paper id": "2410.08861",
        "abstract url": "https://arxiv.org/abs/2410.08861",
        "title": "A foundation model for generalizable disease diagnosis in chest X-ray images",
        "rating": "-1",
        "keywords": [
            [
                "Medical",
                "diagnosis",
                "X-ray",
                "disease",
                "clinical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Medical artificial intelligence (AI) is revolutionizing the interpretation of chest X-ray (CXR) images by providing robust tools for disease diagnosis. However, the effectiveness of these AI models is often limited by their reliance on large amounts of task-specific labeled data and their inability to generalize across diverse clinical settings. To address these challenges, we introduce CXRBase, a foundational model designed to learn versatile representations from unlabelled CXR images, facilitating efficient adaptation to various clinical tasks. CXRBase is initially trained on a substantial dataset of 1.04 million unlabelled CXR images using self-supervised learning methods. This approach allows the model to discern meaningful patterns without the need for explicit labels. After this initial phase, CXRBase is fine-tuned with labeled data to enhance its performance in disease detection, enabling accurate classification of chest diseases. CXRBase provides a generalizable solution to improve model performance and alleviate the annotation workload of experts to enable broad clinical AI applications from chest imaging.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08871",
        "abstract url": "https://arxiv.org/abs/2410.08871",
        "title": "Adaptive optimization of wave energy conversion in oscillatory wave surge converters via SPH simulation and deep reinforcement learning",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "The nonlinear damping characteristics of the oscillating wave surge converter (OWSC) significantly impact the performance of the power take-off system. This study presents a framework by integrating deep reinforcement learning (DRL) with numerical simulations of OWSC to identify optimal adaptive damping policy under varying wave conditions, thereby enhancing wave energy harvesting efficiency. Firstly, the open-source multiphysics libraries SPHinXsys and Simbody are employed to establish the numerical environment for wave interaction with OWSCs. Subsequently, a comparative analysis of three DRL algorithms-proximal policy optimization (PPO), twin delayed deep deterministic policy gradient (TD3), and soft actor-critic (SAC)-is conducted using the two-dimensional (2D) numerical study of OWSC interacting with regular waves. The results reveal that artificial neural networks capture the nonlinear characteristics of wave-structure interactions and provide efficient PTO policies. Notably, the SAC algorithm demonstrates exceptional robustness and accuracy, achieving a 10.61% improvement in wave energy harvesting. Furthermore, policies trained in a 2D environment are successfully applied to the three-dimensional (3D) study, with an improvement of 22.54% in energy harvesting. Additionally, the study shows that energy harvesting is improved by 6.42% for complex irregular waves. However, for the complex dual OWSC system, optimizing the damping characteristics alone is insufficient to enhance energy harvesting.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "67 pages and 25 figures"
    },
    {
        "paper id": "2410.08888",
        "abstract url": "https://arxiv.org/abs/2410.08888",
        "title": "Simulating anisotropic diffusion processes with smoothed particle hydrodynamics",
        "rating": "-1",
        "keywords": [
            [
                "diffusion"
            ]
        ],
        "abstract": "Diffusion problems with anisotropic features arise in the various areas of science and engineering fields. As a Lagrangian mesh-less method, SPH has a special advantage in addressing the diffusion problems due to the the benefit of dealing with the advection term. But its application to solving anisotropic diffusion is still limited since a robust and general SPH formulation is required to obtain accurate approximations of second derivatives. In this paper, we modify a second derivatives model based on the SPH formulation to obtain a full version of Hessian matrix consisting of the Laplacian operator elements. To verify the proposed SPH scheme, firstly, the diffusion of a scalar which distributes following a pre-function within a thin structure is performed by using anisotropic resolution coupling anisotropic kernel. With various anisotropic ratios, excellent agreements with the theoretical solution are achieved. Then, the anisotropic diffusion of a contaminant in fluid is simulated. The simulation results are very consistent with corresponding analytical solutions, showing that the present algorithm can obtain smooth solution without the spurious oscillations for contaminant transport problems with discontinuities, and achieve second-order accuracy. Subsequently, we utilize this newly developed SPH formulation to tackle the problem of the fluid diffusion through a thin porous membrane and the anisotropic transport of transmembrane potential within the left ventricle, demonstrating the capabilities of the proposed SPH framework in solving the complex anisotropic problems.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08903",
        "abstract url": "https://arxiv.org/abs/2410.08903",
        "title": "Dynamic Benchmarks: Spatial and Temporal Alignment for ADS Performance Evaluation",
        "rating": "-1",
        "keywords": [
            [
                "Automated Driving",
                "vehicle"
            ]
        ],
        "abstract": "Deployed SAE level 4+ Automated Driving Systems (ADS) without a human driver are currently operational ride-hailing fleets on surface streets in the United States. This current use case and future applications of this technology will determine where and when the fleets operate, potentially resulting in a divergence from the distribution of driving of some human benchmark population within a given locality. Existing benchmarks for evaluating ADS performance have only done county-level geographical matching of the ADS and benchmark driving exposure in crash rates. This study presents a novel methodology for constructing dynamic human benchmarks that adjust for spatial and temporal variations in driving distribution between an ADS and the overall human driven fleet. Dynamic benchmarks were generated using human police-reported crash data, human vehicle miles traveled (VMT) data, and over 20 million miles of Waymo's rider-only (RO) operational data accumulated across three US counties. The spatial adjustment revealed significant differences across various severity levels in adjusted crash rates compared to unadjusted benchmarks with these differences ranging from 10% to 47% higher in San Francisco, 12% to 20% higher in Maricopa, and 7% lower to 34% higher in Los Angeles counties. The time-of-day adjustment in San Francisco, limited to this region due to data availability, resulted in adjusted crash rates 2% lower to 16% higher than unadjusted rates, depending on severity level. The findings underscore the importance of adjusting for spatial and temporal confounders in benchmarking analysis, which ultimately contributes to a more equitable benchmark for ADS performance evaluations.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08992",
        "abstract url": "https://arxiv.org/abs/2410.08992",
        "title": "Block coupling and rapidly mixing k-heights",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "A $k$-height on a graph $G=(V, E)$ is an assignment $V\\to\\{0, \\ldots, k\\}$ such that the value on ajacent vertices differs by at most $1$. We study the Markov chain on $k$-heights that in each step selects a vertex at random, and, if admissible, increases or decreases the value at this vertex by one. In the cases of $2$-heights and $3$-heights we show that this Markov chain is rapidly mixing on certain families of grid-like graphs and on planar cubic $3$-connected graphs. The result is based on a novel technique called block coupling, which is derived from the well-established monotone coupling approach. This technique may also be effective when analyzing other Markov chains that operate on configurations of spin systems that form a distributive lattice. It is therefore of independent interest.",
        "subjects": [
            "cs.DM",
            "math.CO"
        ],
        "comment": "31 pages, 8 figures. Supplemental code available at Zenodo, doi:10.5281/zenodo.13912818"
    },
    {
        "paper id": "2410.09006",
        "abstract url": "https://arxiv.org/abs/2410.09006",
        "title": "From Interaction to Impact: Towards Safer AI Agents Through Understanding and Evaluating UI Operation Impacts",
        "rating": "-1",
        "keywords": [
            [
                "navigation"
            ]
        ],
        "abstract": "With advances in generative AI, there is increasing work towards creating autonomous agents that can manage daily tasks by operating user interfaces (UIs). While prior research has studied the mechanics of how AI agents might navigate UIs and understand UI structure, the effects of agents and their autonomous actions-particularly those that may be risky or irreversible-remain under-explored. In this work, we investigate the real-world impacts and consequences of UI actions by AI agents. We began by developing a taxonomy of the impacts of UI actions through a series of workshops with domain experts. Following this, we conducted a data synthesis study to gather realistic UI screen traces and action data that users perceive as impactful. We then used our impact categories to annotate our collected data and data repurposed from existing UI navigation datasets. Our quantitative evaluations of different large language models (LLMs) and variants demonstrate how well different LLMs can understand the impacts of UI actions that might be taken by an agent. We show that our taxonomy enhances the reasoning capabilities of these LLMs for understanding the impacts of UI actions, but our findings also reveal significant gaps in their ability to reliably classify more nuanced or complex categories of impact.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09009",
        "abstract url": "https://arxiv.org/abs/2410.09009",
        "title": "Semantic Score Distillation Sampling for Compositional Text-to-3D Generation",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generating high-quality 3D assets from textual descriptions remains a pivotal challenge in computer graphics and vision research. Due to the scarcity of 3D data, state-of-the-art approaches utilize pre-trained 2D diffusion priors, optimized through Score Distillation Sampling (SDS). Despite progress, crafting complex 3D scenes featuring multiple objects or intricate interactions is still difficult. To tackle this, recent methods have incorporated box or layout guidance. However, these layout-guided compositional methods often struggle to provide fine-grained control, as they are generally coarse and lack expressiveness. To overcome these challenges, we introduce a novel SDS approach, Semantic Score Distillation Sampling (SemanticSDS), designed to effectively improve the expressiveness and accuracy of compositional text-to-3D generation. Our approach integrates new semantic embeddings that maintain consistency across different rendering views and clearly differentiate between various objects and parts. These embeddings are transformed into a semantic map, which directs a region-specific SDS process, enabling precise optimization and compositional generation. By leveraging explicit semantic guidance, our method unlocks the compositional capabilities of existing pre-trained diffusion models, thereby achieving superior quality in 3D content generation, particularly for complex objects and scenes. Experimental results demonstrate that our SemanticSDS framework is highly effective for generating state-of-the-art complex 3D content. Code: https://github.com/YangLing0818/SemanticSDS-3D",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project: https://github.com/YangLing0818/SemanticSDS-3D"
    },
    {
        "paper id": "2410.09019",
        "abstract url": "https://arxiv.org/abs/2410.09019",
        "title": "MedMobile: A mobile-sized language model with expert-level clinical capabilities",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Language models (LMs) have demonstrated expert-level reasoning and recall abilities in medicine. However, computational costs and privacy concerns are mounting barriers to wide-scale implementation. We introduce a parsimonious adaptation of phi-3-mini, MedMobile, a 3.8 billion parameter LM capable of running on a mobile device, for medical applications. We demonstrate that MedMobile scores 75.7% on the MedQA (USMLE), surpassing the passing mark for physicians (~60%), and approaching the scores of models 100 times its size. We subsequently perform a careful set of ablations, and demonstrate that chain of thought, ensembling, and fine-tuning lead to the greatest performance gains, while unexpectedly retrieval augmented generation fails to demonstrate significant improvements",
        "subjects": [
            "cs.CL"
        ],
        "comment": "13 pages, 5 figures (2 main, 3 supplementary)"
    },
    {
        "paper id": "2410.09032",
        "abstract url": "https://arxiv.org/abs/2410.09032",
        "title": "Alberta Wells Dataset: Pinpointing Oil and Gas Wells from Satellite Imagery",
        "rating": "-1",
        "keywords": [
            [
                "Remote sensing",
                "Satellite"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Millions of abandoned oil and gas wells are scattered across the world, leaching methane into the atmosphere and toxic compounds into the groundwater. Many of these locations are unknown, preventing the wells from being plugged and their polluting effects averted. Remote sensing is a relatively unexplored tool for pinpointing abandoned wells at scale. We introduce the first large-scale benchmark dataset for this problem, leveraging medium-resolution multi-spectral satellite imagery from Planet Labs. Our curated dataset comprises over 213,000 wells (abandoned, suspended, and active) from Alberta, a region with especially high well density, sourced from the Alberta Energy Regulator and verified by domain experts. We evaluate baseline algorithms for well detection and segmentation, showing the promise of computer vision approaches but also significant room for improvement.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09155",
        "abstract url": "https://arxiv.org/abs/2410.09155",
        "title": "Facial Chick Sexing: An Automated Chick Sexing System From Chick Facial Image",
        "rating": "-1",
        "keywords": [
            [
                "Facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Chick sexing, the process of determining the gender of day-old chicks, is a critical task in the poultry industry due to the distinct roles that each gender plays in production. While effective traditional methods achieve high accuracy, color, and wing feather sexing is exclusive to specific breeds, and vent sexing is invasive and requires trained experts. To address these challenges, we propose a novel approach inspired by facial gender classification techniques in humans: facial chick sexing. This new method does not require expert knowledge and aims to reduce training time while enhancing animal welfare by minimizing chick manipulation. We develop a comprehensive system for training and inference that includes data collection, facial and keypoint detection, facial alignment, and classification. We evaluate our model on two sets of images: Cropped Full Face and Cropped Middle Face, both of which maintain essential facial features of the chick for further analysis. Our experiment demonstrates the promising viability, with a final accuracy of 81.89%, of this approach for future practices in chick sexing by making them more universally applicable.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09174",
        "abstract url": "https://arxiv.org/abs/2410.09174",
        "title": "Context-Aware SQL Error Correction Using Few-Shot Learning -- A Novel Approach Based on NLQ, Error, and SQL Similarity",
        "rating": "-1",
        "keywords": [
            [
                "SQL"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In recent years, the demand for automated SQL generation has increased significantly, driven by the need for efficient data querying in various applications. However, generating accurate SQL queries remains a challenge due to the complexity and variability of natural language inputs. This paper introduces a novel few-shot learning-based approach for error correction in SQL generation, enhancing the accuracy of generated queries by selecting the most suitable few-shot error correction examples for a given natural language question (NLQ). In our experiments with the open-source Gretel dataset, the proposed model offers a 39.2% increase in fixing errors from the baseline approach with no error correction and a 10% increase from a simple error correction method. The proposed technique leverages embedding-based similarity measures to identify the closest matches from a repository of few-shot examples. Each example comprises an incorrect SQL query, the resulting error, the correct SQL query, and detailed steps to transform the incorrect query into the correct one. By employing this method, the system can effectively guide the correction of errors in newly generated SQL queries. Our approach demonstrates significant improvements in SQL generation accuracy by providing contextually relevant examples that facilitate error identification and correction. The experimental results highlight the effectiveness of embedding-based selection in enhancing the few-shot learning process, leading to more precise and reliable SQL query generation. This research contributes to the field of automated SQL generation by offering a robust framework for error correction, paving the way for more advanced and user-friendly database interaction tools.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted for the 1st Workshop on GenAI and RAG Systems for Enterprise @ CIKM 2024"
    },
    {
        "paper id": "2410.09192",
        "abstract url": "https://arxiv.org/abs/2410.09192",
        "title": "Long Range Named Entity Recognition for Marathi Documents",
        "rating": "-1",
        "keywords": [
            [
                "Named Entity Recognition"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The demand for sophisticated natural language processing (NLP) methods, particularly Named Entity Recognition (NER), has increased due to the exponential growth of Marathi-language digital content. In particular, NER is essential for recognizing distant entities and for arranging and understanding unstructured Marathi text data. With an emphasis on managing long-range entities, this paper offers a comprehensive analysis of current NER techniques designed for Marathi documents. It dives into current practices and investigates the BERT transformer model's potential for long-range Marathi NER. Along with analyzing the effectiveness of earlier methods, the report draws comparisons between NER in English literature and suggests adaptation strategies for Marathi literature. The paper discusses the difficulties caused by Marathi's particular linguistic traits and contextual subtleties while acknowledging NER's critical role in NLP. To conclude, this project is a major step forward in improving Marathi NER techniques, with potential wider applications across a range of NLP tasks and domains.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09227",
        "abstract url": "https://arxiv.org/abs/2410.09227",
        "title": "Fast Data-independent KLT Approximations Based on Integer Functions",
        "rating": "-1",
        "keywords": [
            [
                "FPGA"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The Karhunen-Lo\u00e8ve transform (KLT) stands as a well-established discrete transform, demonstrating optimal characteristics in data decorrelation and dimensionality reduction. Its ability to condense energy compression into a select few main components has rendered it instrumental in various applications within image compression frameworks. However, computing the KLT depends on the covariance matrix of the input data, which makes it difficult to develop fast algorithms for its implementation. Approximations for the KLT, utilizing specific rounding functions, have been introduced to reduce its computational complexity. Therefore, our paper introduces a category of low-complexity, data-independent KLT approximations, employing a range of round-off functions. The design methodology of the approximate transform is defined for any block-length $N$, but emphasis is given to transforms of $N = 8$ due to its wide use in image and video compression. The proposed transforms perform well when compared to the exact KLT and approximations considering classical performance measures. For particular scenarios, our proposed transforms demonstrated superior performance when compared to KLT approximations documented in the literature. We also developed fast algorithms for the proposed transforms, further reducing the arithmetic cost associated with their implementation. Evaluation of field programmable gate array (FPGA) hardware implementation metrics was conducted. Practical applications in image encoding showed the relevance of the proposed transforms. In fact, we showed that one of the proposed transforms outperformed the exact KLT given certain compression ratios.",
        "subjects": [
            "eess.SP",
            "cs.CV",
            "eess.IV",
            "math.NA",
            "stat.ME"
        ],
        "comment": "19 pages, 10 figures, 7 tables"
    },
    {
        "paper id": "2410.09234",
        "abstract url": "https://arxiv.org/abs/2410.09234",
        "title": "Fine-Tuning In-House Large Language Models to Infer Differential Diagnosis from Radiology Reports",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "health",
                "Diagnosis",
                "clinical",
                "Radiology"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Radiology reports summarize key findings and differential diagnoses derived from medical imaging examinations. The extraction of differential diagnoses is crucial for downstream tasks, including patient management and treatment planning. However, the unstructured nature of these reports, characterized by diverse linguistic styles and inconsistent formatting, presents significant challenges. Although proprietary large language models (LLMs) such as GPT-4 can effectively retrieve clinical information, their use is limited in practice by high costs and concerns over the privacy of protected health information (PHI). This study introduces a pipeline for developing in-house LLMs tailored to identify differential diagnoses from radiology reports. We first utilize GPT-4 to create 31,056 labeled reports, then fine-tune open source LLM using this dataset. Evaluated on a set of 1,067 reports annotated by clinicians, the proposed model achieves an average F1 score of 92.1\\%, which is on par with GPT-4 (90.8\\%). Through this study, we provide a methodology for constructing in-house LLMs that: match the performance of GPT, reduce dependence on expensive proprietary models, and enhance the privacy and security of PHI.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "10 pages, 2 figures, 4 tables"
    },
    {
        "paper id": "2410.09236",
        "abstract url": "https://arxiv.org/abs/2410.09236",
        "title": "Enhancing Infant Crying Detection with Gradient Boosting for Improved Emotional and Mental Health Diagnostics",
        "rating": "-1",
        "keywords": [
            [
                "Health",
                "physiological"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Infant crying can serve as a crucial indicator of various physiological and emotional states. This paper introduces a comprehensive approach for detecting infant cries within audio data. We integrate Meta's Wav2Vec with traditional audio features, such as Mel-frequency cepstral coefficients (MFCCs), chroma, and spectral contrast, employing Gradient Boosting Machines (GBM) for cry classification. We validate our approach on a real-world dataset, demonstrating significant performance improvements over existing methods.",
        "subjects": [
            "eess.AS",
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09250",
        "abstract url": "https://arxiv.org/abs/2410.09250",
        "title": "Quantum-Trained Convolutional Neural Network for Deepfake Audio Detection",
        "rating": "-1",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "Deepfake"
            ],
            [
                "Quantum"
            ],
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "The rise of deepfake technologies has posed significant challenges to privacy, security, and information integrity, particularly in audio and multimedia content. This paper introduces a Quantum-Trained Convolutional Neural Network (QT-CNN) framework designed to enhance the detection of deepfake audio, leveraging the computational power of quantum machine learning (QML). The QT-CNN employs a hybrid quantum-classical approach, integrating Quantum Neural Networks (QNNs) with classical neural architectures to optimize training efficiency while reducing the number of trainable parameters. Our method incorporates a novel quantum-to-classical parameter mapping that effectively utilizes quantum states to enhance the expressive power of the model, achieving up to 70% parameter reduction compared to classical models without compromising accuracy. Data pre-processing involved extracting essential audio features, label encoding, feature scaling, and constructing sequential datasets for robust model evaluation. Experimental results demonstrate that the QT-CNN achieves comparable performance to traditional CNNs, maintaining high accuracy during training and testing phases across varying configurations of QNN blocks. The QT framework's ability to reduce computational overhead while maintaining performance underscores its potential for real-world applications in deepfake detection and other resource-constrained scenarios. This work highlights the practical benefits of integrating quantum computing into artificial intelligence, offering a scalable and efficient approach to advancing deepfake detection technologies.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "eess.AS",
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09252",
        "abstract url": "https://arxiv.org/abs/2410.09252",
        "title": "ReasonPlanner: Enhancing Autonomous Planning in Dynamic Environments with Temporal Knowledge Graphs and LLMs",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "Graphs"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Planning and performing interactive tasks, such as conducting experiments to determine the melting point of an unknown substance, is straightforward for humans but poses significant challenges for autonomous agents. We introduce ReasonPlanner, a novel generalist agent designed for reflective thinking, planning, and interactive reasoning. This agent leverages LLMs to plan hypothetical trajectories by building a World Model based on a Temporal Knowledge Graph. The agent interacts with the environment using a natural language actor-critic module, where the actor translates the imagined trajectory into a sequence of actionable steps, and the critic determines if replanning is necessary. ReasonPlanner significantly outperforms previous state-of-the-art prompting-based methods on the ScienceWorld benchmark by more than 1.8 times, while being more sample-efficient and interpretable. It relies solely on frozen weights thus requiring no gradient updates. ReasonPlanner can be deployed and utilized without specialized knowledge of Machine Learning, making it accessible to a wide range of users.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09254",
        "abstract url": "https://arxiv.org/abs/2410.09254",
        "title": "Few Exemplar-Based General Medical Image Segmentation via Domain-Aware Selective Adaptation",
        "rating": "-1",
        "keywords": [
            [
                "Medical",
                "healthcare",
                "clinical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Medical image segmentation poses challenges due to domain gaps, data modality variations, and dependency on domain knowledge or experts, especially for low- and middle-income countries (LMICs). Whereas for humans, given a few exemplars (with corresponding labels), we are able to segment different medical images even without exten-sive domain-specific clinical training. In addition, current SAM-based medical segmentation models use fine-grained visual prompts, such as the bounding rectangle generated from manually annotated target segmentation mask, as the bounding box (bbox) prompt during the testing phase. However, in actual clinical scenarios, no such precise prior knowledge is available. Our experimental results also reveal that previous models nearly fail to predict when given coarser bbox prompts. Considering these issues, in this paper, we introduce a domain-aware selective adaptation approach to adapt the general knowledge learned from a large model trained with natural images to the corresponding medical domains/modalities, with access to only a few (e.g. less than 5) exemplars. Our method mitigates the aforementioned limitations, providing an efficient and LMICs-friendly solution. Extensive experimental analysis showcases the effectiveness of our approach, offering potential advancements in healthcare diagnostics and clinical applications in LMICs.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepcted in ACCV 2024"
    },
    {
        "paper id": "2410.09255",
        "abstract url": "https://arxiv.org/abs/2410.09255",
        "title": "MOZART: Ensembling Approach for COVID-19 Detection using Chest X-Ray Imagery",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "healthcare",
                "diagnosis",
                "X-Ray"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "COVID-19, has led to a global pandemic that strained the healthcare systems. Early and accurate detection is crucial for controlling the spread of the virus. While reverse transcription polymerase chain reaction test is the gold standard for diagnosis, it's limited availability, long processing times and extremely high false negative rate, have prompted the exploration of alternative methods. Chest Xray imaging has emerged as a valuable, non invasive tool for identifying COVID-19 related lung abnormalities. Traditional convolutional neural networks (CNNs) achieve impressive accuracy, but there is a need for more robust solutions to minimize false positives and negatives in critical medical applications. Thus We introduce the MOZART framework, an ensemble learning approach that enhances the virus detection. We trained three CNN architectures InceptionV3, Xception, and ResNet50 on a balanced chest X-ray dataset of 3,616 COVID-19 and 3,616 healthy images. Each model underwent a separate preprocessing pipeline, such as normalizing inputs to a range of -1 to 1. The dataset was split into 70% for training, 20% for validation, and 10% for testing, after training the individual models, we trained a shallow neural network on the predictions and to provide a us with the final predictions. Our results show that the MOZART framework with it's sub-experiments MOZART1 and MOZART2 outperforms individual CNN models in key metrics. It achieved an accuracy of 99.17% and an F1 score of 99.16%. MOZART1 excels at minimizing false positives, while MOZART2 is better for reducing false negatives. This work suggests that the MOZART framework can improve reliability in AI-driven medical imaging tasks and should be explored further for other lung diseases.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "This paper was originally intended to be published as part of my B.Sc. graduation project in Electrical and Electronics Engineering at the University of Khartoum in 2021. However, due to political and economic instability, and most recently, the outbreak of conflict in Sudan in April 2023, the publication process was significantly delayed. But yeah, better late than never"
    },
    {
        "paper id": "2410.09257",
        "abstract url": "https://arxiv.org/abs/2410.09257",
        "title": "Two-person positive shortest path games have Nash equlibria in pure stationary strategies",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "We prove that every finite two-person positive shortest path game has a Nash equilibrium (NE) in pure stationary strategies, which can be computed in polynomial time. The existence result holds also for graphs with finite out-degrees. Moreover, we prove that a terminal NE exists provided at least one of two players can guarantee reaching a terminal. If no one can do it, in other words, if each of two players can cut all terminals from the initial position $s$, then, obviously, a cyclic NE exists, although its cost is infinite for both players, since we restrict ourselves to positive games. We conjecture that a terminal NE exists too, provided there exists a directed path from $s$ to a terminal. However, this is open.",
        "subjects": [
            "cs.DM",
            "cs.MA",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09289",
        "abstract url": "https://arxiv.org/abs/2410.09289",
        "title": "AuD-Former: A Hierarchical Transformer Network for Multimodal Audio-Based Disease Prediction",
        "rating": "-1",
        "keywords": [
            [
                "bio-acoustic",
                "medical",
                "diagnosis",
                "Disease",
                "pathological"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Audio-based disease prediction is emerging as a promising supplement to traditional medical diagnosis methods, facilitating early, convenient, and non-invasive disease detection and prevention. Multimodal fusion, which integrates features from various domains within or across bio-acoustic modalities, has proven effective in enhancing diagnostic performance. However, most existing methods in the field employ unilateral fusion strategies that focus solely on either intra-modal or inter-modal fusion. This approach limits the full exploitation of the complementary nature of diverse acoustic feature domains and bio-acoustic modalities. Additionally, the inadequate and isolated exploration of latent dependencies within modality-specific and modality-shared spaces curtails their capacity to manage the inherent heterogeneity in multimodal data. To fill these gaps, we propose AuD-Former, a hierarchical transformer network designed for general multimodal audio-based disease prediction. Specifically, we seamlessly integrate intra-modal and inter-modal fusion in a hierarchical manner and proficiently encode the necessary intra-modal and inter-modal complementary correlations, respectively. Comprehensive experiments demonstrate that AuD-Former achieves state-of-the-art performance in predicting three diseases: COVID-19, Parkinson's disease, and pathological dysarthria, showcasing its promising potential in a broad context of audio-based disease prediction tasks. Additionally, extensive ablation studies and qualitative analyses highlight the significant benefits of each main component within our model.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "cs.LG",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09293",
        "abstract url": "https://arxiv.org/abs/2410.09293",
        "title": "EasyHeC++: Fully Automatic Hand-Eye Calibration with Pretrained Image Models",
        "rating": "-1",
        "keywords": [
            [
                "robotics",
                "robot"
            ]
        ],
        "abstract": "Hand-eye calibration plays a fundamental role in robotics by directly influencing the efficiency of critical operations such as manipulation and grasping. In this work, we present a novel framework, EasyHeC++, designed for fully automatic hand-eye calibration. In contrast to previous methods that necessitate manual calibration, specialized markers, or the training of arm-specific neural networks, our approach is the first system that enables accurate calibration of any robot arm in a marker-free, training-free, and fully automatic manner. Our approach employs a two-step process. First, we initialize the camera pose using a sampling or feature-matching-based method with the aid of pretrained image models. Subsequently, we perform pose optimization through differentiable rendering. Extensive experiments demonstrate the system's superior accuracy in both synthetic and real-world datasets across various robot arms and camera settings. Project page: https://ootts.github.io/easyhec_plus.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted by IROS 2024"
    },
    {
        "paper id": "2410.09299",
        "abstract url": "https://arxiv.org/abs/2410.09299",
        "title": "Hierarchical uncertainty estimation for learning-based registration in neuroimaging",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "MRI"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Over recent years, deep learning based image registration has achieved impressive accuracy in many domains, including medical imaging and, specifically, human neuroimaging with magnetic resonance imaging (MRI). However, the uncertainty estimation associated with these methods has been largely limited to the application of generic techniques (e.g., Monte Carlo dropout) that do not exploit the peculiarities of the problem domain, particularly spatial modeling. Here, we propose a principled way to propagate uncertainties (epistemic or aleatoric) estimated at the level of spatial location by these methods, to the level of global transformation models, and further to downstream tasks. Specifically, we justify the choice of a Gaussian distribution for the local uncertainty modeling, and then propose a framework where uncertainties spread across hierarchical levels, depending on the choice of transformation model. Experiments on publicly available data sets show that Monte Carlo dropout correlates very poorly with the reference registration error, whereas our uncertainty estimates correlate much better. % with the reference registration error. Crucially, the results also show that uncertainty-aware fitting of transformations improves the registration accuracy of brain MRI scans. Finally, we illustrate how sampling from the posterior distribution of the transformations can be used to propagate uncertainties to downstream neuroimaging tasks. Code is available at: https://github.com/HuXiaoling/Regre4Regis.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "eess.IV"
        ],
        "comment": "15 pages, 6 figures"
    },
    {
        "paper id": "2410.09301",
        "abstract url": "https://arxiv.org/abs/2410.09301",
        "title": "Motion Planning for Object Manipulation by Edge-Rolling",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "A common way to manipulate heavy objects is to maintain at least one point of the object in contact with the environment during the manipulation. When the object has a cylindrical shape or, in general, a curved edge, not only sliding and pivoting motions but also rolling the object along the edge can effectively satisfy this condition. Edge-rolling offers several advantages in terms of efficiency and maneuverability. This paper aims to develop a novel approach for approximating the prehensile edge-rolling motion on any path by a sequence of constant screw displacements, leveraging the principles of screw theory. Based on this approach, we proposed an algorithmic method for task-space-based path generation of object manipulation between two given configurations using a sequence of rolling and pivoting motions. The method is based on an optimization algorithm that takes into account the joint limitations of the robot. To validate our approach, we conducted experiments to manipulate a cylinder along linear and curved paths using the Franka Emika Panda manipulator.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, Pre-print, Submitted to IROS 2024"
    },
    {
        "paper id": "2410.09306",
        "abstract url": "https://arxiv.org/abs/2410.09306",
        "title": "TD-Paint: Faster Diffusion Inpainting Through Time Aware Pixel Conditioning",
        "rating": "-1",
        "keywords": [
            [
                "Diffusion",
                "Inpainting"
            ],
            [
                "trajectory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion models have emerged as highly effective techniques for inpainting, however, they remain constrained by slow sampling rates. While recent advances have enhanced generation quality, they have also increased sampling time, thereby limiting scalability in real-world applications. We investigate the generative sampling process of diffusion-based inpainting models and observe that these models make minimal use of the input condition during the initial sampling steps. As a result, the sampling trajectory deviates from the data manifold, requiring complex synchronization mechanisms to realign the generation process. To address this, we propose Time-aware Diffusion Paint (TD-Paint), a novel approach that adapts the diffusion process by modeling variable noise levels at the pixel level. This technique allows the model to efficiently use known pixel values from the start, guiding the generation process toward the target manifold. By embedding this information early in the diffusion process, TD-Paint significantly accelerates sampling without compromising image quality. Unlike conventional diffusion-based inpainting models, which require a dedicated architecture or an expensive generation loop, TD-Paint achieves faster sampling times without architectural modifications. Experimental results across three datasets show that TD-Paint outperforms state-of-the-art diffusion models while maintaining lower complexity.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09309",
        "abstract url": "https://arxiv.org/abs/2410.09309",
        "title": "Adaptive Compliance Policy: Learning Approximate Compliance for Diffusion Guided Control",
        "rating": "-1",
        "keywords": [
            [
                "Diffusion"
            ]
        ],
        "abstract": "Compliance plays a crucial role in manipulation, as it balances between the concurrent control of position and force under uncertainties. Yet compliance is often overlooked by today's visuomotor policies that solely focus on position control. This paper introduces Adaptive Compliance Policy (ACP), a novel framework that learns to dynamically adjust system compliance both spatially and temporally for given manipulation tasks from human demonstrations, improving upon previous approaches that rely on pre-selected compliance parameters or assume uniform constant stiffness. However, computing full compliance parameters from human demonstrations is an ill-defined problem. Instead, we estimate an approximate compliance profile with two useful properties: avoiding large contact forces and encouraging accurate tracking. Our approach enables robots to handle complex contact-rich manipulation tasks and achieves over 50\\% performance improvement compared to state-of-the-art visuomotor policy methods. For result videos, see https://adaptive-compliance.github.io/",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09318",
        "abstract url": "https://arxiv.org/abs/2410.09318",
        "title": "Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation",
        "rating": "-1",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "While Large language model (LLM)-based programming assistants such as CoPilot and ChatGPT can help improve the productivity of professional software developers, they can also facilitate cheating in introductory computer programming courses. Assuming instructors have limited control over the industrial-strength models, this paper investigates the baseline performance of 5 widely used LLMs on a collection of introductory programming problems, examines adversarial perturbations to degrade their performance, and describes the results of a user study aimed at understanding the efficacy of such perturbations in hindering actual code generation for introductory programming assignments. The user study suggests that i) perturbations combinedly reduced the average correctness score by 77%, ii) the drop in correctness caused by these perturbations was affected based on their detectability.",
        "subjects": [
            "cs.CL",
            "cs.CY",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09330",
        "abstract url": "https://arxiv.org/abs/2410.09330",
        "title": "A Framework to Estimate Life Cycle Emissions for Vehicle-Integrated Photovoltaic Systems",
        "rating": "-1",
        "keywords": [
            [
                "Vehicle"
            ]
        ],
        "abstract": "This paper presents a framework to estimate the environmental impact of solar electric vehicles, accounting for the emissions caused by photovoltaic system production as well as vehicle use. We leverage a cradle-to-gate life cycle assessment to estimate the greenhouse gas emissions of the vehicle-integrated photovoltaic system, from the raw material extraction to the final panel assembly, including the effect of the electricity mix both at the factory location and in the country of use. %the vehicle's life cycle, considering both Furthermore, we modify an existing optimization framework for battery electric vehicles to optimally design a solar electric vehicle and estimate its energy consumption. We showcase our framework by analyzing a case study where the mono-crystalline silicon extraction and refinement processes occur in China, while the final assembly of the panel is in The Netherlands, generating 118 kg of CO2 equivalents per square meter of solar panel. The results suggest that it is generally beneficial to operate solar electric vehicles in countries with a high irradiation index. However, when the local electricity mix already displays a low carbon intensity, the additional emissions introduced by the panel are unnecessary, requiring a longer vehicle lifetime to reach an advantageous emission balance.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "6 pages, 8 figures, 2024 IEEE Vehicle Power and Propulsion Conference, Best Paper Award"
    },
    {
        "paper id": "2410.09336",
        "abstract url": "https://arxiv.org/abs/2410.09336",
        "title": "A Novel Multi-Gait Strategy for Stable and Efficient Quadruped Robot Locomotion",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "Taking inspiration from the natural gait transition mechanism of quadrupeds, devising a good gait transition strategy is important for quadruped robots to achieve energy-efficient locomotion on various terrains and velocities. While previous studies have recognized that gait patterns linked to velocities impact two key factors, the Cost of Transport (CoT) and the stability of robot locomotion, only a limited number of studies have effectively combined these factors to design a mechanism that ensures both efficiency and stability in quadruped robot locomotion. In this paper, we propose a multi-gait selection and transition strategy to achieve stable and efficient locomotion across different terrains. Our strategy starts by establishing a gait mapping considering both CoT and locomotion stability to guide the gait selection process during locomotion. Then, we achieve gait switching in time by introducing affine transformations for gait parameters and a designed finite state machine to build the switching order. Comprehensive experiments have been conducted on using our strategy with changing terrains and velocities, and the results indicate that our proposed strategy outperforms baseline methods in achieving simultaneous efficiency in locomotion by considering CoT and stability.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09339",
        "abstract url": "https://arxiv.org/abs/2410.09339",
        "title": "Advanced Gesture Recognition in Autism: Integrating YOLOv7, Video Augmentation and VideoMAE for Video Analysis",
        "rating": "-1",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Deep learning and advancements in contactless sensors have significantly enhanced our ability to understand complex human activities in healthcare settings. In particular, deep learning models utilizing computer vision have been developed to enable detailed analysis of human gesture recognition, especially repetitive gestures which are commonly observed behaviors in children with autism. This research work aims to identify repetitive behaviors indicative of autism by analyzing videos captured in natural settings as children engage in daily activities. The focus is on accurately categorizing real-time repetitive gestures such as spinning, head banging, and arm flapping. To this end, we utilize the publicly accessible Self-Stimulatory Behavior Dataset (SSBD) to classify these stereotypical movements. A key component of the proposed methodology is the use of \\textbf{VideoMAE}, a model designed to improve both spatial and temporal analysis of video data through a masking and reconstruction mechanism. This model significantly outperformed traditional methods, achieving an accuracy of 97.7\\%, a 14.7\\% improvement over the previous state-of-the-art.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.10888",
        "abstract url": "https://arxiv.org/abs/2410.10888",
        "title": "Advancements in Ship Detection: Comparative Analysis of Optical and Hyperspectral Sensors",
        "rating": "-1",
        "keywords": [
            [
                "remote sensing"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "In marine surveillance, applications span military and civilian domains, including ship detection, marine traffic control, and disaster management. Optical and hyperspectral satellites are key for this purpose. This paper focuses on ship detection and classification techniques, particularly comparing optical and hyperspectral remote sensing approaches. It presents a comprehensive analysis of these technologies, covering feature extraction, methodologies, and their suitability for different missions. The study highlights the importance of selecting the right sensor aligned with mission objectives and conditions, aiming to improve detection accuracy through integrated strategies. The paper examines the strengths and limitations of both technologies in various maritime applications, enhancing understanding of their usability in different operational scenarios.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08583",
        "abstract url": "https://arxiv.org/abs/2410.08583",
        "title": "Intent-Enhanced Data Augmentation for Sequential Recommendation",
        "rating": "-1.5",
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The research on intent-enhanced sequential recommendation algorithms focuses on how to better mine dynamic user intent based on user behavior data for sequential recommendation tasks. Various data augmentation methods are widely applied in current sequential recommendation algorithms, effectively enhancing the ability to capture user intent. However, these widely used data augmentation methods often rely on a large amount of random sampling, which can introduce excessive noise into the training data, blur user intent, and thus negatively affect recommendation performance. Additionally, these methods have limited approaches to utilizing augmented data, failing to fully leverage the augmented samples. We propose an intent-enhanced data augmentation method for sequential recommendation(\\textbf{IESRec}), which constructs positive and negative samples based on user behavior sequences through intent-segment insertion. On one hand, the generated positive samples are mixed with the original training data, and they are trained together to improve recommendation performance. On the other hand, the generated positive and negative samples are used to build a contrastive loss function, enhancing recommendation performance through self-supervised training. Finally, the main recommendation task is jointly trained with the contrastive learning loss minimization task. Experiments on three real-world datasets validate the effectiveness of our IESRec model.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": "14 pages, 3 figures"
    },
    {
        "paper id": "2410.08614",
        "abstract url": "https://arxiv.org/abs/2410.08614",
        "title": "Interdependency and cascading failures in co-patenting and shareholding interfirm networks",
        "rating": "-1.5",
        "keywords": [
            [
                "patent"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "This work analyses the interdependent link creation of patent and shareholding links in interfirm networks, and how this dynamics affects the resilience of such networks in the face of cascading failures. Using the Orbis dataset, we construct very large co-patenting and shareholding networks, globally as well as in terms of individual countries. Besides, we construct smaller overlap networks from those firm pairs which have both types of links between them, for nine years between 2008-2016. We use information theoretic measures, such as mutual information, active information storage, and transfer entropy, to characterise the topological similarities and shared topological information between the relevant co-patenting and shareholding networks. We then construct a cascading failure model, and use it to analyse the resilience of interdependent interfirm networks in terms of multiple failure characteristics. We find that there is relatively high level of mutual information between co-patenting networks and the shareholding networks from later years, suggesting that the formation of shareholding links is influenced by the existence of patent links in previous years. We also show that this influence becomes most apparent after a delay of four years between the formation of co-patenting links and shareholding links. Analysing the resilience of shareholding networks against cascading failures, we show that in terms of both mean downtime, and failure proportion of firms, certain countries have less resilient shareholding networks compared to other countries with significant economies. Based on our results, we postulate that an interfirm network model which considers multiple types of relationships together could be a to highlight important features of economic systems around the world.",
        "subjects": [
            "cs.SI",
            "physics.soc-ph"
        ],
        "comment": "17 pages; 19 figures"
    },
    {
        "paper id": "2410.08629",
        "abstract url": "https://arxiv.org/abs/2410.08629",
        "title": "Towards Cross-domain Few-shot Graph Anomaly Detection",
        "rating": "-1.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Few-shot graph anomaly detection (GAD) has recently garnered increasing attention, which aims to discern anomalous patterns among abundant unlabeled test nodes under the guidance of a limited number of labeled training nodes. Existing few-shot GAD approaches typically adopt meta-training methods trained on richly labeled auxiliary networks to facilitate rapid adaptation to target networks that possess sparse labels. However, these proposed methods often assume that the auxiliary and target networks exist in the same data distributions-an assumption rarely holds in practical settings. This paper explores a more prevalent and complex scenario of cross-domain few-shot GAD, where the goal is to identify anomalies within sparsely labeled target graphs using auxiliary graphs from a related, yet distinct domain. The challenge here is nontrivial owing to inherent data distribution discrepancies between the source and target domains, compounded by the uncertainties of sparse labeling in the target domain. In this paper, we propose a simple and effective framework, termed CDFS-GAD, specifically designed to tackle the aforementioned challenges. CDFS-GAD first introduces a domain-adaptive graph contrastive learning module, which is aimed at enhancing cross-domain feature alignment. Then, a prompt tuning module is further designed to extract domain-specific features tailored to each domain. Moreover, a domain-adaptive hypersphere classification loss is proposed to enhance the discrimination between normal and anomalous instances under minimal supervision, utilizing domain-sensitive norms. Lastly, a self-training strategy is introduced to further refine the predicted scores, enhancing its reliability in few-shot settings. Extensive experiments on twelve real-world cross-domain data pairs demonstrate the effectiveness of the proposed CDFS-GAD framework in comparison to various existing GAD methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted by 24th IEEE International Conference on Data Mining (ICDM 2024)"
    },
    {
        "paper id": "2410.08631",
        "abstract url": "https://arxiv.org/abs/2410.08631",
        "title": "CryoFM: A Flow-based Foundation Model for Cryo-EM Densities",
        "rating": "-1.5",
        "keywords": [
            [
                "biology"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Cryo-electron microscopy (cryo-EM) is a powerful technique in structural biology and drug discovery, enabling the study of biomolecules at high resolution. Significant advancements by structural biologists using cryo-EM have led to the production of over 38,626 protein density maps at various resolutions1. However, cryo-EM data processing algorithms have yet to fully benefit from our knowledge of biomolecular density maps, with only a few recent models being data-driven but limited to specific tasks. In this study, we present CryoFM, a foundation model designed as a generative model, learning the distribution of high-quality density maps and generalizing effectively to downstream tasks. Built on flow matching, CryoFM is trained to accurately capture the prior distribution of biomolecular density maps. Furthermore, we introduce a flow posterior sampling method that leverages CRYOFM as a flexible prior for several downstream tasks in cryo-EM and cryo-electron tomography (cryo-ET) without the need for fine-tuning, achieving state-of-the-art performance on most tasks and demonstrating its potential as a foundational model for broader applications in these fields.",
        "subjects": [
            "q-bio.BM",
            "cs.AI",
            "cs.CE",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08665",
        "abstract url": "https://arxiv.org/abs/2410.08665",
        "title": "DistDD: Distributed Data Distillation Aggregation through Gradient Matching",
        "rating": "-1.5",
        "keywords": [
            [
                "federated learning"
            ],
            [
                "architecture search",
                "NAS"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we introduce DistDD, a novel approach within the federated learning framework that reduces the need for repetitive communication by distilling data directly on clients' devices. Unlike traditional federated learning that requires iterative model updates across nodes, DistDD facilitates a one-time distillation process that extracts a global distilled dataset, maintaining the privacy standards of federated learning while significantly cutting down communication costs. By leveraging the DistDD's distilled dataset, the developers of the FL can achieve just-in-time parameter tuning and neural architecture search over FL without repeating the whole FL process multiple times. We provide a detailed convergence proof of the DistDD algorithm, reinforcing its mathematical stability and reliability for practical applications. Our experiments demonstrate the effectiveness and robustness of DistDD, particularly in non-i.i.d. and mislabeled data scenarios, showcasing its potential to handle complex real-world data challenges distinctively from conventional federated learning methods. We also evaluate DistDD's application in the use case and prove its effectiveness and communication-savings in the NAS use case.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08770",
        "abstract url": "https://arxiv.org/abs/2410.08770",
        "title": "Causal machine learning for predicting treatment outcomes",
        "rating": "-1.5",
        "keywords": [
            [
                "health",
                "clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Causal machine learning (ML) offers flexible, data-driven methods for predicting treatment outcomes including efficacy and toxicity, thereby supporting the assessment and safety of drugs. A key benefit of causal ML is that it allows for estimating individualized treatment effects, so that clinical decision-making can be personalized to individual patient profiles. Causal ML can be used in combination with both clinical trial data and real-world data, such as clinical registries and electronic health records, but caution is needed to avoid biased or incorrect predictions. In this Perspective, we discuss the benefits of causal ML (relative to traditional statistical or ML approaches) and outline the key components and steps. Finally, we provide recommendations for the reliable use of causal ML and effective translation into the clinic.",
        "subjects": [
            "cs.LG",
            "stat.AP",
            "stat.ML"
        ],
        "comment": "Accepted version; not Version of Record"
    },
    {
        "paper id": "2410.08777",
        "abstract url": "https://arxiv.org/abs/2410.08777",
        "title": "Compressing regularised dynamics improves link prediction in sparse networks",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Predicting future interactions or novel links in networks is an indispensable tool across diverse domains, including genetic research, online social networks, and recommendation systems. Among the numerous techniques developed for link prediction, those leveraging the networks' community structure have proven highly effective. For example, the recently proposed MapSim predicts links based on a similarity measure derived from the code structure of the map equation, a community-detection objective function that operates on network flows. However, the standard map equation assumes complete observations and typically identifies many small modules in networks where the nodes connect through only a few links. This aspect can degrade MapSim's performance on sparse networks. To overcome this limitation, we incorporate a global regularisation method based on a Bayesian estimate of the transition rates along with three local regularisation methods. The regularised versions of the map equation compensate for incomplete observations and decrease the number of identified communities in sparse networks. The regularised methods outperform standard MapSim and several state-of-the-art embedding methods in highly sparse networks. This performance holds across multiple real-world networks with randomly removed links, simulating incomplete observations. Among the proposed regularisation methods, the global regularisation method provides the most reliable community detection and the highest link prediction performance across different network densities.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08809",
        "abstract url": "https://arxiv.org/abs/2410.08809",
        "title": "DCNet: A Data-Driven Framework for DVL Calibration",
        "rating": "-1.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "robotics",
                "navigation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Autonomous underwater vehicles (AUVs) are underwater robotic platforms used in a variety of applications. An AUV's navigation solution relies heavily on the fusion of inertial sensors and Doppler velocity logs (DVL), where the latter delivers accurate velocity updates. To ensure accurate navigation, a DVL calibration is undertaken before the mission begins to estimate its error terms. During calibration, the AUV follows a complex trajectory and employs nonlinear estimation filters to estimate error terms. In this paper, we introduce DCNet, a data-driven framework that utilizes a two-dimensional convolution kernel in an innovative way. Using DCNet and our proposed DVL error model, we offer a rapid calibration procedure. This can be applied to a trajectory with a nearly constant velocity. To train and test our proposed approach a dataset of 276 minutes long with real DVL recorded measurements was used. We demonstrated an average improvement of 70% in accuracy and 80% improvement in calibration time, compared to the baseline approach, with a low-performance DVL. As a result of those improvements, an AUV employing a low-cost DVL, can achieve higher accuracy, shorter calibration time, and apply a simple nearly constant velocity calibration trajectory. Our results also open up new applications for marine robotics utilizing low-cost, high-accurate DVLs.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "10 Pages, 9 Figures, 5 Tables"
    },
    {
        "paper id": "2410.08822",
        "abstract url": "https://arxiv.org/abs/2410.08822",
        "title": "SOLD: Reinforcement Learning with Slot Object-Centric Latent Dynamics",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Learning a latent dynamics model provides a task-agnostic representation of an agent's understanding of its environment. Leveraging this knowledge for model-based reinforcement learning holds the potential to improve sample efficiency over model-free methods by learning inside imagined rollouts. Furthermore, because the latent space serves as input to behavior models, the informative representations learned by the world model facilitate efficient learning of desired skills. Most existing methods rely on holistic representations of the environment's state. In contrast, humans reason about objects and their interactions, forecasting how actions will affect specific parts of their surroundings. Inspired by this, we propose Slot-Attention for Object-centric Latent Dynamics (SOLD), a novel algorithm that learns object-centric dynamics models in an unsupervised manner from pixel inputs. We demonstrate that the structured latent space not only improves model interpretability but also provides a valuable input space for behavior models to reason over. Our results show that SOLD outperforms DreamerV3, a state-of-the-art model-based RL algorithm, across a range of benchmark robotic environments that evaluate for both relational reasoning and low-level manipulation capabilities. Videos are available at https://slot-latent-dynamics.github.io/.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08867",
        "abstract url": "https://arxiv.org/abs/2410.08867",
        "title": "Prediction by Machine Learning Analysis of Genomic Data Phenotypic Frost Tolerance in Perccottus glenii",
        "rating": "-1.5",
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Analysis of the genome sequence of Perccottus glenii, the only fish known to possess freeze tolerance, holds significant importance for understanding how organisms adapt to extreme environments, Traditional biological analysis methods are time-consuming and have limited accuracy, To address these issues, we will employ machine learning techniques to analyze the gene sequences of Perccottus glenii, with Neodontobutis hainanens as a comparative group, Firstly, we have proposed five gene sequence vectorization methods and a method for handling ultra-long gene sequences, We conducted a comparative study on the three vectorization methods: ordinal encoding, One-Hot encoding, and K-mer encoding, to identify the optimal encoding method, Secondly, we constructed four classification models: Random Forest, LightGBM, XGBoost, and Decision Tree, The dataset used by these classification models was extracted from the National Center for Biotechnology Information database, and we vectorized the sequence matrices using the optimal encoding method, K-mer, The Random Forest model, which is the optimal model, achieved a classification accuracy of up to 99, 98 , Lastly, we utilized SHAP values to conduct an interpretable analysis of the optimal classification model, Through ten-fold cross-validation and the AUC metric, we identified the top 10 features that contribute the most to the model's classification accuracy, This demonstrates that machine learning methods can effectively replace traditional manual analysis in identifying genes associated with the freeze tolerance phenotype in Perccottus glenii.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2410.08949",
        "abstract url": "https://arxiv.org/abs/2410.08949",
        "title": "Transferable Belief Model on Quantum Circuits",
        "rating": "-1.5",
        "keywords": [
            [
                "Quantum"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The transferable belief model, as a semantic interpretation of Dempster-Shafer theory, enables agents to perform reasoning and decision making in imprecise and incomplete environments. The model offers distinct semantics for handling unreliable testimonies, allowing for a more reasonable and general process of belief transfer compared to the Bayesian approach. However, because both the belief masses and the structure of focal sets must be considered when updating belief functions-leading to extra computational complexity during reasoning-the transferable belief model has gradually lost favor among researchers in recent developments. In this paper, we implement the transferable belief model on quantum circuits and demonstrate that belief functions offer a more concise and effective alternative to Bayesian approaches within the quantum computing framework. Furthermore, leveraging the unique characteristics of quantum computing, we propose several novel belief transfer approaches. More broadly, this paper introduces a new perspective on basic information representation for quantum AI models, suggesting that belief functions are more suitable than Bayesian approach for handling uncertainty on quantum circuits.",
        "subjects": [
            "cs.AI",
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08950",
        "abstract url": "https://arxiv.org/abs/2410.08950",
        "title": "On the Adversarial Transferability of Generalized \"Skip Connections\"",
        "rating": "-1.5",
        "keywords": [
            [
                "Architecture Search"
            ],
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Skip connection is an essential ingredient for modern deep models to be deeper and more powerful. Despite their huge success in normal scenarios (state-of-the-art classification performance on natural examples), we investigate and identify an interesting property of skip connections under adversarial scenarios, namely, the use of skip connections allows easier generation of highly transferable adversarial examples. Specifically, in ResNet-like models (with skip connections), we find that using more gradients from the skip connections rather than the residual modules according to a decay factor during backpropagation allows one to craft adversarial examples with high transferability. The above method is termed as Skip Gradient Method (SGM). Although starting from ResNet-like models in vision domains, we further extend SGM to more advanced architectures, including Vision Transformers (ViTs) and models with length-varying paths and other domains, i.e. natural language processing. We conduct comprehensive transfer attacks against various models including ResNets, Transformers, Inceptions, Neural Architecture Search, and Large Language Models (LLMs). We show that employing SGM can greatly improve the transferability of crafted attacks in almost all cases. Furthermore, considering the big complexity for practical use, we further demonstrate that SGM can even improve the transferability on ensembles of models or targeted attacks and the stealthiness against current defenses. At last, we provide theoretical explanations and empirical insights on how SGM works. Our findings not only motivate new adversarial research into the architectural characteristics of models but also open up further challenges for secure model architecture design. Our code is available at https://github.com/mo666666/SGM.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09012",
        "abstract url": "https://arxiv.org/abs/2410.09012",
        "title": "Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Foundation models (FMs) such as large language models (LLMs) have significantly impacted many fields, including software engineering (SE). The interaction between SE and FMs has led to the integration of FMs into SE practices (FM4SE) and the application of SE methodologies to FMs (SE4FM). While several literature surveys exist on academic contributions to these trends, we are the first to provide a practitioner's view. We analyze 155 FM4SE and 997 SE4FM blog posts from leading technology companies, leveraging an FM-powered surveying approach to systematically label and summarize the discussed activities and tasks. We observed that while code generation is the most prominent FM4SE task, FMs are leveraged for many other SE activities such as code understanding, summarization, and API recommendation. The majority of blog posts on SE4FM are about model deployment & operation, and system architecture & orchestration. Although the emphasis is on cloud deployments, there is a growing interest in compressing FMs and deploying them on smaller devices such as edge or mobile devices. We outline eight future research directions inspired by our gained insights, aiming to bridge the gap between academic findings and real-world applications. Our study not only enriches the body of knowledge on practical applications of FM4SE and SE4FM but also demonstrates the utility of FMs as a powerful and efficient approach in conducting literature surveys within technical and grey literature domains. Our dataset, results, code and used prompts can be found in our online replication package at https://github.com/SAILResearch/fmse-blogs.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09043",
        "abstract url": "https://arxiv.org/abs/2410.09043",
        "title": "Transforming In-Vehicle Network Intrusion Detection: VAE-based Knowledge Distillation Meets Explainable AI",
        "rating": "-1.5",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "attack"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In the evolving landscape of autonomous vehicles, ensuring robust in-vehicle network (IVN) security is paramount. This paper introduces an advanced intrusion detection system (IDS) called KD-XVAE that uses a Variational Autoencoder (VAE)-based knowledge distillation approach to enhance both performance and efficiency. Our model significantly reduces complexity, operating with just 1669 parameters and achieving an inference time of 0.3 ms per batch, making it highly suitable for resource-constrained automotive environments. Evaluations in the HCRL Car-Hacking dataset demonstrate exceptional capabilities, attaining perfect scores (Recall, Precision, F1 Score of 100%, and FNR of 0%) under multiple attack types, including DoS, Fuzzing, Gear Spoofing, and RPM Spoofing. Comparative analysis on the CICIoV2024 dataset further underscores its superiority over traditional machine learning models, achieving perfect detection metrics. We furthermore integrate Explainable AI (XAI) techniques to ensure transparency in the model's decisions. The VAE compresses the original feature space into a latent space, on which the distilled model is trained. SHAP(SHapley Additive exPlanations) values provide insights into the importance of each latent dimension, mapped back to original features for intuitive understanding. Our paper advances the field by integrating state-of-the-art techniques, addressing critical challenges in the deployment of efficient, trustworthy, and reliable IDSes for autonomous vehicles, ensuring enhanced protection against emerging cyber threats.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09125",
        "abstract url": "https://arxiv.org/abs/2410.09125",
        "title": "Training on Fake Labels: Mitigating Label Leakage in Split Learning via Secure Dimension Transformation",
        "rating": "-1.5",
        "keywords": [
            [
                "federated learning"
            ],
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Two-party split learning has emerged as a popular paradigm for vertical federated learning. To preserve the privacy of the label owner, split learning utilizes a split model, which only requires the exchange of intermediate representations (IRs) based on the inputs and gradients for each IR between two parties during the learning process. However, split learning has recently been proven to survive label inference attacks. Though several defense methods could be adopted, they either have limited defensive performance or significantly negatively impact the original mission. In this paper, we propose a novel two-party split learning method to defend against existing label inference attacks while maintaining the high utility of the learned models. Specifically, we first craft a dimension transformation module, SecDT, which could achieve bidirectional mapping between original labels and increased K-class labels to mitigate label leakage from the directional perspective. Then, a gradient normalization algorithm is designed to remove the magnitude divergence of gradients from different classes. We propose a softmax-normalized Gaussian noise to mitigate privacy leakage and make our K unknowable to adversaries. We conducted experiments on real-world datasets, including two binary-classification datasets (Avazu and Criteo) and three multi-classification datasets (MNIST, FashionMNIST, CIFAR-10); we also considered current attack schemes, including direction, norm, spectral, and model completion attacks. The detailed experiments demonstrate our proposed method's effectiveness and superiority over existing approaches. For instance, on the Avazu dataset, the attack AUC of evaluated four prominent attacks could be reduced by 0.4532+-0.0127.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09133",
        "abstract url": "https://arxiv.org/abs/2410.09133",
        "title": "MVG-CRPS: A Robust Loss Function for Multivariate Probabilistic Forecasting",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In probabilistic time series forecasting, the multivariate Gaussian (MVG) distribution is widely used as predictive distribution for correlated continuous random variables. Current deep probabilistic models typically employ neural networks to parameterize the mean vector and covariance matrix of the distribution, with log-score (i.e., negative log-likelihood) as the default loss function. However, log-score is highly sensitive to outliers, leading to significant errors when anomalies are present in the data. Motivated by the use of the continuous ranked probability score (CRPS) in learning univariate distributions, we propose a robust loss function specifically designed for high-dimensional MVG outputs. The proposed MVG-CRPS loss function has a closed-form expression based on the neural network outputs, making it easily integrable into deep learning models. We evaluate MVG-CRPS on two probabilistic forecasting tasks -- multivariate autoregressive and univariate sequence-to-sequence (Seq2Seq) forecasting -- both involving observations following MVG distribution. Experimental results on real-world datasets demonstrate that MVG-CRPS achieves both robustness and efficiency, offering enhanced accuracy and uncertainty quantification in probabilistic forecasting.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09186",
        "abstract url": "https://arxiv.org/abs/2410.09186",
        "title": "Learning Algorithms Made Simple",
        "rating": "-1.5",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we discuss learning algorithms and their importance in different types of applications which includes training to identify important patterns and features in a straightforward, easy-to-understand manner. We will review the main concepts of artificial intelligence (AI), machine learning (ML), deep learning (DL), and hybrid models. Some important subsets of Machine Learning algorithms such as supervised, unsupervised, and reinforcement learning are also discussed in this paper. These techniques can be used for some important tasks like prediction, classification, and segmentation. Convolutional Neural Networks (CNNs) are used for image and video processing and many more applications. We dive into the architecture of CNNs and how to integrate CNNs with ML algorithms to build hybrid models. This paper explores the vulnerability of learning algorithms to noise, leading to misclassification. We further discuss the integration of learning algorithms with Large Language Models (LLM) to generate coherent responses applicable to many domains such as healthcare, marketing, and finance by learning important patterns from large volumes of data. Furthermore, we discuss the next generation of learning algorithms and how we may have an unified Adaptive and Dynamic Network to perform important tasks. Overall, this article provides brief overview of learning algorithms, exploring their current state, applications and future direction.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09190",
        "abstract url": "https://arxiv.org/abs/2410.09190",
        "title": "Time to Retrain? Detecting Concept Drifts in Machine Learning Systems",
        "rating": "-1.5",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "With the boom of machine learning (ML) techniques, software practitioners build ML systems to process the massive volume of streaming data for diverse software engineering tasks such as failure prediction in AIOps. Trained using historical data, such ML models encounter performance degradation caused by concept drift, i.e., data and inter-relationship (concept) changes between training and production. It is essential to use concept rift detection to monitor the deployed ML models and re-train the ML models when needed. In this work, we explore applying state-of-the-art (SOTA) concept drift detection techniques on synthetic and real-world datasets in an industrial setting. Such an industrial setting requires minimal manual effort in labeling and maximal generality in ML model architecture. We find that current SOTA semi-supervised methods not only require significant labeling effort but also only work for certain types of ML models. To overcome such limitations, we propose a novel model-agnostic technique (CDSeer) for detecting concept drift. Our evaluation shows that CDSeer has better precision and recall compared to the state-of-the-art while requiring significantly less manual labeling. We demonstrate the effectiveness of CDSeer at concept drift detection by evaluating it on eight datasets from different domains and use cases. Results from internal deployment of CDSeer on an industrial proprietary dataset show a 57.1% improvement in precision while using 99% fewer labels compared to the SOTA concept drift detection method. The performance is also comparable to the supervised concept drift detection method, which requires 100% of the data to be labeled. The improved performance and ease of adoption of CDSeer are valuable in making ML systems more reliable.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09194",
        "abstract url": "https://arxiv.org/abs/2410.09194",
        "title": "AI security and cyber risk in IoT systems",
        "rating": "-1.5",
        "keywords": [
            [
                "IoT"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "We present a dependency model tailored to the context of current challenges in data strategies and make recommendations for the cybersecurity community. The model can be used for cyber risk estimation and assessment and generic risk impact assessment.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09199",
        "abstract url": "https://arxiv.org/abs/2410.09199",
        "title": "An Efficient Contrastive Unimodal Pretraining Method for EHR Time Series Data",
        "rating": "-1.5",
        "keywords": [
            [
                "Health",
                "clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine learning has revolutionized the modeling of clinical timeseries data. Using machine learning, a Deep Neural Network (DNN) can be automatically trained to learn a complex mapping of its input features for a desired task. This is particularly valuable in Electronic Health Record (EHR) databases, where patients often spend extended periods in intensive care units (ICUs). Machine learning serves as an efficient method for extract meaningful information. However, many state-of-the-art (SOTA) methods for training DNNs demand substantial volumes of labeled data, posing significant challenges for clinics in terms of cost and time. Self-supervised learning offers an alternative by allowing practitioners to extract valuable insights from data without the need for costly labels. Yet, current SOTA methods often necessitate large data batches to achieve optimal performance, increasing computational demands. This presents a challenge when working with long clinical timeseries data. To address this, we propose an efficient method of contrastive pretraining tailored for long clinical timeseries data. Our approach utilizes an estimator for negative pair comparison, enabling effective feature extraction. We assess the efficacy of our pretraining using standard self-supervised tasks such as linear evaluation and semi-supervised learning. Additionally, our model demonstrates the ability to impute missing measurements, providing clinicians with deeper insights into patient conditions. We demonstrate that our pretraining is capable of achieving better performance as both the size of the model and the size of the measurement vocabulary scale. Finally, we externally validate our model, trained on the MIMIC-III dataset, using the eICU dataset. We demonstrate that our model is capable of learning robust clinical information that is transferable to other clinics.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09244",
        "abstract url": "https://arxiv.org/abs/2410.09244",
        "title": "Using off-the-shelf LLMs to query enterprise data by progressively revealing ontologies",
        "rating": "-1.5",
        "keywords": [
            [
                "SQL"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Ontologies are known to improve the accuracy of Large Language Models (LLMs) when translating natural language queries into a formal query language like SQL or SPARQL. There are two ways to leverage ontologies when working with LLMs. One is to fine-tune the model, i.e., to enhance it with specific domain knowledge. Another is the zero-shot prompting approach, where the ontology is provided as part of the input question. Unfortunately, modern enterprises typically have ontologies that are too large to fit in a prompt due to LLM's token size limitations. We present a solution that incrementally reveals \"just enough\" of an ontology that is needed to answer a given question.",
        "subjects": [
            "cs.DB",
            "cs.AI"
        ],
        "comment": "5 pages"
    },
    {
        "paper id": "2410.09284",
        "abstract url": "https://arxiv.org/abs/2410.09284",
        "title": "Enhanced Federated Anomaly Detection Through Autoencoders Using Summary Statistics-Based Thresholding",
        "rating": "-1.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In Federated Learning (FL), anomaly detection (AD) is a challenging task due to the decentralized nature of data and the presence of non-IID data distributions. This study introduces a novel federated threshold calculation method that leverages summary statistics from both normal and anomalous data to improve the accuracy and robustness of anomaly detection using autoencoders (AE) in a federated setting. Our approach aggregates local summary statistics across clients to compute a global threshold that optimally separates anomalies from normal data while ensuring privacy preservation. We conducted extensive experiments using publicly available datasets, including Credit Card Fraud Detection, Shuttle, and Covertype, under various data distribution scenarios. The results demonstrate that our method consistently outperforms existing federated and local threshold calculation techniques, particularly in handling non-IID data distributions. This study also explores the impact of different data distribution scenarios and the number of clients on the performance of federated anomaly detection. Our findings highlight the potential of using summary statistics for threshold calculation in improving the scalability and accuracy of federated anomaly detection systems.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09356",
        "abstract url": "https://arxiv.org/abs/2410.09356",
        "title": "Fusion Matrix Prompt Enhanced Self-Attention Spatial-Temporal Interactive Traffic Forecasting Framework",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recently, spatial-temporal forecasting technology has been rapidly developed due to the increasing demand for traffic management and travel planning. However, existing traffic forecasting models still face the following limitations. On one hand, most previous studies either focus too much on real-world geographic information, neglecting the potential traffic correlation between different regions, or overlook geographical position and only model the traffic flow relationship. On the other hand, the importance of different time slices is ignored in time modeling. Therefore, we propose a Fusion Matrix Prompt Enhanced Self-Attention Spatial-Temporal Interactive Traffic Forecasting Framework (FMPESTF), which is composed of spatial and temporal modules for down-sampling traffic data. The network is designed to establish a traffic fusion matrix considering spatial-temporal heterogeneity as a query to reconstruct a data-driven dynamic traffic data structure, which accurately reveal the flow relationship of nodes in the traffic network. In addition, we introduce attention mechanism in time modeling, and design hierarchical spatial-temporal interactive learning to help the model adapt to various traffic scenarios. Through extensive experimental on six real-world traffic datasets, our method is significantly superior to other baseline models, demonstrating its efficiency and accuracy in dealing with traffic forecasting problems.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "THE WEB CONFERENCE 2025"
    },
    {
        "paper id": "2410.09359",
        "abstract url": "https://arxiv.org/abs/2410.09359",
        "title": "Green Recommender Systems: Optimizing Dataset Size for Energy-Efficient Algorithm Performance",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "As recommender systems become increasingly prevalent, the environmental impact and energy efficiency of training large-scale models have come under scrutiny. This paper investigates the potential for energy-efficient algorithm performance by optimizing dataset sizes through downsampling techniques in the context of Green Recommender Systems. We conducted experiments on the MovieLens 100K, 1M, 10M, and Amazon Toys and Games datasets, analyzing the performance of various recommender algorithms under different portions of dataset size. Our results indicate that while more training data generally leads to higher algorithm performance, certain algorithms, such as FunkSVD and BiasedMF, particularly with unbalanced and sparse datasets like Amazon Toys and Games, maintain high-quality recommendations with up to a 50% reduction in training data, achieving nDCG@10 scores within approximately 13% of full dataset performance. These findings suggest that strategic dataset reduction can decrease computational and environmental costs without substantially compromising recommendation quality. This study advances sustainable and green recommender systems by providing insights for reducing energy consumption while maintaining effectiveness.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.11886",
        "abstract url": "https://arxiv.org/abs/2410.11886",
        "title": "Are Grid Cells Hexagonal for Performance or by Convenience?",
        "rating": "-1.5",
        "keywords": [
            [
                "biologically"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "This paper investigates whether the hexagonal structure of grid cells provides any performance benefits or if it merely represents a biologically convenient configuration. Utilizing the Vector-HaSH content addressable memory model as a model of the grid cell -- place cell network of the mammalian brain, we compare the performance of square and hexagonal grid cells in tasks of storing and retrieving spatial memories. Our experiments across different path types, path lengths and grid configurations, reveal that hexagonal grid cells perform similarly to square grid cells with respect to spatial representation and memory recall. Our results show comparable accuracy and robustness across different datasets and noise levels on images to recall. These findings suggest that the brain's use of hexagonal grids may be more a matter of biological convenience and ease of implementation rather than because they provide superior performance over square grid cells (which are easier to implement in silico).",
        "subjects": [
            "q-bio.NC",
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ],
        "comment": "5 pages, accepted at Montreal AI and Neuroscience Conference 2024"
    },
    {
        "paper id": "2410.08534",
        "abstract url": "https://arxiv.org/abs/2410.08534",
        "title": "Quality Prediction of AI Generated Images and Videos: Emerging Trends and Opportunities",
        "rating": "-2",
        "keywords": [
            [
                "text-to-image",
                "super resolution"
            ],
            [
                "Quality Assessment"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The advent of AI has influenced many aspects of human life, from self-driving cars and intelligent chatbots to text-based image and video generation models capable of creating realistic images and videos based on user prompts (text-to-image, image-to-image, and image-to-video). AI-based methods for image and video super resolution, video frame interpolation, denoising, and compression have already gathered significant attention and interest in the industry and some solutions are already being implemented in real-world products and services. However, to achieve widespread integration and acceptance, AI-generated and enhanced content must be visually accurate, adhere to intended use, and maintain high visual quality to avoid degrading the end user's quality of experience (QoE). One way to monitor and control the visual \"quality\" of AI-generated and -enhanced content is by deploying Image Quality Assessment (IQA) and Video Quality Assessment (VQA) models. However, most existing IQA and VQA models measure visual fidelity in terms of \"reconstruction\" quality against a pristine reference content and were not designed to assess the quality of \"generative\" artifacts. To address this, newer metrics and models have recently been proposed, but their performance evaluation and overall efficacy have been limited by datasets that were too small or otherwise lack representative content and/or distortion capacity; and by performance measures that can accurately report the success of an IQA/VQA model for \"GenAI\". This paper examines the current shortcomings and possibilities presented by AI-generated and enhanced image and video content, with a particular focus on end-user perceived quality. Finally, we discuss open questions and make recommendations for future work on the \"GenAI\" quality assessment problems, towards further progressing on this interesting and relevant field of research.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": "\"The abstract field cannot be longer than 1,920 characters\", the abstract appearing here is slightly shorter than that in the PDF file"
    },
    {
        "paper id": "2410.08551",
        "abstract url": "https://arxiv.org/abs/2410.08551",
        "title": "Context-Aware Full Body Anonymization using Text-to-Image Diffusion Models",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion",
                "Text-to-Image"
            ],
            [
                "facial"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Anonymization plays a key role in protecting sensible information of individuals in real world datasets. Self-driving cars for example need high resolution facial features to track people and their viewing direction to predict future behaviour and react accordingly. In order to protect people's privacy whilst keeping important features in the dataset, it is important to replace the full body of a person with a highly detailed anonymized one. In contrast to doing face anonymization, full body replacement decreases the ability of recognizing people by their hairstyle or clothes. In this paper, we propose a workflow for full body person anonymization utilizing Stable Diffusion as a generative backend. Text-to-image diffusion models, like Stable Diffusion, OpenAI's DALL-E or Midjourney, have become very popular in recent time, being able to create photorealistic images from a single text prompt. We show that our method outperforms state-of-the art anonymization pipelines with respect to image quality, resolution, Inception Score (IS) and Frechet Inception Distance (FID). Additionally, our method is invariant with respect to the image generator and thus able to be used with the latest models available.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08588",
        "abstract url": "https://arxiv.org/abs/2410.08588",
        "title": "ViT3D Alignment of LLaMA3: 3D Medical Image Report Generation",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Medical"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Automatic medical report generation (MRG), which aims to produce detailed text reports from medical images, has emerged as a critical task in this domain. MRG systems can enhance radiological workflows by reducing the time and effort required for report writing, thereby improving diagnostic efficiency. In this work, we present a novel approach for automatic MRG utilizing a multimodal large language model. Specifically, we employed the 3D Vision Transformer (ViT3D) image encoder introduced from M3D-CLIP to process 3D scans and use the Asclepius-Llama3-8B as the language model to generate the text reports by auto-regressive decoding. The experiment shows our model achieved an average Green score of 0.3 on the MRG task validation set and an average accuracy of 0.61 on the visual question answering (VQA) task validation set, outperforming the baseline model. Our approach demonstrates the effectiveness of the ViT3D alignment of LLaMA3 for automatic MRG and VQA tasks by tuning the model on a small dataset.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08626",
        "abstract url": "https://arxiv.org/abs/2410.08626",
        "title": "Small Tunes Transformer: Exploring Macro & Micro-Level Hierarchies for Skeleton-Conditioned Melody Generation",
        "rating": "-2",
        "keywords": [
            [
                "Skeleton"
            ],
            [
                "music"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Recently, symbolic music generation has become a focus of numerous deep learning research. Structure as an important part of music, contributes to improving the quality of music, and an increasing number of works start to study the hierarchical structure. In this study, we delve into the multi-level structures within music from macro-level and micro-level hierarchies. At the macro-level hierarchy, we conduct phrase segmentation algorithm to explore how phrases influence the overall development of music, and at the micro-level hierarchy, we design skeleton notes extraction strategy to explore how skeleton notes within each phrase guide the melody generation. Furthermore, we propose a novel Phrase-level Cross-Attention mechanism to capture the intrinsic relationship between macro-level hierarchy and micro-level hierarchy. Moreover, in response to the current lack of research on Chinese-style music, we construct our Small Tunes Dataset: a substantial collection of MIDI files comprising 10088 Small Tunes, a category of traditional Chinese Folk Songs. This dataset serves as the focus of our study. We generate Small Tunes songs utilizing the extracted skeleton notes as conditions, and experiment results indicate that our proposed model, Small Tunes Transformer, outperforms other state-of-the-art models. Besides, we design three novel objective evaluation metrics to evaluate music from both rhythm and melody dimensions.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": "Accepted by 31st International Conference on MultiMedia Modeling (MMM2025)"
    },
    {
        "paper id": "2410.08669",
        "abstract url": "https://arxiv.org/abs/2410.08669",
        "title": "SmartPretrain: Model-Agnostic and Dataset-Agnostic Representation Learning for Motion Prediction",
        "rating": "-2",
        "keywords": [
            [
                "robot"
            ],
            [
                "Forecast"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. However, the scarcity of large-scale driving datasets has hindered the development of robust and generalizable motion prediction models, limiting their ability to capture complex interactions and road geometries. Inspired by recent advances in natural language processing (NLP) and computer vision (CV), self-supervised learning (SSL) has gained significant attention in the motion prediction community for learning rich and transferable scene representations. Nonetheless, existing pre-training methods for motion prediction have largely focused on specific model architectures and single dataset, limiting their scalability and generalizability. To address these challenges, we propose SmartPretrain, a general and scalable SSL framework for motion prediction that is both model-agnostic and dataset-agnostic. Our approach integrates contrastive and reconstructive SSL, leveraging the strengths of both generative and discriminative paradigms to effectively represent spatiotemporal evolution and interactions without imposing architectural constraints. Additionally, SmartPretrain employs a dataset-agnostic scenario sampling strategy that integrates multiple datasets, enhancing data volume, diversity, and robustness. Extensive experiments on multiple datasets demonstrate that SmartPretrain consistently improves the performance of state-of-the-art prediction models across datasets, data splits and main metrics. For instance, SmartPretrain significantly reduces the MissRate of Forecast-MAE by 10.6%. These results highlight SmartPretrain's effectiveness as a unified, scalable solution for motion prediction, breaking free from the limitations of the small-data regime. Codes are available at https://github.com/youngzhou1999/SmartPretrain",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "comment": "11 pages, 5 figures"
    },
    {
        "paper id": "2410.08680",
        "abstract url": "https://arxiv.org/abs/2410.08680",
        "title": "Gait Sequence Upsampling using Diffusion Models for Single LiDAR Sensors",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion",
                "inpainting"
            ],
            [
                "LiDAR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently, 3D LiDAR has emerged as a promising technique in the field of gait-based person identification, serving as an alternative to traditional RGB cameras, due to its robustness under varying lighting conditions and its ability to capture 3D geometric information. However, long capture distances or the use of low-cost LiDAR sensors often result in sparse human point clouds, leading to a decline in identification performance. To address these challenges, we propose a sparse-to-dense upsampling model for pedestrian point clouds in LiDAR-based gait recognition, named LidarGSU, which is designed to improve the generalization capability of existing identification models. Our method utilizes diffusion probabilistic models (DPMs), which have shown high fidelity in generative tasks such as image completion. In this work, we leverage DPMs on sparse sequential pedestrian point clouds as conditional masks in a video-to-video translation approach, applied in an inpainting manner. We conducted extensive experiments on the SUSTeck1K dataset to evaluate the generative quality and recognition performance of the proposed method. Furthermore, we demonstrate the applicability of our upsampling model using a real-world dataset, captured with a low-resolution sensor across varying measurement distances.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08725",
        "abstract url": "https://arxiv.org/abs/2410.08725",
        "title": "The generalized method of solving ECDLP using quantum annealing",
        "rating": "-2",
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "This paper presents a generalization of a method allowing the transformation of the Elliptic Curve Discrete Logarithm Problem (ECDLP) over prime fields to the Quadratic Unconstrained Binary Optimization (QUBO) problem. The original method requires that a given elliptic curve model has complete arithmetic. The new one has no such restriction, which is a breakthrough. Since the mentioned obstacle is no longer a problem, the latest version of the algorithm may be used for any elliptic curve model. As a result, one may use quantum annealing to solve ECDLP on any given model of elliptic curves.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "13 pages, 4 figures, 2 tables"
    },
    {
        "paper id": "2410.08726",
        "abstract url": "https://arxiv.org/abs/2410.08726",
        "title": "5G as Enabler for Industrie 4.0 Use Cases: Challenges and Concepts",
        "rating": "-2",
        "keywords": [
            [
                "5G",
                "industrial"
            ]
        ],
        "abstract": "The increasing demand for highly customized products, as well as flexible production lines, can be seen as trigger for the \"fourth industrial revolution\", referred to as \"Industrie 4.0\". Current systems usually rely on wire-line technologies to connect sensors and actuators. To enable a higher flexibility such as moving robots or drones, these connections need to be replaced by wireless technologies in the future. Furthermore, this facilitates the renewal of brownfield deployments to address Industrie 4.0 requirements. This paper proposes representative use cases, which have been examined in the German Tactile Internet 4.0 (TACNET 4.0) research project. In order to analyze these use cases, this paper identifies the main challenges and requirements of communication networks in Industrie 4.0 and discusses the applicability of 5th generation wireless communication systems (5G).",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08742",
        "abstract url": "https://arxiv.org/abs/2410.08742",
        "title": "OpenWiFiSync: Open Source Implementation of a Clock Synchronization Algorithm using Wi-Fi",
        "rating": "-2",
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "Precise clock synchronization is an important requirement for distributed and networked industrial use cases. As more and more use cases contain mobile devices, clock synchronization has to be performed over wireless communication links. As wireless communication links are currently not as deterministic and reliable as wireline communication systems, novel clock synchronization algorithms have to be investigated. Here, the so-called Reference Broadcast Infrastructure Synchronization Protocol is a well suited solution as it brings up multiple advantages. Most important is the non-invasiveness, meaning it can be used with commercially available components. As a considerably high amount of factories use Wi-Fi as wireless communication system for their mobile use cases, the aforementioned protocol is implemented using Wi-Fi. Furthermore, the usage of Open-Source Software can be seen as driver for highly efficient and interoperable applications. Consequently, the implementation is accessible under the GNU General Public License on GitHub under the designation OpenWiFiSync. Besides the details on concept, its implementation, and the used testbed, first results are outlined within this paper. Additionally, future work and the estimated timeline are presented.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08746",
        "abstract url": "https://arxiv.org/abs/2410.08746",
        "title": "Last-iterate Convergence in Regularized Graphon Mean Field Game",
        "rating": "-2",
        "keywords": [
            [
                "tabular"
            ]
        ],
        "abstract": "To model complex real-world systems, such as traders in stock markets, or the dissemination of contagious diseases, graphon mean-field games (GMFG) have been proposed to model many agents. Despite the empirical success, our understanding of GMFG is limited. Popular algorithms such as mirror descent are deployed but remain unknown for their convergence properties. In this work, we give the first last-iterate convergence rate of mirror descent in regularized monotone GMFG. In tabular monotone GMFG with finite state and action spaces and under bandit feedback, we show a last-iterate convergence rate of $O(T^{-1/4})$. Moreover, when exact knowledge of costs and transitions is available, we improve this convergence rate to $O(T^{-1})$, matching the existing convergence rate observed in strongly convex games. In linear GMFG, our algorithm achieves a last-iterate convergence rate of $O(T^{-1/5})$. Finally, we verify the performance of the studied algorithms by empirically testing them against fictitious play in a variety of tasks.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08780",
        "abstract url": "https://arxiv.org/abs/2410.08780",
        "title": "Optimizing NeRF-based SLAM with Trajectory Smoothness Constraints",
        "rating": "-2",
        "keywords": [
            [
                "NeRF",
                "Radiance Fields"
            ],
            [
                "Trajectory",
                "SLAM"
            ]
        ],
        "abstract": "The joint optimization of Neural Radiance Fields (NeRF) and camera trajectories has been widely applied in SLAM tasks due to its superior dense mapping quality and consistency. NeRF-based SLAM learns camera poses using constraints by implicit map representation. A widely observed phenomenon that results from the constraints of this form is jerky and physically unrealistic estimated camera motion, which in turn affects the map quality. To address this deficiency of current NeRF-based SLAM, we propose in this paper TS-SLAM (TS for Trajectory Smoothness). It introduces smoothness constraints on camera trajectories by representing them with uniform cubic B-splines with continuous acceleration that guarantees smooth camera motion. Benefiting from the differentiability and local control properties of B-splines, TS-SLAM can incrementally learn the control points end-to-end using a sliding window paradigm. Additionally, we regularize camera trajectories by exploiting the dynamics prior to further smooth trajectories. Experimental results demonstrate that TS-SLAM achieves superior trajectory accuracy and improves mapping quality versus NeRF-based SLAM that does not employ the above smoothness constraints.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08797",
        "abstract url": "https://arxiv.org/abs/2410.08797",
        "title": "CoTCoNet: An Optimized Coupled Transformer-Convolutional Network with an Adaptive Graph Reconstruction for Leukemia Detection",
        "rating": "-2",
        "keywords": [
            [
                "Graph"
            ],
            [
                "biological"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Swift and accurate blood smear analysis is an effective diagnostic method for leukemia and other hematological malignancies. However, manual leukocyte count and morphological evaluation using a microscope is time-consuming and prone to errors. Conventional image processing methods also exhibit limitations in differentiating cells due to the visual similarity between malignant and benign cell morphology. This limitation is further compounded by the skewed training data that hinders the extraction of reliable and pertinent features. In response to these challenges, we propose an optimized Coupled Transformer Convolutional Network (CoTCoNet) framework for the classification of leukemia, which employs a well-designed transformer integrated with a deep convolutional network to effectively capture comprehensive global features and scalable spatial patterns, enabling the identification of complex and large-scale hematological features. Further, the framework incorporates a graph-based feature reconstruction module to reveal the hidden or unobserved hard-to-see biological features of leukocyte cells and employs a Population-based Meta-Heuristic Algorithm for feature selection and optimization. To mitigate data imbalance issues, we employ a synthetic leukocyte generator. In the evaluation phase, we initially assess CoTCoNet on a dataset containing 16,982 annotated cells, and it achieves remarkable accuracy and F1-Score rates of 0.9894 and 0.9893, respectively. To broaden the generalizability of our model, we evaluate it across four publicly available diverse datasets, which include the aforementioned dataset. This evaluation demonstrates that our method outperforms current state-of-the-art approaches. We also incorporate an explainability approach in the form of feature visualization closely aligned with cell annotations to provide a deeper understanding of the framework.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08799",
        "abstract url": "https://arxiv.org/abs/2410.08799",
        "title": "Online Learning for Intelligent Thermal Management of Interference-coupled and Passively Cooled Base Stations",
        "rating": "-2",
        "keywords": [
            [
                "Thermal"
            ]
        ],
        "abstract": "Passively cooled base stations (PCBSs) have emerged to deliver better cost and energy efficiency. However, passive cooling necessitates intelligent thermal control via traffic management, i.e., the instantaneous data traffic or throughput of a PCBS directly impacts its thermal performance. This is particularly challenging for outdoor deployment of PCBSs because the heat dissipation efficiency is uncertain and fluctuates over time. What is more, the PCBSs are interference-coupled in multi-cell scenarios. Thus, a higher-throughput PCBS leads to higher interference to the other PCBSs, which, in turn, would require more resource consumption to meet their respective throughput targets. In this paper, we address online decision-making for maximizing the total downlink throughput for a multi-PCBS system subject to constraints related on operating temperature. We demonstrate that a reinforcement learning (RL) approach, specifically soft actor-critic (SAC), can successfully perform throughput maximization while keeping the PCBSs cool, by adapting the throughput to time-varying heat dissipation conditions. Furthermore, we design a denial and reward mechanism that effectively mitigates the risk of overheating during the exploration phase of RL. Simulation results show that our approach achieves up to 88.6% of the global optimum. This is very promising, as our approach operates without prior knowledge of future heat dissipation efficiency, which is required by the global optimum.",
        "subjects": [
            "cs.NI",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08805",
        "abstract url": "https://arxiv.org/abs/2410.08805",
        "title": "MEMROC: Multi-Eye to Mobile RObot Calibration",
        "rating": "-2",
        "keywords": [
            [
                "6-DoF"
            ],
            [
                "RObot"
            ]
        ],
        "abstract": "This paper presents MEMROC (Multi-Eye to Mobile RObot Calibration), a novel motion-based calibration method that simplifies the process of accurately calibrating multiple cameras relative to a mobile robot's reference frame. MEMROC utilizes a known calibration pattern to facilitate accurate calibration with a lower number of images during the optimization process. Additionally, it leverages robust ground plane detection for comprehensive 6-DoF extrinsic calibration, overcoming a critical limitation of many existing methods that struggle to estimate the complete camera pose. The proposed method addresses the need for frequent recalibration in dynamic environments, where cameras may shift slightly or alter their positions due to daily usage, operational adjustments, or vibrations from mobile robot movements. MEMROC exhibits remarkable robustness to noisy odometry data, requiring minimal calibration input data. This combination makes it highly suitable for daily operations involving mobile robots. A comprehensive set of experiments on both synthetic and real data proves MEMROC's efficiency, surpassing existing state-of-the-art methods in terms of accuracy, robustness, and ease of use. To facilitate further research, we have made our code publicly available at https://github.com/davidea97/MEMROC.git.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08864",
        "abstract url": "https://arxiv.org/abs/2410.08864",
        "title": "The Good, the Bad and the Ugly: Watermarks, Transferable Attacks and Adversarial Defenses",
        "rating": "-2",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "watermark"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "We formalize and extend existing definitions of backdoor-based watermarks and adversarial defenses as interactive protocols between two players. The existence of these schemes is inherently tied to the learning tasks for which they are designed. Our main result shows that for almost every discriminative learning task, at least one of the two -- a watermark or an adversarial defense -- exists. The term \"almost every\" indicates that we also identify a third, counterintuitive but necessary option, i.e., a scheme we call a transferable attack. By transferable attack, we refer to an efficient algorithm computing queries that look indistinguishable from the data distribution and fool all efficient defenders. To this end, we prove the necessity of a transferable attack via a construction that uses a cryptographic tool called homomorphic encryption. Furthermore, we show that any task that satisfies our notion of a transferable attack implies a cryptographic primitive, thus requiring the underlying task to be computationally complex. These two facts imply an \"equivalence\" between the existence of transferable attacks and cryptography. Finally, we show that the class of tasks of bounded VC-dimension has an adversarial defense, and a subclass of them has a watermark.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CR"
        ],
        "comment": "42 pages, 6 figures, preliminary version published in ICML 2024 (Workshop on Theoretical Foundations of Foundation Models), see https://openreview.net/pdf?id=WMaFRiggwV"
    },
    {
        "paper id": "2410.08935",
        "abstract url": "https://arxiv.org/abs/2410.08935",
        "title": "Voxel-SLAM: A Complete, Accurate, and Versatile LiDAR-Inertial SLAM System",
        "rating": "-2",
        "keywords": [
            [
                "Voxel"
            ],
            [
                "LiDAR",
                "vehicle",
                "SLAM"
            ]
        ],
        "abstract": "In this work, we present Voxel-SLAM: a complete, accurate, and versatile LiDAR-inertial SLAM system that fully utilizes short-term, mid-term, long-term, and multi-map data associations to achieve real-time estimation and high precision mapping. The system consists of five modules: initialization, odometry, local mapping, loop closure, and global mapping, all employing the same map representation, an adaptive voxel map. The initialization provides an accurate initial state estimation and a consistent local map for subsequent modules, enabling the system to start with a highly dynamic initial state. The odometry, exploiting the short-term data association, rapidly estimates current states and detects potential system divergence. The local mapping, exploiting the mid-term data association, employs a local LiDAR-inertial bundle adjustment (BA) to refine the states (and the local map) within a sliding window of recent LiDAR scans. The loop closure detects previously visited places in the current and all previous sessions. The global mapping refines the global map with an efficient hierarchical global BA. The loop closure and global mapping both exploit long-term and multi-map data associations. We conducted a comprehensive benchmark comparison with other state-of-the-art methods across 30 sequences from three representative scenes, including narrow indoor environments using hand-held equipment, large-scale wilderness environments with aerial robots, and urban environments on vehicle platforms. Other experiments demonstrate the robustness and efficiency of the initialization, the capacity to work in multiple sessions, and relocalization in degenerated environments.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08937",
        "abstract url": "https://arxiv.org/abs/2410.08937",
        "title": "Distributed Quantum Hypothesis Testing under Zero-rate Communication Constraints",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "The trade-offs between error probabilities in quantum hypothesis testing are by now well-understood in the centralized setting, but much less is known for distributed settings. Here, we study a distributed binary hypothesis testing problem to infer a bipartite quantum state shared between two remote parties, where one of these parties communicates classical information to the tester at zero-rate (while the other party communicates classical or quantum information to the tester at zero-rate or higher). As our main contribution, we derive an efficiently computable single-letter formula for the Stein's exponent of this problem, when the state under the alternative is product. For the general case, we show that the Stein's exponent is given by a multi-letter expression involving max-min optimization of regularized measured relative entropy. While this becomes single-letter for the fully classical case, we further prove that this already does not happen in the same way for classical-quantum states in general. As a key tool for proving the converse direction of our results, we develop a quantum version of the blowing-up lemma which may be of independent interest.",
        "subjects": [
            "quant-ph",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08980",
        "abstract url": "https://arxiv.org/abs/2410.08980",
        "title": "Leveraging Internet Principles to Build a Quantum Network",
        "rating": "-2",
        "keywords": [
            [
                "Quantum",
                "physics"
            ]
        ],
        "abstract": "Designing an operational architecture for the Quantum Internet is a challenging task in light of both fundamental limitations imposed by the laws of physics and technological constraints. Here, we propose a method to abstract away most of the quantum-specific elements and formulate a best-effort quantum network architecture based on packet-switching, akin to that of the classical Internet. Such reframing provides an opportunity to exploit the many tools and protocols available and well-understood within the Internet. As an illustration, we tailor and adapt classical congestion control and active queue management protocols to quantum networks, comprising an architecture wherein quantum end- and intermediate nodes effectively regulate demand and resource utilization, respectively. Results show that these classical networking tools can be effectively used to combat quantum memory decoherence and keep end-to-end fidelity around a target value.",
        "subjects": [
            "quant-ph",
            "cs.NI"
        ],
        "comment": "9 pages, 5 figures"
    },
    {
        "paper id": "2410.08999",
        "abstract url": "https://arxiv.org/abs/2410.08999",
        "title": "I-SCOUT: Integrated Sensing and Communications to Uncover Moving Targets in NextG Networks",
        "rating": "-2",
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "Integrated Sensing and Communication (ISAC) represents a transformative approach within 5G and beyond, aiming to merge wireless communication and sensing functionalities into a unified network infrastructure. This integration offers enhanced spectrum efficiency, real-time situational awareness, cost and energy reductions, and improved operational performance. ISAC provides simultaneous communication and sensing capabilities, enhancing the ability to detect, track, and respond to spectrum dynamics and potential threats in complex environments. In this paper, we introduce I-SCOUT, an innovative ISAC solution designed to uncover moving targets in NextG networks. We specifically repurpose the Positioning Reference Signal (PRS) of the 5G waveform, exploiting its distinctive autocorrelation characteristics for environment sensing. The reflected signals from moving targets are processed to estimate both the range and velocity of these targets using the cross ambiguity function (CAF). We conduct an in-depth analysis of the tradeoff between sensing and communication functionalities, focusing on the allocation of PRSs for ISAC purposes. Our study reveals that the number of PRSs dedicated to ISAC has a significant impact on the system's performance, necessitating a careful balance to optimize both sensing accuracy and communication efficiency. Our results demonstrate that I-SCOUT effectively leverages ISAC to accurately determine the range and velocity of moving targets. Moreover, I-SCOUT is capable of distinguishing between multiple targets within a group, showcasing its potential for complex scenarios. These findings underscore the viability of ISAC in enhancing the capabilities of NextG networks, for both commercial and tactical applications where precision and reliability are critical.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Accepted for publication at the MILCOM'24 conference"
    },
    {
        "paper id": "2410.09018",
        "abstract url": "https://arxiv.org/abs/2410.09018",
        "title": "Data-Driven Neural Estimation of Indirect Rate-Distortion Function",
        "rating": "-2",
        "keywords": [
            [
                "remote sensing"
            ]
        ],
        "abstract": "The rate-distortion function (RDF) has long been an information-theoretic benchmark for data compression. As its natural extension, the indirect rate-distortion function (iRDF) corresponds to the scenario where the encoder can only access an observation correlated with the source, rather than the source itself. Such scenario is also relevant for modern applications like remote sensing and goal-oriented communication. The iRDF can be reduced into a standard RDF with the distortion measure replaced by its conditional expectation conditioned upon the observation. This reduction, however, leads to a non-trivial challenge when one needs to estimate the iRDF given datasets only, because without statistical knowledge of the joint probability distribution between the source and its observation, the conditional expectation cannot be evaluated. To tackle this challenge, starting from the well known fact that conditional expectation is the minimum mean-squared error estimator and exploiting a Markovian relationship, we identify a functional equivalence between the reduced distortion measure in the iRDF and the solution of a quadratic loss minimization problem, which can be efficiently approximated by neural network approach. We proceed to reformulate the iRDF as a variational problem corresponding to the Lagrangian representation of the iRDF curve, and propose a neural network based approximate solution, integrating the aforementioned distortion measure estimator. Asymptotic analysis guarantees consistency of the solution, and numerical experimental results demonstrate the accuracy and effectiveness of the algorithm.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09029",
        "abstract url": "https://arxiv.org/abs/2410.09029",
        "title": "Towards a Health-Based Power Grid Optimization in the Artificial Intelligence Era",
        "rating": "-2",
        "keywords": [
            [
                "Health"
            ]
        ],
        "abstract": "The electric power sector is one of the largest contributors to greenhouse gas emissions in the world. In recent years, there has been an unprecedented increase in electricity demand driven by the so-called Artificial Intelligence (AI) revolution. Although AI has and will continue to have a transformative impact, its environmental and health impacts are often overlooked. The standard approach to power grid optimization aims to minimize CO$_2$ emissions. In this paper, we propose a new holistic paradigm. Our proposed optimization directly targets the minimization of adverse health outcomes under energy efficiency and emission constraints. We show the first example of an optimal fuel mix allocation problem aiming to minimize the average number of adverse health effects resulting from exposure to hazardous air pollutants with constraints on the average and marginal emissions. We argue that this new health-based power grid optimization is essential to promote truly sustainable technological advances that align both with global climate goals and public health priorities.",
        "subjects": [
            "eess.SP",
            "eess.SY",
            "stat.AP"
        ],
        "comment": "5 pages, 1 figure"
    },
    {
        "paper id": "2410.09036",
        "abstract url": "https://arxiv.org/abs/2410.09036",
        "title": "Design and Performance Evaluation of an Elbow-Based Biomechanical Energy Harvester",
        "rating": "-2",
        "keywords": [
            [
                "Biomechanical"
            ]
        ],
        "abstract": "Carbon emissions have long been attributed to the increase in climate change. With the effects of climate change escalating in the past few years, there has been an increased effort to find green alternatives to power generation, which has been a major contributor to carbon emissions. One prominent way that has arisen is biomechanical energy, or harvesting energy based on natural human movement. This study will evaluate the feasibility of electric generation using a gear and generator-based biomechanical energy harvester in the elbow joint. The joint was chosen using kinetic arm analysis through MediaPipe, in which the elbow joint showed much higher angular velocity during walking, thus showing more potential as a place to construct the harvester. Leg joints were excluded to not obstruct daily movement. The gear and generator type was decided to maximize energy production in the elbow joint. The device was constructed using a gearbox and a generator. The results show that it generated as much as 0.16 watts using the optimal resistance. This demonstrates the feasibility of electric generation with an elbow joint gear and generator-type biomechanical energy harvester.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 9 figures"
    },
    {
        "paper id": "2410.09169",
        "abstract url": "https://arxiv.org/abs/2410.09169",
        "title": "Efficient Zero-Knowledge Proofs for Set Membership in Blockchain-Based Sensor Networks: A Novel OR-Aggregation Approach",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "Blockchain-based sensor networks offer promising solutions for secure and transparent data management in IoT ecosystems. However, efficient set membership proofs remain a critical challenge, particularly in resource-constrained environments. This paper introduces a novel OR-aggregation approach for zero-knowledge set membership proofs, tailored specifically for blockchain-based sensor networks. We provide a comprehensive theoretical foundation, detailed protocol specification, and rigorous security analysis. Our implementation incorporates optimization techniques for resource-constrained devices and strategies for integration with prominent blockchain platforms. Extensive experimental evaluation demonstrates the superiority of our approach over existing methods, particularly for large-scale deployments. Results show significant improvements in proof size, generation time, and verification efficiency. The proposed OR-aggregation technique offers a scalable and privacy-preserving solution for set membership verification in blockchain-based IoT applications, addressing key limitations of current approaches. Our work contributes to the advancement of efficient and secure data management in large-scale sensor networks, paving the way for wider adoption of blockchain technology in IoT ecosystems.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09181",
        "abstract url": "https://arxiv.org/abs/2410.09181",
        "title": "Can a large language model be a gaslighter?",
        "rating": "-2",
        "keywords": [
            [
                "attacks"
            ],
            [
                "psychological"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have gained human trust due to their capabilities and helpfulness. However, this in turn may allow LLMs to affect users' mindsets by manipulating language. It is termed as gaslighting, a psychological effect. In this work, we aim to investigate the vulnerability of LLMs under prompt-based and fine-tuning-based gaslighting attacks. Therefore, we propose a two-stage framework DeepCoG designed to: 1) elicit gaslighting plans from LLMs with the proposed DeepGaslighting prompting template, and 2) acquire gaslighting conversations from LLMs through our Chain-of-Gaslighting method. The gaslighting conversation dataset along with a corresponding safe dataset is applied to fine-tuning-based attacks on open-source LLMs and anti-gaslighting safety alignment on these LLMs. Experiments demonstrate that both prompt-based and fine-tuning-based attacks transform three open-source LLMs into gaslighters. In contrast, we advanced three safety alignment strategies to strengthen (by 12.05%) the safety guardrail of LLMs. Our safety alignment strategies have minimal impacts on the utility of LLMs. Empirical studies indicate that an LLM may be a potential gaslighter, even if it passed the harmfulness test on general dangerous queries.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.CL",
            "cs.CY",
            "cs.LG"
        ],
        "comment": "10/26 (Main Body/Total), 8 figures"
    },
    {
        "paper id": "2410.09191",
        "abstract url": "https://arxiv.org/abs/2410.09191",
        "title": "Testing the Unknown: A Framework for OpenMP Testing via Random Program Generation",
        "rating": "-2",
        "keywords": [
            [
                "grammar"
            ]
        ],
        "abstract": "We present a randomized differential testing approach to test OpenMP implementations. In contrast to previous work that manually creates dozens of verification and validation tests, our approach is able to randomly generate thousands of tests, exposing OpenMP implementations to a wide range of program behaviors. We represent the space of possible random OpenMP tests using a grammar and implement our method as an extension of the Varity program generator. By generating 1,800 OpenMP tests, we find various performance anomalies and correctness issues when we apply it to three OpenMP implementations: GCC, Clang, and Intel. We also present several case studies that analyze the anomalies and give more details about the classes of tests that our approach creates.",
        "subjects": [
            "cs.SE",
            "cs.PF",
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09210",
        "abstract url": "https://arxiv.org/abs/2410.09210",
        "title": "Cross-Domain Distribution Alignment for Segmentation of Private Unannotated 3D Medical Images",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Manual annotation of 3D medical images for segmentation tasks is tedious and time-consuming. Moreover, data privacy limits the applicability of crowd sourcing to perform data annotation in medical domains. As a result, training deep neural networks for medical image segmentation can be challenging. We introduce a new source-free Unsupervised Domain Adaptation (UDA) method to address this problem. Our idea is based on estimating the internally learned distribution of a relevant source domain by a base model and then generating pseudo-labels that are used for enhancing the model refinement through self-training. We demonstrate that our approach leads to SOTA performance on a real-world 3D medical dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09229",
        "abstract url": "https://arxiv.org/abs/2410.09229",
        "title": "Quantitative Monoidal Algebra: Axiomatising Distance with String Diagrams",
        "rating": "-2",
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "(Symmetric) monoidal theories encapsulate presentations by generators and equations for (symmetric) monoidal categories. Terms of a monoidal theory are typically represented pictorially using string diagrams. In this work we introduce and study a quantitative version of monoidal theories, where instead of equality one may reason more abstractly about distance between string diagrams. This is in analogy with quantitative algebraic theories by Mardare et al., but developed in a monoidal rather than cartesian setting. Our framework paves the way for a quantitative analysis of string diagrammatic calculi for resource-sensitive processes, as found e.g. in quantum theory, machine learning, cryptography, and digital circuit theory.",
        "subjects": [
            "cs.LO",
            "math.CT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09249",
        "abstract url": "https://arxiv.org/abs/2410.09249",
        "title": "Failure Prediction from Limited Hardware Demonstrations",
        "rating": "-2",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "Prediction of failures in real-world robotic systems either requires accurate model information or extensive testing. Partial knowledge of the system model makes simulation-based failure prediction unreliable. Moreover, obtaining such demonstrations is expensive, and could potentially be risky for the robotic system to repeatedly fail during data collection. This work presents a novel three-step methodology for discovering failures that occur in the true system by using a combination of a limited number of demonstrations from the true system and the failure information processed through sampling-based testing of a model dynamical system. Given a limited budget $N$ of demonstrations from true system and a model dynamics (with potentially large modeling errors), the proposed methodology comprises of a) exhaustive simulations for discovering algorithmic failures using the model dynamics; b) design of initial $N_1$ demonstrations of the true system using Bayesian inference to learn a Gaussian process regression (GPR)-based failure predictor; and c) iterative $N - N_1$ demonstrations of the true system for updating the failure predictor. To illustrate the efficacy of the proposed methodology, we consider: a) the failure discovery for the task of pushing a T block to a fixed target region with UR3E collaborative robot arm using a diffusion policy; and b) the failure discovery for an F1-Tenth racing car tracking a given raceline under an LQR control policy.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 7 figures"
    },
    {
        "paper id": "2410.09279",
        "abstract url": "https://arxiv.org/abs/2410.09279",
        "title": "Optimal Interval Observers for Bounded Jacobian Nonlinear Dynamical Systems",
        "rating": "-2",
        "keywords": [
            [
                "CT"
            ]
        ],
        "abstract": "In this chapter, we introduce two interval observer designs for discrete-time (DT) and continuous-time (CT) nonlinear systems with bounded Jacobians that are affected by bounded uncertainties. Our proposed methods utilize the concepts of mixed-monotone decomposition and embedding systems to design correct-by-construction interval framers, i.e., the interval framers inherently bound the true state of the system without needing any additional constraints. Further, our methods leverage techniques for positive/cooperative systems to guarantee global uniform ultimate boundedness of the framer error, i.e., the proposed interval observer is input-to-state stable. Specifically, our two interval observer designs minimize the $\\mathcal{H}_{\\infty}$ and $L_1$ gains, respectively, of the associated linear comparison system of the framer error dynamics. Moreover, our designs adopt a multiple-gain observer structure, which offers additional degrees of freedom, along with coordinate transformations that may improve the feasibility of the resulting optimization programs. We will also discuss and propose computationally tractable optimization formulations to compute the observer gains. Finally, we compare the efficacy of the proposed designs against existing DT and CT interval observers.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Submitted to Springer as a book chapter"
    },
    {
        "paper id": "2410.09292",
        "abstract url": "https://arxiv.org/abs/2410.09292",
        "title": "SurgicalGS: Dynamic 3D Gaussian Splatting for Accurate Robotic-Assisted Surgical Scene Reconstruction",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "point cloud",
                "depth"
            ],
            [
                "Surgical",
                "surgery",
                "endoscopic"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Accurate 3D reconstruction of dynamic surgical scenes from endoscopic video is essential for robotic-assisted surgery. While recent 3D Gaussian Splatting methods have shown promise in achieving high-quality reconstructions with fast rendering speeds, their use of inverse depth loss functions compresses depth variations. This can lead to a loss of fine geometric details, limiting their ability to capture precise 3D geometry and effectiveness in intraoperative application. To address these challenges, we present SurgicalGS, a dynamic 3D Gaussian Splatting framework specifically designed for surgical scene reconstruction with improved geometric accuracy. Our approach first initialises a Gaussian point cloud using depth priors, employing binary motion masks to identify pixels with significant depth variations and fusing point clouds from depth maps across frames for initialisation. We use the Flexible Deformation Model to represent dynamic scene and introduce a normalised depth regularisation loss along with an unsupervised depth smoothness constraint to ensure more accurate geometric reconstruction. Extensive experiments on two real surgical datasets demonstrate that SurgicalGS achieves state-of-the-art reconstruction quality, especially in terms of accurate geometry, advancing the usability of 3D Gaussian Splatting in robotic-assisted surgery.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "7 pages"
    },
    {
        "paper id": "2410.09312",
        "abstract url": "https://arxiv.org/abs/2410.09312",
        "title": "Towards Multi-Modal Animal Pose Estimation: An In-Depth Analysis",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "biomechanics"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Animal pose estimation (APE) aims to locate the animal body parts using a diverse array of sensor and modality inputs, which is crucial for research across neuroscience, biomechanics, and veterinary medicine. By evaluating 178 papers since 2013, APE methods are categorised by sensor and modality types, learning paradigms, experimental setup, and application domains, presenting detailed analyses of current trends, challenges, and future directions in single- and multi-modality APE systems. The analysis also highlights the transition between human and animal pose estimation. Additionally, 2D and 3D APE datasets and evaluation metrics based on different sensors and modalities are provided. A regularly updated project page is provided here: https://github.com/ChennyDeng/MM-APE.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "35 pages, 5 figures, 8 tables"
    },
    {
        "paper id": "2410.09317",
        "abstract url": "https://arxiv.org/abs/2410.09317",
        "title": "Cross-Detection and Dual-Side Monitoring Schemes for FPGA-Based High-Accuracy and High-Precision Time-to-Digital Converters",
        "rating": "-2",
        "keywords": [
            [
                "FPGA"
            ]
        ],
        "abstract": "This study presents a novel field-programmable gate array (FPGA)-based Time-to-Digital Converter (TDC) design suitable for high timing resolution applications, utilizing two new techniques. First, a cross-detection (CD) method is introduced that minimizes the occurrence of bubbles, which cause inaccuracy in the timing measurement of a TDC in thermometer codes, by altering the conventional sampling pattern, thereby yielding an average bin size half of its typical size. The second technique employs dual-side monitoring (DSM) of thermometer codes, including end-of-propagation (EOP) and start-of-propagation (SOP). Distinct from conventional TDCs, which focus solely on SOP thermometer codes, this technique utilizes EOP to calibrate SOP, simultaneously enhancing time resolution and the TDC's stability against changes in temperature and location. The proposed DSM scheme necessitates only an additional CARRY4 for capturing the EOP thermometer code, rendering it a resource-efficient solution. The CD-DSM TDC has been successfully implemented on a Virtex-7 Xilinx FPGA (a 28-nm process), with an average bin size of 6.1 ps and a root mean square of 3.8 ps. Compared to conventional TDCs, the CD-DSM TDC offers superior linearity. The successful measurement of ultra-high coincidence timing resolution (CTR) from two Cerenkov radiator integrated microchannel plate photomultiplier tubes (CRI-MCP-PMTs) was conducted with the CD-DSM TDCs for sub-100 ps timing measurements. A comparison with current-edge TDCs further highlights the superior performance of the CD-DSM TDCs.",
        "subjects": [
            "physics.ins-det",
            "eess.SP",
            "physics.med-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09326",
        "abstract url": "https://arxiv.org/abs/2410.09326",
        "title": "QOPS: A Compiler Framework for Quantum Circuit Simulation Acceleration with Profile Guided Optimizations",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum circuit simulation is important in the evolution of quantum software and hardware. Novel algorithms can be developed and evaluated by performing quantum circuit simulations on classical computers before physical quantum computers are available. Unfortunately, compared with a physical quantum computer, a prolonged simulation time hampers the rapid development of quantum algorithms. Inspired by the feedback-directed optimization scheme used by classical compilers to improve the generated code, this work proposes a quantum compiler framework QOPS to enable profile-guided optimization (PGO) for quantum circuit simulation acceleration. The QOPS compiler instruments a quantum simulator to collect performance data during the circuit simulation and it then generates the optimized version of the quantum circuit based on the collected data. Experimental results show the PGO can effectively shorten the simulation time on our tested benchmark programs. Especially, the simulator-specific PGO (virtual swap) can be applied to the benchmarks to accelerate the simulation speed by a factor of 1.19. As for the hardware-independent PGO, compared with the brute force mechanism (turning on all available compilation flags), which achieves 21% performance improvement against the non-optimized version, the PGO can achieve 16% speedup with a factor of 63 less compilation time than the brute force approach.",
        "subjects": [
            "quant-ph",
            "cs.PF",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.11887",
        "abstract url": "https://arxiv.org/abs/2410.11887",
        "title": "Thermal Comfort in Sight: Thermal Affordance and its Visual Assessment for Sustainable Streetscape Design",
        "rating": "-2",
        "keywords": [
            [
                "Thermal"
            ]
        ],
        "abstract": "In response to climate change and urban heat island effects, enhancing human thermal comfort in cities is crucial for sustainable urban development. Traditional methods for investigating the urban thermal environment and corresponding human thermal comfort level are often resource intensive, inefficient, and limited in scope. To address these challenges, we (1) introduce the concept of thermal affordance, which represents the inherent capacity of a streetscape to influence human thermal comfort based on its visual and physical features; and (2) a method to evaluate it (visual assessment of thermal affordance -- VATA), which combines street view imagery (SVI), online and in-filed surveys, and statistical learning algorithms. VATA extracts five categories of image features from SVI data and establishes 19 visual-perceptual indicators for streetscape visual assessment. Using a multi-task neural network and elastic net regression, we model their chained relationship to predict and comprehend thermal affordance for Singapore. VATA predictions are validated with field-investigated OTC data, providing a cost-effective and scalable method to assess the thermal comfort potential of urban streetscape. This framework can inform streetscape design to support sustainable, livable, and resilient urban environments.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08576",
        "abstract url": "https://arxiv.org/abs/2410.08576",
        "title": "A Theoretical Framework for AI-driven data quality monitoring in high-volume data environments",
        "rating": "-2.5",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "quality assessment"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper presents a theoretical framework for an AI-driven data quality monitoring system designed to address the challenges of maintaining data quality in high-volume environments. We examine the limitations of traditional methods in managing the scale, velocity, and variety of big data and propose a conceptual approach leveraging advanced machine learning techniques. Our framework outlines a system architecture that incorporates anomaly detection, classification, and predictive analytics for real-time, scalable data quality management. Key components include an intelligent data ingestion layer, adaptive preprocessing mechanisms, context-aware feature extraction, and AI-based quality assessment modules. A continuous learning paradigm is central to our framework, ensuring adaptability to evolving data patterns and quality requirements. We also address implications for scalability, privacy, and integration within existing data ecosystems. While practical results are not provided, it lays a robust theoretical foundation for future research and implementations, advancing data quality management and encouraging the exploration of AI-driven solutions in dynamic environments.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08651",
        "abstract url": "https://arxiv.org/abs/2410.08651",
        "title": "Edge AI Collaborative Learning: Bayesian Approaches to Uncertainty Estimation",
        "rating": "-2.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "IoT"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent advancements in edge computing have significantly enhanced the AI capabilities of Internet of Things (IoT) devices. However, these advancements introduce new challenges in knowledge exchange and resource management, particularly addressing the spatiotemporal data locality in edge computing environments. This study examines algorithms and methods for deploying distributed machine learning within autonomous, network-capable, AI-enabled edge devices. We focus on determining confidence levels in learning outcomes considering the spatial variability of data encountered by independent agents. Using collaborative mapping as a case study, we explore the application of the Distributed Neural Network Optimization (DiNNO) algorithm extended with Bayesian neural networks (BNNs) for uncertainty estimation. We implement a 3D environment simulation using the Webots platform to simulate collaborative mapping tasks, decouple the DiNNO algorithm into independent processes for asynchronous network communication in distributed learning, and integrate distributed uncertainty estimation using BNNs. Our experiments demonstrate that BNNs can effectively support uncertainty estimation in a distributed learning context, with precise tuning of learning hyperparameters crucial for effective uncertainty assessment. Notably, applying Kullback-Leibler divergence for parameter regularization resulted in a 12-30% reduction in validation loss during distributed BNN training compared to other regularization strategies.",
        "subjects": [
            "cs.LG",
            "cs.DC",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08656",
        "abstract url": "https://arxiv.org/abs/2410.08656",
        "title": "radarODE-MTL: A Multi-Task Learning Framework with Eccentric Gradient Alignment for Robust Radar-Based ECG Reconstruction",
        "rating": "-2.5",
        "keywords": [
            [
                "Radar"
            ],
            [
                "cardiac"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Millimeter-wave radar is promising to provide robust and accurate vital sign monitoring in an unobtrusive manner. However, the radar signal might be distorted in propagation by ambient noise or random body movement, ruining the subtle cardiac activities and destroying the vital sign recovery. In particular, the recovery of electrocardiogram (ECG) signal heavily relies on the deep-learning model and is sensitive to noise. Therefore, this work creatively deconstructs the radar-based ECG recovery into three individual tasks and proposes a multi-task learning (MTL) framework, radarODE-MTL, to increase the robustness against consistent and abrupt noises. In addition, to alleviate the potential conflicts in optimizing individual tasks, a novel multi-task optimization strategy, eccentric gradient alignment (EGA), is proposed to dynamically trim the task-specific gradients based on task difficulties in orthogonal space. The proposed radarODE-MTL with EGA is evaluated on the public dataset with prominent improvements in accuracy, and the performance remains consistent under noises. The experimental results indicate that radarODE-MTL could reconstruct accurate ECG signals robustly from radar signals and imply the application prospect in real-life situations. The code is available at: http://github.com/ZYY0844/radarODE-MTL.",
        "subjects": [
            "eess.SP",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08727",
        "abstract url": "https://arxiv.org/abs/2410.08727",
        "title": "Losing dimensions: Geometric memorization in generative diffusion",
        "rating": "-2.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Generative diffusion processes are state-of-the-art machine learning models deeply connected with fundamental concepts in statistical physics. Depending on the dataset size and the capacity of the network, their behavior is known to transition from an associative memory regime to a generalization phase in a phenomenon that has been described as a glassy phase transition. Here, using statistical physics techniques, we extend the theory of memorization in generative diffusion to manifold-supported data. Our theoretical and experimental findings indicate that different tangent subspaces are lost due to memorization effects at different critical times and dataset sizes, which depend on the local variance of the data along their directions. Perhaps counterintuitively, we find that, under some conditions, subspaces of higher variance are lost first due to memorization effects. This leads to a selective loss of dimensionality where some prominent features of the data are memorized without a full collapse on any individual training point. We validate our theory with a comprehensive set of experiments on networks trained both in image datasets and on linear manifolds, which result in a remarkable qualitative agreement with the theoretical predictions.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09124",
        "abstract url": "https://arxiv.org/abs/2410.09124",
        "title": "SoK: Verifiable Cross-Silo FL",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL) is a widespread approach that allows training machine learning (ML) models with data distributed across multiple devices. In cross-silo FL, which often appears in domains like healthcare or finance, the number of participants is moderate, and each party typically represents a well-known organization. For instance, in medicine data owners are often hospitals or data hubs which are well-established entities. However, malicious parties may still attempt to disturb the training procedure in order to obtain certain benefits, for example, a biased result or a reduction in computational load. While one can easily detect a malicious agent when data used for training is public, the problem becomes much more acute when it is necessary to maintain the privacy of the training dataset. To address this issue, there is recently growing interest in developing verifiable protocols, where one can check that parties do not deviate from the training procedure and perform computations correctly. In this paper, we present a systematization of knowledge on verifiable cross-silo FL. We analyze various protocols, fit them in a taxonomy, and compare their efficiency and threat models. We also analyze Zero-Knowledge Proof (ZKP) schemes and discuss how their overall cost in a FL context can be minimized. Lastly, we identify research gaps and discuss potential directions for future scientific work.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09126",
        "abstract url": "https://arxiv.org/abs/2410.09126",
        "title": "Convolutional Neural Network Design and Evaluation for Real-Time Multivariate Time Series Fault Detection in Spacecraft Attitude Sensors",
        "rating": "-2.5",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "drone"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Traditional anomaly detection techniques onboard satellites are based on reliable, yet limited, thresholding mechanisms which are designed to monitor univariate signals and trigger recovery actions according to specific European Cooperation for Space Standardization (ECSS) standards. However, Artificial Intelligence-based Fault Detection, Isolation and Recovery (FDIR) solutions have recently raised with the prospect to overcome the limitations of these standard methods, expanding the range of detectable failures and improving response times. This paper presents a novel approach to detecting stuck values within the Accelerometer and Inertial Measurement Unit of a drone-like spacecraft for the exploration of Small Solar System Bodies (SSSB), leveraging a multi-channel Convolutional Neural Network (CNN) to perform multi-target classification and independently detect faults in the sensors. Significant attention has been dedicated to ensuring the compatibility of the algorithm within the onboard FDIR system, representing a step forward to the in-orbit validation of a technology that remains experimental until its robustness is thoroughly proven. An integration methodology is proposed to enable the network to effectively detect anomalies and trigger recovery actions at the system level. The detection performances and the capability of the algorithm in reaction triggering are evaluated employing a set of custom-defined detection and system metrics, showing the outstanding performances of the algorithm in performing its FDIR task.",
        "subjects": [
            "eess.SY",
            "cs.AI"
        ],
        "comment": "submitted to Advances in Space Research"
    },
    {
        "paper id": "2410.09128",
        "abstract url": "https://arxiv.org/abs/2410.09128",
        "title": "TIGER: Temporally Improved Graph Entity Linker",
        "rating": "-2.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "recommendation"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Knowledge graphs change over time, for example, when new entities are introduced or entity descriptions change. This impacts the performance of entity linking, a key task in many uses of knowledge graphs such as web search and recommendation. Specifically, entity linking models exhibit temporal degradation - their performance decreases the further a knowledge graph moves from its original state on which an entity linking model was trained. To tackle this challenge, we introduce \\textbf{TIGER}: a \\textbf{T}emporally \\textbf{I}mproved \\textbf{G}raph \\textbf{E}ntity Linke\\textbf{r}. By incorporating structural information between entities into the model, we enhance the learned representation, making entities more distinguishable over time. The core idea is to integrate graph-based information into text-based information, from which both distinct and shared embeddings are based on an entity's feature and structural relationships and their interaction. Experiments on three datasets show that our model can effectively prevent temporal degradation, demonstrating a 16.24\\% performance boost over the state-of-the-art in a temporal setting when the time gap is one year and an improvement to 20.93\\% as the gap expands to three years. The code and data are made available at \\url{https://github.com/pengyu-zhang/TIGER-Temporally-Improved-Graph-Entity-Linker}.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09163",
        "abstract url": "https://arxiv.org/abs/2410.09163",
        "title": "Learning to Walk from Three Minutes of Real-World Data with Semi-structured Dynamics Models",
        "rating": "-2.5",
        "keywords": [
            [
                "robot"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Traditionally, model-based reinforcement learning (MBRL) methods exploit neural networks as flexible function approximators to represent a priori unknown environment dynamics. However, training data are typically scarce in practice, and these black-box models often fail to generalize. Modeling architectures that leverage known physics can substantially reduce the complexity of system-identification, but break down in the face of complex phenomena such as contact. We introduce a novel framework for learning semi-structured dynamics models for contact-rich systems which seamlessly integrates structured first principles modeling techniques with black-box auto-regressive models. Specifically, we develop an ensemble of probabilistic models to estimate external forces, conditioned on historical observations and actions, and integrate these predictions using known Lagrangian dynamics. With this semi-structured approach, we can make accurate long-horizon predictions with substantially less data than prior methods. We leverage this capability and propose Semi-Structured Reinforcement Learning (SSRL) a simple model-based learning framework which pushes the sample complexity boundary for real-world learning. We validate our approach on a real-world Unitree Go1 quadruped robot, learning dynamic gaits -- from scratch -- on both hard and soft surfaces with just a few minutes of real-world data. Video and code are available at: https://sites.google.com/utexas.edu/ssrl",
        "subjects": [
            "cs.RO",
            "cs.LG",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09206",
        "abstract url": "https://arxiv.org/abs/2410.09206",
        "title": "pyhgf: A neural network library for predictive coding",
        "rating": "-2.5",
        "keywords": [
            [
                "graphs"
            ],
            [
                "Biological"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Bayesian models of cognition have gained considerable traction in computational neuroscience and psychiatry. Their scopes are now expected to expand rapidly to artificial intelligence, providing general inference frameworks to support embodied, adaptable, and energy-efficient autonomous agents. A central theory in this domain is predictive coding, which posits that learning and behaviour are driven by hierarchical probabilistic inferences about the causes of sensory inputs. Biological realism constrains these networks to rely on simple local computations in the form of precision-weighted predictions and prediction errors. This can make this framework highly efficient, but its implementation comes with unique challenges on the software development side. Embedding such models in standard neural network libraries often becomes limiting, as these libraries' compilation and differentiation backends can force a conceptual separation between optimization algorithms and the systems being optimized. This critically departs from other biological principles such as self-monitoring, self-organisation, cellular growth and functional plasticity. In this paper, we introduce \\texttt{pyhgf}: a Python package backed by JAX and Rust for creating, manipulating and sampling dynamic networks for predictive coding. We improve over other frameworks by enclosing the network components as transparent, modular and malleable variables in the message-passing steps. The resulting graphs can implement arbitrary computational complexities as beliefs propagation. But the transparency of core variables can also translate into inference processes that leverage self-organisation principles, and express structure learning, meta-learning or causal discovery as the consequence of network structural adaptation to surprising inputs. The code, tutorials and documentation are hosted at: https://github.com/ilabcode/pyhgf.",
        "subjects": [
            "cs.NE",
            "cs.AI",
            "cs.LG",
            "q-bio.NC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09290",
        "abstract url": "https://arxiv.org/abs/2410.09290",
        "title": "Ranking over Regression for Bayesian Optimization and Molecule Selection",
        "rating": "-2.5",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "chemical"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Bayesian optimization (BO) has become an indispensable tool for autonomous decision-making across diverse applications from autonomous vehicle control to accelerated drug and materials discovery. With the growing interest in self-driving laboratories, BO of chemical systems is crucial for machine learning (ML) guided experimental planning. Typically, BO employs a regression surrogate model to predict the distribution of unseen parts of the search space. However, for the selection of molecules, picking the top candidates with respect to a distribution, the relative ordering of their properties may be more important than their exact values. In this paper, we introduce Rank-based Bayesian Optimization (RBO), which utilizes a ranking model as the surrogate. We present a comprehensive investigation of RBO's optimization performance compared to conventional BO on various chemical datasets. Our results demonstrate similar or improved optimization performance using ranking models, particularly for datasets with rough structure-property landscapes and activity cliffs. Furthermore, we observe a high correlation between the surrogate ranking ability and BO performance, and this ability is maintained even at early iterations of BO optimization when using ranking surrogate models. We conclude that RBO is an effective alternative to regression-based BO, especially for optimizing novel chemical compounds.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "14 + 4 pages, 5 + 3 figures"
    },
    {
        "paper id": "2410.08560",
        "abstract url": "https://arxiv.org/abs/2410.08560",
        "title": "Enhanced Robot Planning and Perception through Environment Prediction",
        "rating": "-3",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "Robot",
                "navigation"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Mobile robots rely on maps to navigate through an environment. In the absence of any map, the robots must build the map online from partial observations as they move in the environment. Traditional methods build a map using only direct observations. In contrast, humans identify patterns in the observed environment and make informed guesses about what to expect ahead. Modeling these patterns explicitly is difficult due to the complexity of the environments. However, these complex models can be approximated well using learning-based methods in conjunction with large training data. By extracting patterns, robots can use direct observations and predictions of what lies ahead to better navigate an unknown environment. In this dissertation, we present several learning-based methods to equip mobile robots with prediction capabilities for efficient and safer operation. In the first part of the dissertation, we learn to predict using geometrical and structural patterns in the environment. Partially observed maps provide invaluable cues for accurately predicting the unobserved areas. We first demonstrate the capability of general learning-based approaches to model these patterns for a variety of overhead map modalities. Then we employ task-specific learning for faster navigation in indoor environments by predicting 2D occupancy in the nearby regions. This idea is further extended to 3D point cloud representation for object reconstruction. Predicting the shape of the full object from only partial views, our approach paves the way for efficient next-best-view planning. In the second part of the dissertation, we learn to predict using spatiotemporal patterns in the environment. We focus on dynamic tasks such as target tracking and coverage where we seek decentralized coordination between robots. We first show how graph neural networks can be used for more scalable and faster inference.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "289 pages, 81 figures, 16 tables; Dissertation submitted to UMD to fulfill PhD requirement"
    },
    {
        "paper id": "2410.08600",
        "abstract url": "https://arxiv.org/abs/2410.08600",
        "title": "Energy-Cautious Designation of Kinematic Parameters for a Sustainable Parallel-Serial Heavy-Duty Manipulator Driven by Electromechanical Linear Actuator",
        "rating": "-3",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "industrial"
            ]
        ],
        "abstract": "Electrification, a key strategy in combating climate change, is transforming industries, and off-highway machines (OHM) will be next to transition from combustion engines and hydraulic actuation to sustainable fully electrified machines. Electromechanical linear actuators (EMLAs) offer superior efficiency, safety, and reduced maintenance, and they unlock vast potential for high-performance autonomous operations. However, a key challenge lies in optimizing the kinematic parameters of OHMs' on-board manipulators for EMLA integration to exploit the full capabilities of actuation systems and maximize their performance. This work addresses this challenge by delving into the structural optimization of a prevalent closed kinematic chain configuration commonly employed in OHM manipulators. Our approach aims to retain the manipulator's existing capabilities while reducing its energy expenditure, paving the way for a greener future in industrial automation, one in which sustainable and high-performing robotized OHMs can evolve. The feasibility of our methodology is validated through simulation results obtained on a commercially available parallel-serial heavy-duty manipulator mounted on a battery electric vehicle. The results demonstrate the efficacy of our approach in modifying kinematic parameters to facilitate the replacement of conventional hydraulic actuators with EMLAs, all while minimizing the overall energy consumption of the system.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": "This work is accepted for presentation at IEEE VTC 2024-Washington USA"
    },
    {
        "paper id": "2410.08602",
        "abstract url": "https://arxiv.org/abs/2410.08602",
        "title": "Achieving multi uav best viewpoint coordination in obstructed environments",
        "rating": "-3",
        "keywords": [
            [
                "robot"
            ],
            [
                "uav"
            ]
        ],
        "abstract": "Wildfire suppression is a complex task that poses high risks to humans. Using robotic teams for wildfire suppression enhances the safety and efficiency of detecting, monitoring, and extinguishing fires. We propose a control architecture based on task hierarchical control for the autonomous steering of a system of flying robots in wildfire suppression. We incorporate a novel line-of-sight obstacle avoidance method that calculates the best viewpoints and ensures an occlusion-free view for the suppression robot during the mission. Path integral control generates optimal trajectories towards the goals. We conduct an ablation study to assess the effectiveness of our approach by comparing it to scenarios where these key components are excluded, in order to validate the approach in simulations using Matlab and Unity. The results demonstrate significant performance improvements, with 44.0 % increase in effectiveness with the new line-of-sight obstacle avoidance task and up to 39.6 % improvement when using path integral control.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "6 pages, 5 figures, submitted to joint ACC and L-CSS"
    },
    {
        "paper id": "2410.08650",
        "abstract url": "https://arxiv.org/abs/2410.08650",
        "title": "Extended Friction Models for the Physics Simulation of Servo Actuators",
        "rating": "-3",
        "keywords": [
            [
                "robot"
            ],
            [
                "Physics"
            ]
        ],
        "abstract": "Accurate physical simulation is crucial for the development and validation of control algorithms in robotic systems. Recent works in Reinforcement Learning (RL) take notably advantage of extensive simulations to produce efficient robot control. State-of-the-art servo actuator models generally fail at capturing the complex friction dynamics of these systems. This limits the transferability of simulated behaviors to real-world applications. In this work, we present extended friction models that allow to more accurately simulate servo actuator dynamics. We propose a comprehensive analysis of various friction models, present a method for identifying model parameters using recorded trajectories from a pendulum test bench, and demonstrate how these models can be integrated into physics engines. The proposed friction models are validated on four distinct servo actuators and tested on 2R manipulators, showing significant improvements in accuracy over the standard Coulomb-Viscous model. Our results highlight the importance of considering advanced friction effects in the simulation of servo actuators to enhance the realism and reliability of robotic simulations.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08729",
        "abstract url": "https://arxiv.org/abs/2410.08729",
        "title": "Smart PRACH Jamming: A Serious Threat for 5G Campus Networks",
        "rating": "-3",
        "keywords": [
            [
                "attacks"
            ],
            [
                "5G",
                "industrial"
            ]
        ],
        "abstract": "Smart jamming attacks on cellular campus networks represent an enormous potential threat, especially in the industrial environment. In complex production processes, the disruption of a single wireless connected Cyber-Physical System (CPS) is enough to cause a large-scale failure. In this paper, a smart jamming attack on the Physical Random Access Channel (PRACH) of a 5G system is modeled. This is followed by a practical implementation of the jammer on a testbed based on Open Air Interface (OAI) and Software Defined Radios (SDRs). It is shown that the designed jammer design can interfere a legitimate transmission of a PRACH preamble with a ratio of more than 99.9%. While less than one percent of the cell resources are interfered compared to broadband jamming. In addition, two different types of jamming signal spectra are compared in relation to their interference capacity. The developed attack can be re-implemented based on publicly available source code and Commercial Off-The-Shelf (COTS) hardware.",
        "subjects": [
            "cs.NI",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08807",
        "abstract url": "https://arxiv.org/abs/2410.08807",
        "title": "Robust Variable-Horizon MPC with Adaptive Terminal Constraints",
        "rating": "-3",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "satellite"
            ]
        ],
        "abstract": "This paper presents a novel robust variable-horizon model predictive control scheme designed to intercept a target moving along a known trajectory, in finite time. Linear discrete-time systems affected by bounded process disturbances are considered and a tube-based MPC approach is adopted. The main contribution is an adaptive mechanism for choosing the terminal constraint set sequence in the MPC optimization problem. This mechanism is designed to ensure recursive feasibility while promoting minimization of the final distance to the target. Finite-time convergence of the proposed control scheme is proven. In order to evaluate its effectiveness, the designed control law is tested through numerical simulations, including a case study involving orbital rendezvous of a satellite with a tumbling object. The results indicate a significant reduction in conservatism compared to existing state-of-the-art methods using a fixed terminal set sequence.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08856",
        "abstract url": "https://arxiv.org/abs/2410.08856",
        "title": "FlowMRI-Net: A generalizable self-supervised physics-driven 4D Flow MRI reconstruction network for aortic and cerebrovascular applications",
        "rating": "-3",
        "keywords": [
            [
                "diagnosis",
                "MRI",
                "clinical"
            ],
            [
                "physics"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "In this work, we propose FlowMRI-Net, a novel deep learning-based framework for fast reconstruction of accelerated 4D flow magnetic resonance imaging (MRI) using physics-driven unrolled optimization and a complexvalued convolutional recurrent neural network trained in a self-supervised manner. The generalizability of the framework is evaluated using aortic and cerebrovascular 4D flow MRI acquisitions acquired on systems from two different vendors for various undersampling factors (R=8,16,24) and compared to state-of-the-art compressed sensing (CS-LLR) and deep learning-based (FlowVN) reconstructions. Evaluation includes quantitative analysis of image magnitudes, velocity magnitudes, and peak velocity curves. FlowMRINet outperforms CS-LLR and FlowVN for aortic 4D flow MRI reconstruction, resulting in vectorial normalized root mean square errors of $0.239\\pm0.055$, $0.308\\pm0.066$, and $0.302\\pm0.085$ and mean directional errors of $0.023\\pm0.015$, $0.036\\pm0.018$, and $0.039\\pm0.025$ for velocities in the thoracic aorta for R=16, respectively. Furthermore, FlowMRI-Net outperforms CS-LLR for cerebrovascular 4D flow MRI reconstruction, where no FlowVN can be trained due to the lack of a highquality reference, resulting in a consistent increase in SNR of around 6 dB and more accurate peak velocity curves for R=8,16,24. Reconstruction times ranged from 1 to 7 minutes on commodity CPU/GPU hardware. FlowMRI-Net enables fast and accurate quantification of aortic and cerebrovascular flow dynamics, with possible applications to other vascular territories. This will improve clinical adaptation of 4D flow MRI and hence may aid in the diagnosis and therapeutic management of cardiovascular diseases.",
        "subjects": [
            "physics.med-ph",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08894",
        "abstract url": "https://arxiv.org/abs/2410.08894",
        "title": "Conditional Generative Models for Contrast-Enhanced Synthesis of T1w and T1 Maps in Brain MRI",
        "rating": "-3",
        "keywords": [
            [
                "voxel"
            ],
            [
                "diffusion"
            ],
            [
                "diagnosis",
                "MRI",
                "tumor"
            ],
            [
                "cs.AI",
                "eess.IV"
            ]
        ],
        "abstract": "Contrast enhancement by Gadolinium-based contrast agents (GBCAs) is a vital tool for tumor diagnosis in neuroradiology. Based on brain MRI scans of glioblastoma before and after Gadolinium administration, we address enhancement prediction by neural networks with two new contributions. Firstly, we study the potential of generative models, more precisely conditional diffusion and flow matching, for uncertainty quantification in virtual enhancement. Secondly, we examine the performance of T1 scans from quantitive MRI versus T1-weighted scans. In contrast to T1-weighted scans, these scans have the advantage of a physically meaningful and thereby comparable voxel range. To compare network prediction performance of these two modalities with incompatible gray-value scales, we propose to evaluate segmentations of contrast-enhanced regions of interest using Dice and Jaccard scores. Across models, we observe better segmentations with T1 scans than with T1-weighted scans.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "q-bio.NC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08909",
        "abstract url": "https://arxiv.org/abs/2410.08909",
        "title": "Implicit Graph Search for Planning on Graphs of Convex Sets",
        "rating": "-3",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "robotics"
            ],
            [
                "Graph"
            ]
        ],
        "abstract": "Graphs of Convex Sets (GCS) is a recent method for synthesizing smooth trajectories by decomposing the planning space into convex sets, forming a graph to encode the adjacency relationships within the decomposition, and then simultaneously searching this graph and optimizing parts of the trajectory to obtain the final trajectory. To do this, one must solve a Mixed Integer Convex Program (MICP) and to mitigate computational time, GCS proposes a convex relaxation that is empirically very tight. Despite this tight relaxation, motion planning with GCS for real-world robotics problems translates to solving the simultaneous batch optimization problem that may contain millions of constraints and therefore can be slow. This is further exacerbated by the fact that the size of the GCS problem is invariant to the planning query. Motivated by the observation that the trajectory solution lies only on a fraction of the set of convex sets, we present two implicit graph search methods for planning on the graph of convex sets called INSATxGCS (IxG) and IxG*. INterleaved Search And Trajectory optimization (INSAT) is a previously developed algorithm that alternates between searching on a graph and optimizing partial paths to find a smooth trajectory. By using an implicit graph search method INSAT on the graph of convex sets, we achieve faster planning while ensuring stronger guarantees on completeness and optimality. Moveover, introducing a search-based technique to plan on the graph of convex sets enables us to easily leverage well-established techniques such as search parallelization, lazy planning, anytime planning, and replanning as future work. Numerical comparisons against GCS demonstrate the superiority of IxG across several applications, including planning for an 18-degree-of-freedom multi-arm assembly scenario.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08983",
        "abstract url": "https://arxiv.org/abs/2410.08983",
        "title": "DEL: Discrete Element Learner for Learning 3D Particle Dynamics with Neural Rendering",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "graph"
            ],
            [
                "physics"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Learning-based simulators show great potential for simulating particle dynamics when 3D groundtruth is available, but per-particle correspondences are not always accessible. The development of neural rendering presents a new solution to this field to learn 3D dynamics from 2D images by inverse rendering. However, existing approaches still suffer from ill-posed natures resulting from the 2D to 3D uncertainty, for example, specific 2D images can correspond with various 3D particle distributions. To mitigate such uncertainty, we consider a conventional, mechanically interpretable framework as the physical priors and extend it to a learning-based version. In brief, we incorporate the learnable graph kernels into the classic Discrete Element Analysis (DEA) framework to implement a novel mechanics-integrated learning system. In this case, the graph network kernels are only used for approximating some specific mechanical operators in the DEA framework rather than the whole dynamics mapping. By integrating the strong physics priors, our methods can effectively learn the dynamics of various materials from the partial 2D observations in a unified manner. Experiments show that our approach outperforms other learned simulators by a large margin in this context and is robust to different renderers, fewer training samples, and fewer camera views.",
        "subjects": [
            "cs.CV",
            "cs.GR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09122",
        "abstract url": "https://arxiv.org/abs/2410.09122",
        "title": "Study on (r,s)- Generalised Transformation Graphs, A Novel Perspective Based on Transformation Graphs",
        "rating": "-3",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "chemical"
            ]
        ],
        "abstract": "For a graph $\\mathbb{Q}=(\\mathbb{V},\\mathbb{E})$, the transformation graphs are defined as graphs with vertex set being $\\mathbb{V(Q)} \\cup \\mathbb{E(Q)}$ and edge set is described following certain conditions. In comparison to the structure descriptor of the original graph $\\mathbb{Q}$, the topological descriptor of its transformation graphs displays distinct characteristics related to structure. Thus, a compound's transformation graphs descriptors can be used to model a variety of structural features of the underlying chemical. In this work, the concept of transformation graphs are extended giving rise to novel class of graphs, the $(r,s)$- generalised transformation graphs, whose vertex set is union of $r$ copies of $\\mathbb{V(Q)}$ and $s$ copies of $\\mathbb{E(Q)}$, where, $r, s \\in N$ and the edge set are defined under certain conditions. Further, these class of graphs are analysed with the help of first Zagreb index. Mainly, there are eight transformation graphs based on the criteria for edge set, but under the concept of $(r,s)$- generalised transformation graphs, infinite number of graphs can be described and analysed.",
        "subjects": [
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09188",
        "abstract url": "https://arxiv.org/abs/2410.09188",
        "title": "MFIT: Multi-Fidelity Thermal Modeling for 2.5D and 3D Multi-Chiplet Architectures",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "Thermal"
            ]
        ],
        "abstract": "Rapidly evolving artificial intelligence and machine learning applications require ever-increasing computational capabilities, while monolithic 2D design technologies approach their limits. Heterogeneous integration of smaller chiplets using a 2.5D silicon interposer and 3D packaging has emerged as a promising paradigm to address this limit and meet performance demands. These approaches offer a significant cost reduction and higher manufacturing yield than monolithic 2D integrated circuits. However, the compact arrangement and high compute density exacerbate the thermal management challenges, potentially compromising performance. Addressing these thermal modeling challenges is critical, especially as system sizes grow and different design stages require varying levels of accuracy and speed. Since no single thermal modeling technique meets all these needs, this paper introduces MFIT, a range of multi-fidelity thermal models that effectively balance accuracy and speed. These multi-fidelity models can enable efficient design space exploration and runtime thermal management. Our extensive testing on systems with 16, 36, and 64 2.5D integrated chiplets and 16x3 3D integrated chiplets demonstrates that these models can reduce execution times from days to mere seconds and milliseconds with negligible loss in accuracy.",
        "subjects": [
            "cs.AR"
        ],
        "comment": "Preprint for MFIT: Multi-Fidelity Thermal Modeling for 2.5D and 3D Multi-Chiplet Architectures"
    },
    {
        "paper id": "2410.09200",
        "abstract url": "https://arxiv.org/abs/2410.09200",
        "title": "Crowd Size Estimation for Non-Uniform Spatial Distributions with mmWave Radar",
        "rating": "-3",
        "keywords": [
            [
                "Radar"
            ],
            [
                "6G"
            ]
        ],
        "abstract": "Sensing with RF signals such as mmWave radar has gained considerable interest in recent years. This is particularly relevant to 6G networks, which aim to integrate sensing and communication (ISAC) capabilities for enhanced functionality. The contextual information provided by such sensing, whether collected by standalone non-ISAC units or integrated within ISAC, can not only optimize cellular network assets but can also serve as a valuable tool for a wide range of applications beyond network optimization. In this context, we present a novel methodology for crowd size estimation using monostatic mmWave radar, which is capable of accurately counting large crowds that are unevenly distributed across space. Our estimation approach relies on the rigorous derivation of occlusion probabilities, which are then used to mathematically characterize the probability distributions that describe the number of agents visible to the radar as a function of the crowd size. We then estimate the true crowd size by comparing these derived mathematical models to the empirical distribution of the number of visible agents detected by the radar. This method requires minimal sensing capabilities (e.g., angle-of-arrival information is not needed), thus being well suited for either a dedicated mmWave radar or an ISAC system. Extensive numerical simulations validate our methodology, demonstrating strong performance across diverse spatial distributions and for crowd sizes of up to (and including) 30 agents. We achieve a mean absolute error (MAE) of 0.48 agents, significantly outperforming a baseline which assumes that the agents are uniformly distributed in the area. Overall, our approach holds significant promise for a variety of applications including network resource allocation, crowd management, and urban planning.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09213",
        "abstract url": "https://arxiv.org/abs/2410.09213",
        "title": "iFANnpp: Nuclear Power Plant Digital Twin for Robots and Autonomous Intelligence",
        "rating": "-3",
        "keywords": [
            [
                "Robotics",
                "robot",
                "navigation"
            ],
            [
                "physics"
            ]
        ],
        "abstract": "Robotics has gained significant attention due to its autonomy and ability to automate in the nuclear industry. However, the increasing complexity of robots has led to a growing demand for advanced simulation and control methods to predict robot behavior and optimize plant performance. Most existing digital twins only address parts of systems and do not offer an overall design of nuclear power plants. Furthermore, they are often designed for specific algorithms or tasks, making them unsuitable for broader research applications or other potential projects. In response, we propose a comprehensive nuclear power plant designed to enhance real-time monitoring, operational efficiency, and predictive maintenance. We selected to model a full-scope nuclear power plant in Unreal Engine 5 to incorporate the complexities and various phenomena. The high-resolution simulation environment is integrated with a General Pressurized Water Reactor Simulator, a high-fidelity physics-driven software, to create a realistic flow of nuclear power plant and a real-time updating virtual environment. Furthermore, the virtual environment provides various features and a Python bridge for researchers to test custom algorithms and frameworks easily. The digital twin's performance is presented, and several research ideas - such as multi-robot task scheduling and robot navigation in the radiation area - using implemented features are presented.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "12 pages, 9 figures"
    },
    {
        "paper id": "2410.09240",
        "abstract url": "https://arxiv.org/abs/2410.09240",
        "title": "nach0-pc: Multi-task Language Model with Molecular Point Cloud Encoder",
        "rating": "-3",
        "keywords": [
            [
                "3D",
                "Point Cloud"
            ],
            [
                "diffusion"
            ],
            [
                "chemical"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements have integrated Language Models (LMs) into a drug discovery pipeline. However, existing models mostly work with SMILES and SELFIES chemical string representations, which lack spatial features vital for drug discovery. Additionally, attempts to translate chemical 3D structures into text format encounter issues such as excessive length and insufficient atom connectivity information. To address these issues, we introduce nach0-pc, a model combining domain-specific encoder and textual representation to handle spatial arrangement of atoms effectively. Our approach utilizes a molecular point cloud encoder for concise and order-invariant structure representation. We introduce a novel pre-training scheme for molecular point clouds to distillate the knowledge from spatial molecular structures datasets. After fine-tuning within both single-task and multi-task frameworks, nach0-pc demonstrates performance comparable with other diffusion models in terms of generated samples quality across several established spatial molecular generation tasks. Notably, our model is a multi-task approach, in contrast to diffusion models being limited to single tasks. Additionally, it is capable of processing point cloud-related data, which language models are not capable of handling due to memory limitations. These lead to our model having reduced training and inference time while maintaining on par performance.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.10889",
        "abstract url": "https://arxiv.org/abs/2410.10889",
        "title": "Analysing Osteoporosis Detection: A Comparative Study of CNN and FNN",
        "rating": "-3",
        "keywords": [
            [
                "medical",
                "health",
                "diagnosis",
                "X-ray"
            ],
            [
                "Mineral"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Osteoporosis causes progressive loss of bone density and strength, causing a more elevated risk of fracture than in normal healthy bones. It is estimated that some 1 in 3 women and 1 in 5 men over the age of 50 will experience osteoporotic fractures, which poses osteoporosis as an important public health problem worldwide. The basis of diagnosis is based on Bone Mineral Density (BMD) tests, with Dual-energy X-ray Absorptiometry (DEXA) being the most common. A T-score of -2.5 or lower defines osteoporosis. This paper focuses on the application of medical imaging analytics towards the detection of osteoporosis by conducting a comparative study of the efficiency of CNN and FNN in DEXA image analytics. Both models are very promising, although, at 95%, the FNN marginally outperformed the CNN at 93%. Hence, this research underlines the probable capability of deep learning techniques in improving the detection of osteoporosis and optimizing diagnostic tools in order to achieve better patient outcomes.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08827",
        "abstract url": "https://arxiv.org/abs/2410.08827",
        "title": "Do Unlearning Methods Remove Information from Language Model Weights?",
        "rating": "-3.5",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "attacks"
            ],
            [
                "bioweapons"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large Language Models' knowledge of how to perform cyber-security attacks, create bioweapons, and manipulate humans poses risks of misuse. Previous work has proposed methods to unlearn this knowledge. Historically, it has been unclear whether unlearning techniques are removing information from the model weights or just making it harder to access. To disentangle these two objectives, we propose an adversarial evaluation method to test for the removal of information from model weights: we give an attacker access to some facts that were supposed to be removed, and using those, the attacker tries to recover other facts from the same distribution that cannot be guessed from the accessible facts. We show that using fine-tuning on the accessible facts can recover 88% of the pre-unlearning accuracy when applied to current unlearning methods, revealing the limitations of these methods in removing information from the model weights.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08877",
        "abstract url": "https://arxiv.org/abs/2410.08877",
        "title": "Interdependency Matters: Graph Alignment for Multivariate Time Series Anomaly Detection",
        "rating": "-3.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "industrial"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Anomaly detection in multivariate time series (MTS) is crucial for various applications in data mining and industry. Current industrial methods typically approach anomaly detection as an unsupervised learning task, aiming to identify deviations by estimating the normal distribution in noisy, label-free datasets. These methods increasingly incorporate interdependencies between channels through graph structures to enhance accuracy. However, the role of interdependencies is more critical than previously understood, as shifts in interdependencies between MTS channels from normal to anomalous data are significant. This observation suggests that \\textit{anomalies could be detected by changes in these interdependency graph series}. To capitalize on this insight, we introduce MADGA (MTS Anomaly Detection via Graph Alignment), which redefines anomaly detection as a graph alignment (GA) problem that explicitly utilizes interdependencies for anomaly detection. MADGA dynamically transforms subsequences into graphs to capture the evolving interdependencies, and Graph alignment is performed between these graphs, optimizing an alignment plan that minimizes cost, effectively minimizing the distance for normal data and maximizing it for anomalous data. Uniquely, our GA approach involves explicit alignment of both nodes and edges, employing Wasserstein distance for nodes and Gromov-Wasserstein distance for edges. To our knowledge, this is the first application of GA to MTS anomaly detection that explicitly leverages interdependency for this purpose. Extensive experiments on diverse real-world datasets validate the effectiveness of MADGA, demonstrating its capability to detect anomalies and differentiate interdependencies, consistently achieving state-of-the-art across various scenarios.",
        "subjects": [
            "cs.LG",
            "cs.DB",
            "cs.IR",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08938",
        "abstract url": "https://arxiv.org/abs/2410.08938",
        "title": "KinDEL: DNA-Encoded Library Dataset for Kinase Inhibitors",
        "rating": "-3.5",
        "keywords": [
            [
                "biophysical",
                "DNA"
            ],
            [
                "chemical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "DNA-Encoded Libraries (DEL) are combinatorial small molecule libraries that offer an efficient way to characterize diverse chemical spaces. Selection experiments using DELs are pivotal to drug discovery efforts, enabling high-throughput screens for hit finding. However, limited availability of public DEL datasets hinders the advancement of computational techniques designed to process such data. To bridge this gap, we present KinDEL, one of the first large, publicly available DEL datasets on two kinases: Mitogen-Activated Protein Kinase 14 (MAPK14) and Discoidin Domain Receptor Tyrosine Kinase 1 (DDR1). Interest in this data modality is growing due to its ability to generate extensive supervised chemical data that densely samples around select molecular structures. Demonstrating one such application of the data, we benchmark different machine learning techniques to develop predictive models for hit identification; in particular, we highlight recent structure-based probabilistic approaches. Finally, we provide biophysical assay data, both on- and off-DNA, to validate our models on a smaller subset of molecules. Data and code for our benchmarks can be found at: https://github.com/insitro/kindel.",
        "subjects": [
            "q-bio.QM",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09219",
        "abstract url": "https://arxiv.org/abs/2410.09219",
        "title": "Understanding the First Wave of AI Safety Institutes: Characteristics, Functions, and Challenges",
        "rating": "-3.5",
        "keywords": [
            [
                "biological"
            ],
            [
                "chemical"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "In November 2023, the UK and US announced the creation of their AI Safety Institutes (AISIs). Five other jurisdictions have followed in establishing AISIs or similar institutions, with more likely to follow. While there is considerable variation between these institutions, there are also key similarities worth identifying. This primer describes one cluster of similar AISIs, the \"first wave,\" consisting of the Japan, UK, and US AISIs. First-wave AISIs have several fundamental characteristics in common: they are technical government institutions, have a clear mandate related to the safety of advanced AI systems, and lack regulatory powers. Safety evaluations are at the center of first-wave AISIs. These techniques test AI systems across tasks to understand their behavior and capabilities on relevant risks, such as cyber, chemical, and biological misuse. They also share three core functions: research, standards, and cooperation. These functions are critical to AISIs' work on safety evaluations but also support other activities such as scientific consensus-building and foundational AI safety research. Despite its growing popularity as an institutional model, the AISI model is not free from challenges and limitations. Some analysts have criticized the first wave of AISIs for specializing too much in a sub-area and for being potentially redundant with existing institutions, for example. Future developments may rapidly change this landscape, and particularities of individual AISIs may not be captured by our broad-strokes description. This policy brief aims to outline the core elements of first-wave AISIs as a way of encouraging and improving conversations on this novel institutional model, acknowledging this is just a simplified snapshot rather than a timeless prescription.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08691",
        "abstract url": "https://arxiv.org/abs/2410.08691",
        "title": "Bio-inspired reconfigurable stereo vision for robotics using omnidirectional cameras",
        "rating": "-4",
        "keywords": [
            [
                "depth"
            ],
            [
                "robotics",
                "robot"
            ],
            [
                "Bio-inspired"
            ]
        ],
        "abstract": "This work introduces a novel bio-inspired reconfigurable stereo vision system for robotics, leveraging omnidirectional cameras and a novel algorithm to achieve flexible visual capabilities. Inspired by the adaptive vision of various species, our visual system addresses traditional stereo vision limitations, i.e., immutable camera alignment with narrow fields of view, by introducing a reconfigurable stereo vision system to robotics. Our key innovations include the reconfigurable stereo vision strategy that allows dynamic camera alignment, a robust depth measurement system utilizing a nonrectified geometrical method combined with a deep neural network for feature matching, and a geometrical compensation technique to enhance visual accuracy. Implemented on a metamorphic robot, this vision system demonstrates its great adaptability to various scenarios by switching its configurations of 316\u00b0 monocular with 79\u00b0 binocular field for fast target seeking and 242\u00b0 monocular with 150\u00b0 binocular field for detailed close inspection.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages, 8 figures, submitted to IEEE ICRA 2025"
    },
    {
        "paper id": "2410.08649",
        "abstract url": "https://arxiv.org/abs/2410.08649",
        "title": "E-Motion: Future Motion Simulation via Event Sequence Diffusion",
        "rating": "-4.5",
        "keywords": [
            [
                "event camera"
            ],
            [
                "Diffusion"
            ],
            [
                "trajectory",
                "vehicle"
            ],
            [
                "navigation"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Forecasting a typical object's future motion is a critical task for interpreting and interacting with dynamic environments in computer vision. Event-based sensors, which could capture changes in the scene with exceptional temporal granularity, may potentially offer a unique opportunity to predict future motion with a level of detail and precision previously unachievable. Inspired by that, we propose to integrate the strong learning capacity of the video diffusion model with the rich motion information of an event camera as a motion simulation framework. Specifically, we initially employ pre-trained stable video diffusion models to adapt the event sequence dataset. This process facilitates the transfer of extensive knowledge from RGB videos to an event-centric domain. Moreover, we introduce an alignment mechanism that utilizes reinforcement learning techniques to enhance the reverse generation trajectory of the diffusion model, ensuring improved performance and accuracy. Through extensive testing and validation, we demonstrate the effectiveness of our method in various complex scenarios, showcasing its potential to revolutionize motion flow prediction in computer vision applications such as autonomous vehicle guidance, robotic navigation, and interactive media. Our findings suggest a promising direction for future research in enhancing the interpretative power and predictive accuracy of computer vision systems.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "NeurIPS 2024"
    },
    {
        "paper id": "2410.08833",
        "abstract url": "https://arxiv.org/abs/2410.08833",
        "title": "Symmetry-Constrained Generation of Diverse Low-Bandgap Molecules with Monte Carlo Tree Search",
        "rating": "-4.5",
        "keywords": [
            [
                "infrared"
            ],
            [
                "biomedical"
            ],
            [
                "patent"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Organic optoelectronic materials are a promising avenue for next-generation electronic devices due to their solution processability, mechanical flexibility, and tunable electronic properties. In particular, near-infrared (NIR) sensitive molecules have unique applications in night-vision equipment and biomedical imaging. Molecular engineering has played a crucial role in developing non-fullerene acceptors (NFAs) such as the Y-series molecules, which have significantly improved the power conversion efficiency (PCE) of solar cells and enhanced spectral coverage in the NIR region. However, systematically designing molecules with targeted optoelectronic properties while ensuring synthetic accessibility remains a challenge. To address this, we leverage structural priors from domain-focused, patent-mined datasets of organic electronic molecules using a symmetry-aware fragment decomposition algorithm and a fragment-constrained Monte Carlo Tree Search (MCTS) generator. Our approach generates candidates that retain symmetry constraints from the patent dataset, while also exhibiting red-shifted absorption, as validated by TD-DFT calculations.",
        "subjects": [
            "physics.chem-ph",
            "cond-mat.mtrl-sci",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08837",
        "abstract url": "https://arxiv.org/abs/2410.08837",
        "title": "A physics-guided neural network for flooding area detection using SAR imagery and local river gauge observations",
        "rating": "-4.5",
        "keywords": [
            [
                "radar"
            ],
            [
                "satellite"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The flooding extent area in a river valley is related to river gauge observations. The higher the water elevation, the larger the flooding area. Due to synthetic aperture radar\\textquoteright s (SAR) capabilities to penetrate through clouds, radar images have been commonly used to estimate flooding extent area with various methods, from simple thresholding to deep learning models. In this study, we propose a physics-guided neural network for flooding area detection. Our approach takes as input data the Sentinel 1 time-series images and the water elevations in the river assigned to each image. We apply the Pearson correlation coefficient between the predicted sum of water extent areas and the local water level observations of river water elevations as the loss function. The effectiveness of our method is evaluated in five different study areas by comparing the predicted water maps with reference water maps obtained from digital terrain models and optical satellite images. The highest Intersection over Union (IoU) score achieved by our models was 0.89 for the water class and 0.96 for the non-water class. Additionally, we compared the results with other unsupervised methods. The proposed neural network provided a higher IoU than the other methods, especially for SAR images registered during low water elevation in the river.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "18 pages, 6 figures, 57 cited references"
    },
    {
        "paper id": "2410.08762",
        "abstract url": "https://arxiv.org/abs/2410.08762",
        "title": "Cross-chain Sharing of Personal Health Records: Heterogeneous and Interoperable Blockchains",
        "rating": "-5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "medical",
                "Health"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "With the widespread adoption of medical informatics, a wealth of valuable personal health records (PHR) has been generated. Concurrently, blockchain technology has enhanced the security of medical institutions. However, these institutions often function as isolated data silos, limiting the potential value of PHRs. As the demand for data sharing between hospitals on different blockchains grows, addressing the challenge of cross-chain data sharing becomes crucial. When sharing PHRs across blockchains, the limited storage and computational capabilities of medical Internet of Things (IoT) devices complicate the storage of large volumes of PHRs and the handling of complex calculations. Additionally, varying blockchain cryptosystems and the risk of internal attacks further complicate the cross-chain sharing of PHRs. This paper proposes a scheme for sharing PHRs across heterogeneous and interoperable blockchains. Medical IoT devices can encrypt and store real-time PHRs in an InterPlanetary File System, requiring only simple operations for data sharing. An enhanced proxy re-encryption(PRE) algorithm addresses the differences in blockchain cryptosystems. Multi-dimensional analysis demonstrates that this scheme offers robust security and excellent performance.",
        "subjects": [
            "cs.CR",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09003",
        "abstract url": "https://arxiv.org/abs/2410.09003",
        "title": "Design and Control of an Omnidirectional Aerial Robot with a Miniaturized Haptic Joystick for Physical Interaction",
        "rating": "-5",
        "keywords": [
            [
                "6-DoF"
            ],
            [
                "flight"
            ],
            [
                "Robot"
            ],
            [
                "physic"
            ]
        ],
        "abstract": "Fully actuated aerial robot proved their superiority for Aerial Physical Interaction (APhI) over the past years. This work proposes a minimal setup for aerial telemanipulation, enhancing accessibility of these technologies. The design and the control of a 6-DoF joystick with 4-DoF haptic feedback is detailed. It is the first haptic device with standard Remote Controller (RC) form factor for APhI. By miniaturizing haptic device, it enhances RC with the sense of touch, increasing physical awareness. The goal is to give operators an extra sense, other than vision and sound, to help to perform safe APhI. To the best of the authors knowledge, this is the first teleoperation system able to decouple each single axis input command. On the omnidirectional quadrotor, by reducing the number of components with a new design, we aim a simplified maintenance, and improved force and thrust to weight ratio. Open-sourced physic based simulation and successful preliminary flight tests highlighted the tool as promising for future APhI applications.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "6 pages, 6 figures"
    },
    {
        "paper id": "2410.09034",
        "abstract url": "https://arxiv.org/abs/2410.09034",
        "title": "PEAR: A Robust and Flexible Automation Framework for Ptychography Enabled by Multiple Large Language Model Agents",
        "rating": "-6",
        "keywords": [
            [
                "Robot"
            ],
            [
                "biology",
                "X-ray"
            ],
            [
                "industrial",
                "recommendation",
                "chemistry"
            ],
            [
                "physics"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Ptychography is an advanced computational imaging technique in X-ray and electron microscopy. It has been widely adopted across scientific research fields, including physics, chemistry, biology, and materials science, as well as in industrial applications such as semiconductor characterization. In practice, obtaining high-quality ptychographic images requires simultaneous optimization of numerous experimental and algorithmic parameters. Traditionally, parameter selection often relies on trial and error, leading to low-throughput workflows and potential human bias. In this work, we develop the \"Ptychographic Experiment and Analysis Robot\" (PEAR), a framework that leverages large language models (LLMs) to automate data analysis in ptychography. To ensure high robustness and accuracy, PEAR employs multiple LLM agents for tasks including knowledge retrieval, code generation, parameter recommendation, and image reasoning. Our study demonstrates that PEAR's multi-agent design significantly improves the workflow success rate, even with smaller open-weight models such as LLaMA 3.1 8B. PEAR also supports various automation levels and is designed to work with customized local knowledge bases, ensuring flexibility and adaptability across different research environments.",
        "subjects": [
            "cs.CE",
            "cs.AI",
            "cs.CL",
            "cs.MA"
        ],
        "comment": "18 pages, 5 figures, technical preview report"
    },
    {
        "paper id": "2410.08556",
        "abstract url": "https://arxiv.org/abs/2410.08556",
        "title": "A System of Bidirectional Power Routing Toward Multi-energy Management",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we propose a system of bidirectional power routing for inter-house multi-energy management systems that utilize electricity and hydrogen as energy carriers. The key is to share private facilities such as photovoltaic panels and batteries among a group of houses along with a common hydrogen system. A power router of line switching type is introduced as a physical interface to realize the sharing economy between households. The proposed system offers a unique measure to address the urgent challenges of today's multi-energy system, namely increasing the renewables' self-consumption, enhancing the energy system's resilience, and providing traceability of hydrogen in terms of renewability certification. We also present an experimental demonstration under a simplified scenario using prototype hardware.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Accepted for presentation at 2024 Annual Conference of the IEEE Industrial Electronics Society"
    },
    {
        "paper id": "2410.08561",
        "abstract url": "https://arxiv.org/abs/2410.08561",
        "title": "Towards Effective Deep Neural Network Approach for Multi-Trial P300-based Character Recognition in Brain-Computer Interfaces",
        "rating": "-10",
        "keywords": [],
        "abstract": "Brain-computer interfaces (BCIs) enable direct interaction between users and computers by decoding brain signals. This study addresses the challenges of detecting P300 event-related potentials in electroencephalograms (EEGs) and integrating these P300 responses for character spelling, particularly within oddball paradigms characterized by uneven P300 distribution, low target probability, and poor signal-to-noise ratio (SNR). This work proposes a weighted ensemble spatio-sequential convolutional neural network (WE-SPSQ-CNN) to improve classification accuracy and SNR by mitigating signal variability for character identification. We evaluated the proposed WE-SPSQ-CNN on dataset II from the BCI Competition III, achieving P300 classification accuracies of 69.7\\% for subject A and 79.9\\% for subject B across fifteen epochs. For character recognition, the model achieved average accuracies of 76.5\\%, 87.5\\%, and 94.5\\% with five, ten, and fifteen repetitions, respectively. Our proposed model outperformed state-of-the-art models in the five-repetition and delivered comparable performance in the ten and fifteen repetitions.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08570",
        "abstract url": "https://arxiv.org/abs/2410.08570",
        "title": "Predictive Tree-based Virtual Keyboard for Improved Gaze Typing",
        "rating": "-10",
        "keywords": [],
        "abstract": "On-screen keyboard eye-typing systems are limited due to the lack of predictive text and user-centred approaches, resulting in low text entry rates and frequent recalibration. This work proposes integrating the prediction by partial matching (PPM) technique into a tree-based virtual keyboard. We developed the Flex-Tree on-screen keyboard using a two-stage tree-based character selection system with ten commands, testing it with three degree of PPM (PPM1, PPM2, PPM3). Flex-Tree provides access to 72 English characters, including upper- and lower-case letters, numbers, and special characters, and offers functionalities like the delete command for corrections. The system was evaluated with sixteen healthy volunteers using two specially designed typing tasks, including the hand-picked and random-picked sentences. The spelling task was performed using two input modalities: (i) a mouse and (ii) a portable eye-tracker. Two experiments were conducted, encompassing 24 different conditions. The typing performance of Flex-Tree was compared with that of a tree-based virtual keyboard with an alphabetic arrangement (NoPPM) and the Dasher on-screen keyboard for new users. Flex-Tree with PPM3 outperformed the other keyboards, achieving average text entry speeds of 27.7 letters/min with a mouse and 16.3 letters/min with an eye-tracker. Using the eye-tracker, the information transfer rates at the command and letter levels were 108.4 bits/min and 100.7 bits/min, respectively. Flex-Tree, across all three degree of PPM, received high ratings on the system usability scale and low-weighted ratings on the NASA Task Load Index for both input modalities, highlighting its user-centred design.",
        "subjects": [
            "cs.HC",
            "cs.ET"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08585",
        "abstract url": "https://arxiv.org/abs/2410.08585",
        "title": "Blind and robust reconstruction of adaptive optics point spread functions for asteroid deconvolution and moon detection",
        "rating": "-10",
        "keywords": [],
        "abstract": "Initially designed to detect and characterize exoplanets, extreme adaptive optics systems (AO) open a new window on the solar system by resolving its small bodies. Nonetheless, despite the always increasing performances of AO systems, the correction is not perfect, degrading their image and producing a bright halo that can hide faint and close moons. Using a reference point spread function (PSF) is not always sufficient due to the random nature of the turbulence. In this work, we present our method to overcome this limitation. It blindly reconstructs the AO-PSF directly in the data of interest, without any prior on the instrument nor the asteroid's shape. This is done by first estimating the PSF core parameters under the assumption of a sharp-edge and flat object, allowing the image of the main body to be deconvolved. Then, the PSF faint extensions are reconstructed with a robust penalization optimization, discarding outliers on-the-fly such as cosmic rays, defective pixels and moons. This allows to properly model and remove the asteroid's halo. Finally, moons can be detected in the residuals, using the reconstructed PSF and the knowledge of the outliers learned with the robust method. We show that our method can be easily applied to different instruments (VLT/SPHERE, Keck/NIRC2), efficiently retrieving the features of AO-PSFs. Compared with state-of-the-art moon enhancement algorithms, moon signal is greatly improved and our robust detection method manages to discriminate faint moons from outliers.",
        "subjects": [
            "eess.SP",
            "astro-ph.IM"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2407.21548"
    },
    {
        "paper id": "2410.08599",
        "abstract url": "https://arxiv.org/abs/2410.08599",
        "title": "Synthesis from LTL with Reward Optimization in Sampled Oblivious Environments",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper addresses the synthesis of reactive systems that enforce hard constraints while optimizing for quality-based soft constraints. We build on recent advancements in combining reactive synthesis with example-based guidance to handle both types of constraints in stochastic, oblivious environments accessible only through sampling. Our approach constructs examples that satisfy LTL-based hard constraints while maximizing expected rewards-representing the soft constraints-on samples drawn from the environment. We formally define this synthesis problem, prove it to be NP-complete, and propose an SMT-based solution, demonstrating its effectiveness with a case study.",
        "subjects": [
            "cs.FL"
        ],
        "comment": "19 pages, serve as complete version for reference"
    },
    {
        "paper id": "2410.08616",
        "abstract url": "https://arxiv.org/abs/2410.08616",
        "title": "Dual-AEB: Synergizing Rule-Based and Multimodal Large Language Models for Effective Emergency Braking",
        "rating": "-10",
        "keywords": [],
        "abstract": "Automatic Emergency Braking (AEB) systems are a crucial component in ensuring the safety of passengers in autonomous vehicles. Conventional AEB systems primarily rely on closed-set perception modules to recognize traffic conditions and assess collision risks. To enhance the adaptability of AEB systems in open scenarios, we propose Dual-AEB, a system combines an advanced multimodal large language model (MLLM) for comprehensive scene understanding and a conventional rule-based rapid AEB to ensure quick response times. To the best of our knowledge, Dual-AEB is the first method to incorporate MLLMs within AEB systems. Through extensive experimentation, we have validated the effectiveness of our method. The source code will be available at https://github.com/ChipsICU/Dual-AEB.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08618",
        "abstract url": "https://arxiv.org/abs/2410.08618",
        "title": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems with In-Network Coordination",
        "rating": "-10",
        "keywords": [],
        "abstract": "Distributed filesystems typically employ synchronous metadata updates, facing inherent challenges for access efficiency, load balancing, and directory contention, especially under dynamic and skewed workloads. This paper argues that synchronous updates are overly conservative for distributed filesystems. We propose AsyncFS with asynchronous metadata updates, allowing operations to return early and defer directory updates until respective read to enable latency hiding and conflict resolution. The key challenge is efficiently maintaining the synchronous semantics of metadata updates. To address this, AsyncFS is co-designed with a programmable switch, leveraging the constrained on-switch resources to holistically track directory states in the network with negligible cost. This allows AsyncFS to timely aggregate and efficiently apply delayed updates using batching and consolidation before directory reads. Evaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$ higher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art distributed filesystems, InfiniFS and CFS-KV, respectively, on skewed workloads. For real-world workloads, AsyncFS improves end-to-end throughput by 21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV, respectively.",
        "subjects": [
            "cs.DC",
            "cs.OS",
            "cs.PF"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08619",
        "abstract url": "https://arxiv.org/abs/2410.08619",
        "title": "TactileAR: Active Tactile Pattern Reconstruction",
        "rating": "-10",
        "keywords": [],
        "abstract": "High-resolution (HR) contact surface information is essential for robotic grasping and precise manipulation tasks. However, it remains a challenge for current taxel-based sensors to obtain HR tactile information. In this paper, we focus on utilizing low-resolution (LR) tactile sensors to reconstruct the localized, dense, and HR representation of contact surfaces. In particular, we build a Gaussian triaxial tactile sensor degradation model and propose a tactile pattern reconstruction framework based on the Kalman filter. This framework enables the reconstruction of 2-D HR contact surface shapes using collected LR tactile sequences. In addition, we present an active exploration strategy to enhance the reconstruction efficiency. We evaluate the proposed method in real-world scenarios with comparison to existing prior-information-based approaches. Experimental results confirm the efficiency of the proposed approach and demonstrate satisfactory reconstructions of complex contact surface shapes. Code: https://github.com/wmtlab/tactileAR",
        "subjects": [
            "cs.RO"
        ],
        "comment": "accepted by ICRA 2024"
    },
    {
        "paper id": "2410.08647",
        "abstract url": "https://arxiv.org/abs/2410.08647",
        "title": "To Repair or Not to Repair: Assessing Fault Resilience in MPI Stencil Applications",
        "rating": "-10",
        "keywords": [],
        "abstract": "With the increasing size of HPC computations, faults are becoming more and more relevant in the HPC field. The MPI standard does not define the application behaviour after a fault, leaving the burden of fault management to the user, who usually resorts to checkpoint and restart mechanisms. This trend is especially true in stencil applications, as their regular pattern simplifies the selection of checkpoint locations. However, checkpoint and restart mechanisms introduce non-negligible overhead, disk load, and scalability concerns. In this paper, we show an alternative through fault resilience, enabled by the features provided by the User Level Fault Mitigation extension and shipped within the Legio fault resilience framework. Through fault resilience, we continue executing only the non-failed processes, thus sacrificing result accuracy for faster fault recovery. Our experiments on a specimen stencil application show that, despite the fault impact visible in the result, we produced meaningful values usable for scientific research, proving the possibilities of a fault resilience approach in a stencil scenario.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08653",
        "abstract url": "https://arxiv.org/abs/2410.08653",
        "title": "From gymnastics to virtual nonholonomic constraints: energy injection, dissipation, and regulation for the acrobot",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this article we study virtual nonholonomic constraints, which are relations between the generalized coordinates and momenta of a mechanical system that can be enforced via feedback control. We design a constraint which emulates gymnastics giant motion in an acrobot, and prove that this constraint can inject or dissipate energy based on the sign of a design parameter. The proposed constraint is tested both in simulation and experimentally on a real-world acrobot, demonstrating highly effective energy regulation properties and robustness to a variety of disturbances.",
        "subjects": [
            "cs.RO",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08676",
        "abstract url": "https://arxiv.org/abs/2410.08676",
        "title": "The Design Space of in-IDE Human-AI Experience",
        "rating": "-10",
        "keywords": [],
        "abstract": "Nowadays, integration of AI-driven tools within Integrated Development Environments (IDEs) is reshaping the software development lifecycle. Existing research highlights that users expect these tools to be efficient, context-aware, accurate, user-friendly, customizable, and secure. However, a major gap remains in understanding developers' needs and challenges, particularly when interacting with AI systems in IDEs and from the perspectives of different user groups. In this work, we address this gap through structured interviews with 35 developers from three different groups: Adopters, Churners, and Non-Users of AI in IDEs to create a comprehensive Design Space of in-IDE Human-AI Experience. Our results highlight key areas of Technology Improvement, Interaction, and Alignment in in-IDE AI systems, as well as Simplifying Skill Building and Programming Tasks. Our key findings stress the need for AI systems that are more personalized, proactive, and reliable. We also emphasize the importance of context-aware and privacy-focused solutions and better integration with existing workflows. Furthermore, our findings show that while Adopters appreciate advanced features and non-interruptive integration, Churners emphasize the need for improved reliability and privacy. Non-Users, in contrast, focus on skill development and ethical concerns as barriers to adoption. Lastly, we provide recommendations for industry practitioners aiming to enhance AI integration within developer workflows.",
        "subjects": [
            "cs.SE",
            "cs.HC"
        ],
        "comment": "11 pages, 2 figures, submitted to ICSE SEIP 2025"
    },
    {
        "paper id": "2410.08692",
        "abstract url": "https://arxiv.org/abs/2410.08692",
        "title": "Contrastive Knowledge Distillation for Robust Multimodal Sentiment Analysis",
        "rating": "-10",
        "keywords": [],
        "abstract": "Multimodal sentiment analysis (MSA) systems leverage information from different modalities to predict human sentiment intensities. Incomplete modality is an important issue that may cause a significant performance drop in MSA systems. By generative imputation, i.e., recovering the missing data from available data, systems may achieve robust performance but will lead to high computational costs. This paper introduces a knowledge distillation method, called `Multi-Modal Contrastive Knowledge Distillation' (MM-CKD), to address the issue of incomplete modality in video sentiment analysis with lower computation cost, as a novel non-imputation-based method. We employ Multi-view Supervised Contrastive Learning (MVSC) to transfer knowledge from a teacher model to student models. This approach not only leverages cross-modal knowledge but also introduces cross-sample knowledge with supervision, jointly improving the performance of both teacher and student models through online learning. Our method gives competitive results with significantly lower computational costs than state-of-the-art imputation-based methods.",
        "subjects": [
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08701",
        "abstract url": "https://arxiv.org/abs/2410.08701",
        "title": "Obelia: Scaling DAG-Based Blockchains to Hundreds of Validators",
        "rating": "-10",
        "keywords": [],
        "abstract": "Obelia improves upon structured DAG-based consensus protocols used in proof-of-stake systems, allowing them to effectively scale to accommodate hundreds of validators. Obelia implements a two-tier validator system. A core group of high-stake validators that propose blocks as in current protocols and a larger group of lower-stake auxiliary validators that occasionally author blocks. Obelia incentivizes auxiliary validators to assist recovering core validators and integrates seamlessly with existing protocols. We show that Obelia does not introduce visible overhead compared to the original protocol, even when scaling to hundreds of validators, or when a large number of auxiliary validators are unreliable.",
        "subjects": [
            "cs.CR",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08706",
        "abstract url": "https://arxiv.org/abs/2410.08706",
        "title": "Goal-Oriented Communications for Real-time Inference with Two-Way Delay",
        "rating": "-10",
        "keywords": [],
        "abstract": "We design a goal-oriented communication strategy for remote inference, where an intelligent model (e.g., a pre-trained neural network) at the receiver side predicts the real-time value of a target signal based on data packets transmitted from a remote location. The inference error depends on both the Age of Information (AoI) and the length of the data packets. Previous formulations of this problem either assumed IID transmission delays with immediate feedback or focused only on monotonic relations where inference performance degrades as the input data ages. In contrast, we consider a possibly non-monotonic relationship between the inference error and AoI. We show how to minimize the expected time-average inference error under two-way delay, where the delay process can have memory. Simulation results highlight the significant benefits of adopting such a goal-oriented communication strategy for remote inference, especially under highly variable delay scenarios.",
        "subjects": [
            "cs.NI",
            "eess.SP"
        ],
        "comment": "12 pages, 8 figures"
    },
    {
        "paper id": "2410.08711",
        "abstract url": "https://arxiv.org/abs/2410.08711",
        "title": "On-Chip Learning via Transformer In-Context Learning",
        "rating": "-10",
        "keywords": [],
        "abstract": "Autoregressive decoder-only transformers have become key components for scalable sequence processing and generation models. However, the transformer's self-attention mechanism requires transferring prior token projections from the main memory at each time step (token), thus severely limiting their performance on conventional processors. Self-attention can be viewed as a dynamic feed-forward layer, whose matrix is input sequence-dependent similarly to the result of local synaptic plasticity. Using this insight, we present a neuromorphic decoder-only transformer model that utilizes an on-chip plasticity processor to compute self-attention. Interestingly, the training of transformers enables them to ``learn'' the input context during inference. We demonstrate this in-context learning ability of transformers on the Loihi 2 processor by solving a few-shot classification problem. With this we emphasize the importance of pretrained models especially their ability to find simple, local, backpropagation free, learning rules enabling on-chip learning and adaptation in a hardware friendly manner.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08714",
        "abstract url": "https://arxiv.org/abs/2410.08714",
        "title": "A Simple yet Exact Analysis of the MultiQueue",
        "rating": "-10",
        "keywords": [],
        "abstract": "The MultiQueue is a relaxed concurrent priority queue consisting of $n$ internal priority queues, where an insertion uses a random queue and a deletion considers two random queues and deletes the minimum from the one with the smaller minimum. The rank error of the deletion is the number of smaller elements in the MultiQueue. Alistarh et al. [2] have demonstrated in a sophisticated potential argument that the expected rank error remains bounded by $O(n)$ over long sequences of deletions. In this paper we present a simpler analysis by identifying the stable distribution of an underlying Markov chain and with it the long-term distribution of the rank error exactly. Simple calculations then reveal the expected long-term rank error to be $\\tfrac{5}{6}n-1+\\tfrac{1}{6n}$. Our arguments generalize to deletion schemes where the probability to delete from a given queue depends only on the rank of the queue. Specifically, this includes deleting from the best of $c$ randomly selected queues for any $c>1$.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08722",
        "abstract url": "https://arxiv.org/abs/2410.08722",
        "title": "A generic Branch-and-Cut algorithm for bi-objective binary linear programs",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents the first generic bi-objective binary linear branch-and-cut algorithm. Studying the impact of valid inequalities in solution and objective spaces, two cutting frameworks are proposed. The multi-point separation problem is introduced together with a cutting algorithm to efficiently generate valid inequalities violating multiple points simultaneously. The other main idea is to invoke state-of-the-art integer linear programming solver's internal advanced techniques such as cut separators. Aggregation techniques are proposed to use these frameworks with a trade-off among efficient cut separations, tight lower and upper bound sets and advanced branching strategies. Experiments on various types of instances in the literature exhibit the promising efficiency of the algorithm that solves instances with up to 2800 binary variables in less than one hour of CPU time. Our algorithms are easy to extend for more than two objectives and integer variables.",
        "subjects": [
            "cs.DM",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08723",
        "abstract url": "https://arxiv.org/abs/2410.08723",
        "title": "Investigating Human-Computer Interaction and Visual Comprehension in Text Generation Process of Natural Language Generation Models",
        "rating": "-10",
        "keywords": [],
        "abstract": "Natural language generation (NLG) models are becoming a highly sought-after research focus in the field of natural language processing (NLP), demonstrating strong capabilities in text generation tasks such as writing and dialogue generation. Despite the impressive performance of NLG models, their complex architecture and extensive model weights result in a lack of interpretability. This limitation hampers their adoption in many critical decision-making scenarios. Fortunately, the intervention of human-computer interaction and visual comprehension provides users with the possibility of opening the \"black box\". In this paper, we conduct a investigation addressing the roles and limitations of human-computer interactive and visual comprehension in text generation process of NLG models. We present a taxonomy of interaction methods and visualization techniques, providing a structured overview of the three main research subjects and their corresponding six tasks within the application process of large language models (LLMs). Finally, we summarize the shortcomings in the existing work and investigate the key challenges and emerging opportunities in the era of LLMs.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08737",
        "abstract url": "https://arxiv.org/abs/2410.08737",
        "title": "Bad Neighbors: On Understanding VPN Provider Networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "Virtual Private Network (VPN) solutions are used to connect private networks securely over the Internet. Besides their benefits in corporate environments, VPNs are also marketed to privacy-minded users to preserve their privacy, and to bypass geolocation-based content blocking and censorship. This has created a market for turnkey VPN services offering a multitude of vantage points all over the world for a monthly price. While VPN providers are heavily using privacy and security benefits in their marketing, such claims are generally hard to measure and substantiate. While there exist some studies on the VPN ecosystem, all prior works omit a critical part in their analyses: (i) How well do the providers configure and secure their own network infrastructure? and (ii) How well are they protecting their customers from other customers? To answer these questions, we have developed an automated measurement system with which we conduct a large-scale analysis of VPN providers and their thousands of VPN endpoints. Considering the fact that VPNs work internally using non-Internet-routable IP addresses, they might enable access to otherwise inaccessible networks. If not properly secured, this can inadvertently expose internal networks of these providers, or worse, even other clients connected to their services. Our results indicate a widespread lack of traffic filtering towards internally routable networks on the majority of tested VPN service providers, even in cases where no other VPN customers were directly exposed. We have disclosed our findings to the affected providers and other stakeholders, and offered guidance to improve the situation.",
        "subjects": [
            "cs.CR",
            "cs.NI"
        ],
        "comment": "15 pages, 7 tables, 6 figures"
    },
    {
        "paper id": "2410.08744",
        "abstract url": "https://arxiv.org/abs/2410.08744",
        "title": "No Tick-Size Too Small: A General Method for Modelling Small Tick Limit Order Books",
        "rating": "-10",
        "keywords": [],
        "abstract": "We investigate the disparity in the microstructural properties of the Limit Order Book (LOB) across different relative tick sizes. Tick sizes not only influence the granularity of the price formation process but also affect market agents' behavior. A key contribution of this study is the identification of several stylized facts, which are used to differentiate between large, medium, and small tick stocks, along with clear metrics for their measurement. We provide cross-asset visualizations to illustrate how these attributes vary with relative tick size. Further, we propose a Hawkes Process model that accounts for sparsity, multi-tick level price moves, and the shape of the book in small-tick stocks. Through simulation studies, we demonstrate the universality of the model and identify key variables that determine whether a simulated LOB resembles a large-tick or small-tick stock. Our tests show that stylized facts like sparsity, shape, and relative returns distribution can be smoothly transitioned from a large-tick to a small-tick asset using our model. We test this model's assumptions, showcase its challenges and propose questions for further directions in this area of research.",
        "subjects": [
            "q-fin.TR",
            "cs.CE",
            "q-fin.CP"
        ],
        "comment": "Presented at ALGODEFI 2024, Milan"
    },
    {
        "paper id": "2410.08755",
        "abstract url": "https://arxiv.org/abs/2410.08755",
        "title": "PILLAR: an AI-Powered Privacy Threat Modeling Tool",
        "rating": "-10",
        "keywords": [],
        "abstract": "The rapid evolution of Large Language Models (LLMs) has unlocked new possibilities for applying artificial intelligence across a wide range of fields, including privacy engineering. As modern applications increasingly handle sensitive user data, safeguarding privacy has become more critical than ever. To protect privacy effectively, potential threats need to be identified and addressed early in the system development process. Frameworks like LINDDUN offer structured approaches for uncovering these risks, but despite their value, they often demand substantial manual effort, expert input, and detailed system knowledge. This makes the process time-consuming and prone to errors. Current privacy threat modeling methods, such as LINDDUN, typically rely on creating and analyzing complex data flow diagrams (DFDs) and system descriptions to pinpoint potential privacy issues. While these approaches are thorough, they can be cumbersome, relying heavily on the precision of the data provided by users. Moreover, they often generate a long list of threats without clear guidance on how to prioritize them, leaving developers unsure of where to focus their efforts. In response to these challenges, we introduce PILLAR (Privacy risk Identification with LINDDUN and LLM Analysis Report), a new tool that integrates LLMs with the LINDDUN framework to streamline and enhance privacy threat modeling. PILLAR automates key parts of the LINDDUN process, such as generating DFDs, classifying threats, and prioritizing risks. By leveraging the capabilities of LLMs, PILLAR can take natural language descriptions of systems and transform them into comprehensive threat models with minimal input from users, reducing the workload on developers and privacy experts while improving the efficiency and accuracy of the process.",
        "subjects": [
            "cs.CR",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08756",
        "abstract url": "https://arxiv.org/abs/2410.08756",
        "title": "Privacy-Preserving Optimal State Estimation with Low Complexity via Cram\u00e9r-Rao Lower Bound Approach",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper addresses the optimal state estimation problem for dynamic systems while preserving private information against an adversary. To dominate the adversary's estimation accuracy about private information in the mean square error (MSE) sense, the Cram\u00e9r-Rao lower bound (CRLB) is employed to evaluate privacy level. The problem is formulated as a constrained optimization, which minimizes the MSE of the state estimate with a constraint on privacy level, achieving a trade-off between privacy and utility. To solve the constrained optimization problem, an explicit expression for CRLB is first provided using the information inequality. To overcome the increasing sizes of the involved matrices over time, a low-complexity approach is then proposed to achieve online calculation for CRLB, significantly reducing computational complexity. Next, the optimization problem is relaxed to a semi-definite programming problem, and a relaxed solution is provided. Finally, a privacy-preserving state estimation algorithm with low complexity is developed and proved to achieve differential privacy. Two illustrative examples, including a practical case of building occupancy, demonstrate the effectiveness of the proposed algorithm.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08801",
        "abstract url": "https://arxiv.org/abs/2410.08801",
        "title": "A Methodology for Evaluating RAG Systems: A Case Study On Configuration Dependency Validation",
        "rating": "-10",
        "keywords": [],
        "abstract": "Retrieval-augmented generation (RAG) is an umbrella of different components, design decisions, and domain-specific adaptations to enhance the capabilities of large language models and counter their limitations regarding hallucination and outdated and missing knowledge. Since it is unclear which design decisions lead to a satisfactory performance, developing RAG systems is often experimental and needs to follow a systematic and sound methodology to gain sound and reliable results. However, there is currently no generally accepted methodology for RAG evaluation despite a growing interest in this technology. In this paper, we propose a first blueprint of a methodology for a sound and reliable evaluation of RAG systems and demonstrate its applicability on a real-world software engineering research task: the validation of configuration dependencies across software technologies. In summary, we make two novel contributions: (i) A novel, reusable methodological design for evaluating RAG systems, including a demonstration that represents a guideline, and (ii) a RAG system, which has been developed following this methodology, that achieves the highest accuracy in the field of dependency validation. For the blueprint's demonstration, the key insights are the crucial role of choosing appropriate baselines and metrics, the necessity for systematic RAG refinements derived from qualitative failure analysis, as well as the reporting practices of key design decision to foster replication and evaluation.",
        "subjects": [
            "cs.SE",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.08839",
        "abstract url": "https://arxiv.org/abs/2410.08839",
        "title": "Exploiting Multiple Polarizations in Extra Large Holographic MIMO",
        "rating": "-10",
        "keywords": [],
        "abstract": "The proliferation of large multi-antenna configurations operating in high frequency bands has recently challenged the conventional far-field, rich-scattering paradigm of wireless channels. Extra large antenna arrays must usually work in the near field and in the absence of multipath, which are far from traditional assumptions in conventional wireless communication systems. The present study proposes to analyze the spatial multiplexing capabilities of large multi-antenna configurations under line-of-sight, near field conditions by considering the use of multiple orthogonal diversities at both transmitter and receiver. The analysis is carried out using a holographic approximation to the problem, whereby the number of radiating elements is assumed to become large while their separation becomes asymptotically negligible. This emulates the operation of a continuous aperture of infinitesimal radiating elements, also recently known as holographic surfaces. The present study characterizes the asymptotic MIMO channel as seen by extra large uniform linear and planar arrays, as well as their associated achievable rates assuming access to perfect channel state information (CSI). It is shown, in particular, that for a given distance between the receiver and the center of the array and a given signal quality, there exists an optimum dimension of the multi-antenna surface that maximizes the spectral efficiency.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "submitted for Journal publication"
    },
    {
        "paper id": "2410.08858",
        "abstract url": "https://arxiv.org/abs/2410.08858",
        "title": "Decoding Secret Memorization in Code LLMs Through Token-Level Characterization",
        "rating": "-10",
        "keywords": [],
        "abstract": "Code Large Language Models (LLMs) have demonstrated remarkable capabilities in generating, understanding, and manipulating programming code. However, their training process inadvertently leads to the memorization of sensitive information, posing severe privacy risks. Existing studies on memorization in LLMs primarily rely on prompt engineering techniques, which suffer from limitations such as widespread hallucination and inefficient extraction of the target sensitive information. In this paper, we present a novel approach to characterize real and fake secrets generated by Code LLMs based on token probabilities. We identify four key characteristics that differentiate genuine secrets from hallucinated ones, providing insights into distinguishing real and fake secrets. To overcome the limitations of existing works, we propose DESEC, a two-stage method that leverages token-level features derived from the identified characteristics to guide the token decoding process. DESEC consists of constructing an offline token scoring model using a proxy Code LLM and employing the scoring model to guide the decoding process by reassigning token likelihoods. Through extensive experiments on four state-of-the-art Code LLMs using a diverse dataset, we demonstrate the superior performance of DESEC in achieving a higher plausible rate and extracting more real secrets compared to existing baselines. Our findings highlight the effectiveness of our token-level approach in enabling an extensive assessment of the privacy leakage risks associated with Code LLMs.",
        "subjects": [
            "cs.CR",
            "cs.SE"
        ],
        "comment": "12 pages, 8 figures"
    },
    {
        "paper id": "2410.08986",
        "abstract url": "https://arxiv.org/abs/2410.08986",
        "title": "Best-of-Both-Worlds Fairness of the Envy-Cycle-Elimination Algorithm",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider the problem of fairly dividing indivisible goods among agents with additive valuations. It is known that an Epistemic EFX and $2/3$-MMS allocation can be obtained using the Envy-Cycle-Elimination (ECE) algorithm. In this work, we explore whether this algorithm can be randomized to also ensure ex-ante proportionality. For two agents, we show that a randomized variant of ECE can compute an ex-post EFX and ex-ante envy-free allocation in near-linear time. However, for three agents, we show that several natural randomization methods for ECE fail to achieve ex-ante proportionality.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09011",
        "abstract url": "https://arxiv.org/abs/2410.09011",
        "title": "Transformer Temperature Management and Voltage Control in Electric Distribution Systems with High Solar PV Penetration",
        "rating": "-10",
        "keywords": [],
        "abstract": "The increasing penetration of photovoltaic (PV) systems in distribution grids can lead to overvoltage and transformer overloading issues. While voltage regulation has been extensively studied and some research has addressed transformer temperature control, there is limited work on simultaneously managing both challenges. This paper addresses this gap by proposing an optimization-based strategy that efficiently manages voltage regulation and transformer temperature while minimizing the curtailment of PV generation. In order to make this problem convex, a relaxation is applied to the transformer temperature dynamics constraint. We also provide analysis to determine under which conditions this relaxation remains tight. The proposed approach is validated through simulations, demonstrating its effectiveness in achieving the desired control objectives.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09031",
        "abstract url": "https://arxiv.org/abs/2410.09031",
        "title": "Improved List Size for Folded Reed-Solomon Codes",
        "rating": "-10",
        "keywords": [],
        "abstract": "Folded Reed-Solomon (FRS) codes are variants of Reed-Solomon codes, known for their optimal list decoding radius. We show explicit FRS codes with rate $R$ that can be list decoded up to radius $1-R-\u03b5$ with lists of size $\\mathcal{O}(1/ \u03b5^2)$. This improves the best known list size among explicit list decoding capacity achieving codes. We also show a more general result that for any $k\\geq 1$, there are explicit FRS codes with rate $R$ and distance $1-R$ that can be list decoded arbitrarily close to radius $\\frac{k}{k+1}(1-R)$ with lists of size $(k-1)^2+1$. Our results are based on a new and simple combinatorial viewpoint of the intersections between Hamming balls and affine subspaces that recovers previously known parameters. We then use folded Wronskian determinants to carry out an inductive proof that yields sharper bounds.",
        "subjects": [
            "cs.IT",
            "cs.CC"
        ],
        "comment": "Accepted to SODA 2025"
    },
    {
        "paper id": "2410.09048",
        "abstract url": "https://arxiv.org/abs/2410.09048",
        "title": "Towards Trustworthy LLMs for Code: A Data-Centric Synergistic Auditing Framework",
        "rating": "-10",
        "keywords": [],
        "abstract": "LLM-powered coding and development assistants have become prevalent to programmers' workflows. However, concerns about the trustworthiness of LLMs for code persist despite their widespread use. Much of the existing research focused on either training or evaluation, raising questions about whether stakeholders in training and evaluation align in their understanding of model trustworthiness and whether they can move toward a unified direction. In this paper, we propose a vision for a unified trustworthiness auditing framework, DataTrust, which adopts a data-centric approach that synergistically emphasizes both training and evaluation data and their correlations. DataTrust aims to connect model trustworthiness indicators in evaluation with data quality indicators in training. It autonomously inspects training data and evaluates model trustworthiness using synthesized data, attributing potential causes from specific evaluation data to corresponding training data and refining indicator connections. Additionally, a trustworthiness arena powered by DataTrust will engage crowdsourced input and deliver quantitative outcomes. We outline the benefits that various stakeholders can gain from DataTrust and discuss the challenges and opportunities it presents.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Short Vision Paper"
    },
    {
        "paper id": "2410.09139",
        "abstract url": "https://arxiv.org/abs/2410.09139",
        "title": "\"ChatGPT, Don't Tell Me What to Do\": Designing AI for Context Analysis in Humanitarian Frontline Negotiations",
        "rating": "-10",
        "keywords": [],
        "abstract": "Frontline humanitarian negotiators are increasingly exploring ways to use AI tools in their workflows. However, current AI-tools in negotiation primarily focus on outcomes, neglecting crucial aspects of the negotiation process. Through iterative co-design with experienced frontline negotiators (n=32), we found that flexible tools that enable contextualizing cases and exploring options (with associated risks) are more effective than those providing direct recommendations of negotiation strategies. Surprisingly, negotiators demonstrated tolerance for occasional hallucinations and biases of AI. Our findings suggest that the design of AI-assisted negotiation tools should build on practitioners' existing practices, such as weighing different compromises and validating information with peers. This approach leverages negotiators' expertise while enhancing their decision-making capabilities. We call for technologists to learn from and collaborate closely with frontline negotiators, applying these insights to future AI designs and jointly developing professional guidelines for AI use in humanitarian negotiations.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09166",
        "abstract url": "https://arxiv.org/abs/2410.09166",
        "title": "Towards Input-Convex Neural Network Modeling for Battery Optimization in Power Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Battery energy storage systems (BESS) play an increasingly vital role in integrating renewable generation into power grids due to their ability to dynamically balance supply. Grid-tied batteries typically employ power converters, where part-load efficiencies vary non-linearly. While this non-linearity can be modeled with high accuracy, it poses challenges for optimization, particularly in ensuring computational tractability. In this paper, we consider a non-linear BESS formulation based on the Energy Reservoir Model (ERM). A data-driven approach is introduced with the input-convex neural network (ICNN) to approximate the nonlinear efficiency with a convex function. The epigraph of the convex function is used to engender a convex program for battery ERM optimization. This relaxed ICNN method is applied to two battery optimization use-cases: PV smoothing and revenue maximization, and it is compared with three other ERM formulations (nonlinear, linear, and mixed-integer). Specifically, ICNN-based methods appear to be promising for future battery optimization with desirable feasibility and optimality outcomes across both use-cases.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09172",
        "abstract url": "https://arxiv.org/abs/2410.09172",
        "title": "Testing GPU Numerics: Finding Numerical Differences Between NVIDIA and AMD GPUs",
        "rating": "-10",
        "keywords": [],
        "abstract": "As scientific codes are ported between GPU platforms, continuous testing is required to ensure numerical robustness and identify numerical differences. Compiler-induced numerical differences occur when a program is compiled and run on different GPUs, and the numerical outcomes are different for the same input. We present a study of compiler-induced numerical differences between NVIDIA and AMD GPUs. Our approach uses Varity to generate thousands of short numerical tests in CUDA and HIP, and their inputs; then, we use differential testing to check if the program produced a numerical inconsistency when run on these GPUs. We also use the HIPIFY tool to convert CUDA tests into HIP and check if there are numerical inconsistencies induced by HIPIFY. We generated more than 600,000 tests and found subtle numerical differences that come from (1) math library calls, (2) differences in floating-point precision (FP64 versus FP32), and (3) converting code with HIPIFY.",
        "subjects": [
            "math.NA",
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09201",
        "abstract url": "https://arxiv.org/abs/2410.09201",
        "title": "The Condorcet Dimension of Metric Spaces",
        "rating": "-10",
        "keywords": [],
        "abstract": "A Condorcet winning set is a set of candidates such that no other candidate is preferred by at least half the voters over all members of the set. The Condorcet dimension, which is the minimum cardinality of a Condorcet winning set, is known to be at most logarithmic in the number of candidates. We study the case of elections where voters and candidates are located in a $2$-dimensional space with preferences based upon proximity voting. Our main result is that the Condorcet dimension is at most $3$, under both the Manhattan norm and the infinity norm, natural measures in electoral systems.",
        "subjects": [
            "cs.GT",
            "cs.MA"
        ],
        "comment": "9 pages"
    },
    {
        "paper id": "2410.09222",
        "abstract url": "https://arxiv.org/abs/2410.09222",
        "title": "Optimal Feedback Stabilizing Control of Bounded Jacobian Discrete-Time Systems via Interval Observers",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper addresses optimal feedback stabilizing control for bounded Jacobian nonlinear discrete-time (DT) systems with nonlinear observations, affected by state and process noise. Instead of directly stabilizing the uncertain system, we propose stabilizing a higher-dimensional interval observer whose states enclose the true system states. Our nonlinear control approach introduces additional flexibility compared to linear methods, compensating for system nonlinearities and allowing potentially tighter closed-loop intervals. We also establish a separation principle, enabling independent design of observer and control gains, and derive tractable linear matrix inequalities, resulting in a stable closed-loop system.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Submitted to ACC'25"
    },
    {
        "paper id": "2410.09231",
        "abstract url": "https://arxiv.org/abs/2410.09231",
        "title": "On The MCMC Performance In Bernoulli Group Testing And The Random Max Set-Cover Problem",
        "rating": "-10",
        "keywords": [],
        "abstract": "The group testing problem is a canonical inference task where one seeks to identify $k$ infected individuals out of a population of $n$ people, based on the outcomes of $m$ group tests. Of particular interest is the case of Bernoulli group testing (BGT), where each individual participates in each test independently and with a fixed probability. BGT is known to be an \"information-theoretically\" optimal design, as there exists a decoder that can identify with high probability as $n$ grows the infected individuals using $m^*=\\log_2 \\binom{n}{k}$ BGT tests, which is the minimum required number of tests among \\emph{all} group testing designs. An important open question in the field is if a polynomial-time decoder exists for BGT which succeeds also with $m^*$ samples. In a recent paper (Iliopoulos, Zadik COLT '21) some evidence was presented (but no proof) that a simple low-temperature MCMC method could succeed. The evidence was based on a first-moment (or \"annealed\") analysis of the landscape, as well as simulations that show the MCMC success for $n \\approx 1000s$. In this work, we prove that, despite the intriguing success in simulations for small $n$, the class of MCMC methods proposed in previous work for BGT with $m^*$ samples takes super-polynomial-in-$n$ time to identify the infected individuals, when $k=n^\u03b1$ for $\u03b1\\in (0,1)$ small enough. Towards obtaining our results, we establish the tight max-satisfiability thresholds of the random $k$-set cover problem, a result of potentially independent interest in the study of random constraint satisfaction problems.",
        "subjects": [
            "math.ST",
            "cs.DS",
            "math.PR"
        ],
        "comment": "71 pages"
    },
    {
        "paper id": "2410.09235",
        "abstract url": "https://arxiv.org/abs/2410.09235",
        "title": "Relative-error monotonicity testing",
        "rating": "-10",
        "keywords": [],
        "abstract": "The standard model of Boolean function property testing is not well suited for testing $\\textit{sparse}$ functions which have few satisfying assignments, since every such function is close (in the usual Hamming distance metric) to the constant-0 function. In this work we propose and investigate a new model for property testing of Boolean functions, called $\\textit{relative-error testing}$, which provides a natural framework for testing sparse functions. This new model defines the distance between two functions $f, g: \\{0,1\\}^n \\to \\{0,1\\}$ to be $$\\textsf{reldist}(f,g) := { \\frac{|f^{-1}(1) \\triangle g^{-1}(1)|} {|f^{-1}(1)|}}.$$ This is a more demanding distance measure than the usual Hamming distance ${ {|f^{-1}(1) \\triangle g^{-1}(1)|}/{2^n}}$ when $|f^{-1}(1)| \\ll 2^n$; to compensate for this, algorithms in the new model have access both to a black-box oracle for the function $f$ being tested and to a source of independent uniform satisfying assignments of $f$. In this paper we first give a few general results about the relative-error testing model; then, as our main technical contribution, we give a detailed study of algorithms and lower bounds for relative-error testing of $\\textit{monotone}$ Boolean functions. We give upper and lower bounds which are parameterized by $N=|f^{-1}(1)|$, the sparsity of the function $f$ being tested. Our results show that there are interesting differences between relative-error monotonicity testing of sparse Boolean functions, and monotonicity testing in the standard model. These results motivate further study of the testability of Boolean function properties in the relative-error model.",
        "subjects": [
            "cs.CC",
            "cs.DM",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09241",
        "abstract url": "https://arxiv.org/abs/2410.09241",
        "title": "Large Language Models for Energy-Efficient Code: Emerging Results and Future Directions",
        "rating": "-10",
        "keywords": [],
        "abstract": "Energy-efficient software helps improve mobile device experiences and reduce the carbon footprint of data centers. However, energy goals are often de-prioritized in order to meet other requirements. We take inspiration from recent work exploring the use of large language models (LLMs) for different software engineering activities. We propose a novel application of LLMs: as code optimizers for energy efficiency. We describe and evaluate a prototype, finding that over 6 small programs our system can improve energy efficiency in 3 of them, up to 2x better than compiler optimizations alone. From our experience, we identify some of the challenges of energy-efficient LLM code optimization and propose a research agenda.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09285",
        "abstract url": "https://arxiv.org/abs/2410.09285",
        "title": "Contribution Rate Imputation Theory: A Conceptual Model",
        "rating": "-10",
        "keywords": [],
        "abstract": "The \"Theory of Contribution Rate Imputation\" estimates developer effort by analyzing historical commit data and typical development rates. Building on the Time-Delta Method, this approach calculates unobserved work periods using metrics like cyclomatic complexity and Levenshtein distance. The Contribution Rate Imputation Method (CRIM) improves upon traditional productivity metrics, offering a more accurate estimation of person-hours spent on software contributions. This method provides valuable insights for project management and resource allocation, helping organizations better understand and optimize developer productivity.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "14 pages"
    },
    {
        "paper id": "2410.09296",
        "abstract url": "https://arxiv.org/abs/2410.09296",
        "title": "The 2020 United States Decennial Census Is More Private Than You (Might) Think",
        "rating": "-10",
        "keywords": [],
        "abstract": "The U.S. Decennial Census serves as the foundation for many high-profile policy decision-making processes, including federal funding allocation and redistricting. In 2020, the Census Bureau adopted differential privacy to protect the confidentiality of individual responses through a disclosure avoidance system that injects noise into census data tabulations. The Bureau subsequently posed an open question: Could sharper privacy guarantees be obtained for the 2020 U.S. Census compared to their published guarantees, or equivalently, had the nominal privacy budgets been fully utilized? In this paper, we affirmatively address this open problem by demonstrating that between 8.50% and 13.76% of the privacy budget for the 2020 U.S. Census remains unused for each of the eight geographical levels, from the national level down to the block level. This finding is made possible through our precise tracking of privacy losses using $f$-differential privacy, applied to the composition of private queries across various geographical levels. Our analysis indicates that the Census Bureau introduced unnecessarily high levels of injected noise to achieve the claimed privacy guarantee for the 2020 U.S. Census. Consequently, our results enable the Bureau to reduce noise variances by 15.08% to 24.82% while maintaining the same privacy budget for each geographical level, thereby enhancing the accuracy of privatized census statistics. We empirically demonstrate that reducing noise injection into census statistics mitigates distortion caused by privacy constraints in downstream applications of private census data, illustrated through a study examining the relationship between earnings and education.",
        "subjects": [
            "cs.CR",
            "cs.DS",
            "stat.AP",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09305",
        "abstract url": "https://arxiv.org/abs/2410.09305",
        "title": "Analyzing Wage Theft in Day Labor Markets via Principal Agent Models",
        "rating": "-10",
        "keywords": [],
        "abstract": "In day labor markets, workers are particularly vulnerable to wage theft. This paper introduces a principal-agent model to analyze the conditions required to mitigate wage theft through fines and establishes the necessary and sufficient conditions to reduce theft. We find that the fines necessary to eliminate theft are significantly larger than those imposed by current labor laws, making wage theft likely to persist under penalty-based methods alone. Through numerical analysis, we show how wage theft disproportionately affects workers with lower reservation utilities and observe that workers with similar reservation utilities experience comparable impacts, regardless of their skill levels. To address the limitations of penalty-based approaches, we extend the model to a dynamic game incorporating worker awareness. We prove that wage theft can be fully eliminated if workers accurately predict theft using historical data and employers follow optimal fixed wage strategy. Additionally, sharing wage theft information becomes an effective long-term solution when employers use any given fixed wage strategies, emphasizing the importance of raising worker awareness through various channels.",
        "subjects": [
            "math.OC",
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09310",
        "abstract url": "https://arxiv.org/abs/2410.09310",
        "title": "Directed Testing of ORAN using a Partially Specified Declarative Digital Twin",
        "rating": "-10",
        "keywords": [],
        "abstract": "Real Time performance testing can be divided into two distinct parts: system test and algorithm test. System test checks that the right functions operate on the right data within power, latency, and other constraints under all conditions. Major RAN OEMs, put as much effort into system test and debug as they do into algorithm test, to ensure a competitive product. An algorithm tester will provide little insight into real time and hardware-software (HW-SW) capacity as it is unaware of the system implementation. In this paper we present an innovative Digital Twin technology, which we call Declarative Digital Twin (DDT). A DDT can describe the system requirements of the RAN such that critical corner cases can be found via automation, that would normally be missed by conventional testing. This is possible even when the RAN requirements are only partially specified. We present a Domain Specific Language (DSL) for declarative description of the RAN and show results from an automated solver that demonstrate how potential HW-SW implementation related corner cases can be identified from the DDT of an ORAN DU.",
        "subjects": [
            "cs.PL",
            "cs.SE",
            "eess.SY"
        ],
        "comment": "5 pages, 7 figures, 1 table, presented at the First RitiRAN Workshop co-located with VTC Fall 2024"
    },
    {
        "paper id": "2410.09320",
        "abstract url": "https://arxiv.org/abs/2410.09320",
        "title": "Towards an Incident Management Framework in Proprietary Software Ecosystems",
        "rating": "-10",
        "keywords": [],
        "abstract": "In the evolving landscape of Software Engineering, the paradigm of software ecosystems has emerged, giving rise to proprietary software ecosystems (PSECO), with their central organizations known as keystones. PSECO is characterized by the contribution of various technologies produced as private and protected by intellectual property and confidentiality agreements, centered on common technological platforms. Sustaining these PSECO technological platforms is vital, as any incident can have substantial repercussions. This work introduces a framework for incident management to support the organizations' management teams in the PSECO context, called IM Framework. The IM Framework was developed in close collaboration with practitioners across a large international organization. We grounded the IM Framework based on the results of a rapid review study that retrieved 293 studies, of which 23 were selected after applying review procedures. This framework comprises five core categories: organizational goals, practices, success factors, associated benefits, and prevalent barriers. The IM Framework offers practical guidance for the PSECO management team, focusing on real-world applications to enhance reliability and resilience in a complex and dynamic software environment. Our study also promises to fill the gap in incident management governance by supporting the PSECO organization's management team and maintaining robust technological platforms amidst evolving business demands and market pressures.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "51 pages, 5 figures"
    },
    {
        "paper id": "2410.09321",
        "abstract url": "https://arxiv.org/abs/2410.09321",
        "title": "Simultaneously Approximating All Norms for Massively Parallel Correlation Clustering",
        "rating": "-10",
        "keywords": [],
        "abstract": "We revisit the simultaneous approximation model for the correlation clustering problem introduced by Davies, Moseley, and Newman[DMN24]. The objective is to find a clustering that minimizes given norms of the disagreement vector over all vertices. We present an efficient algorithm that produces a clustering that is simultaneously a $63.3$-approximation for all monotone symmetric norms. This significantly improves upon the previous approximation ratio of $6348$ due to Davies, Moseley, and Newman[DMN24], which works only for $\\ell_p$-norms. To achieve this result, we first reduce the problem to approximating all top-$k$ norms simultaneously, using the connection between monotone symmetric norms and top-$k$ norms established by Chakrabarty and Swamy [CS19]. Then we develop a novel procedure that constructs a $12.66$-approximate fractional clustering for all top-$k$ norms. Our $63.3$-approximation ratio is obtained by combining this with the $5$-approximate rounding algorithm by Kalhan, Makarychev, and Zhou[KMZ19]. We then demonstrate that with a loss of $\u03b5$ in the approximation ratio, the algorithm can be adapted to run in nearly linear time and in the MPC (massively parallel computation) model with poly-logarithmic number of rounds. By allowing a further trade-off in the approximation ratio to $(359+\u03b5)$, the number of MPC rounds can be reduced to a constant.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09341",
        "abstract url": "https://arxiv.org/abs/2410.09341",
        "title": "BAKUP: Automated, Flexible, and Capital-Efficient Insurance Protocol for Decentralized Finance",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper introduces BAKUP, a smart contract insurance design for decentralized finance users to mitigate risks arising from platform vulnerabilities. While providing automated claim payout, BAKUP utilizes a modular structure to harmonize three key features: the platform's resilience against vulnerabilities, the flexibility of underwritten policies, and capital efficiency. An immutable core module performs capital accounting while ensuring robustness against external vulnerabilities, a customizable oracle module enables the underwriting of novel policies, and an optional and peripheral yield module allows users to independently manage additional yield. The implementation incorporates binary conditional tokens that are tradable on automated market maker (AMM)-based exchanges. Finally, the paper examines specific liquidity provision strategies for the conditional tokens, demonstrating that a conservative strategy and parameterization can effectively reduce the divergence loss of liquidity providers by more than 47 % compared to a naive strategy in the worst-case scenario.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "9 pages"
    },
    {
        "paper id": "2410.09358",
        "abstract url": "https://arxiv.org/abs/2410.09358",
        "title": "Exploiting Moving Arrays for Near-Field Sensing",
        "rating": "-10",
        "keywords": [],
        "abstract": "This letter exploits moving arrays to enable nearfield multiple-input multiple-output (MIMO) sensing via a limited number of antenna elements. We consider a scenario where a base station (BS) is equipped with a uniform linear array (ULA) on a moving platform. The objective is to locate a point target in the two-dimensional (2D) space by leveraging the near-field channel characteristics created by the movement of antenna arrays. Under this setup, we analyze the Cramer-Rao bound (CRB) for estimating the target's 2D coordinate, which provides the fundamental sensing performance limits for localization. It is revealed that our proposed design with a moving array achieves a CRB that is proportional to the CRB obtained by an equivalent extremely large ULA matching the platform's size. This shows that the movement of antenna array significantly enlarges its effective aperture to enable near-field sensing. Numerical results show that the proposed moving array design substantially enhances the target estimation performance compared to the conventional fixed array benchmark.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "5 pages, 7 figures"
    }
]