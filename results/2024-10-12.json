[
    {
        "paper id": "2410.09416",
        "abstract url": "https://arxiv.org/abs/2410.09416",
        "title": "Can Vision-Language Models Replace Human Annotators: A Case Study with CelebA Dataset",
        "rating": "2.5",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "cs.AI",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "This study evaluates the capability of Vision-Language Models (VLMs) in image data annotation by comparing their performance on the CelebA dataset in terms of quality and cost-effectiveness against manual annotation. Annotations from the state-of-the-art LLaVA-NeXT model on 1000 CelebA images are in 79.5% agreement with the original human annotations. Incorporating re-annotations of disagreed cases into a majority vote boosts AI annotation consistency to 89.1% and even higher for more objective labels. Cost assessments demonstrate that AI annotation significantly reduces expenditures compared to traditional manual methods -- representing less than 1% of the costs for manual annotation in the CelebA dataset. These findings support the potential of VLMs as a viable, cost-effective alternative for specific annotation tasks, reducing both financial burden and ethical concerns associated with large-scale manual data annotation. The AI annotations and re-annotations utilized in this study are available on https://github.com/evev2024/EVEV2024_CelebA.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Accepted by NeurIPS 2024 Workshop (EvalEval 2024)"
    },
    {
        "paper id": "2410.09489",
        "abstract url": "https://arxiv.org/abs/2410.09489",
        "title": "Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks",
        "rating": "2.5",
        "keywords": [
            [
                "parameter efficient",
                "PEFT",
                "efficient fine-tuning"
            ],
            [
                "Visual-Language"
            ],
            [
                "3D"
            ],
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Recent advancements in large language models have demonstrated enhanced capabilities in visual reasoning tasks by employing additional encoders for aligning different modalities. While the Q-Former has been widely used as a general encoder for aligning several modalities including image, video, audio, and 3D with large language models, previous works on its efficient training and the analysis of its individual components have been limited. In this work, we investigate the effectiveness of parameter efficient fine-tuning (PEFT) the Q-Former using InstructBLIP with visual reasoning benchmarks ScienceQA and IconQA. We observe that applying PEFT to the Q-Former achieves comparable performance to full fine-tuning using under 2% of the trainable parameters. Additionally, we employ AdaLoRA for dynamic parameter budget reallocation to examine the relative importance of the Q-Former's sublayers with 4 different benchmarks. Our findings reveal that the self-attention layers are noticeably more important in perceptual visual-language reasoning tasks, and relative importance of FFN layers depends on the complexity of visual-language patterns involved in tasks. The code is available at https://github.com/AttentionX/InstructBLIP_PEFT.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "EMNLP 2024 Findings"
    },
    {
        "paper id": "2410.09421",
        "abstract url": "https://arxiv.org/abs/2410.09421",
        "title": "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment",
        "rating": "1.5",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "attacks"
            ],
            [
                "cs.CV",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "As large vision-language models (LVLMs) evolve rapidly, the demand for high-quality and diverse data to align these models becomes increasingly crucial. However, the creation of such data with human supervision proves costly and time-intensive. In this paper, we investigate the efficacy of AI feedback to scale supervision for aligning LVLMs. We introduce VLFeedback, the first large-scale vision-language feedback dataset, comprising over 82K multi-modal instructions and comprehensive rationales generated by off-the-shelf models without human annotations. To evaluate the effectiveness of AI feedback for vision-language alignment, we train Silkie, an LVLM fine-tuned via direct preference optimization on VLFeedback. Silkie showcases exceptional performance regarding helpfulness, visual faithfulness, and safety metrics. It outperforms its base model by 6.9\\% and 9.5\\% in perception and cognition tasks, reduces hallucination issues on MMHal-Bench, and exhibits enhanced resilience against red-teaming attacks. Furthermore, our analysis underscores the advantage of AI feedback, particularly in fostering preference diversity to deliver more comprehensive improvements. Our dataset, training code and models are available at https://vlf-silkie.github.io.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": "EMNLP 2024 Main Conference camera-ready version. This article supersedes arXiv:2312.10665"
    },
    {
        "paper id": "2410.09432",
        "abstract url": "https://arxiv.org/abs/2410.09432",
        "title": "Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation Models",
        "rating": "1.5",
        "keywords": [
            [
                "Efficient Fine-Tuning"
            ],
            [
                "federated learning"
            ],
            [
                "cs.CV",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Low-Rank Adaptation (LoRA) is a popular technique for efficient fine-tuning of foundation models. However, applying LoRA in federated learning environments, where data is distributed across multiple clients, presents unique challenges. Existing methods rely on traditional federated averaging of LoRA adapters, resulting in inexact updates. To address this, we propose Federated Exact LoRA, or FedEx-LoRA, which adds a residual error term to the pretrained frozen weight matrix. Our approach achieves exact updates with minimal computational and communication overhead, preserving LoRA's efficiency. We evaluate the method on various Natural Language Understanding (NLU) and Natural Language Generation (NLG) tasks, showing consistent performance gains over state-of-the-art methods across multiple settings. Through extensive analysis, we quantify that the deviations in updates from the ideal solution are significant, highlighting the need for exact aggregation. Our method's simplicity, efficiency, and broad applicability position it as a promising solution for accurate and effective federated fine-tuning of foundation models.",
        "subjects": [
            "cs.DC",
            "cs.CL",
            "cs.CV"
        ],
        "comment": "RS and KP contributed equally to this work: 18 Pages, 9 Figures, and 8 Tables. Another version of the paper accepted at NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability"
    },
    {
        "paper id": "2410.09454",
        "abstract url": "https://arxiv.org/abs/2410.09454",
        "title": "Skipping Computations in Multimodal LLMs",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in both textual and multimodal domains. However, this success often comes with substantial computational costs, particularly when handling lengthy sequences of multimodal inputs. This has sparked many efforts focusing on enhancing efficiency during training and inference. In this study, we investigate the computation redundancy in Multimodal Large Language Models (MLLMs) during inference. We propose different methods to skip computations, such as skipping entire blocks, FFN or self-attention (SA) layers. Additionally, we explore parallelizing certain layers, such as FFN and SA layers. Our findings validate that (1) significant amount of computations can be avoided at inference time, especially for tasks such as Visual Question Answering (VQA). (2) Skipping computations during training can recover 97% of the original performance, even when skipping half of the blocks or removing 70% of the weights. Alternatively, (3) properly training with smaller LLMs can yield comparable performance to LLMs 2 or 3 times larger. To conclude, we extend our investigation to recent MLLMs, such as LLaVA-1.5, showing similar observations. Our work show that there is redundant computations inside MLLMs and thus the potential for significantly improving inference costs without sacrificing performance. The code is available here: https://github.com/mshukor/ima-lmms.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Accepted at NeurIPS 2024 Workshop RBFM. Code: https://github.com/mshukor/ima-lmms"
    },
    {
        "paper id": "2410.09527",
        "abstract url": "https://arxiv.org/abs/2410.09527",
        "title": "LexSumm and LexT5: Benchmarking and Modeling Legal Summarization Tasks in English",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "In the evolving NLP landscape, benchmarks serve as yardsticks for gauging progress. However, existing Legal NLP benchmarks only focus on predictive tasks, overlooking generative tasks. This work curates LexSumm, a benchmark designed for evaluating legal summarization tasks in English. It comprises eight English legal summarization datasets, from diverse jurisdictions, such as the US, UK, EU and India. Additionally, we release LexT5, legal oriented sequence-to-sequence model, addressing the limitation of the existing BERT-style encoder-only models in the legal domain. We assess its capabilities through zero-shot probing on LegalLAMA and fine-tuning on LexSumm. Our analysis reveals abstraction and faithfulness errors even in summaries generated by zero-shot LLMs, indicating opportunities for further improvements. LexSumm benchmark and LexT5 model are available at https://github.com/TUMLegalTech/LexSumm-LexT5.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to NLLP Workshop, EMNLP 2024"
    },
    {
        "paper id": "2410.09554",
        "abstract url": "https://arxiv.org/abs/2410.09554",
        "title": "Exploring space efficiency in a tree-based linear model for extreme multi-label classification",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Extreme multi-label classification (XMC) aims to identify relevant subsets from numerous labels. Among the various approaches for XMC, tree-based linear models are effective due to their superior efficiency and simplicity. However, the space complexity of tree-based methods is not well-studied. Many past works assume that storing the model is not affordable and apply techniques such as pruning to save space, which may lead to performance loss. In this work, we conduct both theoretical and empirical analyses on the space to store a tree model under the assumption of sparse data, a condition frequently met in text data. We found that, some features may be unused when training binary classifiers in a tree method, resulting in zero values in the weight vectors. Hence, storing only non-zero elements can greatly save space. Our experimental results indicate that tree models can achieve up to a 95% reduction in storage space compared to the standard one-vs-rest method for multi-label text classification. Our research provides a simple procedure to estimate the size of a tree model before training any classifier in the tree nodes. Then, if the model size is already acceptable, this approach can help avoid modifying the model through weight pruning or other techniques.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": "EMNLP 2024"
    },
    {
        "paper id": "2410.09605",
        "abstract url": "https://arxiv.org/abs/2410.09605",
        "title": "Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Understanding the training dynamics of transformers is important to explain the impressive capabilities behind large language models. In this work, we study the dynamics of training a shallow transformer on a task of recognizing co-occurrence of two designated words. In the literature of studying training dynamics of transformers, several simplifications are commonly adopted such as weight reparameterization, attention linearization, special initialization, and lazy regime. In contrast, we analyze the gradient flow dynamics of simultaneously training three attention matrices and a linear MLP layer from random initialization, and provide a framework of analyzing such dynamics via a coupled dynamical system. We establish near minimum loss and characterize the attention model after training. We discover that gradient flow serves as an inherent mechanism that naturally divide the training process into two phases. In Phase 1, the linear MLP quickly aligns with the two target signals for correct classification, whereas the softmax attention remains almost unchanged. In Phase 2, the attention matrices and the MLP evolve jointly to enlarge the classification margin and reduce the loss to a near minimum value. Technically, we prove a novel property of the gradient flow, termed \\textit{automatic balancing of gradients}, which enables the loss values of different samples to decrease almost at the same rate and further facilitates the proof of near minimum training loss. We also conduct experiments to verify our theoretical results.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": "Accepted by NeurIPS 2024"
    },
    {
        "paper id": "2410.09615",
        "abstract url": "https://arxiv.org/abs/2410.09615",
        "title": "SLiM: One-shot Quantized Sparse Plus Low-rank Approximation of LLMs",
        "rating": "1.5",
        "keywords": [
            [
                "parameter-efficient",
                "efficient fine-tuning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have revolutionized natural language understanding and generation tasks but suffer from high memory consumption and slow inference times due to their large parameter sizes. Traditional model compression techniques, such as quantization and pruning, mitigate these issues but often require retraining to maintain accuracy, which is computationally expensive. This paper introduces SLiM, a novel approach for compressing LLMs using a one-shot Quantized Sparse Plus Low-rank Approximation. SLiM eliminates the need for costly retraining by combining a symmetric quantization method (SLiM-Quant) with a saliency-based low-rank approximation. Our method reduces quantization error while leveraging sparse representations compatible with accelerated hardware architectures. Additionally, we propose a parameter-efficient fine-tuning recipe that significantly reduces overhead compared to conventional quantization-aware training. SLiM achieves up to a 5.4% improvement in model accuracy for sparsity patterns like 2:4, and the fine-tuning step further enhances accuracy by up to 5.8%, demonstrating state-of-the-art performance. This work provides a pathway for efficiently deploying large models in memory-constrained environments without compromising accuracy.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.PF"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09623",
        "abstract url": "https://arxiv.org/abs/2410.09623",
        "title": "Quebec Automobile Insurance Question-Answering With Retrieval-Augmented Generation",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Large Language Models (LLMs) perform outstandingly in various downstream tasks, and the use of the Retrieval-Augmented Generation (RAG) architecture has been shown to improve performance for legal question answering (Nuruzzaman and Hussain, 2020; Louis et al., 2024). However, there are limited applications in insurance questions-answering, a specific type of legal document. This paper introduces two corpora: the Quebec Automobile Insurance Expertise Reference Corpus and a set of 82 Expert Answers to Layperson Automobile Insurance Questions. Our study leverages both corpora to automatically and manually assess a GPT4-o, a state-of-the-art LLM, to answer Quebec automobile insurance questions. Our results demonstrate that, on average, using our expertise reference corpus generates better responses on both automatic and manual evaluation metrics. However, they also highlight that LLM QA is unreliable enough for mass utilization in critical areas. Indeed, our results show that between 5% to 13% of answered questions include a false statement that could lead to customer misunderstanding.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted to NLLP 2024 EMNLP workshop"
    },
    {
        "paper id": "2410.09649",
        "abstract url": "https://arxiv.org/abs/2410.09649",
        "title": "Learning the Bitter Lesson: Empirical Evidence from 20 Years of CVPR Proceedings",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "This study examines the alignment of \\emph{Conference on Computer Vision and Pattern Recognition} (CVPR) research with the principles of the \"bitter lesson\" proposed by Rich Sutton. We analyze two decades of CVPR abstracts and titles using large language models (LLMs) to assess the field's embracement of these principles. Our methodology leverages state-of-the-art natural language processing techniques to systematically evaluate the evolution of research approaches in computer vision. The results reveal significant trends in the adoption of general-purpose learning algorithms and the utilization of increased computational resources. We discuss the implications of these findings for the future direction of computer vision research and its potential impact on broader artificial intelligence development. This work contributes to the ongoing dialogue about the most effective strategies for advancing machine learning and computer vision, offering insights that may guide future research priorities and methodologies in the field.",
        "subjects": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "comment": "NLP4Sceince Workshop, EMNLP 2024"
    },
    {
        "paper id": "2410.09379",
        "abstract url": "https://arxiv.org/abs/2410.09379",
        "title": "Multi-granularity Contrastive Cross-modal Collaborative Generation for End-to-End Long-term Video Question Answering",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Long-term Video Question Answering (VideoQA) is a challenging vision-and-language bridging task focusing on semantic understanding of untrimmed long-term videos and diverse free-form questions, simultaneously emphasizing comprehensive cross-modal reasoning to yield precise answers. The canonical approaches often rely on off-the-shelf feature extractors to detour the expensive computation overhead, but often result in domain-independent modality-unrelated representations. Furthermore, the inherent gradient blocking between unimodal comprehension and cross-modal interaction hinders reliable answer generation. In contrast, recent emerging successful video-language pre-training models enable cost-effective end-to-end modeling but fall short in domain-specific ratiocination and exhibit disparities in task formulation. Toward this end, we present an entirely end-to-end solution for long-term VideoQA: Multi-granularity Contrastive cross-modal collaborative Generation (MCG) model. To derive discriminative representations possessing high visual concepts, we introduce Joint Unimodal Modeling (JUM) on a clip-bone architecture and leverage Multi-granularity Contrastive Learning (MCL) to harness the intrinsically or explicitly exhibited semantic correspondences. To alleviate the task formulation discrepancy problem, we propose a Cross-modal Collaborative Generation (CCG) module to reformulate VideoQA as a generative task instead of the conventional classification scheme, empowering the model with the capability for cross-modal high-semantic fusion and generation so as to rationalize and answer. Extensive experiments conducted on six publicly available VideoQA datasets underscore the superiority of our proposed method.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Transactions on Image Processing"
    },
    {
        "paper id": "2410.09380",
        "abstract url": "https://arxiv.org/abs/2410.09380",
        "title": "Prompting Video-Language Foundation Models with Domain-specific Fine-grained Heuristics for Video Question Answering",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Video Question Answering (VideoQA) represents a crucial intersection between video understanding and language processing, requiring both discriminative unimodal comprehension and sophisticated cross-modal interaction for accurate inference. Despite advancements in multi-modal pre-trained models and video-language foundation models, these systems often struggle with domain-specific VideoQA due to their generalized pre-training objectives. Addressing this gap necessitates bridging the divide between broad cross-modal knowledge and the specific inference demands of VideoQA tasks. To this end, we introduce HeurVidQA, a framework that leverages domain-specific entity-action heuristics to refine pre-trained video-language foundation models. Our approach treats these models as implicit knowledge engines, employing domain-specific entity-action prompters to direct the model's focus toward precise cues that enhance reasoning. By delivering fine-grained heuristics, we improve the model's ability to identify and interpret key entities and actions, thereby enhancing its reasoning capabilities. Extensive evaluations across multiple VideoQA datasets demonstrate that our method significantly outperforms existing models, underscoring the importance of integrating domain-specific knowledge into video-language models for more accurate and context-aware VideoQA.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
        "paper id": "2410.09382",
        "abstract url": "https://arxiv.org/abs/2410.09382",
        "title": "CLIP-SCGI: Synthesized Caption-Guided Inversion for Person Re-Identification",
        "rating": "1",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "Re-Identification"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Person re-identification (ReID) has recently benefited from large pretrained vision-language models such as Contrastive Language-Image Pre-Training (CLIP). However, the absence of concrete descriptions necessitates the use of implicit text embeddings, which demand complicated and inefficient training strategies. To address this issue, we first propose one straightforward solution by leveraging existing image captioning models to generate pseudo captions for person images, and thereby boost person re-identification with large vision language models. Using models like the Large Language and Vision Assistant (LLAVA), we generate high-quality captions based on fixed templates that capture key semantic attributes such as gender, clothing, and age. By augmenting ReID training sets from uni-modality (image) to bi-modality (image and text), we introduce CLIP-SCGI, a simple yet effective framework that leverages synthesized captions to guide the learning of discriminative and robust representations. Built on CLIP, CLIP-SCGI fuses image and text embeddings through two modules to enhance the training process. To address quality issues in generated captions, we introduce a caption-guided inversion module that captures semantic attributes from images by converting relevant visual information into pseudo-word tokens based on the descriptions. This approach helps the model better capture key information and focus on relevant regions. The extracted features are then utilized in a cross-modal fusion module, guiding the model to focus on regions semantically consistent with the caption, thereby facilitating the optimization of the visual encoder to extract discriminative and robust representations. Extensive experiments on four popular ReID benchmarks demonstrate that CLIP-SCGI outperforms the state-of-the-art by a significant margin.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09397",
        "abstract url": "https://arxiv.org/abs/2410.09397",
        "title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for Backward Passes",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in processing long-context information. However, the quadratic complexity of attention computation with respect to sequence length poses significant computational challenges, and I/O aware algorithms have been proposed. This paper presents a comprehensive analysis of the I/O complexity for attention mechanisms, focusing on backward passes by categorizing into small and large cache scenarios. Using the red-blue pebble game framework, we establish tight bounds on I/O complexity across all cache sizes. We confirm that the de facto standard I/O aware algorithm FlashAttention is optimal for both forward and backward passes for the large cache size scenario. For small cache sizes, we provide an algorithm that improves over existing methods and achieves the tight bounds. Additionally, we extend our analysis to sparse attention, a mainstream speeding-up approach, deriving fine-grained lower bounds for both forward and backward passes and both small and large caches. Our findings complete the theoretical foundation for I/O complexity in attention mechanisms, offering insights for designing efficient algorithms of LLM training and inference.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CC",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09398",
        "abstract url": "https://arxiv.org/abs/2410.09398",
        "title": "MITA: Bridging the Gap between Model and Data for Test-time Adaptation",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Test-Time Adaptation (TTA) has emerged as a promising paradigm for enhancing the generalizability of models. However, existing mainstream TTA methods, predominantly operating at batch level, often exhibit suboptimal performance in complex real-world scenarios, particularly when confronting outliers or mixed distributions. This phenomenon stems from a pronounced over-reliance on statistical patterns over the distinct characteristics of individual instances, resulting in a divergence between the distribution captured by the model and data characteristics. To address this challenge, we propose Meet-In-The-Middle based Test-Time Adaptation ($\\textbf{MITA}$), which introduces energy-based optimization to encourage mutual adaptation of the model and data from opposing directions, thereby meeting in the middle. MITA pioneers a significant departure from traditional approaches that focus solely on aligning the model to the data, facilitating a more effective bridging of the gap between model's distribution and data characteristics. Comprehensive experiments with MITA across three distinct scenarios (Outlier, Mixture, and Pure) demonstrate its superior performance over SOTA methods, highlighting its potential to significantly enhance generalizability in practical applications.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09403",
        "abstract url": "https://arxiv.org/abs/2410.09403",
        "title": "Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "The rapid advancement of scientific progress requires innovative tools that can accelerate discovery. While recent AI methods, particularly large language models (LLMs), have shown promise in tasks such as hypothesis generation and experimental design, they fall short in replicating the collaborative nature of real-world scientific practices, where diverse teams of experts work together to tackle complex problems. To address the limitation, we propose an LLM-based multi-agent system, i.e., Virtual Scientists (VirSci), designed to mimic the teamwork inherent in scientific research. VirSci organizes a team of agents to collaboratively generate, evaluate, and refine research ideas. Through comprehensive experiments, we demonstrate that this multi-agent approach outperforms the state-of-the-art method in producing novel and impactful scientific ideas, showing potential in aligning with key insights in the Science of Science field. Our findings suggest that integrating collaborative agents can lead to more innovative scientific outputs, offering a robust system for autonomous scientific discovery.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09407",
        "abstract url": "https://arxiv.org/abs/2410.09407",
        "title": "CAMPHOR: Collaborative Agents for Multi-input Planning and High-Order Reasoning On Device",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "While server-side Large Language Models (LLMs) demonstrate proficiency in function calling and complex reasoning, deploying Small Language Models (SLMs) directly on devices brings opportunities to improve latency and privacy but also introduces unique challenges for accuracy and memory. We introduce CAMPHOR, an innovative on-device SLM multi-agent framework designed to handle multiple user inputs and reason over personal context locally, ensuring privacy is maintained. CAMPHOR employs a hierarchical architecture where a high-order reasoning agent decomposes complex tasks and coordinates expert agents responsible for personal context retrieval, tool interaction, and dynamic plan generation. By implementing parameter sharing across agents and leveraging prompt compression, we significantly reduce model size, latency, and memory usage. To validate our approach, we present a novel dataset capturing multi-agent task trajectories centered on personalized mobile assistant use-cases. Our experiments reveal that fine-tuned SLM agents not only surpass closed-source LLMs in task completion F1 by~35\\% but also eliminate the need for server-device communication, all while enhancing privacy.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09409",
        "abstract url": "https://arxiv.org/abs/2410.09409",
        "title": "Distribution-aware Noisy-label Crack Segmentation",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Road crack segmentation is critical for robotic systems tasked with the inspection, maintenance, and monitoring of road infrastructures. Existing deep learning-based methods for crack segmentation are typically trained on specific datasets, which can lead to significant performance degradation when applied to unseen real-world scenarios. To address this, we introduce the SAM-Adapter, which incorporates the general knowledge of the Segment Anything Model (SAM) into crack segmentation, demonstrating enhanced performance and generalization capabilities. However, the effectiveness of the SAM-Adapter is constrained by noisy labels within small-scale training sets, including omissions and mislabeling of cracks. In this paper, we present an innovative joint learning framework that utilizes distribution-aware domain-specific semantic knowledge to guide the discriminative learning process of the SAM-Adapter. To our knowledge, this is the first approach that effectively minimizes the adverse effects of noisy labels on the supervised learning of the SAM-Adapter. Our experimental results on two public pavement crack segmentation datasets confirm that our method significantly outperforms existing state-of-the-art techniques. Furthermore, evaluations on the completely unseen CFD dataset demonstrate the high cross-domain generalization capability of our model, underscoring its potential for practical applications in crack segmentation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09412",
        "abstract url": "https://arxiv.org/abs/2410.09412",
        "title": "FB-Bench: A Fine-Grained Multi-Task Benchmark for Evaluating LLMs' Responsiveness to Human Feedback",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Human feedback is crucial in the interactions between humans and Large Language Models (LLMs). However, existing research primarily focuses on benchmarking LLMs in single-turn dialogues. Even in benchmarks designed for multi-turn dialogues, the user inputs are often independent, neglecting the nuanced and complex nature of human feedback within real-world usage scenarios. To fill this research gap, we introduce FB-Bench, a fine-grained, multi-task benchmark designed to evaluate LLMs' responsiveness to human feedback in real-world usage scenarios. Drawing from the two main interaction scenarios, FB-Bench comprises 734 meticulously curated samples, encompassing eight task types, five deficiency types of response, and nine feedback types. We extensively evaluate a broad array of popular LLMs, revealing significant variations in their performance across different interaction scenarios. Further analysis indicates that task, human feedback, and deficiencies of previous responses can also significantly impact LLMs' responsiveness. Our findings underscore both the strengths and limitations of current models, providing valuable insights and directions for future research. Both the toolkits and the dataset of FB-Bench are available at https://github.com/PKU-Baichuan-MLSystemLab/FB-Bench.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09418",
        "abstract url": "https://arxiv.org/abs/2410.09418",
        "title": "Beyond Exact Match: Semantically Reassessing Event Extraction by Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Event extraction has gained extensive research attention due to its broad range of applications. However, the current mainstream evaluation method for event extraction relies on token-level exact match, which misjudges numerous semantic-level correct cases. This reliance leads to a significant discrepancy between the evaluated performance of models under exact match criteria and their real performance. To address this problem, we propose RAEE, an automatic evaluation framework that accurately assesses event extraction results at semantic-level instead of token-level. Specifically, RAEE leverages Large Language Models (LLMs) as automatic evaluation agents, incorporating chain-of-thought prompting and an adaptive mechanism to achieve interpretable and adaptive evaluations for precision and recall of triggers and arguments. Extensive experimental results demonstrate that: (1) RAEE achieves a very high correlation with the human average; (2) after reassessing 14 models, including advanced LLMs, on 10 datasets, there is a significant performance gap between exact match and RAEE. The exact match evaluation significantly underestimates the performance of existing event extraction models, particularly underestimating the capabilities of LLMs; (3) fine-grained analysis under RAEE evaluation reveals insightful phenomena worth further exploration. The evaluation toolkit of our proposed RAEE will be publicly released.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09426",
        "abstract url": "https://arxiv.org/abs/2410.09426",
        "title": "FlatQuant: Flatness Matters for LLM Quantization",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Recently, quantization has been widely used for the compression and acceleration of large language models~(LLMs). Due to the outliers in LLMs, it is crucial to flatten weights and activations to minimize quantization error with the equally spaced quantization points. Prior research explores various pre-quantization transformations to suppress outliers, such as per-channel scaling and Hadamard transformation. However, we observe that these transformed weights and activations can still remain steep and outspread. In this paper, we propose FlatQuant (Fast and Learnable Affine Transformation), a new post-training quantization approach to enhance flatness of weights and activations. Our approach identifies optimal affine transformations tailored to each linear layer, calibrated in hours via a lightweight objective. To reduce runtime overhead, we apply Kronecker decomposition to the transformation matrices, and fuse all operations in FlatQuant into a single kernel. Extensive experiments show that FlatQuant sets up a new state-of-the-art quantization benchmark. For instance, it achieves less than $\\textbf{1}\\%$ accuracy drop for W4A4 quantization on the LLaMA-3-70B model, surpassing SpinQuant by $\\textbf{7.5}\\%$. For inference latency, FlatQuant reduces the slowdown induced by pre-quantization transformation from 0.26x of QuaRot to merely $\\textbf{0.07x}$, bringing up to $\\textbf{2.3x}$ speedup for prefill and $\\textbf{1.7x}$ speedup for decoding, respectively. Code is available at: \\url{https://github.com/ruikangliu/FlatQuant}.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "23 pages"
    },
    {
        "paper id": "2410.09428",
        "abstract url": "https://arxiv.org/abs/2410.09428",
        "title": "Declarative Knowledge Distillation from Large Language Models for Visual Question Answering Datasets",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Visual Question Answering (VQA) is the task of answering a question about an image and requires processing multimodal input and reasoning to obtain the answer. Modular solutions that use declarative representations within the reasoning component have a clear advantage over end-to-end trained systems regarding interpretability. The downside is that crafting the rules for such a component can be an additional burden on the developer. We address this challenge by presenting an approach for declarative knowledge distillation from Large Language Models (LLMs). Our method is to prompt an LLM to extend an initial theory on VQA reasoning, given as an answer-set program, to meet the requirements of the VQA task. Examples from the VQA dataset are used to guide the LLM, validate the results, and mend rules if they are not correct by using feedback from the ASP solver. We demonstrate that our approach works on the prominent CLEVR and GQA datasets. Our results confirm that distilling knowledge from LLMs is in fact a promising direction besides data-driven rule learning approaches.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": "Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)"
    },
    {
        "paper id": "2410.09448",
        "abstract url": "https://arxiv.org/abs/2410.09448",
        "title": "Solving the Challenge Set without Solving the Task: On Winograd Schemas as a Test of Pronominal Coreference Resolution",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Challenge sets such as the Winograd Schema Challenge (WSC) are used to benchmark systems' ability to resolve ambiguities in natural language. If one assumes as in existing work that solving a given challenge set is at least as difficult as solving some more general task, then high performance on the challenge set should indicate high performance on the general task overall. However, we show empirically that this assumption of difficulty does not always hold. In particular, we demonstrate that despite the strong performance of prompted language models (LMs) on the WSC and its variants, these same modeling techniques perform relatively poorly at resolving certain pronominal ambiguities attested in OntoNotes and related datasets that are perceived to be easier. Motivated by these findings, we propose a method for ensembling a prompted LM with a supervised, task-specific system that is overall more accurate at resolving pronominal coreference across datasets. Finally, we emphasize that datasets involving the same linguistic phenomenon draw on distinct, but overlapping, capabilities, and evaluating on any one dataset alone does not provide a complete picture of a system's overall capability.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "CoNLL 2024"
    },
    {
        "paper id": "2410.09455",
        "abstract url": "https://arxiv.org/abs/2410.09455",
        "title": "VERITAS-NLI : Validation and Extraction of Reliable Information Through Automated Scraping and Natural Language Inference",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "In today's day and age where information is rapidly spread through online platforms, the rise of fake news poses an alarming threat to the integrity of public discourse, societal trust, and reputed news sources. Classical machine learning and Transformer-based models have been extensively studied for the task of fake news detection, however they are hampered by their reliance on training data and are unable to generalize on unseen headlines. To address these challenges, we propose our novel solution, leveraging web-scraping techniques and Natural Language Inference (NLI) models to retrieve external knowledge necessary for verifying the accuracy of a headline. Our system is evaluated on a diverse self-curated evaluation dataset spanning over multiple news channels and broad domains. Our best performing pipeline achieves an accuracy of 84.3% surpassing the best classical Machine Learning model by 33.3% and Bidirectional Encoder Representations from Transformers (BERT) by 31.0% . This highlights the efficacy of combining dynamic web-scraping with Natural Language Inference to find support for a claimed headline in the corresponding externally retrieved knowledge for the task of fake news detection.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Preprint, 15 pages, 7 figures"
    },
    {
        "paper id": "2410.09456",
        "abstract url": "https://arxiv.org/abs/2410.09456",
        "title": "Automatic Speech Recognition with BERT and CTC Transformers: A Review",
        "rating": "1",
        "keywords": [
            [
                "cs.CL",
                "eess.AS"
            ]
        ],
        "abstract": "This review paper provides a comprehensive analysis of recent advances in automatic speech recognition (ASR) with bidirectional encoder representations from transformers BERT and connectionist temporal classification (CTC) transformers. The paper first introduces the fundamental concepts of ASR and discusses the challenges associated with it. It then explains the architecture of BERT and CTC transformers and their potential applications in ASR. The paper reviews several studies that have used these models for speech recognition tasks and discusses the results obtained. Additionally, the paper highlights the limitations of these models and outlines potential areas for further research. All in all, this review provides valuable insights for researchers and practitioners who are interested in ASR with BERT and CTC transformers.",
        "subjects": [
            "cs.CL",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09472",
        "abstract url": "https://arxiv.org/abs/2410.09472",
        "title": "DRCap: Decoding CLAP Latents with Retrieval-augmented Generation for Zero-shot Audio Captioning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "While automated audio captioning (AAC) has made notable progress, traditional fully supervised AAC models still face two critical challenges: the need for expensive audio-text pair data for training and performance degradation when transferring across domains. To overcome these limitations, we present DRCap, a data-efficient and flexible zero-shot audio captioning system that requires text-only data for training and can quickly adapt to new domains without additional fine-tuning. DRCap integrates a contrastive language-audio pre-training (CLAP) model and a large-language model (LLM) as its backbone. During training, the model predicts the ground-truth caption with a fixed text encoder from CLAP, whereas, during inference, the text encoder is replaced with the audio encoder to generate captions for audio clips in a zero-shot manner. To mitigate the modality gap of the CLAP model, we use both the projection strategy from the encoder side and the retrieval-augmented generation strategy from the decoder side. Specifically, audio embeddings are first projected onto a text embedding support to absorb extensive semantic information within the joint multi-modal space of CLAP. At the same time, similar captions retrieved from a datastore are fed as prompts to instruct the LLM, incorporating external knowledge to take full advantage of its strong generative capability. Conditioned on both the projected CLAP embedding and the retrieved similar captions, the model is able to produce a more accurate and semantically rich textual description. By tailoring the text embedding support and the caption datastore to the target domain, DRCap acquires a robust ability to adapt to new domains in a training-free manner. Experimental results demonstrate that DRCap outperforms all other zero-shot models in in-domain scenarios and achieves state-of-the-art performance in cross-domain scenarios.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09474",
        "abstract url": "https://arxiv.org/abs/2410.09474",
        "title": "Distilling Invariant Representations with Dual Augmentation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Knowledge distillation (KD) has been widely used to transfer knowledge from large, accurate models (teachers) to smaller, efficient ones (students). Recent methods have explored enforcing consistency by incorporating causal interpretations to distill invariant representations. In this work, we extend this line of research by introducing a dual augmentation strategy to promote invariant feature learning in both teacher and student models. Our approach leverages different augmentations applied to both models during distillation, pushing the student to capture robust, transferable features. This dual augmentation strategy complements invariant causal distillation by ensuring that the learned representations remain stable across a wider range of data variations and transformations. Extensive experiments on CIFAR-100 demonstrate the effectiveness of this approach, achieving competitive results in same-architecture KD.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "8 pages, 1 figure, 3 tables. This paper presents preliminary results from a project that we have since discontinued, as our research focus has shifted to new directions"
    },
    {
        "paper id": "2410.09507",
        "abstract url": "https://arxiv.org/abs/2410.09507",
        "title": "AERA Chat: An Interactive Platform for Automated Explainable Student Answer Assessment",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Generating rationales that justify scoring decisions has emerged as a promising approach to enhance explainability in the development of automated scoring systems. However, the scarcity of publicly available rationale data and the high cost of annotation have resulted in existing methods typically relying on noisy rationales generated by large language models (LLMs). To address these challenges, we have developed AERA Chat, an interactive platform, to provide visually explained assessment of student answers and streamline the verification of rationales. Users can input questions and student answers to obtain automated, explainable assessment results from LLMs. The platform's innovative visualization features and robust evaluation tools make it useful for educators to assist their marking process, and for researchers to evaluate assessment performance and quality of rationales generated by different LLMs, or as a tool for efficient annotation. We evaluated three rationale generation approaches on our platform to demonstrate its capability.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09529",
        "abstract url": "https://arxiv.org/abs/2410.09529",
        "title": "Preserving Old Memories in Vivid Detail: Human-Interactive Photo Restoration Framework",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Photo restoration technology enables preserving visual memories in photographs. However, physical prints are vulnerable to various forms of deterioration, ranging from physical damage to loss of image quality, etc. While restoration by human experts can improve the quality of outcomes, it often comes at a high price in terms of cost and time for restoration. In this work, we present the AI-based photo restoration framework composed of multiple stages, where each stage is tailored to enhance and restore specific types of photo damage, accelerating and automating the photo restoration process. By integrating these techniques into a unified architecture, our framework aims to offer a one-stop solution for restoring old and deteriorated photographs. Furthermore, we present a novel old photo restoration dataset because we lack a publicly available dataset for our evaluation.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09533",
        "abstract url": "https://arxiv.org/abs/2410.09533",
        "title": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced Local Feature Correspondence",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Visual correspondence is a crucial step in key computer vision tasks, including camera localization, image registration, and structure from motion. The most effective techniques for matching keypoints currently involve using learned sparse or dense matchers, which need pairs of images. These neural networks have a good general understanding of features from both images, but they often struggle to match points from different semantic areas. This paper presents a new method that uses semantic cues from foundation vision model features (like DINOv2) to enhance local feature matching by incorporating semantic reasoning into existing descriptors. Therefore, the learned descriptors do not require image pairs at inference time, allowing feature caching and fast matching using similarity search, unlike learned matchers. We present adapted versions of six existing descriptors, with an average increase in performance of 29% in camera localization, with comparable accuracy to existing matchers as LightGlue and LoFTR in two existing benchmarks. Both code and trained models are available at https://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted in ACCV 2024"
    },
    {
        "paper id": "2410.09542",
        "abstract url": "https://arxiv.org/abs/2410.09542",
        "title": "MIRAGE: Evaluating and Explaining Inductive Reasoning Process in Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Inductive reasoning is an essential capability for large language models (LLMs) to achieve higher intelligence, which requires the model to generalize rules from observed facts and then apply them to unseen examples. We present {\\scshape Mirage}, a synthetic dataset that addresses the limitations of previous work, specifically the lack of comprehensive evaluation and flexible test data. In it, we evaluate LLMs' capabilities in both the inductive and deductive stages, allowing for flexible variation in input distribution, task scenario, and task difficulty to analyze the factors influencing LLMs' inductive reasoning. Based on these multi-faceted evaluations, we demonstrate that the LLM is a poor rule-based reasoner. In many cases, when conducting inductive reasoning, they do not rely on a correct rule to answer the unseen case. From the perspectives of different prompting methods, observation numbers, and task forms, models tend to consistently conduct correct deduction without correct inductive rules. Besides, we find that LLMs are good neighbor-based reasoners. In the inductive reasoning process, the model tends to focus on observed facts that are close to the current test example in feature space. By leveraging these similar examples, the model maintains strong inductive capabilities within a localized region, significantly improving its deductive performance.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "25 pages,9 figures, under review"
    },
    {
        "paper id": "2410.09556",
        "abstract url": "https://arxiv.org/abs/2410.09556",
        "title": "A Speaker Turn-Aware Multi-Task Adversarial Network for Joint User Satisfaction Estimation and Sentiment Analysis",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "User Satisfaction Estimation is an important task and increasingly being applied in goal-oriented dialogue systems to estimate whether the user is satisfied with the service. It is observed that whether the user's needs are met often triggers various sentiments, which can be pertinent to the successful estimation of user satisfaction, and vice versa. Thus, User Satisfaction Estimation (USE) and Sentiment Analysis (SA) should be treated as a joint, collaborative effort, considering the strong connections between the sentiment states of speakers and the user satisfaction. Existing joint learning frameworks mainly unify the two highly pertinent tasks over cascade or shared-bottom implementations, however they fail to distinguish task-specific and common features, which will produce sub-optimal utterance representations for downstream tasks. In this paper, we propose a novel Speaker Turn-Aware Multi-Task Adversarial Network (STMAN) for dialogue-level USE and utterance-level SA. Specifically, we first introduce a multi-task adversarial strategy which trains a task discriminator to make utterance representation more task-specific, and then utilize a speaker-turn aware multi-task interaction strategy to extract the common features which are complementary to each task. Extensive experiments conducted on two real-world service dialogue datasets show that our model outperforms several state-of-the-art methods.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09563",
        "abstract url": "https://arxiv.org/abs/2410.09563",
        "title": "Robust Optical Flow Computation: A Higher-Order Differential Approach",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In the domain of computer vision, optical flow stands as a cornerstone for unraveling dynamic visual scenes. However, the challenge of accurately estimating optical flow under conditions of large nonlinear motion patterns remains an open question. The image flow constraint is vulnerable to substantial displacements, and rapid spatial transformations. Inaccurate approximations inherent in numerical differentiation techniques can further amplify such intricacies. In response, this research proposes an innovative algorithm for optical flow computation, utilizing the higher precision of second-order Taylor series approximation within the differential estimation framework. By embracing this mathematical underpinning, the research seeks to extract more information about the behavior of the function under complex real-world scenarios and estimate the motion of areas with a lack of texture. An impressive showcase of the algorithm's capabilities emerges through its performance on renowned optical flow benchmarks such as KITTI (2015) and Middlebury. The average endpoint error (AEE), which computes the Euclidian distance between the calculated flow field and the ground truth flow field, stands notably diminished, validating the effectiveness of the algorithm in handling complex motion patterns.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2410.09564",
        "abstract url": "https://arxiv.org/abs/2410.09564",
        "title": "Extended Japanese Commonsense Morality Dataset with Masked Token and Label Enhancement",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Rapid advancements in artificial intelligence (AI) have made it crucial to integrate moral reasoning into AI systems. However, existing models and datasets often overlook regional and cultural differences. To address this shortcoming, we have expanded the JCommonsenseMorality (JCM) dataset, the only publicly available dataset focused on Japanese morality. The Extended JCM (eJCM) has grown from the original 13,975 sentences to 31,184 sentences using our proposed sentence expansion method called Masked Token and Label Enhancement (MTLE). MTLE selectively masks important parts of sentences related to moral judgment and replaces them with alternative expressions generated by a large language model (LLM), while re-assigning appropriate labels. The model trained using our eJCM achieved an F1 score of 0.857, higher than the scores for the original JCM (0.837), ChatGPT one-shot classification (0.841), and data augmented using AugGPT, a state-of-the-art augmentation method (0.850). Specifically, in complex moral reasoning tasks unique to Japanese culture, the model trained with eJCM showed a significant improvement in performance (increasing from 0.681 to 0.756) and achieved a performance close to that of GPT-4 Turbo (0.787). These results demonstrate the validity of the eJCM dataset and the importance of developing models and datasets that consider the cultural context.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09566",
        "abstract url": "https://arxiv.org/abs/2410.09566",
        "title": "Bridging Text and Image for Artist Style Transfer via Contrastive Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Image style transfer has attracted widespread attention in the past few years. Despite its remarkable results, it requires additional style images available as references, making it less flexible and inconvenient. Using text is the most natural way to describe the style. More importantly, text can describe implicit abstract styles, like styles of specific artists or art movements. In this paper, we propose a Contrastive Learning for Artistic Style Transfer (CLAST) that leverages advanced image-text encoders to control arbitrary style transfer. We introduce a supervised contrastive training strategy to effectively extract style descriptions from the image-text model (i.e., CLIP), which aligns stylization with the text description. To this end, we also propose a novel and efficient adaLN based state space models that explore style-content fusion. Finally, we achieve a text-driven image style transfer. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods in artistic style transfer. More importantly, it does not require online fine-tuning and can render a 512x512 image in 0.03s.",
        "subjects": [
            "cs.CV",
            "cs.HC"
        ],
        "comment": "18 pages, 8 figures"
    },
    {
        "paper id": "2410.09569",
        "abstract url": "https://arxiv.org/abs/2410.09569",
        "title": "Are You Human? An Adversarial Benchmark to Expose LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated an alarming ability to impersonate humans in conversation, raising concerns about their potential misuse in scams and deception. Humans have a right to know if they are conversing to an LLM. We evaluate text-based prompts designed as challenges to expose LLM imposters in real-time. To this end we compile and release an open-source benchmark dataset that includes 'implicit challenges' that exploit an LLM's instruction-following mechanism to cause role deviation, and 'exlicit challenges' that test an LLM's ability to perform simple tasks typically easy for humans but difficult for LLMs. Our evaluation of 9 leading models from the LMSYS leaderboard revealed that explicit challenges successfully detected LLMs in 78.4% of cases, while implicit challenges were effective in 22.9% of instances. User studies validate the real-world applicability of our methods, with humans outperforming LLMs on explicit challenges (78% vs 22% success rate). Our framework unexpectedly revealed that many study participants were using LLMs to complete tasks, demonstrating its effectiveness in detecting both AI impostors and human misuse of AI tools. This work addresses the critical need for reliable, real-time LLM detection methods in high-stakes conversations.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09575",
        "abstract url": "https://arxiv.org/abs/2410.09575",
        "title": "Reconstructive Visual Instruction Tuning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces reconstructive visual instruction tuning (ROSS), a family of Large Multimodal Models (LMMs) that exploit vision-centric supervision signals. In contrast to conventional visual instruction tuning approaches that exclusively supervise text outputs, ROSS prompts LMMs to supervise visual outputs via reconstructing input images. By doing so, it capitalizes on the inherent richness and detail present within input images themselves, which are often lost in pure text supervision. However, producing meaningful feedback from natural images is challenging due to the heavy spatial redundancy of visual signals. To address this issue, ROSS employs a denoising objective to reconstruct latent representations of input images, avoiding directly regressing exact raw RGB values. This intrinsic activation design inherently encourages LMMs to maintain image detail, thereby enhancing their fine-grained comprehension capabilities and reducing hallucinations. Empirically, ROSS consistently brings significant improvements across different visual encoders and language models. In comparison with extrinsic assistance state-of-the-art alternatives that aggregate multiple visual experts, ROSS delivers competitive performance with a single SigLIP visual encoder, demonstrating the efficacy of our vision-centric supervision tailored for visual outputs.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09576",
        "abstract url": "https://arxiv.org/abs/2410.09576",
        "title": "The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In recent years, large language models (LLMs) and generative AI have revolutionized natural language processing (NLP), offering unprecedented capabilities in education. This chapter explores the transformative potential of LLMs in automated question generation and answer assessment. It begins by examining the mechanisms behind LLMs, emphasizing their ability to comprehend and generate human-like text. The chapter then discusses methodologies for creating diverse, contextually relevant questions, enhancing learning through tailored, adaptive strategies. Key prompting techniques, such as zero-shot and chain-of-thought prompting, are evaluated for their effectiveness in generating high-quality questions, including open-ended and multiple-choice formats in various languages. Advanced NLP methods like fine-tuning and prompt-tuning are explored for their role in generating task-specific questions, despite associated costs. The chapter also covers the human evaluation of generated questions, highlighting quality variations across different methods and areas for improvement. Furthermore, it delves into automated answer assessment, demonstrating how LLMs can accurately evaluate responses, provide constructive feedback, and identify nuanced understanding or misconceptions. Examples illustrate both successful assessments and areas needing improvement. The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Book Chapter (Under Review)"
    },
    {
        "paper id": "2410.09584",
        "abstract url": "https://arxiv.org/abs/2410.09584",
        "title": "Toward General Instruction-Following Alignment for Retrieval-Augmented Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Following natural instructions is crucial for the effective application of Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in Large Language Models (LLMs), research on assessing and improving instruction-following (IF) alignment within the RAG domain remains limited. To address this issue, we propose VIF-RAG, the first automated, scalable, and verifiable synthetic pipeline for instruction-following alignment in RAG systems. We start by manually crafting a minimal set of atomic instructions (<100) and developing combination rules to synthesize and verify complex instructions for a seed set. We then use supervised models for instruction rewriting while simultaneously generating code to automate the verification of instruction quality via a Python executor. Finally, we integrate these instructions with extensive RAG and general data samples, scaling up to a high-quality VIF-RAG-QA dataset (>100k) through automated processes. To further bridge the gap in instruction-following auto-evaluation for RAG systems, we introduce FollowRAG Benchmark, which includes approximately 3K test samples, covering 22 categories of general instruction constraints and four knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks. Using FollowRAG and eight widely-used IF and foundational abilities benchmarks for LLMs, we demonstrate that VIF-RAG markedly enhances LLM performance across a broad range of general instruction constraints while effectively leveraging its capabilities in RAG scenarios. Further analysis offers practical insights for achieving IF alignment in RAG systems. Our code and datasets are released at https://FollowRAG.github.io.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "comment": "Working in progress"
    },
    {
        "paper id": "2410.09608",
        "abstract url": "https://arxiv.org/abs/2410.09608",
        "title": "I or Not I: Unraveling the Linguistic Echoes of Identity in Samuel Beckett's \"Not I\" Through Natural Language Processing",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Exploring the depths of Samuel Beckett's \"Not I\" through advanced natural language processing techniques, this research uncovers the intricate linguistic structures that underpin the text. By analyzing word frequency, detecting emotional sentiments with a BERT-based model, and examining repetitive motifs, we unveil how Beckett's minimalist yet complex language reflects the protagonist's fragmented psyche. Our results demonstrate that recurring themes of time, memory, and existential angst are artfully woven through recursive linguistic patterns and rhythmic repetition. This innovative approach not only deepens our understanding of Beckett's stylistic contributions but also highlights his unique role in modern literature, where language transcends simple communication to explore profound existential questions.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09609",
        "abstract url": "https://arxiv.org/abs/2410.09609",
        "title": "Traversing Emotional Landscapes and Linguistic Patterns in Bernard-Marie Kolt\u00e8s' Plays: An NLP Perspective",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "This study employs Natural Language Processing (NLP) to analyze the intricate linguistic and emotional dimensions within the plays of Bernard-Marie Kolt\u00e8s, a central figure in contemporary French theatre. By integrating advanced computational techniques, we dissect Kolt\u00e8s' narrative style, revealing the subtle interplay between language and emotion across his dramatic oeuvre. Our findings highlight how Kolt\u00e8s crafts his narratives, enriching our understanding of his thematic explorations and contributing to the broader field of digital humanities in literary analysis.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09636",
        "abstract url": "https://arxiv.org/abs/2410.09636",
        "title": "Can We Estimate Purchase Intention Based on Zero-shot Speech Emotion Recognition?",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "eess.AS"
            ]
        ],
        "abstract": "This paper proposes a zero-shot speech emotion recognition (SER) method that estimates emotions not previously defined in the SER model training. Conventional methods are limited to recognizing emotions defined by a single word. Moreover, we have the motivation to recognize unknown bipolar emotions such as ``I want to buy - I do not want to buy.'' In order to allow the model to define classes using sentences freely and to estimate unknown bipolar emotions, our proposed method expands upon the contrastive language-audio pre-training (CLAP) framework by introducing multi-class and multi-task settings. We also focus on purchase intention as a bipolar emotion and investigate the model's performance to zero-shot estimate it. This study is the first attempt to estimate purchase intention from speech directly. Experiments confirm that the results of zero-shot estimation by the proposed method are at the same level as those of the model trained by supervised learning.",
        "subjects": [
            "eess.AS",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "5 pages, 3 figures, accepted for APSIPA 2024 ASC"
    },
    {
        "paper id": "2410.09637",
        "abstract url": "https://arxiv.org/abs/2410.09637",
        "title": "ReLU's Revival: On the Entropic Overload in Normalization-Free Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "LayerNorm is a critical component in modern large language models (LLMs) for stabilizing training and ensuring smooth optimization. However, it introduces significant challenges in mechanistic interpretability, outlier feature suppression, faithful signal propagation, and computational and communication complexity of private inference. This work explores desirable activation functions in normalization-free decoder-only LLMs. Contrary to the conventional preference for the GELU in transformer-based models, our empirical findings demonstrate an {\\em opposite trend} -- ReLU significantly outperforms GELU in LayerNorm-free models, leading to an {\\bf 8.2\\%} perplexity improvement. We discover a key issue with GELU, where early layers experience entropic overload, leading to the under-utilization of the representational capacity of attention heads. This highlights that smoother activations like GELU are {\\em ill-suited} for LayerNorm-free architectures, whereas ReLU's geometrical properties -- specialization in input space and intra-class selectivity -- lead to improved learning dynamics and better information retention in the absence of LayerNorm. This study offers key insights for optimizing transformer architectures where LayerNorm introduces significant challenges.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "NeurIPS 2024 Workshop on Attributing Model Behavior at Scale"
    },
    {
        "paper id": "2410.09640",
        "abstract url": "https://arxiv.org/abs/2410.09640",
        "title": "Provable Acceleration of Nesterov's Accelerated Gradient for Rectangular Matrix Factorization and Linear Neural Networks",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We study the convergence rate of first-order methods for rectangular matrix factorization, which is a canonical nonconvex optimization problem. Specifically, given a rank-$r$ matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$, we prove that gradient descent (GD) can find a pair of $\u03b5$-optimal solutions $\\mathbf{X}_T\\in\\mathbb{R}^{m\\times d}$ and $\\mathbf{Y}_T\\in\\mathbb{R}^{n\\times d}$, where $d\\geq r$, satisfying $\\lVert\\mathbf{X}_T\\mathbf{Y}_T^\\top-\\mathbf{A}\\rVert_\\mathrm{F}\\leq\u03b5\\lVert\\mathbf{A}\\rVert_\\mathrm{F}$ in $T=O(\u03ba^2\\log\\frac{1}\u03b5)$ iterations with high probability, where $\u03ba$ denotes the condition number of $\\mathbf{A}$. Furthermore, we prove that Nesterov's accelerated gradient (NAG) attains an iteration complexity of $O(\u03ba\\log\\frac{1}\u03b5)$, which is the best-known bound of first-order methods for rectangular matrix factorization. Different from small balanced random initialization in the existing literature, we adopt an unbalanced initialization, where $\\mathbf{X}_0$ is large and $\\mathbf{Y}_0$ is $0$. Moreover, our initialization and analysis can be further extended to linear neural networks, where we prove that NAG can also attain an accelerated linear convergence rate. In particular, we only require the width of the network to be greater than or equal to the rank of the output label matrix. In contrast, previous results achieving the same rate require excessive widths that additionally depend on the condition number and the rank of the input data matrix.",
        "subjects": [
            "cs.LG",
            "math.OC",
            "stat.ML"
        ],
        "comment": "Accepted by 38th Conference on Neural Information Processing Systems (NeurIPS 2024)"
    },
    {
        "paper id": "2410.09642",
        "abstract url": "https://arxiv.org/abs/2410.09642",
        "title": "RepMatch: Quantifying Cross-Instance Similarities in Representation Space",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Advances in dataset analysis techniques have enabled more sophisticated approaches to analyzing and characterizing training data instances, often categorizing data based on attributes such as ``difficulty''. In this work, we introduce RepMatch, a novel method that characterizes data through the lens of similarity. RepMatch quantifies the similarity between subsets of training instances by comparing the knowledge encoded in models trained on them, overcoming the limitations of existing analysis methods that focus solely on individual instances and are restricted to within-dataset analysis. Our framework allows for a broader evaluation, enabling similarity comparisons across arbitrary subsets of instances, supporting both dataset-to-dataset and instance-to-dataset analyses. We validate the effectiveness of RepMatch across multiple NLP tasks, datasets, and models. Through extensive experimentation, we demonstrate that RepMatch can effectively compare datasets, identify more representative subsets of a dataset (that lead to better performance than randomly selected subsets of equivalent size), and uncover heuristics underlying the construction of some challenge datasets.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09644",
        "abstract url": "https://arxiv.org/abs/2410.09644",
        "title": "Adapters for Altering LLM Vocabularies: What Languages Benefit the Most?",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Vocabulary adaptation, which integrates new vocabulary into pre-trained language models (LMs), enables expansion to new languages and mitigates token over-fragmentation. However, existing approaches are limited by their reliance on heuristic or external embeddings. We propose VocADT, a novel method for vocabulary adaptation using adapter modules that are trained to learn the optimal linear combination of existing embeddings while keeping the model's weights fixed. VocADT offers a flexible and scalable solution without requiring external resources or language constraints. Across 11 languages-with various scripts, resource availability, and fragmentation-we demonstrate that VocADT outperforms the original Mistral model and other baselines across various multilingual tasks. We find that Latin-script languages and highly fragmented languages benefit the most from vocabulary adaptation. We further fine-tune the adapted model on the generative task of machine translation and find that vocabulary adaptation is still beneficial after fine-tuning and that VocADT is the most effective method.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09655",
        "abstract url": "https://arxiv.org/abs/2410.09655",
        "title": "Interpolated-MLPs: Controllable Inductive Bias",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Due to their weak inductive bias, Multi-Layer Perceptrons (MLPs) have subpar performance at low-compute levels compared to standard architectures such as convolution-based networks (CNN). Recent work, however, has shown that the performance gap drastically reduces as the amount of compute is increased without changing the amount of inductive bias. In this work, we study the converse: in the low-compute regime, how does the incremental increase of inductive bias affect performance? To quantify inductive bias, we propose a \"soft MLP\" approach, which we coin Interpolated MLP (I-MLP). We control the amount of inductive bias in the standard MLP by introducing a novel algorithm based on interpolation between fixed weights from a prior model with high inductive bias. We showcase our method using various prior models, including CNNs and the MLP-Mixer architecture. This interpolation scheme allows fractional control of inductive bias, which may be attractive when full inductive bias is not desired (e.g. in the mid-compute regime). We find experimentally that for Vision Tasks in the low-compute regime, there is a continuous and two-sided logarithmic relationship between inductive bias and performance when using CNN and MLP-Mixer prior models.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "13 pages, 3 figures, ICML HiLD 2024 Workshop: 2nd Workshop on High-dimensional Learning Dynamics"
    },
    {
        "paper id": "2410.09671",
        "abstract url": "https://arxiv.org/abs/2410.09671",
        "title": "OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In this technical report, we introduce OpenR, an open-source framework designed to integrate key components for enhancing the reasoning capabilities of large language models (LLMs). OpenR unifies data acquisition, reinforcement learning training (both online and offline), and non-autoregressive decoding into a cohesive software platform. Our goal is to establish an open-source platform and community to accelerate the development of LLM reasoning. Inspired by the success of OpenAI's o1 model, which demonstrated improved reasoning abilities through step-by-step reasoning and reinforcement learning, OpenR integrates test-time compute, reinforcement learning, and process supervision to improve reasoning in LLMs. Our work is the first to provide an open-source framework that explores the core techniques of OpenAI's o1 model with reinforcement learning, achieving advanced reasoning capabilities beyond traditional autoregressive methods. We demonstrate the efficacy of OpenR by evaluating it on the MATH dataset, utilising publicly available data and search methods. Our initial experiments confirm substantial gains, with relative improvements in reasoning and performance driven by test-time computation and reinforcement learning through process reward models. The OpenR framework, including code, models, and datasets, is accessible at https://openreasoner.github.io.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09675",
        "abstract url": "https://arxiv.org/abs/2410.09675",
        "title": "COrAL: Order-Agnostic Language Modeling for Efficient Iterative Refinement",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Iterative refinement has emerged as an effective paradigm for enhancing the capabilities of large language models (LLMs) on complex tasks. However, existing approaches typically implement iterative refinement at the application or prompting level, relying on autoregressive (AR) modeling. The sequential token generation in AR models can lead to high inference latency. To overcome these challenges, we propose Context-Wise Order-Agnostic Language Modeling (COrAL), which incorporates iterative refinement directly into the LLM architecture while maintaining computational efficiency. Our approach models multiple token dependencies within manageable context windows, enabling the model to perform iterative refinement internally during the generation process. Leveraging the order-agnostic nature of COrAL, we introduce sliding blockwise order-agnostic decoding, which performs multi-token forward prediction and backward reconstruction within context windows. This allows the model to iteratively refine its outputs in parallel in the sliding block, effectively capturing diverse dependencies without the high inference cost of sequential generation. Empirical evaluations on reasoning tasks demonstrate that COrAL improves performance and inference speed, respectively, achieving absolute accuracy gains of $4.6\\%$ on GSM8K and $4.0\\%$ on LogiQA, along with inference speedups of up to $3.9\\times$ over next-token baselines. Preliminary results on code generation indicate a drop in pass rates due to inconsistencies in order-agnostic outputs, highlighting the inherent quality--speed trade-off. Our code is publicly available at https://github.com/YuxiXie/COrAL.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "12 pages, 7 figures, 3 tables (23 pages, 9 figures, 4 tables including references and appendices)"
    },
    {
        "paper id": "2410.09687",
        "abstract url": "https://arxiv.org/abs/2410.09687",
        "title": "MoIN: Mixture of Introvert Experts to Upcycle an LLM",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The goal of this paper is to improve (upcycle) an existing large language model without the prohibitive requirements of continued pre-training of the full-model. The idea is to split the pre-training data into semantically relevant groups and train an expert on each subset. An expert takes the form of a lightweight adapter added on the top of a frozen base model. During inference, an incoming query is first routed to the most relevant expert which is then loaded onto the base model for the forward pass. Unlike typical Mixture of Experts (MoE) models, the experts in our method do not work with other experts for a single query. Hence, we dub them \"introvert\" experts. Freezing the base model and keeping the experts as lightweight adapters allows extreme parallelism during training and inference. Training of all experts can be done in parallel without any communication channels between them. Similarly, the inference can also be heavily parallelized by distributing experts on different GPUs and routing each request to the GPU containing its relevant expert. We implement a proof-of-concept version of this method and show the validity of our approach.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09701",
        "abstract url": "https://arxiv.org/abs/2410.09701",
        "title": "Transformers as Game Players: Provable In-context Game-playing Capabilities of Pre-trained Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "The in-context learning (ICL) capability of pre-trained models based on the transformer architecture has received growing interest in recent years. While theoretical understanding has been obtained for ICL in reinforcement learning (RL), the previous results are largely confined to the single-agent setting. This work proposes to further explore the in-context learning capabilities of pre-trained transformer models in competitive multi-agent games, i.e., in-context game-playing (ICGP). Focusing on the classical two-player zero-sum games, theoretical guarantees are provided to demonstrate that pre-trained transformers can provably learn to approximate Nash equilibrium in an in-context manner for both decentralized and centralized learning settings. As a key part of the proof, constructional results are established to demonstrate that the transformer architecture is sufficiently rich to realize celebrated multi-agent game-playing algorithms, in particular, decentralized V-learning and centralized VI-ULCB.",
        "subjects": [
            "stat.ML",
            "cs.IT",
            "cs.LG",
            "cs.MA"
        ],
        "comment": "Accepted to NeurIPS 2024"
    },
    {
        "paper id": "2410.09706",
        "abstract url": "https://arxiv.org/abs/2410.09706",
        "title": "ECVC: Exploiting Non-Local Correlations in Multiple Frames for Contextual Video Compression",
        "rating": "1",
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "In Learned Video Compression (LVC), improving inter prediction, such as enhancing temporal context mining and mitigating accumulated errors, is crucial for boosting rate-distortion performance. Existing LVCs mainly focus on mining the temporal movements within adjacent frames, neglecting non-local correlations among frames. Additionally, current contextual video compression models use a single reference frame, which is insufficient for handling complex movements. To address these issues, we propose leveraging non-local correlations across multiple frames to enhance temporal priors, significantly boosting rate-distortion performance. To mitigate error accumulation, we introduce a partial cascaded fine-tuning strategy that supports fine-tuning on full-length sequences with constrained computational resources. This method reduces the train-test mismatch in sequence lengths and significantly decreases accumulated errors. Based on the proposed techniques, we present a video compression scheme ECVC. Experiments demonstrate that our ECVC achieves state-of-the-art performance, reducing 7.3% and 10.5% more bit-rates than DCVC-DC and DCVC-FM over VTM-13.2 low delay B (LDB), respectively, when the intra period (IP) is 32. Additionally, ECVC reduces 11.1% more bit-rate than DCVC-FM over VTM-13.2 LDB when the IP is -1. Our Code will be available at https://github.com/JiangWeibeta/ECVC.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "Code will be available at https://github.com/JiangWeibeta/ECVC"
    },
    {
        "paper id": "2410.09714",
        "abstract url": "https://arxiv.org/abs/2410.09714",
        "title": "AM-SAM: Automated Prompting and Mask Calibration for Segment Anything Model",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Segment Anything Model (SAM) has gained significant recognition in the field of semantic segmentation due to its versatile capabilities and impressive performance. Despite its success, SAM faces two primary limitations: (1) it relies heavily on meticulous human-provided prompts like key points, bounding boxes or text messages, which is labor-intensive; (2) the mask decoder's feature representation is sometimes inaccurate, as it solely employs dot product operations at the end of mask decoder, which inadequately captures the necessary correlations for precise segmentation. Current solutions to these problems such as fine-tuning SAM often require retraining a large number of parameters, which needs huge amount of time and computing resources. To address these limitations, we propose an automated prompting and mask calibration method called AM-SAM based on a bi-level optimization framework. Our approach automatically generates prompts for an input image, eliminating the need for human involvement with a good performance in early training epochs, achieving faster convergence. Additionally, we freeze the main part of SAM, and modify the mask decoder with Low-Rank Adaptation (LoRA), enhancing the mask decoder's feature representation by incorporating advanced techniques that go beyond simple dot product operations to more accurately capture and utilize feature correlations. Our experimental results demonstrate that AM-SAM achieves significantly accurate segmentation, matching or exceeding the effectiveness of human-generated and default prompts. Notably, on the body segmentation dataset, our method yields a 5% higher dice score with a 4-example few-shot training set compared to the SOTA method, underscoring its superiority in semantic segmentation tasks.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09724",
        "abstract url": "https://arxiv.org/abs/2410.09724",
        "title": "Taming Overconfidence in LLMs: Reward Calibration in RLHF",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Language model calibration refers to the alignment between the confidence of the model and the actual performance of its responses. While previous studies point out the overconfidence phenomenon in Large Language Models (LLMs) and show that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) are overconfident with a more sharpened output probability, in this study, we reveal that RLHF tends to lead models to express verbalized overconfidence in their own responses. We investigate the underlying cause of this overconfidence and demonstrate that reward models used for Proximal Policy Optimization (PPO) exhibit inherent biases towards high-confidence scores regardless of the actual quality of responses. Building upon this insight, we propose two PPO variants: PPO-M: PPO with Calibrated Reward Modeling and PPO-C: PPO with Calibrated Reward Calculation. PPO-M integrates explicit confidence scores in reward model training, which calibrates reward models to better capture the alignment between response quality and verbalized confidence. PPO-C adjusts the reward score during PPO based on the difference between the current reward and the moving average of past rewards. Both PPO-M and PPO-C can be seamlessly integrated into the current PPO pipeline and do not require additional golden labels. We evaluate our methods on both Llama3-8B and Mistral-7B across six diverse datasets including multiple-choice and open-ended generation. Experiment results demonstrate that both of our methods can reduce calibration error and maintain performance comparable to standard PPO. We further show that they do not compromise model capabilities in open-ended conversation settings.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09368",
        "abstract url": "https://arxiv.org/abs/2410.09368",
        "title": "Towards a Domain-Specific Modelling Environment for Reinforcement Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In recent years, machine learning technologies have gained immense popularity and are being used in a wide range of domains. However, due to the complexity associated with machine learning algorithms, it is a challenge to make it user-friendly, easy to understand and apply. Machine learning applications are especially challenging for users who do not have proficiency in this area. In this paper, we use model-driven engineering (MDE) methods and tools for developing a domain-specific modelling environment to contribute towards providing a solution for this problem. We targeted reinforcement learning from the machine learning domain, and evaluated the proposed language, reinforcement learning modelling language (RLML), with multiple applications. The tool supports syntax-directed editing, constraint checking, and automatic generation of code from RLML models. The environment also provides support for comparing results generated with multiple RL algorithms. With our proposed MDE approach, we were able to help in abstracting reinforcement learning technologies and improve the learning curve for RL users.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": "24 pages"
    },
    {
        "paper id": "2410.09375",
        "abstract url": "https://arxiv.org/abs/2410.09375",
        "title": "Looped ReLU MLPs May Be All You Need as Practical Programmable Computers",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Previous work has demonstrated that attention mechanisms are Turing complete. More recently, it has been shown that a looped 13-layer Transformer can function as a universal programmable computer. In contrast, the multi-layer perceptrons with $\\mathsf{ReLU}$ activation ($\\mathsf{ReLU}$-$\\mathsf{MLP}$), one of the most fundamental components of neural networks, is known to be expressive; specifically, a two-layer neural network is a universal approximator given an exponentially large number of hidden neurons. However, it remains unclear whether a $\\mathsf{ReLU}$-$\\mathsf{MLP}$ can be made into a universal programmable computer using a practical number of weights. In this work, we provide an affirmative answer that a looped 23-layer $\\mathsf{ReLU}$-$\\mathsf{MLP}$ is capable to perform the basic necessary operations, effectively functioning as a programmable computer. This indicates that simple modules have stronger expressive power than previously expected and have not been fully explored. Our work provides insights into the mechanisms of neural networks and demonstrates that complex tasks, such as functioning as a programmable computer, do not necessarily require advanced architectures like Transformers.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09383",
        "abstract url": "https://arxiv.org/abs/2410.09383",
        "title": "Deep Transfer Learning: Model Framework and Error Analysis",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper presents a framework for deep transfer learning, which aims to leverage information from multi-domain upstream data with a large number of samples $n$ to a single-domain downstream task with a considerably smaller number of samples $m$, where $m \\ll n$, in order to enhance performance on downstream task. Our framework has several intriguing features. First, it allows the existence of both shared and specific features among multi-domain data and provides a framework for automatic identification, achieving precise transfer and utilization of information. Second, our model framework explicitly indicates the upstream features that contribute to downstream tasks, establishing a relationship between upstream domains and downstream tasks, thereby enhancing interpretability. Error analysis demonstrates that the transfer under our framework can significantly improve the convergence rate for learning Lipschitz functions in downstream supervised tasks, reducing it from $\\tilde{O}(m^{-\\frac{1}{2(d+2)}}+n^{-\\frac{1}{2(d+2)}})$ (\"no transfer\") to $\\tilde{O}(m^{-\\frac{1}{2(d^*+3)}} + n^{-\\frac{1}{2(d+2)}})$ (\"partial transfer\"), and even to $\\tilde{O}(m^{-1/2}+n^{-\\frac{1}{2(d+2)}})$ (\"complete transfer\"), where $d^* \\ll d$ and $d$ is the dimension of the observed data. Our theoretical findings are substantiated by empirical experiments conducted on image classification datasets, along with a regression dataset.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09401",
        "abstract url": "https://arxiv.org/abs/2410.09401",
        "title": "A Novel Approach to Malicious Code Detection Using CNN-BiLSTM and Feature Fusion",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "With the rapid advancement of Internet technology, the threat of malware to computer systems and network security has intensified. Malware affects individual privacy and security and poses risks to critical infrastructures of enterprises and nations. The increasing quantity and complexity of malware, along with its concealment and diversity, challenge traditional detection techniques. Static detection methods struggle against variants and packed malware, while dynamic methods face high costs and risks that limit their application. Consequently, there is an urgent need for novel and efficient malware detection techniques to improve accuracy and robustness. This study first employs the minhash algorithm to convert binary files of malware into grayscale images, followed by the extraction of global and local texture features using GIST and LBP algorithms. Additionally, the study utilizes IDA Pro to decompile and extract opcode sequences, applying N-gram and tf-idf algorithms for feature vectorization. The fusion of these features enables the model to comprehensively capture the behavioral characteristics of malware. In terms of model construction, a CNN-BiLSTM fusion model is designed to simultaneously process image features and opcode sequences, enhancing classification performance. Experimental validation on multiple public datasets demonstrates that the proposed method significantly outperforms traditional detection techniques in terms of accuracy, recall, and F1 score, particularly in detecting variants and obfuscated malware with greater stability. The research presented in this paper offers new insights into the development of malware detection technologies, validating the effectiveness of feature and model fusion, and holds promising application prospects.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09408",
        "abstract url": "https://arxiv.org/abs/2410.09408",
        "title": "C-Adapter: Adapting Deep Classifiers for Efficient Conformal Prediction Sets",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Conformal prediction, as an emerging uncertainty quantification technique, typically functions as post-hoc processing for the outputs of trained classifiers. To optimize the classifier for maximum predictive efficiency, Conformal Training rectifies the training objective with a regularization that minimizes the average prediction set size at a specific error rate. However, the regularization term inevitably deteriorates the classification accuracy and leads to suboptimal efficiency of conformal predictors. To address this issue, we introduce \\textbf{Conformal Adapter} (C-Adapter), an adapter-based tuning method to enhance the efficiency of conformal predictors without sacrificing accuracy. In particular, we implement the adapter as a class of intra order-preserving functions and tune it with our proposed loss that maximizes the discriminability of non-conformity scores between correctly and randomly matched data-label pairs. Using C-Adapter, the model tends to produce extremely high non-conformity scores for incorrect labels, thereby enhancing the efficiency of prediction sets across different coverage rates. Extensive experiments demonstrate that C-Adapter can effectively adapt various classifiers for efficient prediction sets, as well as enhance the conformal training method.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09411",
        "abstract url": "https://arxiv.org/abs/2410.09411",
        "title": "Towards the Effect of Examples on In-Context Learning: A Theoretical Case Study",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In-context learning (ICL) has emerged as a powerful capability for large language models (LLMs) to adapt to downstream tasks by leveraging a few (demonstration) examples. Despite its effectiveness, the mechanism behind ICL remains underexplored. To better understand how ICL integrates the examples with the knowledge learned by the LLM during pre-training (i.e., pre-training knowledge) and how the examples impact ICL, this paper conducts a theoretical study in binary classification tasks. In particular, we introduce a probabilistic model extending from the Gaussian mixture model to exactly quantify the impact of pre-training knowledge, label frequency, and label noise on the prediction accuracy. Based on our analysis, when the pre-training knowledge contradicts the knowledge in the examples, whether ICL prediction relies more on the pre-training knowledge or the examples depends on the number of examples. In addition, the label frequency and label noise of the examples both affect the accuracy of the ICL prediction, where the minor class has a lower accuracy, and how the label noise impacts the accuracy is determined by the specific noise level of the two classes. Extensive simulations are conducted to verify the correctness of the theoretical results, and real-data experiments also align with the theoretical insights. Our work reveals the role of pre-training knowledge and examples in ICL, offering a deeper understanding of LLMs' behaviors in classification tasks.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09420",
        "abstract url": "https://arxiv.org/abs/2410.09420",
        "title": "Anderson Acceleration in Nonsmooth Problems: Local Convergence via Active Manifold Identification",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Anderson acceleration is an effective technique for enhancing the efficiency of fixed-point iterations; however, analyzing its convergence in nonsmooth settings presents significant challenges. In this paper, we investigate a class of nonsmooth optimization algorithms characterized by the active manifold identification property. This class includes a diverse array of methods such as the proximal point method, proximal gradient method, proximal linear method, proximal coordinate descent method, Douglas-Rachford splitting (or the alternating direction method of multipliers), and the iteratively reweighted $\\ell_1$ method, among others. Under the assumption that the optimization problem possesses an active manifold at a stationary point, we establish a local R-linear convergence rate for the Anderson-accelerated algorithm. Our extensive numerical experiments further highlight the robust performance of the proposed Anderson-accelerated methods.",
        "subjects": [
            "math.OC",
            "cs.LG",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09457",
        "abstract url": "https://arxiv.org/abs/2410.09457",
        "title": "Power-Softmax: Towards Secure LLM Inference over Encrypted Data",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Modern cryptographic methods for implementing privacy-preserving LLMs such as Homomorphic Encryption (HE) require the LLMs to have a polynomial form. Forming such a representation is challenging because Transformers include non-polynomial components, such as Softmax and layer normalization. Previous approaches have either directly approximated pre-trained models with large-degree polynomials, which are less efficient over HE, or replaced non-polynomial components with easier-to-approximate primitives before training, e.g., Softmax with pointwise attention. The latter approach might introduce scalability challenges. We present a new HE-friendly variant of self-attention that offers a stable form for training and is easy to approximate with polynomials for secure inference. Our work introduces the first polynomial LLMs with 32 layers and over a billion parameters, exceeding the size of previous models by more than tenfold. The resulting models demonstrate reasoning and in-context learning (ICL) capabilities comparable to standard transformers of the same size, representing a breakthrough in the field. Finally, we provide a detailed latency breakdown for each computation over encrypted data, paving the way for further optimization, and explore the differences in inductive bias between transformers relying on our HE-friendly variant and standard transformers. Our code is attached as a supplement.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09463",
        "abstract url": "https://arxiv.org/abs/2410.09463",
        "title": "e-Fold Cross-Validation for energy-aware Machine Learning Evaluations",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "This paper introduces e-fold cross-validation, an energy-efficient alternative to k-fold cross-validation. It dynamically adjusts the number of folds based on a stopping criterion. The criterion checks after each fold whether the standard deviation of the evaluated folds has consistently decreased or remained stable. Once met, the process stops early. We tested e-fold cross-validation on 15 datasets and 10 machine-learning algorithms. On average, it required 4 fewer folds than 10-fold cross-validation, reducing evaluation time, computational resources, and energy use by about 40%. Performance differences between e-fold and 10-fold cross-validation were less than 2% for larger datasets. More complex models showed even smaller discrepancies. In 96% of iterations, the results were within the confidence interval, confirming statistical significance. E-fold cross-validation offers a reliable and efficient alternative to k-fold, reducing computational costs while maintaining accuracy.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09466",
        "abstract url": "https://arxiv.org/abs/2410.09466",
        "title": "Reinforcement Learning in Hyperbolic Spaces: Models and Experiments",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We examine five setups where an agent (or two agents) seeks to explore unknown environment without any prior information. Although seemingly very different, all of them can be formalized as Reinforcement Learning (RL) problems in hyperbolic spaces. More precisely, it is natural to endow the action spaces with the hyperbolic metric. We introduce statistical and dynamical models necessary for addressing problems of this kind and implement algorithms based on this framework. Throughout the paper we view RL through the lens of the black-box optimization.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09480",
        "abstract url": "https://arxiv.org/abs/2410.09480",
        "title": "Identification of Non-causal Graphical Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The paper considers the problem to estimate non-causal graphical models whose edges encode smoothing relations among the variables. We propose a new covariance extension problem and show that the solution minimizing the transportation distance with respect to white noise process is a double-sided autoregressive non-causal graphical model. Then, we generalize the paradigm to a class of graphical autoregressive moving-average models. Finally, we test the performance of the proposed method through some numerical experiments.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.OC"
        ],
        "comment": "Accepted to the IEEE CDC 2024 conference"
    },
    {
        "paper id": "2410.09484",
        "abstract url": "https://arxiv.org/abs/2410.09484",
        "title": "Bridging Gaps: Federated Multi-View Clustering in Heterogeneous Hybrid Views",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recently, federated multi-view clustering (FedMVC) has emerged to explore cluster structures in multi-view data distributed on multiple clients. Existing approaches often assume that clients are isomorphic and all of them belong to either single-view clients or multi-view clients. Despite their success, these methods also present limitations when dealing with practical FedMVC scenarios involving heterogeneous hybrid views, where a mixture of both single-view and multi-view clients exhibit varying degrees of heterogeneity. In this paper, we propose a novel FedMVC framework, which concurrently addresses two challenges associated with heterogeneous hybrid views, i.e., client gap and view gap. To address the client gap, we design a local-synergistic contrastive learning approach that helps single-view clients and multi-view clients achieve consistency for mitigating heterogeneity among all clients. To address the view gap, we develop a global-specific weighting aggregation method, which encourages global models to learn complementary features from hybrid views. The interplay between local-synergistic contrastive learning and global-specific weighting aggregation mutually enhances the exploration of the data cluster structures distributed on multiple clients. Theoretical analysis and extensive experiments demonstrate that our method can handle the heterogeneous hybrid views in FedMVC and outperforms state-of-the-art methods. The code is available at \\url{https://github.com/5Martina5/FMCSC}.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09486",
        "abstract url": "https://arxiv.org/abs/2410.09486",
        "title": "ActSafe: Active Exploration with Safety Constraints for Reinforcement Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Reinforcement learning (RL) is ubiquitous in the development of modern AI systems. However, state-of-the-art RL agents require extensive, and potentially unsafe, interactions with their environments to learn effectively. These limitations confine RL agents to simulated environments, hindering their ability to learn directly in real-world settings. In this work, we present ActSafe, a novel model-based RL algorithm for safe and efficient exploration. ActSafe learns a well-calibrated probabilistic model of the system and plans optimistically w.r.t. the epistemic uncertainty about the unknown dynamics, while enforcing pessimism w.r.t. the safety constraints. Under regularity assumptions on the constraints and dynamics, we show that ActSafe guarantees safety during learning while also obtaining a near-optimal policy in finite time. In addition, we propose a practical variant of ActSafe that builds on latest model-based RL advancements and enables safe exploration even in high-dimensional settings such as visual control. We empirically show that ActSafe obtains state-of-the-art performance in difficult exploration tasks on standard safe deep RL benchmarks while ensuring safety during learning.",
        "subjects": [
            "cs.LG",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09506",
        "abstract url": "https://arxiv.org/abs/2410.09506",
        "title": "Distribution-Aware Mean Estimation under User-level Local Differential Privacy",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We consider the problem of mean estimation under user-level local differential privacy, where $n$ users are contributing through their local pool of data samples. Previous work assume that the number of data samples is the same across users. In contrast, we consider a more general and realistic scenario where each user $u \\in [n]$ owns $m_u$ data samples drawn from some generative distribution $\u03bc$; $m_u$ being unknown to the statistician but drawn from a known distribution $M$ over $\\mathbb{N}^\\star$. Based on a distribution-aware mean estimation algorithm, we establish an $M$-dependent upper bounds on the worst-case risk over $\u03bc$ for the task of mean estimation. We then derive a lower bound. The two bounds are asymptotically matching up to logarithmic factors and reduce to known bounds when $m_u = m$ for any user $u$.",
        "subjects": [
            "stat.ME",
            "cs.AI",
            "cs.CR",
            "cs.LG",
            "stat.ML"
        ],
        "comment": "25 pages, 1 figure"
    },
    {
        "paper id": "2410.09528",
        "abstract url": "https://arxiv.org/abs/2410.09528",
        "title": "Boosting Deductive Reasoning with Step Signals In RLHF",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Logical reasoning is a crucial task for Large Language Models (LLMs), enabling them to tackle complex problems. Among reasoning tasks, multi-step reasoning poses a particular challenge. Grounded in the theory of formal logic, we have developed an automated method, Multi-step Deduction (MuseD), for deductive reasoning data. MuseD has allowed us to create training and testing datasets for multi-step reasoning. Our generation method enables control over the complexity of the generated instructions, facilitating training and evaluation of models across different difficulty levels. Through RLHF training, our training data has demonstrated significant improvements in logical capabilities for both in-domain of out-of-domain reasoning tasks. Additionally, we have conducted tests to assess the multi-step reasoning abilities of various models.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09541",
        "abstract url": "https://arxiv.org/abs/2410.09541",
        "title": "LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning",
        "rating": "0.5",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Large language models (LLMs) sometimes demonstrate poor performance on knowledge-intensive tasks, commonsense reasoning is one of them. Researchers typically address these issues by retrieving related knowledge from knowledge graphs or employing self-enhancement methods to elicit knowledge in LLMs. However, noisy knowledge and invalid reasoning issues hamper their ability to answer questions accurately. To this end, we propose a novel method named eliciting, filtering and integrating knowledge in large language model (LINKED). In it, we design a reward model to filter out the noisy knowledge and take the marginal consistent reasoning module to reduce invalid reasoning. With our comprehensive experiments on two complex commonsense reasoning benchmarks, our method outperforms SOTA baselines (up to 9.0% improvement of accuracy). Besides, to measure the positive and negative impact of the injected knowledge, we propose a new metric called effectiveness-preservation score for the knowledge enhancement works. Finally, through extensive experiments, we conduct an in-depth analysis and find many meaningful conclusions about LLMs in commonsense reasoning tasks.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Accepted by EMNLP 2024 Findings"
    },
    {
        "paper id": "2410.09543",
        "abstract url": "https://arxiv.org/abs/2410.09543",
        "title": "Boltzmann-Aligned Inverse Folding Model as a Predictor of Mutational Effects on Protein-Protein Interactions",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Predicting the change in binding free energy ($\u0394\u0394G$) is crucial for understanding and modulating protein-protein interactions, which are critical in drug design. Due to the scarcity of experimental $\u0394\u0394G$ data, existing methods focus on pre-training, while neglecting the importance of alignment. In this work, we propose the Boltzmann Alignment technique to transfer knowledge from pre-trained inverse folding models to $\u0394\u0394G$ prediction. We begin by analyzing the thermodynamic definition of $\u0394\u0394G$ and introducing the Boltzmann distribution to connect energy with protein conformational distribution. However, the protein conformational distribution is intractable; therefore, we employ Bayes' theorem to circumvent direct estimation and instead utilize the log-likelihood provided by protein inverse folding models for $\u0394\u0394G$ estimation. Compared to previous inverse folding-based methods, our method explicitly accounts for the unbound state of protein complex in the $\u0394\u0394G$ thermodynamic cycle, introducing a physical inductive bias and achieving both supervised and unsupervised state-of-the-art (SoTA) performance. Experimental results on SKEMPI v2 indicate that our method achieves Spearman coefficients of 0.3201 (unsupervised) and 0.5134 (supervised), significantly surpassing the previously reported SoTA values of 0.2632 and 0.4324, respectively. Futhermore, we demonstrate the capability of our method on binding energy prediction, protein-protein docking and antibody optimization tasks.",
        "subjects": [
            "cs.CE",
            "cs.AI",
            "q-bio.BM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09568",
        "abstract url": "https://arxiv.org/abs/2410.09568",
        "title": "Second-Order Min-Max Optimization with Lazy Hessians",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper studies second-order methods for convex-concave minimax optimization. Monteiro and Svaiter (2012) proposed a method to solve the problem with an optimal iteration complexity of $\\mathcal{O}(\u03b5^{-3/2})$ to find an $\u03b5$-saddle point. However, it is unclear whether the computational complexity, $\\mathcal{O}((N+ d^2) d \u03b5^{-2/3})$, can be improved. In the above, we follow Doikov et al. (2023) and assume the complexity of obtaining a first-order oracle as $N$ and the complexity of obtaining a second-order oracle as $dN$. In this paper, we show that the computation cost can be reduced by reusing Hessian across iterations. Our methods take the overall computational complexity of $ \\tilde{\\mathcal{O}}( (N+d^2)(d+ d^{2/3}\u03b5^{-2/3}))$, which improves those of previous methods by a factor of $d^{1/3}$. Furthermore, we generalize our method to strongly-convex-strongly-concave minimax problems and establish the complexity of $\\tilde{\\mathcal{O}}((N+d^2) (d + d^{2/3} \u03ba^{2/3}) )$ when the condition number of the problem is $\u03ba$, enjoying a similar speedup upon the state-of-the-art method. Numerical experiments on both real and synthetic datasets also verify the efficiency of our method.",
        "subjects": [
            "math.OC",
            "cs.CC",
            "cs.CR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09597",
        "abstract url": "https://arxiv.org/abs/2410.09597",
        "title": "A Complete Characterization of Learnability for Stochastic Noisy Bandits",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We study the stochastic noisy bandit problem with an unknown reward function $f^*$ in a known function class $\\mathcal{F}$. Formally, a model $M$ maps arms $\u03c0$ to a probability distribution $M(\u03c0)$ of reward. A model class $\\mathcal{M}$ is a collection of models. For each model $M$, define its mean reward function $f^M(\u03c0)=\\mathbb{E}_{r \\sim M(\u03c0)}[r]$. In the bandit learning problem, we proceed in rounds, pulling one arm $\u03c0$ each round and observing a reward sampled from $M(\u03c0)$. With knowledge of $\\mathcal{M}$, supposing that the true model $M\\in \\mathcal{M}$, the objective is to identify an arm $\\hat\u03c0$ of near-maximal mean reward $f^M(\\hat\u03c0)$ with high probability in a bounded number of rounds. If this is possible, then the model class is said to be learnable. Importantly, a result of \\cite{hanneke2023bandit} shows there exist model classes for which learnability is undecidable. However, the model class they consider features deterministic rewards, and they raise the question of whether learnability is decidable for classes containing sufficiently noisy models. For the first time, we answer this question in the positive by giving a complete characterization of learnability for model classes with arbitrary noise. In addition to that, we also describe the full spectrum of possible optimal query complexities. Further, we prove adaptivity is sometimes necessary to achieve the optimal query complexity. Last, we revisit an important complexity measure for interactive decision making, the Decision-Estimation-Coefficient \\citep{foster2021statistical,foster2023tight}, and propose a new variant of the DEC which also characterizes learnability in this setting.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09600",
        "abstract url": "https://arxiv.org/abs/2410.09600",
        "title": "The Fragility of Fairness: Causal Sensitivity Analysis for Fair Machine Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Fairness metrics are a core tool in the fair machine learning literature (FairML), used to determine that ML models are, in some sense, \"fair\". Real-world data, however, are typically plagued by various measurement biases and other violated assumptions, which can render fairness assessments meaningless. We adapt tools from causal sensitivity analysis to the FairML context, providing a general framework which (1) accommodates effectively any combination of fairness metric and bias that can be posed in the \"oblivious setting\"; (2) allows researchers to investigate combinations of biases, resulting in non-linear sensitivity; and (3) enables flexible encoding of domain-specific constraints and assumptions. Employing this framework, we analyze the sensitivity of the most common parity metrics under 3 varieties of classifier across 14 canonical fairness datasets. Our analysis reveals the striking fragility of fairness assessments to even minor dataset biases. We show that causal sensitivity analysis provides a powerful and necessary toolkit for gauging the informativeness of parity metric evaluations. Our repository is available here: https://github.com/Jakefawkes/fragile_fair.",
        "subjects": [
            "cs.LG",
            "cs.CY"
        ],
        "comment": "Published at Neurips 2024 in the Dataset and Benchmarks Track"
    },
    {
        "paper id": "2410.09638",
        "abstract url": "https://arxiv.org/abs/2410.09638",
        "title": "On Goodhart's law, with an application to value alignment",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "``When a measure becomes a target, it ceases to be a good measure'', this adage is known as {\\it Goodhart's law}. In this paper, we investigate formally this law and prove that it critically depends on the tail distribution of the discrepancy between the true goal and the measure that is optimized. Discrepancies with long-tail distributions favor a Goodhart's law, that is, the optimization of the measure can have a counter-productive effect on the goal. We provide a formal setting to assess Goodhart's law by studying the asymptotic behavior of the correlation between the goal and the measure, as the measure is optimized. Moreover, we introduce a distinction between a {\\it weak} Goodhart's law, when over-optimizing the metric is useless for the true goal, and a {\\it strong} Goodhart's law, when over-optimizing the metric is harmful for the true goal. A distinction which we prove to depend on the tail distribution. We stress the implications of this result to large-scale decision making and policies that are (and have to be) based on metrics, and propose numerous research directions to better assess the safety of such policies in general, and to the particularly concerning case where these policies are automated with algorithms.",
        "subjects": [
            "stat.ML",
            "cs.AI",
            "cs.LG",
            "math.ST"
        ],
        "comment": "47 pages, 11 figures"
    },
    {
        "paper id": "2410.09645",
        "abstract url": "https://arxiv.org/abs/2410.09645",
        "title": "AI Model Registries: A Foundational Tool for AI Governance",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "In this report, we propose the implementation of national registries for frontier AI models as a foundational tool for AI governance. We explore the rationale, design, and implementation of such registries, drawing on comparisons with registries in analogous industries to make recommendations for a registry that is efficient, unintrusive, and which will bring AI governance closer to parity with the governmental insight into other high-impact industries. We explore key information that should be collected, including model architecture, model size, compute and data used during training, and we survey the viability and utility of evaluations developed specifically for AI. Our proposal is designed to provide governmental insight and enhance AI safety while fostering innovation and minimizing the regulatory burden on developers. By providing a framework that respects intellectual property concerns and safeguards sensitive information, this registry approach supports responsible AI development without impeding progress. We propose that timely and accurate registration should be encouraged primarily through injunctive action, by requiring third parties to use only registered models, and secondarily through direct financial penalties for non-compliance. By providing a comprehensive framework for AI model registries, we aim to support policymakers in developing foundational governance structures to monitor and mitigate risks associated with advanced AI systems.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09660",
        "abstract url": "https://arxiv.org/abs/2410.09660",
        "title": "Structured Regularization for Constrained Optimization on the SPD Manifold",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Matrix-valued optimization tasks, including those involving symmetric positive definite (SPD) matrices, arise in a wide range of applications in machine learning, data science and statistics. Classically, such problems are solved via constrained Euclidean optimization, where the domain is viewed as a Euclidean space and the structure of the matrices (e.g., positive definiteness) enters as constraints. More recently, geometric approaches that leverage parametrizations of the problem as unconstrained tasks on the corresponding matrix manifold have been proposed. While they exhibit algorithmic benefits in many settings, they cannot directly handle additional constraints, such as inequality or sparsity constraints. A remedy comes in the form of constrained Riemannian optimization methods, notably, Riemannian Frank-Wolfe and Projected Gradient Descent. However, both algorithms require potentially expensive subroutines that can introduce computational bottlenecks in practise. To mitigate these shortcomings, we introduce a class of structured regularizers, based on symmetric gauge functions, which allow for solving constrained optimization on the SPD manifold with faster unconstrained methods. We show that our structured regularizers can be chosen to preserve or induce desirable structure, in particular convexity and \"difference of convex\" structure. We demonstrate the effectiveness of our approach in numerical experiments.",
        "subjects": [
            "math.OC",
            "cs.LG",
            "math.DG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09678",
        "abstract url": "https://arxiv.org/abs/2410.09678",
        "title": "Learning Orthogonal Multi-Index Models: A Fine-Grained Information Exponent Analysis",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The information exponent (Ben Arous et al. [2021]) -- which is equivalent to the lowest degree in the Hermite expansion of the link function for Gaussian single-index models -- has played an important role in predicting the sample complexity of online stochastic gradient descent (SGD) in various learning tasks. In this work, we demonstrate that, for multi-index models, focusing solely on the lowest degree can miss key structural details of the model and result in suboptimal rates. Specifically, we consider the task of learning target functions of form $f_*(\\mathbf{x}) = \\sum_{k=1}^{P} \u03c6(\\mathbf{v}_k^* \\cdot \\mathbf{x})$, where $P \\ll d$, the ground-truth directions $\\{ \\mathbf{v}_k^* \\}_{k=1}^P$ are orthonormal, and only the second and $2L$-th Hermite coefficients of the link function $\u03c6$ can be nonzero. Based on the theory of information exponent, when the lowest degree is $2L$, recovering the directions requires $d^{2L-1}\\mathrm{poly}(P)$ samples, and when the lowest degree is $2$, only the relevant subspace (not the exact directions) can be recovered due to the rotational invariance of the second-order terms. In contrast, we show that by considering both second- and higher-order terms, we can first learn the relevant space via the second-order terms, and then the exact directions using the higher-order terms, and the overall sample and complexity of online SGD is $d \\mathrm{poly}(P)$.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09692",
        "abstract url": "https://arxiv.org/abs/2410.09692",
        "title": "ALLoRA: Adaptive Learning Rate Mitigates LoRA Fatal Flaws",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Low-Rank Adaptation (LoRA) is the bread and butter of Large Language Model (LLM) finetuning. LoRA learns an additive low-rank perturbation, $AB$, of a pretrained matrix parameter $W$ to align the model to a new task or dataset with $W+AB$. We identify three core limitations to LoRA for finetuning--a setting that employs limited amount of data and training steps. First, LoRA employs Dropout to prevent overfitting. We prove that Dropout is only suitable for long training episodes but fails to converge to a reliable regularizer for short training episodes. Second, LoRA's initialization of $B$ at $0$ creates a slow training dynamic between $A$ and $B$. That dynamic is also exacerbated by Dropout that further slows the escape from $0$ for $B$ which is particularly harmful for short training episodes. Third, the scaling factor multiplying each LoRA additive perturbation creates ``short-sighted'' interactions between the LoRA modules of different layers. Motivated by principled analysis of those limitations, we find an elegant solution: a Dropout-free, scaling-free, LoRA with Adaptive Learning rate--coined ALLoRA. By scaling the per sample and per parameter gradients with a coefficient inversely proportional to parameters' $\\ell_2$ norm, ALLoRA alleviates those three limitations. As a by-product, ALLoRA removes two hyper-parameters from LoRA: the scaling factor and the dropout rate. Empirical results show that ALLoRA admits better accuracy than LoRA on various settings, including against recent LoRA variants such as Weight-Decomposed Low-Rank Adaptation (DoRA). Ablation studies show our solution is the optimal in a family of weight-dependent / output-dependent approaches on various LLMs including the latest Llama3.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09695",
        "abstract url": "https://arxiv.org/abs/2410.09695",
        "title": "Can In-context Learning Really Generalize to Out-of-distribution Tasks?",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In this work, we explore the mechanism of in-context learning (ICL) on out-of-distribution (OOD) tasks that were not encountered during training. To achieve this, we conduct synthetic experiments where the objective is to learn OOD mathematical functions through ICL using a GPT-2 model. We reveal that Transformers may struggle to learn OOD task functions through ICL. Specifically, ICL performance resembles implementing a function within the pretraining hypothesis space and optimizing it with gradient descent based on the in-context examples. Additionally, we investigate ICL's well-documented ability to learn unseen abstract labels in context. We demonstrate that such ability only manifests in the scenarios without distributional shifts and, therefore, may not serve as evidence of new-task-learning ability. Furthermore, we assess ICL's performance on OOD tasks when the model is pretrained on multiple tasks. Both empirical and theoretical analyses demonstrate the existence of the \\textbf{low-test-error preference} of ICL, where it tends to implement the pretraining function that yields low test error in the testing context. We validate this through numerical experiments. This new theoretical result, combined with our empirical findings, elucidates the mechanism of ICL in addressing OOD tasks.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Preprint, under review"
    },
    {
        "paper id": "2410.09697",
        "abstract url": "https://arxiv.org/abs/2410.09697",
        "title": "Provable Convergence and Limitations of Geometric Tempering for Langevin Dynamics",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Geometric tempering is a popular approach to sampling from challenging multi-modal probability distributions by instead sampling from a sequence of distributions which interpolate, using the geometric mean, between an easier proposal distribution and the target distribution. In this paper, we theoretically investigate the soundness of this approach when the sampling algorithm is Langevin dynamics, proving both upper and lower bounds. Our upper bounds are the first analysis in the literature under functional inequalities. They assert the convergence of tempered Langevin in continuous and discrete-time, and their minimization leads to closed-form optimal tempering schedules for some pairs of proposal and target distributions. Our lower bounds demonstrate a simple case where the geometric tempering takes exponential time, and further reveal that the geometric tempering can suffer from poor functional inequalities and slow convergence, even when the target distribution is well-conditioned. Overall, our results indicate that geometric tempering may not help, and can even be harmful for convergence.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "stat.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09713",
        "abstract url": "https://arxiv.org/abs/2410.09713",
        "title": "Agentic Information Retrieval",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "What will information entry look like in the next generation of digital products? Since the 1970s, user access to relevant information has relied on domain-specific architectures of information retrieval (IR). Over the past two decades, the advent of modern IR systems, including web search engines and personalized recommender systems, has greatly improved the efficiency of retrieving relevant information from vast data corpora. However, the core paradigm of these IR systems remains largely unchanged, relying on filtering a predefined set of candidate items. Since 2022, breakthroughs in large language models (LLMs) have begun transforming how information is accessed, establishing a new technical paradigm. In this position paper, we introduce Agentic Information Retrieval (Agentic IR), a novel IR paradigm shaped by the capabilities of LLM agents. Agentic IR expands the scope of accessible tasks and leverages a suite of new techniques to redefine information retrieval. We discuss three types of cutting-edge applications of agentic IR and the challenges faced. We propose that agentic IR holds promise for generating innovative applications, potentially becoming a central information entry point in future digital ecosystems.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": "11 pages, position paper"
    },
    {
        "paper id": "2410.10894",
        "abstract url": "https://arxiv.org/abs/2410.10894",
        "title": "COME: Test-time adaption by Conservatively Minimizing Entropy",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine learning models must continuously self-adjust themselves for novel data distribution in the open world. As the predominant principle, entropy minimization (EM) has been proven to be a simple yet effective cornerstone in existing test-time adaption (TTA) methods. While unfortunately its fatal limitation (i.e., overconfidence) tends to result in model collapse. For this issue, we propose to Conservatively Minimize the Entropy (COME), which is a simple drop-in replacement of traditional EM to elegantly address the limitation. In essence, COME explicitly models the uncertainty by characterizing a Dirichlet prior distribution over model predictions during TTA. By doing so, COME naturally regularizes the model to favor conservative confidence on unreliable samples. Theoretically, we provide a preliminary analysis to reveal the ability of COME in enhancing the optimization stability by introducing a data-adaptive lower bound on the entropy. Empirically, our method achieves state-of-the-art performance on commonly used benchmarks, showing significant improvements in terms of classification accuracy and uncertainty estimation under various settings including standard, life-long and open-world TTA, i.e., up to $34.5\\%$ improvement on accuracy and $15.1\\%$ on false positive rate.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "Ongoing work"
    },
    {
        "paper id": "2410.10896",
        "abstract url": "https://arxiv.org/abs/2410.10896",
        "title": "AT-MoE: Adaptive Task-planning Mixture of Experts via LoRA Approach",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The advent of Large Language Models (LLMs) has ushered in a new era of artificial intelligence, with the potential to transform various sectors through automation and insightful analysis. The Mixture of Experts (MoE) architecture has been proposed as a solution to enhance model performance in complex tasks. Yet, existing MoE models struggle with task-specific learning and interpretability, especially in fields like medicine where precision is critical. This paper introduces the Adaptive Task-planing Mixture of Experts(AT-MoE), an innovative architecture designed to address these limitations. We first train task-specific experts via LoRA approach to enhance problem-solving capabilities and interpretability in specialized areas. Subsequently, we introduce a layer-wise adaptive grouped routing module that optimizes module fusion based on complex task instructions, ensuring optimal task resolution. The grouped routing module first perform overall weight allocation from the dimension of the expert group, and then conduct local weight normalization adjustments within the group. This design maintains multi-dimensional balance, controllability, and interpretability, while facilitating task-specific fusion in response to complex instructions.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.11576",
        "abstract url": "https://arxiv.org/abs/2410.11576",
        "title": "The Best of Both Worlds: On the Dilemma of Out-of-distribution Detection",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Out-of-distribution (OOD) detection is essential for model trustworthiness which aims to sensitively identify semantic OOD samples and robustly generalize for covariate-shifted OOD samples. However, we discover that the superior OOD detection performance of state-of-the-art methods is achieved by secretly sacrificing the OOD generalization ability. Specifically, the classification accuracy of these models could deteriorate dramatically when they encounter even minor noise. This phenomenon contradicts the goal of model trustworthiness and severely restricts their applicability in real-world scenarios. What is the hidden reason behind such a limitation? In this work, we theoretically demystify the ``\\textit{sensitive-robust}'' dilemma that lies in many existing OOD detection methods. Consequently, a theory-inspired algorithm is induced to overcome such a dilemma. By decoupling the uncertainty learning objective from a Bayesian perspective, the conflict between OOD detection and OOD generalization is naturally harmonized and a dual-optimal performance could be expected. Empirical studies show that our method achieves superior performance on standard benchmarks. To our best knowledge, this work is the first principled OOD detection method that achieves state-of-the-art OOD detection performance without compromising OOD generalization ability. Our code is available at \\href{https://github.com/QingyangZhang/DUL}{https://github.com/QingyangZhang/DUL}.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "Accepted by NeurlPS24. Code is available at https://github.com/QingyangZhang/DUL"
    },
    {
        "paper id": "2410.09377",
        "abstract url": "https://arxiv.org/abs/2410.09377",
        "title": "GEM-VPC: A dual Graph-Enhanced Multimodal integration for Video Paragraph Captioning",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video Paragraph Captioning (VPC) aims to generate paragraph captions that summarises key events within a video. Despite recent advancements, challenges persist, notably in effectively utilising multimodal signals inherent in videos and addressing the long-tail distribution of words. The paper introduces a novel multimodal integrated caption generation framework for VPC that leverages information from various modalities and external knowledge bases. Our framework constructs two graphs: a 'video-specific' temporal graph capturing major events and interactions between multimodal information and commonsense knowledge, and a 'theme graph' representing correlations between words of a specific theme. These graphs serve as input for a transformer network with a shared encoder-decoder architecture. We also introduce a node selection module to enhance decoding efficiency by selecting the most relevant nodes from the graphs. Our results demonstrate superior performance across benchmark datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09396",
        "abstract url": "https://arxiv.org/abs/2410.09396",
        "title": "ExpGest: Expressive Speaker Generation Using Diffusion Model and Hybrid Audio-Text Guidance",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Existing gesture generation methods primarily focus on upper body gestures based on audio features, neglecting speech content, emotion, and locomotion. These limitations result in stiff, mechanical gestures that fail to convey the true meaning of audio content. We introduce ExpGest, a novel framework leveraging synchronized text and audio information to generate expressive full-body gestures. Unlike AdaIN or one-hot encoding methods, we design a noise emotion classifier for optimizing adversarial direction noise, avoiding melody distortion and guiding results towards specified emotions. Moreover, aligning semantic and gestures in the latent space provides better generalization capabilities. ExpGest, a diffusion model-based gesture generation framework, is the first attempt to offer mixed generation modes, including audio-driven gestures and text-shaped motion. Experiments show that our framework effectively learns from combined text-driven motion and audio-induced gesture datasets, and preliminary results demonstrate that ExpGest achieves more expressive, natural, and controllable global motion in speakers compared to state-of-the-art models.",
        "subjects": [
            "cs.SD",
            "cs.CV",
            "eess.AS"
        ],
        "comment": "Accepted by ICME 2024"
    },
    {
        "paper id": "2410.09400",
        "abstract url": "https://arxiv.org/abs/2410.09400",
        "title": "CtrLoRA: An Extensible and Efficient Framework for Controllable Image Generation",
        "rating": "0",
        "keywords": [
            [
                "diffusion",
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently, large-scale diffusion models have made impressive progress in text-to-image (T2I) generation. To further equip these T2I models with fine-grained spatial control, approaches like ControlNet introduce an extra network that learns to follow a condition image. However, for every single condition type, ControlNet requires independent training on millions of data pairs with hundreds of GPU hours, which is quite expensive and makes it challenging for ordinary users to explore and develop new types of conditions. To address this problem, we propose the CtrLoRA framework, which trains a Base ControlNet to learn the common knowledge of image-to-image generation from multiple base conditions, along with condition-specific LoRAs to capture distinct characteristics of each condition. Utilizing our pretrained Base ControlNet, users can easily adapt it to new conditions, requiring as few as 1,000 data pairs and less than one hour of single-GPU training to obtain satisfactory results in most scenarios. Moreover, our CtrLoRA reduces the learnable parameters by 90% compared to ControlNet, significantly lowering the threshold to distribute and deploy the model weights. Extensive experiments on various types of conditions demonstrate the efficiency and effectiveness of our method. Codes and model weights will be released at https://github.com/xyfJASON/ctrlora.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09437",
        "abstract url": "https://arxiv.org/abs/2410.09437",
        "title": "MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning",
        "rating": "0",
        "keywords": [
            [
                "Parameter-efficient",
                "PEFT",
                "efficient fine-tuning"
            ],
            [
                "industrial"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Parameter-efficient fine-tuning (PEFT) has been widely employed for domain adaptation, with LoRA being one of the most prominent methods due to its simplicity and effectiveness. However, in multi-task learning (MTL) scenarios, LoRA tends to obscure the distinction between tasks by projecting sparse high-dimensional features from different tasks into the same dense low-dimensional intrinsic space. This leads to task interference and suboptimal performance for LoRA and its variants. To tackle this challenge, we propose MTL-LoRA, which retains the advantages of low-rank adaptation while significantly enhancing multi-task learning capabilities. MTL-LoRA augments LoRA by incorporating additional task-adaptive parameters that differentiate task-specific information and effectively capture shared knowledge across various tasks within low-dimensional spaces. This approach enables large language models (LLMs) pre-trained on general corpus to adapt to different target task domains with a limited number of trainable parameters. Comprehensive experimental results, including evaluations on public academic benchmarks for natural language understanding, commonsense reasoning, and image-text understanding, as well as real-world industrial text Ads relevance datasets, demonstrate that MTL-LoRA outperforms LoRA and its various variants with comparable or even fewer learnable parameters in multitask learning.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "12 Pages, 4 Figures"
    },
    {
        "paper id": "2410.09492",
        "abstract url": "https://arxiv.org/abs/2410.09492",
        "title": "A Simple yet Effective Subway Self-positioning Method based on Aerial-view Sleeper Detection",
        "rating": "0",
        "keywords": [
            [
                "navigation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the rapid development of urban underground rail vehicles,subway positioning, which plays a fundamental role in the traffic navigation and collision avoidance systems, has become a research hot-spot these years. Most current subway positioning methods rely on localization beacons densely pre-installed alongside the railway tracks, requiring massive costs for infrastructure and maintenance, while commonly lacking flexibility and anti-interference ability. In this paper, we propose a low-cost and real-time visual-assisted self-localization framework to address the robust and convenient positioning problem for subways. Firstly, we perform aerial view rail sleeper detection based on the fast and efficient YOLOv8n network. The detection results are then used to achieve real-time correction of mileage values combined with geometric positioning information, obtaining precise subway locations. Front camera Videos for subway driving scenes along a 6.9 km route are collected and annotated from the simulator for validation of the proposed method. Experimental results show that our aerial view sleeper detection algorithm can efficiently detect sleeper positions with F1-score of 0.929 at 1111 fps, and that the proposed positioning framework achieves a mean percentage error of 0.1\\%, demonstrating its continuous and high-precision self-localization capability.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "11 pages,8 figures, under review for IEEE Sensors Journal publication"
    },
    {
        "paper id": "2410.09503",
        "abstract url": "https://arxiv.org/abs/2410.09503",
        "title": "SLAM-AAC: Enhancing Audio Captioning with Paraphrasing Augmentation and CLAP-Refine through LLMs",
        "rating": "0",
        "keywords": [
            [
                "SLAM"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Automated Audio Captioning (AAC) aims to generate natural textual descriptions for input audio signals. Recent progress in audio pre-trained models and large language models (LLMs) has significantly enhanced audio understanding and textual reasoning capabilities, making improvements in AAC possible. In this paper, we propose SLAM-AAC to further enhance AAC with paraphrasing augmentation and CLAP-Refine through LLMs. Our approach uses the self-supervised EAT model to extract fine-grained audio representations, which are then aligned with textual embeddings via lightweight linear layers. The caption generation LLM is efficiently fine-tuned using the LoRA adapter. Drawing inspiration from the back-translation method in machine translation, we implement paraphrasing augmentation to expand the Clotho dataset during pre-training. This strategy helps alleviate the limitation of scarce audio-text pairs and generates more diverse captions from a small set of audio clips. During inference, we introduce the plug-and-play CLAP-Refine strategy to fully exploit multiple decoding outputs, akin to the n-best rescoring strategy in speech recognition. Using the CLAP model for audio-text similarity calculation, we could select the textual descriptions generated by multiple searching beams that best match the input audio. Experimental results show that SLAM-AAC achieves state-of-the-art performance on Clotho V2 and AudioCaps, surpassing previous mainstream models.",
        "subjects": [
            "eess.AS",
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09508",
        "abstract url": "https://arxiv.org/abs/2410.09508",
        "title": "CollabEdit: Towards Non-destructive Collaborative Knowledge Editing",
        "rating": "0",
        "keywords": [
            [
                "Knowledge Editing"
            ],
            [
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "Collaborative learning of large language models (LLMs) has emerged as a new paradigm for utilizing private data from different parties to guarantee efficiency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also garnered increased attention due to its ability to manipulate the behaviors of LLMs explicitly, yet leaves the collaborative KE case (in which knowledge edits of multiple parties are aggregated in a privacy-preserving and continual manner) unexamined. To this end, this manuscript dives into the first investigation of collaborative KE, in which we start by carefully identifying the unique three challenges therein, including knowledge overlap, knowledge conflict, and knowledge forgetting. We then propose a non-destructive collaborative KE framework, COLLABEDIT, which employs a novel model merging mechanism to mimic the global KE behavior while preventing the severe performance drop. Extensive experiments on two canonical datasets demonstrate the superiority of COLLABEDIT compared to other destructive baselines, and results shed light on addressing three collaborative KE challenges and future applications.",
        "subjects": [
            "cs.CL",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09510",
        "abstract url": "https://arxiv.org/abs/2410.09510",
        "title": "Scito2M: A 2 Million, 30-Year Cross-disciplinary Dataset for Temporal Scientometric Analysis",
        "rating": "0",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Understanding the creation, evolution, and dissemination of scientific knowledge is crucial for bridging diverse subject areas and addressing complex global challenges such as pandemics, climate change, and ethical AI. Scientometrics, the quantitative and qualitative study of scientific literature, provides valuable insights into these processes. We introduce Scito2M, a longitudinal scientometric dataset with over two million academic publications, providing comprehensive contents information and citation graphs to support cross-disciplinary analyses. Using Scito2M, we conduct a temporal study spanning over 30 years to explore key questions in scientometrics: the evolution of academic terminology, citation patterns, and interdisciplinary knowledge exchange. Our findings reveal critical insights, such as disparities in epistemic cultures, knowledge production modes, and citation practices. For example, rapidly developing, application-driven fields like LLMs exhibit significantly shorter citation age (2.48 years) compared to traditional theoretical disciplines like oral history (9.71 years).",
        "subjects": [
            "cs.DL",
            "cs.CL"
        ],
        "comment": "19 pages"
    },
    {
        "paper id": "2410.09519",
        "abstract url": "https://arxiv.org/abs/2410.09519",
        "title": "Pic@Point: Cross-Modal Learning by Local and Global Point-Picture Correspondence",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Self-supervised pre-training has achieved remarkable success in NLP and 2D vision. However, these advances have yet to translate to 3D data. Techniques like masked reconstruction face inherent challenges on unstructured point clouds, while many contrastive learning tasks lack in complexity and informative value. In this paper, we present Pic@Point, an effective contrastive learning method based on structural 2D-3D correspondences. We leverage image cues rich in semantic and contextual knowledge to provide a guiding signal for point cloud representations at various abstraction levels. Our lightweight approach outperforms state-of-the-art pre-training methods on several 3D benchmarks.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Accepted at ACML 2024"
    },
    {
        "paper id": "2410.09580",
        "abstract url": "https://arxiv.org/abs/2410.09580",
        "title": "SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search",
        "rating": "0",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "Recommendation"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Conversational Recommender Systems (CRS) proactively engage users in interactive dialogues to elicit user preferences and provide personalized recommendations. Existing methods train Reinforcement Learning (RL)-based agent with greedy action selection or sampling strategy, and may suffer from suboptimal conversational planning. To address this, we present a novel Monte Carlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a conversational agent (S-agent) and a conversational planner (S-planner). S-planner builds a conversational search tree with MCTS based on the initial actions proposed by S-agent to find conversation plans. The best conversation plans from S-planner are used to guide the training of S-agent, creating a self-training loop where S-agent can iteratively improve its capability for conversational planning. Furthermore, we propose an efficient variant SAPIENT-e for trade-off between training efficiency and performance. Extensive experiments on four benchmark datasets validate the effectiveness of our approach, showing that SAPIENT outperforms the state-of-the-art baselines.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09592",
        "abstract url": "https://arxiv.org/abs/2410.09592",
        "title": "ControLRM: Fast and Controllable 3D Generation via Large Reconstruction Model",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Despite recent advancements in 3D generation methods, achieving controllability still remains a challenging issue. Current approaches utilizing score-distillation sampling are hindered by laborious procedures that consume a significant amount of time. Furthermore, the process of first generating 2D representations and then mapping them to 3D lacks internal alignment between the two forms of representation. To address these challenges, we introduce ControLRM, an end-to-end feed-forward model designed for rapid and controllable 3D generation using a large reconstruction model (LRM). ControLRM comprises a 2D condition generator, a condition encoding transformer, and a triplane decoder transformer. Instead of training our model from scratch, we advocate for a joint training framework. In the condition training branch, we lock the triplane decoder and reuses the deep and robust encoding layers pretrained with millions of 3D data in LRM. In the image training branch, we unlock the triplane decoder to establish an implicit alignment between the 2D and 3D representations. To ensure unbiased evaluation, we curate evaluation samples from three distinct datasets (G-OBJ, GSO, ABO) rather than relying on cherry-picking manual generation. The comprehensive experiments conducted on quantitative and qualitative comparisons of 3D controllability and generation quality demonstrate the strong generalization capacity of our proposed approach.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Draft version. This paper is still in submission. For access to our project page and code, please visit: https://toughstonex.github.io/controlrm.github.io/"
    },
    {
        "paper id": "2410.09612",
        "abstract url": "https://arxiv.org/abs/2410.09612",
        "title": "RailYolact -- A Yolact Focused on edge for Real-Time Rail Segmentation",
        "rating": "0",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Ensuring obstacle avoidance on the rail surface is crucial for the safety of autonomous driving trains and its first step is to segment the regions of the rail. We chose to build upon Yolact for our work. To address the issue of rough edge in the rail masks predicted by the model, we incorporated the edge information extracted by edge operator into the original Yolact's loss function to emphasize the model's focus on rail edges. Additionally, we applied box filter to smooth the jagged ground truth mask edges cause by linear interpolation. Since the integration of edge information and smooth process only occurred during the training process, the inference speed of the model remained unaffected. The experiments results on our custom rail dataset demonstrated an improvement in the prediction accuracy. Moreover, the results on Cityscapes showed a 4.1 and 4.6 improvement in $AP$ and $AP_{50}$ , respectively, compared to Yolact.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09613",
        "abstract url": "https://arxiv.org/abs/2410.09613",
        "title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ",
        "rating": "0",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in transformer-based language models have sparked research into their logical reasoning capabilities. Most of the benchmarks used to evaluate these models are simple: generated from short (fragments of) first-order logic sentences with only a few logical operators and quantifiers. We construct the natural language dataset, DELTA$_D$, using the expressive description logic language $\\mathcal{ALCQ}$. DELTA$_D$ comprises 384K examples and increases in two dimensions: i) reasoning depth, and ii) linguistic complexity. In this way, we systematically investigate the logical reasoning capabilities of a supervised fine-tuned DeBERTa-based model and two large language models (GPT-3.5, GPT-4) with few-shot prompting. We show that the DeBERTa-based model fine-tuned on our dataset can master the entailment checking task. Moreover, the performance of GPTs can improve significantly even when a small number of samples is provided (9 shots). We open-source our code and datasets.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)"
    },
    {
        "paper id": "2410.09614",
        "abstract url": "https://arxiv.org/abs/2410.09614",
        "title": "Exploring Behavior-Relevant and Disentangled Neural Dynamics with Generative Diffusion Models",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Understanding the neural basis of behavior is a fundamental goal in neuroscience. Current research in large-scale neuro-behavioral data analysis often relies on decoding models, which quantify behavioral information in neural data but lack details on behavior encoding. This raises an intriguing scientific question: ``how can we enable in-depth exploration of neural representations in behavioral tasks, revealing interpretable neural dynamics associated with behaviors''. However, addressing this issue is challenging due to the varied behavioral encoding across different brain regions and mixed selectivity at the population level. To tackle this limitation, our approach, named ``BeNeDiff'', first identifies a fine-grained and disentangled neural subspace using a behavior-informed latent variable model. It then employs state-of-the-art generative diffusion models to synthesize behavior videos that interpret the neural dynamics of each latent factor. We validate the method on multi-session datasets containing widefield calcium imaging recordings across the dorsal cortex. Through guiding the diffusion model to activate individual latent factors, we verify that the neural dynamics of latent factors in the disentangled neural subspace provide interpretable quantifications of the behaviors of interest. At the same time, the neural subspace in BeNeDiff demonstrates high disentanglement and neural reconstruction quality.",
        "subjects": [
            "q-bio.NC",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09690",
        "abstract url": "https://arxiv.org/abs/2410.09690",
        "title": "FAMOUS: High-Fidelity Monocular 3D Human Digitization Using View Synthesis",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The advancement in deep implicit modeling and articulated models has significantly enhanced the process of digitizing human figures in 3D from just a single image. While state-of-the-art methods have greatly improved geometric precision, the challenge of accurately inferring texture remains, particularly in obscured areas such as the back of a person in frontal-view images. This limitation in texture prediction largely stems from the scarcity of large-scale and diverse 3D datasets, whereas their 2D counterparts are abundant and easily accessible. To address this issue, our paper proposes leveraging extensive 2D fashion datasets to enhance both texture and shape prediction in 3D human digitization. We incorporate 2D priors from the fashion dataset to learn the occluded back view, refined with our proposed domain alignment strategy. We then fuse this information with the input image to obtain a fully textured mesh of the given person. Through extensive experimentation on standard 3D human benchmarks, we demonstrate the superior performance of our approach in terms of both texture and geometry. Code and dataset is available at https://github.com/humansensinglab/FAMOUS.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09699",
        "abstract url": "https://arxiv.org/abs/2410.09699",
        "title": "Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don't Know\", and Reducing Hallucination in RAG",
        "rating": "0",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Hallucination is a key roadblock for applications of Large Language Models (LLMs), particularly for enterprise applications that are sensitive to information accuracy. To address this issue, two general approaches have been explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated information as context, and fine-tuning the LLMs with new information and desired output styles. In this paper, we propose Honest AI: a novel strategy to fine-tune \"small\" language models to say \"I don't know\" to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark. In addition, our approach emphasizes the use of relatively small models with fewer than 10 billion parameters, promoting resource efficiency.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09704",
        "abstract url": "https://arxiv.org/abs/2410.09704",
        "title": "EchoPrime: A Multi-Video View-Informed Vision-Language Model for Comprehensive Echocardiography Interpretation",
        "rating": "0",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "healthcare",
                "clinical",
                "cardiac"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Echocardiography is the most widely used cardiac imaging modality, capturing ultrasound video data to assess cardiac structure and function. Artificial intelligence (AI) in echocardiography has the potential to streamline manual tasks and improve reproducibility and precision. However, most echocardiography AI models are single-view, single-task systems that do not synthesize complementary information from multiple views captured during a full exam, and thus lead to limited performance and scope of applications. To address this problem, we introduce EchoPrime, a multi-view, view-informed, video-based vision-language foundation model trained on over 12 million video-report pairs. EchoPrime uses contrastive learning to train a unified embedding model for all standard views in a comprehensive echocardiogram study with representation of both rare and common diseases and diagnoses. EchoPrime then utilizes view-classification and a view-informed anatomic attention model to weight video-specific interpretations that accurately maps the relationship between echocardiographic views and anatomical structures. With retrieval-augmented interpretation, EchoPrime integrates information from all echocardiogram videos in a comprehensive study and performs holistic comprehensive clinical echocardiography interpretation. In datasets from two independent healthcare systems, EchoPrime achieves state-of-the art performance on 23 diverse benchmarks of cardiac form and function, surpassing the performance of both task-specific approaches and prior foundation models. Following rigorous clinical evaluation, EchoPrime can assist physicians in the automated preliminary assessment of comprehensive echocardiography.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "30 pages, 3 tables, 3 figures"
    },
    {
        "paper id": "2410.09477",
        "abstract url": "https://arxiv.org/abs/2410.09477",
        "title": "Efficient Bipartite Graph Embedding Induced by Clustering Constraints",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Bipartite graph embedding (BGE) maps nodes to compressed embedding vectors that can reflect the hidden topological features of the network, and learning high-quality BGE is crucial for facilitating downstream applications such as recommender systems. However, most existing methods either struggle to efficiently learn embeddings suitable for users and items with fewer interactions, or exhibit poor scalability to handle large-scale networks. In this paper, we propose a Clustering Constraints induced BIpartite graph Embedding (CCBIE) as an integrated solution to both problems. CCBIE facilitates automatic and dynamic soft clustering of items in a top-down manner, and capturing macro-preference information of users through clusters. Specifically, by leveraging the cluster embedding matrix of items, CCBIE calculates the cluster assignment matrix for items and also captures the extent of user preferences across different clusters, thereby elucidating the similarity between users and items on a macro-scale level. CCBIE effectively preserves the global properties of bipartite graphs, maintaining the cluster structure of isomorphic nodes and accounting for long-range dependencies among heterogeneous nodes. Our approach significantly enhances user-item collaborative relation modeling by integrating adaptive clustering for relationship learning, thereby markedly improving prediction performance, particularly benefiting cold users and items. Extensive experiments indicate that CCBIE consistently and significantly improves accuracy over baseline models, particularly on sparse graphs, while also enhancing training speed and reducing memory requirements on large-scale bipartite graphs.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "10 pages, 7 figures"
    },
    {
        "paper id": "2410.09531",
        "abstract url": "https://arxiv.org/abs/2410.09531",
        "title": "PrivQuant: Communication-Efficient Private Inference with Quantized Network/Protocol Co-Optimization",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Private deep neural network (DNN) inference based on secure two-party computation (2PC) enables secure privacy protection for both the server and the client. However, existing secure 2PC frameworks suffer from a high inference latency due to enormous communication. As the communication of both linear and non-linear DNN layers reduces with the bit widths of weight and activation, in this paper, we propose PrivQuant, a framework that jointly optimizes the 2PC-based quantized inference protocols and the network quantization algorithm, enabling communication-efficient private inference. PrivQuant proposes DNN architecture-aware optimizations for the 2PC protocols for communication-intensive quantized operators and conducts graph-level operator fusion for communication reduction. Moreover, PrivQuant also develops a communication-aware mixed precision quantization algorithm to improve inference efficiency while maintaining high accuracy. The network/protocol co-optimization enables PrivQuant to outperform prior-art 2PC frameworks. With extensive experiments, we demonstrate PrivQuant reduces communication by $11\\times, 2.5\\times \\mathrm{and}~ 2.8\\times$, which results in $8.7\\times, 1.8\\times ~ \\mathrm{and}~ 2.4\\times$ latency reduction compared with SiRNN, COINN, and CoPriv, respectively.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": "ICCAD 2024"
    },
    {
        "paper id": "2410.09570",
        "abstract url": "https://arxiv.org/abs/2410.09570",
        "title": "GETS: Ensemble Temperature Scaling for Calibration in Graph Neural Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph Neural Networks deliver strong classification results but often suffer from poor calibration performance, leading to overconfidence or underconfidence. This is particularly problematic in high stakes applications where accurate uncertainty estimates are essential. Existing post hoc methods, such as temperature scaling, fail to effectively utilize graph structures, while current GNN calibration methods often overlook the potential of leveraging diverse input information and model ensembles jointly. In the paper, we propose Graph Ensemble Temperature Scaling, a novel calibration framework that combines input and model ensemble strategies within a Graph Mixture of Experts archi SOTA calibration techniques, reducing expected calibration error by 25 percent across 10 GNN benchmark datasets. Additionally, GETS is computationally efficient, scalable, and capable of selecting effective input combinations for improved calibration performance.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09590",
        "abstract url": "https://arxiv.org/abs/2410.09590",
        "title": "Bayesian Sheaf Neural Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Equipping graph neural networks with a convolution operation defined in terms of a cellular sheaf offers advantages for learning expressive representations of heterophilic graph data. The most flexible approach to constructing the sheaf is to learn it as part of the network as a function of the node features. However, this leaves the network potentially overly sensitive to the learned sheaf. As a counter-measure, we propose a variational approach to learning cellular sheaves within sheaf neural networks, yielding an architecture we refer to as a Bayesian sheaf neural network. As part of this work, we define a novel family of reparameterizable probability distributions on the rotation group $SO(n)$ using the Cayley transform. We evaluate the Bayesian sheaf neural network on several graph datasets, and show that our Bayesian sheaf models outperform deterministic sheaf models when training data is limited, and are less sensitive to the choice of hyperparameters.",
        "subjects": [
            "cs.LG",
            "cs.SI"
        ],
        "comment": "19 pages, 2 figures"
    },
    {
        "paper id": "2410.09596",
        "abstract url": "https://arxiv.org/abs/2410.09596",
        "title": "Mastering AI: Big Data, Deep Learning, and the Evolution of Large Language Models -- AutoML from Basics to State-of-the-Art Techniques",
        "rating": "-0.5",
        "keywords": [
            [
                "Architecture Search",
                "NAS"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This manuscript presents a comprehensive guide to Automated Machine Learning (AutoML), covering fundamental principles, practical implementations, and future trends. The paper is structured to assist both beginners and experienced practitioners, with detailed discussions on popular AutoML tools such as TPOT, AutoGluon, and Auto-Keras. It also addresses emerging topics like Neural Architecture Search (NAS) and AutoML's applications in deep learning. We believe this work will contribute to ongoing research and development in the field of AI and machine learning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "This book contains 170 pages and 5 figures"
    },
    {
        "paper id": "2410.09629",
        "abstract url": "https://arxiv.org/abs/2410.09629",
        "title": "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models",
        "rating": "-0.5",
        "keywords": [
            [
                "biomedicine"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Large language models (LLMs) are proficient in capturing factual knowledge across various domains. However, refining their capabilities on previously seen knowledge or integrating new knowledge from external sources remains a significant challenge. In this work, we propose a novel synthetic knowledge ingestion method called Ski, which leverages fine-grained synthesis, interleaved generation, and assemble augmentation strategies to construct high-quality data representations from raw knowledge sources. We then integrate Ski and its variations with three knowledge injection techniques: Retrieval Augmented Generation (RAG), Supervised Fine-tuning (SFT), and Continual Pre-training (CPT) to inject and refine knowledge in language models. Extensive empirical experiments are conducted on various question-answering tasks spanning finance, biomedicine, and open-generation domains to demonstrate that Ski significantly outperforms baseline methods by facilitating effective knowledge injection. We believe that our work is an important step towards enhancing the factual accuracy of LLM outputs by refining knowledge representation and injection capabilities.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "comment": "EMNLP 2024 main conference long paper"
    },
    {
        "paper id": "2410.09633",
        "abstract url": "https://arxiv.org/abs/2410.09633",
        "title": "DuoDiff: Accelerating Diffusion Models with a Dual-Backbone Approach",
        "rating": "-0.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Diffusion models have achieved unprecedented performance in image generation, yet they suffer from slow inference due to their iterative sampling process. To address this, early-exiting has recently been proposed, where the depth of the denoising network is made adaptive based on the (estimated) difficulty of each sampling step. Here, we discover an interesting \"phase transition\" in the sampling process of current adaptive diffusion models: the denoising network consistently exits early during the initial sampling steps, until it suddenly switches to utilizing the full network. Based on this, we propose accelerating generation by employing a shallower denoising network in the initial sampling steps and a deeper network in the later steps. We demonstrate empirically that our dual-backbone approach, DuoDiff, outperforms existing early-exit diffusion methods in both inference speed and generation quality. Importantly, DuoDiff is easy to implement and complementary to existing approaches for accelerating diffusion.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to NeurIPS, see https://openreview.net/forum?id=G7E4tNmmHD"
    },
    {
        "paper id": "2410.09667",
        "abstract url": "https://arxiv.org/abs/2410.09667",
        "title": "EquiJump: Protein Dynamics Simulation via SO(3)-Equivariant Stochastic Interpolants",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Mapping the conformational dynamics of proteins is crucial for elucidating their functional mechanisms. While Molecular Dynamics (MD) simulation enables detailed time evolution of protein motion, its computational toll hinders its use in practice. To address this challenge, multiple deep learning models for reproducing and accelerating MD have been proposed drawing on transport-based generative methods. However, existing work focuses on generation through transport of samples from prior distributions, that can often be distant from the data manifold. The recently proposed framework of stochastic interpolants, instead, enables transport between arbitrary distribution endpoints. Building upon this work, we introduce EquiJump, a transferable SO(3)-equivariant model that bridges all-atom protein dynamics simulation time steps directly. Our approach unifies diverse sampling methods and is benchmarked against existing models on trajectory data of fast folding proteins. EquiJump achieves state-of-the-art results on dynamics simulation with a transferable model on all of the fast folding proteins.",
        "subjects": [
            "cs.LG",
            "physics.chem-ph",
            "q-bio.BM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09681",
        "abstract url": "https://arxiv.org/abs/2410.09681",
        "title": "LoRD: Adapting Differentiable Driving Policies to Distribution Shifts",
        "rating": "-0.5",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Distribution shifts between operational domains can severely affect the performance of learned models in self-driving vehicles (SDVs). While this is a well-established problem, prior work has mostly explored naive solutions such as fine-tuning, focusing on the motion prediction task. In this work, we explore novel adaptation strategies for differentiable autonomy stacks consisting of prediction, planning, and control, perform evaluation in closed-loop, and investigate the often-overlooked issue of catastrophic forgetting. Specifically, we introduce two simple yet effective techniques: a low-rank residual decoder (LoRD) and multi-task fine-tuning. Through experiments across three models conducted on two real-world autonomous driving datasets (nuPlan, exiD), we demonstrate the effectiveness of our methods and highlight a significant performance gap between open-loop and closed-loop evaluation in prior approaches. Our approach improves forgetting by up to 23.33% and the closed-loop OOD driving score by 8.83% in comparison to standard fine-tuning.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Under Review"
    },
    {
        "paper id": "2410.09686",
        "abstract url": "https://arxiv.org/abs/2410.09686",
        "title": "Generalization of Compositional Tasks with Logical Specification via Implicit Planning",
        "rating": "-0.5",
        "keywords": [
            [
                "GNN",
                "graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In this work, we study the problem of learning generalizable policies for compositional tasks given by a logic specification. These tasks are composed by temporally extended subgoals. Due to dependencies of subgoals and long task horizon, previous reinforcement learning (RL) algorithms, e.g., task-conditioned and goal-conditioned policies, still suffer from slow convergence and sub-optimality when solving the generalization problem of compositional tasks. In order to tackle these issues, this paper proposes a new hierarchical RL framework for the efficient and optimal generalization of compositional tasks. In the high level, we propose a new implicit planner designed specifically for generalizing compositional tasks. Specifically, the planner produces the selection of next sub-task and estimates the multi-step return of completing the rest of task from current state. It learns a latent transition model and conducts planning in the latent space based on a graph neural network (GNN). Then, the next sub-task selected by the high level guides the low-level agent efficiently to solve long-horizon tasks and the multi-step return makes the low-level policy consider dependencies of future sub-tasks. We conduct comprehensive experiments to show the advantage of proposed framework over previous methods in terms of optimality and efficiency.",
        "subjects": [
            "cs.AI",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09693",
        "abstract url": "https://arxiv.org/abs/2410.09693",
        "title": "Neural Solver Selection for Combinatorial Optimization",
        "rating": "-0.5",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Machine learning has increasingly been employed to solve NP-hard combinatorial optimization problems, resulting in the emergence of neural solvers that demonstrate remarkable performance, even with minimal domain-specific knowledge. To date, the community has created numerous open-source neural solvers with distinct motivations and inductive biases. While considerable efforts are devoted to designing powerful single solvers, our findings reveal that existing solvers typically demonstrate complementary performance across different problem instances. This suggests that significant improvements could be achieved through effective coordination of neural solvers at the instance level. In this work, we propose the first general framework to coordinate the neural solvers, which involves feature extraction, selection model, and selection strategy, aiming to allocate each instance to the most suitable solvers. To instantiate, we collect several typical neural solvers with state-of-the-art performance as alternatives, and explore various methods for each component of the framework. We evaluated our framework on two extensively studied combinatorial optimization problems, Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP). Experimental results show that the proposed framework can effectively distribute instances and the resulting composite solver can achieve significantly better performance (e.g., reduce the optimality gap by 0.88\\% on TSPLIB and 0.71\\% on CVRPLIB) than the best individual neural solver with little extra time cost.",
        "subjects": [
            "math.OC",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09696",
        "abstract url": "https://arxiv.org/abs/2410.09696",
        "title": "Scalable Weibull Graph Attention Autoencoder for Modeling Document Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Although existing variational graph autoencoders (VGAEs) have been widely used for modeling and generating graph-structured data, most of them are still not flexible enough to approximate the sparse and skewed latent node representations, especially those of document relational networks (DRNs) with discrete observations. To analyze a collection of interconnected documents, a typical branch of Bayesian models, specifically relational topic models (RTMs), has proven their efficacy in describing both link structures and document contents of DRNs, which motives us to incorporate RTMs with existing VGAEs to alleviate their potential issues when modeling the generation of DRNs. In this paper, moving beyond the sophisticated approximate assumptions of traditional RTMs, we develop a graph Poisson factor analysis (GPFA), which provides analytic conditional posteriors to improve the inference accuracy, and extend GPFA to a multi-stochastic-layer version named graph Poisson gamma belief network (GPGBN) to capture the hierarchical document relationships at multiple semantic levels. Then, taking GPGBN as the decoder, we combine it with various Weibull-based graph inference networks, resulting in two variants of Weibull graph auto-encoder (WGAE), equipped with model inference algorithms. Experimental results demonstrate that our models can extract high-quality hierarchical latent document representations and achieve promising performance on various graph analytic tasks.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "Submit to T-PAMI"
    },
    {
        "paper id": "2410.09708",
        "abstract url": "https://arxiv.org/abs/2410.09708",
        "title": "Control the GNN: Utilizing Neural Controller with Lyapunov Stability for Test-Time Feature Reconstruction",
        "rating": "-0.5",
        "keywords": [
            [
                "GNN",
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The performance of graph neural networks (GNNs) is susceptible to discrepancies between training and testing sample distributions. Prior studies have attempted to enhance GNN performance by reconstructing node features during the testing phase without modifying the model parameters. However, these approaches lack theoretical analysis of the proximity between predictions and ground truth at test time. In this paper, we propose a novel node feature reconstruction method grounded in Lyapunov stability theory. Specifically, we model the GNN as a control system during the testing phase, considering node features as control variables. A neural controller that adheres to the Lyapunov stability criterion is then employed to reconstruct these node features, ensuring that the predictions progressively approach the ground truth at test time. We validate the effectiveness of our approach through extensive experiments across multiple datasets, demonstrating significant performance improvements.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.11577",
        "abstract url": "https://arxiv.org/abs/2410.11577",
        "title": "Breaking the Memory Wall for Heterogeneous Federated Learning via Model Splitting",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL) enables multiple devices to collaboratively train a shared model while preserving data privacy. Ever-increasing model complexity coupled with limited memory resources on the participating devices severely bottlenecks the deployment of FL in real-world scenarios. Thus, a framework that can effectively break the memory wall while jointly taking into account the hardware and statistical heterogeneity in FL is urgently required. In this paper, we propose SmartSplit, a framework that effectively reduces the memory footprint on the device side while guaranteeing the training progress and model accuracy for heterogeneous FL through model splitting.Towards this end, SmartSplit employs a hierarchical structure to adaptively guide the overall training process. In each training round, the central manager, hosted on the server, dynamically selects the participating devices and sets the cutting layer by jointly considering the memory budget, training capacity, and data distribution of each device. The MEC manager, deployed within the edge server, proceeds to split the local model and perform training of the server-side portion. Meanwhile, it fine-tunes the splitting points based on the time-evolving statistical importance. The on-device manager, embedded inside each mobile device, continuously monitors the local training status while employing cost-aware checkpointing to match the runtime dynamic memory budget. Extensive experiments on representative datasets are conducted on both commercial off-the-shelf mobile device testbeds. The experimental results show that SmartSplit excels in FL training on highly memory-constrained mobile SoCs, offering up to a 94% peak latency reduction and 100-fold memory savings. It enhances accuracy performance by 1.49%-57.18% and adaptively adjusts to dynamic memory budgets through cost-aware recomputation.",
        "subjects": [
            "cs.DC",
            "cs.LG"
        ],
        "comment": "Accepted by TPDS"
    },
    {
        "paper id": "2410.09374",
        "abstract url": "https://arxiv.org/abs/2410.09374",
        "title": "ESVO2: Direct Visual-Inertial Odometry with Stereo Event Cameras",
        "rating": "-1",
        "keywords": [
            [
                "Event Cameras"
            ],
            [
                "SLAM"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Event-based visual odometry is a specific branch of visual Simultaneous Localization and Mapping (SLAM) techniques, which aims at solving tracking and mapping sub-problems in parallel by exploiting the special working principles of neuromorphic (ie, event-based) cameras. Due to the motion-dependent nature of event data, explicit data association ie, feature matching under large-baseline view-point changes is hardly established, making direct methods a more rational choice. However, state-of-the-art direct methods are limited by the high computational complexity of the mapping sub-problem and the degeneracy of camera pose tracking in certain degrees of freedom (DoF) in rotation. In this paper, we resolve these issues by building an event-based stereo visual-inertial odometry system on top of our previous direct pipeline Event-based Stereo Visual Odometry. Specifically, to speed up the mapping operation, we propose an efficient strategy for sampling contour points according to the local dynamics of events. The mapping performance is also improved in terms of structure completeness and local smoothness by merging the temporal stereo and static stereo results. To circumvent the degeneracy of camera pose tracking in recovering the pitch and yaw components of general six-DoF motion, we introduce IMU measurements as motion priors via pre-integration. To this end, a compact back-end is proposed for continuously updating the IMU bias and predicting the linear velocity, enabling an accurate motion prediction for camera pose tracking. The resulting system scales well with modern high-resolution event cameras and leads to better global positioning accuracy in large-scale outdoor environments. Extensive evaluations on five publicly available datasets featuring different resolutions and scenarios justify the superior performance of the proposed system against five state-of-the-art methods.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09405",
        "abstract url": "https://arxiv.org/abs/2410.09405",
        "title": "Procedural Generation of Communication Networks in Power Systems",
        "rating": "-1",
        "keywords": [
            [
                "attack"
            ]
        ],
        "abstract": "Power system communication networks enable operators to remotely monitor and control field equipment. The sophistication of these networks is also increasing as operators continue the trend towards digitization, which is beneficial in integrating distributed energy resources. However, as the attack surface increases in size so too does the risk of cyberattacks. The topology, configuration and composition of communication networks is therefore confidential since this can provide information to attackers. As a result, the number of benchmarks available for research purposes is limited. A tool for procedurally generating communication network topologies is therefore proposed. While primarily intended as an enabler for public research into communication networks, this tool also allows general insights to be gained into the effect of communication network design on the vulnerability of networks to cyberattacks. The tool includes the ability to encapsulate network characteristics in JSON specification files, which is demonstrated with example Advanced Metering Infrastructure (AMI), Supervisory Control and Data Acquisition (SCADA) and Wide Area Monitoring (WAM) specification files. The SCADA network generation is then compared to a real-world case. Finally, the effect of network redundancy on the networks cyber resilience is investigated.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "9 pages, 11 figures, originally presented at the 15th DACH+ Energy Informatics Doctoral Workshop"
    },
    {
        "paper id": "2410.09406",
        "abstract url": "https://arxiv.org/abs/2410.09406",
        "title": "Quantum Neural Network for Accelerated Magnetic Resonance Imaging",
        "rating": "-1",
        "keywords": [
            [
                "Quantum"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Magnetic resonance image reconstruction starting from undersampled k-space data requires the recovery of many potential nonlinear features, which is very difficult for algorithms to recover these features. In recent years, the development of quantum computing has discovered that quantum convolution can improve network accuracy, possibly due to potential quantum advantages. This article proposes a hybrid neural network containing quantum and classical networks for fast magnetic resonance imaging, and conducts experiments on a quantum computer simulation system. The experimental results indicate that the hybrid network has achieved excellent reconstruction results, and also confirm the feasibility of applying hybrid quantum-classical neural networks into the image reconstruction of rapid magnetic resonance imaging.",
        "subjects": [
            "eess.IV",
            "cs.ET",
            "quant-ph"
        ],
        "comment": "Accepted at 2024 IEEE International Conference on Imaging Systems and Techniques (IST 2024)"
    },
    {
        "paper id": "2410.09431",
        "abstract url": "https://arxiv.org/abs/2410.09431",
        "title": "REGNet V2: End-to-End REgion-based Grasp Detection Network for Grippers of Different Sizes in Point Clouds",
        "rating": "-1",
        "keywords": [
            [
                "robotics"
            ]
        ],
        "abstract": "Grasping has been a crucial but challenging problem in robotics for many years. One of the most important challenges is how to make grasping generalizable and robust to novel objects as well as grippers in unstructured environments. We present \\regnet, a robotic grasping system that can adapt to different parallel jaws to grasp diversified objects. To support different grippers, \\regnet embeds the gripper parameters into point clouds, based on which it predicts suitable grasp configurations. It includes three components: Score Network (SN), Grasp Region Network (GRN), and Refine Network (RN). In the first stage, SN is used to filter suitable points for grasping by grasp confidence scores. In the second stage, based on the selected points, GRN generates a set of grasp proposals. Finally, RN refines the grasp proposals for more accurate and robust predictions. We devise an analytic policy to choose the optimal grasp to be executed from the predicted grasp set. To train \\regnet, we construct a large-scale grasp dataset containing collision-free grasp configurations using different parallel-jaw grippers. The experimental results demonstrate that \\regnet with the analytic policy achieves the highest success rate of $74.98\\%$ in real-world clutter scenes with $20$ objects, significantly outperforming several state-of-the-art methods, including GPD, PointNetGPD, and S4G. The code and dataset are available at https://github.com/zhaobinglei/REGNet-V2.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09434",
        "abstract url": "https://arxiv.org/abs/2410.09434",
        "title": "Greediness is not always a vice: Efficient Discovery Algorithms for Assignment Problems",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "Finding a maximum-weight matching is a classical and well-studied problem in computer science, solvable in cubic time in general graphs. We consider the specialization called assignment problem where the input is a bipartite graph, and introduce in this work the ``discovery'' variant considering edge weights that are not provided as input but must be queried, requiring additional and costly computations. We develop here discovery algorithms aiming to minimize the number of queried weights while providing guarantees on the computed solution. We first show in this work the inherent challenges of designing discovery algorithms for general assignment problems. We then provide and analyze several efficient greedy algorithms that can make use of natural assumptions about the order in which the nodes are processed by the algorithms. Our motivations for exploring this problem stem from finding practical solutions to a variation of maximum-weight matching in bipartite hypergraphs, a problem recently emerging in the formation of peer-to-peer energy sharing communities.",
        "subjects": [
            "cs.DS",
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09444",
        "abstract url": "https://arxiv.org/abs/2410.09444",
        "title": "Diabetic retinopathy image classification method based on GreenBen data augmentation",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "retinal"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "For the diagnosis of diabetes retinopathy (DR) images, this paper proposes a classification method based on artificial intelligence. The core lies in a new data augmentation method, GreenBen, which first extracts the green channel grayscale image from the retinal image and then performs Ben enhancement. Considering that diabetes macular edema (DME) is a complication closely related to DR, this paper constructs a joint classification framework of DR and DME based on multi task learning and attention module, and uses GreenBen to enhance its data to reduce the difference of DR images and improve the accuracy of model classification. We conducted extensive experiments on three publicly available datasets, and our method achieved the best results. For GreenBen, whether based on the ResNet50 network or the Swin Transformer network, whether for individual classification or joint DME classification, compared with other data augmentation methods, GreenBen achieved stable and significant improvements in DR classification results, with an accuracy increase of 10%.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09449",
        "abstract url": "https://arxiv.org/abs/2410.09449",
        "title": "Interpretable Video based Stress Detection with Self-Refine Chain-of-thought Reasoning",
        "rating": "-1",
        "keywords": [
            [
                "health",
                "healthcare",
                "physiological"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Stress detection is a critical area of research with significant implications for health monitoring and intervention systems. In this paper, we propose a novel interpretable approach for video-based stress detection, leveraging self-refine chain-of-thought reasoning to enhance both accuracy and transparency in decision-making processes. Our method focuses on extracting subtle behavioral and physiological cues from video sequences that indicate stress levels. By incorporating a chain-of-thought reasoning mechanism, the system refines its predictions iteratively, ensuring that the decision-making process can be traced and explained. The model also learns to self-refine through feedback loops, improving its reasoning capabilities over time. We evaluate our approach on several public and private datasets, demonstrating its superior performance in comparison to traditional video-based stress detection methods. Additionally, we provide comprehensive insights into the interpretability of the model's predictions, making the system highly valuable for applications in both healthcare and human-computer interaction domains.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Under Progress"
    },
    {
        "paper id": "2410.09501",
        "abstract url": "https://arxiv.org/abs/2410.09501",
        "title": "Fine-grained subjective visual quality assessment for high-fidelity compressed images",
        "rating": "-1",
        "keywords": [
            [
                "quality assessment"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Advances in image compression, storage, and display technologies have made high-quality images and videos widely accessible. At this level of quality, distinguishing between compressed and original content becomes difficult, highlighting the need for assessment methodologies that are sensitive to even the smallest visual quality differences. Conventional subjective visual quality assessments often use absolute category rating scales, ranging from ``excellent'' to ``bad''. While suitable for evaluating more pronounced distortions, these scales are inadequate for detecting subtle visual differences. The JPEG standardization project AIC is currently developing a subjective image quality assessment methodology for high-fidelity images. This paper presents the proposed assessment methods, a dataset of high-quality compressed images, and their corresponding crowdsourced visual quality ratings. It also outlines a data analysis approach that reconstructs quality scale values in just noticeable difference (JND) units. The assessment method uses boosting techniques on visual stimuli to help observers detect compression artifacts more clearly. This is followed by a rescaling process that adjusts the boosted quality values back to the original perceptual scale. This reconstruction yields a fine-grained, high-precision quality scale in JND units, providing more informative results for practical applications. The dataset and code to reproduce the results will be available at https://github.com/jpeg-aic/dataset-BTC-PTC-24.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Michela Testolina, Mohsen Jenadeleh contributed equally to this work, submitted to the Data Compression Conference (DCC) 2025"
    },
    {
        "paper id": "2410.09523",
        "abstract url": "https://arxiv.org/abs/2410.09523",
        "title": "Monitoring Drug-Induced Brain Activity Changes with Functional Ultrasound Imaging and Convolutional Neural Networks",
        "rating": "-1",
        "keywords": [
            [
                "red blood cells"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Functional ultrasound imaging (fUSI) is a cutting-edge technology that measures changes in cerebral blood volume (CBV) by detecting backscattered echoes from red blood cells moving within its field of view (FOV). It offers high spatiotemporal resolution and sensitivity, allowing for detailed visualization of cerebral blood flow dynamics. While fUSI has been utilized in preclinical drug development studies to explore the mechanisms of action of various drugs targeting the central nervous system, many of these studies have primarily focused on predetermined regions of interest (ROIs). This focus may overlook relevant brain activity outside these specific areas, which could influence the results. To address this limitation, we combined convolutional neural networks (CNNs) with fUSI to comprehensively understand the pharmacokinetic process of Dizocilpine, also known as MK-801, a drug that blocks the N-Methyl-D-aspartate (NMDA) receptor in the central nervous system. CNN and class activation mapping (CAM) revealed the spatiotemporal effects of MK-801, which originated in the cortex and propagated to the hippocampus, demonstrating the ability to detect dynamic drug effects over time. Additionally, CNN and CAM assessed the impact of anesthesia on the spatiotemporal hemodynamics of the brain, revealing no distinct patterns between early and late stages. The integration of fUSI and CNN provides a powerful tool to gain insights into the spatiotemporal dynamics of drug action in the brain. This combination enables a comprehensive and unbiased assessment of drug effects on brain function, potentially accelerating the development of new therapies in neuropharmacological studies.",
        "subjects": [
            "q-bio.NC",
            "eess.IV",
            "physics.med-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09524",
        "abstract url": "https://arxiv.org/abs/2410.09524",
        "title": "Emphasis Rendering for Conversational Text-to-Speech with Multi-modal Multi-scale Context Modeling",
        "rating": "-1",
        "keywords": [
            [
                "Text-to-Speech"
            ],
            [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Conversational Text-to-Speech (CTTS) aims to accurately express an utterance with the appropriate style within a conversational setting, which attracts more attention nowadays. While recognizing the significance of the CTTS task, prior studies have not thoroughly investigated speech emphasis expression, which is essential for conveying the underlying intention and attitude in human-machine interaction scenarios, due to the scarcity of conversational emphasis datasets and the difficulty in context understanding. In this paper, we propose a novel Emphasis Rendering scheme for the CTTS model, termed ER-CTTS, that includes two main components: 1) we simultaneously take into account textual and acoustic contexts, with both global and local semantic modeling to understand the conversation context comprehensively; 2) we deeply integrate multi-modal and multi-scale context to learn the influence of context on the emphasis expression of the current utterance. Finally, the inferred emphasis feature is fed into the neural speech synthesizer to generate conversational speech. To address data scarcity, we create emphasis intensity annotations on the existing conversational dataset (DailyTalk). Both objective and subjective evaluations suggest that our model outperforms the baseline models in emphasis rendering within a conversational setting. The code and audio samples are available at https://github.com/CodeStoreTTS/ER-CTTS.",
        "subjects": [
            "cs.CL",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "submitted to IEEE Transaction"
    },
    {
        "paper id": "2410.09539",
        "abstract url": "https://arxiv.org/abs/2410.09539",
        "title": "Bi-temporal Gaussian Feature Dependency Guided Change Detection in Remote Sensing Images",
        "rating": "-1",
        "keywords": [
            [
                "Remote Sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Change Detection (CD) enables the identification of alterations between images of the same area captured at different times. However, existing CD methods still struggle to address pseudo changes resulting from domain information differences in multi-temporal images and instances of detail errors caused by the loss and contamination of detail features during the upsampling process in the network. To address this, we propose a bi-temporal Gaussian distribution feature-dependent network (BGFD). Specifically, we first introduce the Gaussian noise domain disturbance (GNDD) module, which approximates distribution using image statistical features to characterize domain information, samples noise to perturb the network for learning redundant domain information, addressing domain information differences from a more fundamental perspective. Additionally, within the feature dependency facilitation (FDF) module, we integrate a novel mutual information difference loss ($L_{MI}$) and more sophisticated attention mechanisms to enhance the capabilities of the network, ensuring the acquisition of essential domain information. Subsequently, we have designed a novel detail feature compensation (DFC) module, which compensates for detail feature loss and contamination introduced during the upsampling process from the perspectives of enhancing local features and refining global features. The BGFD has effectively reduced pseudo changes and enhanced the detection capability of detail information. It has also achieved state-of-the-art performance on four publicly available datasets - DSIFN-CD, SYSU-CD, LEVIR-CD, and S2Looking, surpassing baseline models by +8.58%, +1.28%, +0.31%, and +3.76% respectively, in terms of the F1-Score metric.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09550",
        "abstract url": "https://arxiv.org/abs/2410.09550",
        "title": "DiffuTraj: A Stochastic Vessel Trajectory Prediction Approach via Guided Diffusion Process",
        "rating": "-1",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Trajectory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Maritime vessel maneuvers, characterized by their inherent complexity and indeterminacy, requires vessel trajectory prediction system capable of modeling the multi-modality nature of future motion states. Conventional stochastic trajectory prediction methods utilize latent variables to represent the multi-modality of vessel motion, however, tends to overlook the complexity and dynamics inherent in maritime behavior. In contrast, we explicitly simulate the transition of vessel motion from uncertainty towards a state of certainty, effectively handling future indeterminacy in dynamic scenes. In this paper, we present a novel framework (\\textit{DiffuTraj}) to conceptualize the trajectory prediction task as a guided reverse process of motion pattern uncertainty diffusion, in which we progressively remove uncertainty from maritime regions to delineate the intended trajectory. Specifically, we encode the previous states of the target vessel, vessel-vessel interactions, and the environment context as guiding factors for trajectory generation. Subsequently, we devise a transformer-based conditional denoiser to capture spatio-temporal dependencies, enabling the generation of trajectories better aligned for particular maritime environment. Comprehensive experiments on vessel trajectory prediction benchmarks demonstrate the superiority of our method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "containing 14pages, 9 figures and 3 tables; Submitted to IEEE Transactions on Intelligent Transportation Systems on 17-June-2024"
    },
    {
        "paper id": "2410.09561",
        "abstract url": "https://arxiv.org/abs/2410.09561",
        "title": "Distributed Area Coverage Control with Imprecise Robot Localization",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "This article examines the problem of area coverage for a network of mobile robots with imprecise agent localization. Each robot has uniform radial sensing ability, governed by first order kinodynamics. The convex-space is partitioned based on the Guaranteed Voronoi (GV) principle and each robot's area of responsibility corresponds to its GV-cell, bounded by hyperbolic arcs. The proposed control law is distributed, demands the positioning information about its GV-Delaunay neighbors and has an inherent collision avoidance property.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "In proceedings of the 24th Mediterranean Conference on Control and Automation, 2016. 6 pages, 10 figures, video available at https://sotiris.papatheodorou.xyz/papers/2016_MED_PST/2016_MED_PST.mp4"
    },
    {
        "paper id": "2410.09583",
        "abstract url": "https://arxiv.org/abs/2410.09583",
        "title": "POPoS: Improving Efficient and Robust Facial Landmark Detection with Parallel Optimal Position Search",
        "rating": "-1",
        "keywords": [
            [
                "Facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Achieving a balance between accuracy and efficiency is a critical challenge in facial landmark detection (FLD). This paper introduces the Parallel Optimal Position Search (POPoS), a high-precision encoding-decoding framework designed to address the fundamental limitations of traditional FLD methods. POPoS employs three key innovations: (1) Pseudo-range multilateration is utilized to correct heatmap errors, enhancing the precision of landmark localization. By integrating multiple anchor points, this approach minimizes the impact of individual heatmap inaccuracies, leading to robust overall positioning. (2) To improve the pseudo-range accuracy of selected anchor points, a new loss function, named multilateration anchor loss, is proposed. This loss function effectively enhances the accuracy of the distance map, mitigates the risk of local optima, and ensures optimal solutions. (3) A single-step parallel computation algorithm is introduced, significantly enhancing computational efficiency and reducing processing time. Comprehensive evaluations across five benchmark datasets demonstrate that POPoS consistently outperforms existing methods, particularly excelling in low-resolution scenarios with minimal computational overhead. These features establish POPoS as a highly efficient and accurate tool for FLD, with broad applicability in real-world scenarios. The code is available at https://github.com/teslatasy/PoPoS",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The experimental setup and results require further modifications and improvements to ensure the accuracy and quality of the paper. Therefore, we are requesting to withdraw the submission"
    },
    {
        "paper id": "2410.09595",
        "abstract url": "https://arxiv.org/abs/2410.09595",
        "title": "FiRework: Field Refinement Framework for Efficient Enhancement of Deformable Registration",
        "rating": "-1",
        "keywords": [
            [
                "MRI",
                "clinical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Deformable image registration remains a fundamental task in clinical practice, yet solving registration problems involving complex deformations remains challenging. Current deep learning-based registration methods employ continuous deformation to model large deformations, which often suffer from accumulated registration errors and interpolation inaccuracies. Moreover, achieving satisfactory results with these frameworks typically requires a large number of cascade stages, demanding substantial computational resources. Therefore, we propose a novel approach, the field refinement framework (FiRework), tailored for unsupervised deformable registration, aiming to address these challenges. In FiRework, we redesign the continuous deformation framework to mitigate the aforementioned errors. Notably, our FiRework requires only one level of recursion during training and supports continuous inference, offering improved efficacy compared to continuous deformation frameworks. We conducted experiments on two brain MRI datasets, enhancing two existing deformable registration networks with FiRework. The experimental results demonstrate the superior performance of our proposed framework in deformable registration. The code is publicly available at https://github.com/ZAX130/FiRework.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09628",
        "abstract url": "https://arxiv.org/abs/2410.09628",
        "title": "Enhanced Electronic Health Records Text Summarization Using Large Language Models",
        "rating": "-1",
        "keywords": [
            [
                "Health",
                "healthcare",
                "clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The development of Electronic Health Records summarization systems has revolutionized patient data management. Previous research advanced this field by adapting Large Language Models for clinical tasks, using diverse datasets to generate general EHR summaries. However, clinicians often require specific, focused summaries for quicker insights. This project builds on prior work by creating a system that generates clinician-preferred, focused summaries, improving EHR summarization for more efficient patient care. The proposed system leverages the Google Flan-T5 model to generate tailored EHR summaries based on clinician-specified topics. The approach involved fine-tuning the Flan-T5 model on an EHR question-answering dataset formatted in the Stanford Question Answering Dataset (SQuAD) style, which is a large-scale reading comprehension dataset with questions and answers. Fine-tuning utilized the Seq2SeqTrainer from the Hugging Face Transformers library with optimized hyperparameters. Key evaluation metrics demonstrated promising results: the system achieved an Exact Match (EM) score of 81.81%. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics showed strong performance, with ROUGE-1 at 96.03%, ROUGE-2 at 86.67%, and ROUGE-L at 96.10%. Additionally, the Bilingual Evaluation Understudy (BLEU) score was 63%, reflecting the model's coherence in generating summaries. By enhancing EHR summarization through LLMs, this project supports digital transformation efforts in healthcare, streamlining workflows, and enabling more personalized patient care.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09631",
        "abstract url": "https://arxiv.org/abs/2410.09631",
        "title": "Society of Medical Simplifiers",
        "rating": "-1",
        "keywords": [
            [
                "biomedical",
                "Medical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Medical text simplification is crucial for making complex biomedical literature more accessible to non-experts. Traditional methods struggle with the specialized terms and jargon of medical texts, lacking the flexibility to adapt the simplification process dynamically. In contrast, recent advancements in large language models (LLMs) present unique opportunities by offering enhanced control over text simplification through iterative refinement and collaboration between specialized agents. In this work, we introduce the Society of Medical Simplifiers, a novel LLM-based framework inspired by the \"Society of Mind\" (SOM) philosophy. Our approach leverages the strengths of LLMs by assigning five distinct roles, i.e., Layperson, Simplifier, Medical Expert, Language Clarifier, and Redundancy Checker, organized into interaction loops. This structure allows the agents to progressively improve text simplification while maintaining the complexity and accuracy of the original content. Evaluations on the Cochrane text simplification dataset demonstrate that our framework is on par with or outperforms state-of-the-art methods, achieving superior readability and content preservation through controlled simplification processes.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by Third Workshop on Text Simplification, Accessibility and Readability"
    },
    {
        "paper id": "2410.09632",
        "abstract url": "https://arxiv.org/abs/2410.09632",
        "title": "SciGisPy: a Novel Metric for Biomedical Text Simplification via Gist Inference Score",
        "rating": "-1",
        "keywords": [
            [
                "Biomedical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Biomedical literature is often written in highly specialized language, posing significant comprehension challenges for non-experts. Automatic text simplification (ATS) offers a solution by making such texts more accessible while preserving critical information. However, evaluating ATS for biomedical texts is still challenging due to the limitations of existing evaluation metrics. General-domain metrics like SARI, BLEU, and ROUGE focus on surface-level text features, and readability metrics like FKGL and ARI fail to account for domain-specific terminology or assess how well the simplified text conveys core meanings (gist). To address this, we introduce SciGisPy, a novel evaluation metric inspired by Gist Inference Score (GIS) from Fuzzy-Trace Theory (FTT). SciGisPy measures how well a simplified text facilitates the formation of abstract inferences (gist) necessary for comprehension, especially in the biomedical domain. We revise GIS for this purpose by introducing domain-specific enhancements, including semantic chunking, Information Content (IC) theory, and specialized embeddings, while removing unsuitable indexes. Our experimental evaluation on the Cochrane biomedical text simplification dataset demonstrates that SciGisPy outperforms the original GIS formulation, with a significant increase in correctly identified simplified texts (84% versus 44.8%). The results and a thorough ablation study confirm that SciGisPy better captures the essential meaning of biomedical content, outperforming existing approaches.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by he Third Workshop on Text Simplification, Accessibility and Readability"
    },
    {
        "paper id": "2410.09639",
        "abstract url": "https://arxiv.org/abs/2410.09639",
        "title": "Unique MS Lesion Identification from MRI",
        "rating": "-1",
        "keywords": [
            [
                "MRI",
                "Lesion"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Unique identification of multiple sclerosis (MS) white matter lesions (WMLs) is important to help characterize MS progression. WMLs are routinely identified from magnetic resonance images (MRIs) but the resultant total lesion load does not correlate well with EDSS; whereas mean unique lesion volume has been shown to correlate with EDSS. Our approach builds on prior work by incorporating Hessian matrix computation from lesion probability maps before using the random walker algorithm to estimate the volume of each unique lesion. Synthetic images demonstrate our ability to accurately count the number of lesions present. The takeaways, are: 1) that our method correctly identifies all lesions including many that are missed by previous methods; 2) we can better separate confluent lesions; and 3) we can accurately capture the total volume of WMLs in a given probability map. This work will allow new more meaningful statistics to be computed from WMLs in brain MRIs",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "5 pages, 5 figures, submitted to SPIE medical imaging conference"
    },
    {
        "paper id": "2410.09657",
        "abstract url": "https://arxiv.org/abs/2410.09657",
        "title": "Geometric Optimal Control of Mechanical Systems with Gravitational and Resistive Force",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "Optimal control plays a crucial role in numerous mechanical and robotic applications. Broadly, optimal control methods are divided into direct methods (which optimize trajectories directly via discretization) and indirect methods (which transform optimality conditions into equations that guarantee optimal trajectories). While direct methods could mask geometric insights into system dynamics due to discretization, indirect methods offer a deeper understanding of the system's geometry. In this paper, we propose a geometric framework for understanding optimal control in mechanical systems, focusing on the combined effects of inertia, drag, and gravitational forces. By modeling mechanical systems as configuration manifolds equipped with kinetic and drag metrics, alongside a potential field, we explore how these factors influence trajectory optimization. We derive optimal control equations incorporating these effects and apply them to two-link and UR5 robotic manipulators, demonstrating how manifold curvature and resistive forces shape optimal trajectories. This work offers a comprehensive geometric approach to optimal control, with broad applications to robotic systems.",
        "subjects": [
            "cs.RO",
            "math.OC"
        ],
        "comment": "6 pages, submitted to The International Conference on Robotics and Automation (ICRA)"
    },
    {
        "paper id": "2410.09674",
        "abstract url": "https://arxiv.org/abs/2410.09674",
        "title": "EG-SpikeFormer: Eye-Gaze Guided Transformer on Spiking Neural Networks for Medical Image Analysis",
        "rating": "-1",
        "keywords": [
            [
                "Medical",
                "healthcare",
                "clinical"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Neuromorphic computing has emerged as a promising energy-efficient alternative to traditional artificial intelligence, predominantly utilizing spiking neural networks (SNNs) implemented on neuromorphic hardware. Significant advancements have been made in SNN-based convolutional neural networks (CNNs) and Transformer architectures. However, their applications in the medical imaging domain remain underexplored. In this study, we introduce EG-SpikeFormer, an SNN architecture designed for clinical tasks that integrates eye-gaze data to guide the model's focus on diagnostically relevant regions in medical images. This approach effectively addresses shortcut learning issues commonly observed in conventional models, especially in scenarios with limited clinical data and high demands for model reliability, generalizability, and transparency. Our EG-SpikeFormer not only demonstrates superior energy efficiency and performance in medical image classification tasks but also enhances clinical relevance. By incorporating eye-gaze data, the model improves interpretability and generalization, opening new directions for the application of neuromorphic computing in healthcare.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09679",
        "abstract url": "https://arxiv.org/abs/2410.09679",
        "title": "Stability and Transparency in Mixed Reality Bilateral Human Teleoperation",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Recent work introduced the concept of human teleoperation (HT), where the remote robot typically considered in conventional bilateral teleoperation is replaced by a novice person wearing a mixed reality head mounted display and tracking the motion of a virtual tool controlled by an expert. HT has advantages in cost, complexity, and patient acceptance for telemedicine in low-resource communities or remote locations. However, the stability, transparency, and performance of bilateral HT are unexplored. In this paper, we therefore develop a mathematical model and simulation of the HT system using test data. We then analyze various control architectures with this model and implement them with the HT system to find the achievable performance, investigate stability, and determine the most promising teleoperation scheme in the presence of time delays. We show that instability in HT, while not destructive or dangerous, makes the system impossible to use. However, stable and transparent teleoperation are possible with small time delays (<200 ms) through 3-channel teleoperation, or with large time delays through model-mediated teleoperation with local pose and force feedback for the novice.",
        "subjects": [
            "cs.RO",
            "cs.HC",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09684",
        "abstract url": "https://arxiv.org/abs/2410.09684",
        "title": "Technical Design Review of Duke Robotics Club's Oogway: An AUV for RoboSub 2024",
        "rating": "-1",
        "keywords": [
            [
                "Robotics",
                "robot"
            ]
        ],
        "abstract": "The Duke Robotics Club is proud to present our robot for the 2024 RoboSub Competition: Oogway. Now in its second year, Oogway has been dramatically upgraded in both its capabilities and reliability. Oogway was built on the principle of independent, well-integrated, and reliable subsystems. Individual components and subsystems were tested and designed separately. Oogway's most advanced capabilities are a result of the tight integration between these subsystems. Such examples include a re-envisioned controls system, an entirely new electrical stack, advanced sonar integration, additional cameras and system monitoring, a new marker dropper, and a watertight capsule mechanism. These additions enabled Oogway to prequalify for Robosub 2024.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09691",
        "abstract url": "https://arxiv.org/abs/2410.09691",
        "title": "Robust 3D Point Clouds Classification based on Declarative Defenders",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "3D point cloud classification requires distinct models from 2D image classification due to the divergent characteristics of the respective input data. While 3D point clouds are unstructured and sparse, 2D images are structured and dense. Bridging the domain gap between these two data types is a non-trivial challenge to enable model interchangeability. Recent research using Lattice Point Classifier (LPC) highlights the feasibility of cross-domain applicability. However, the lattice projection operation in LPC generates 2D images with disconnected projected pixels. In this paper, we explore three distinct algorithms for mapping 3D point clouds into 2D images. Through extensive experiments, we thoroughly examine and analyze their performance and defense mechanisms. Leveraging current large foundation models, we scrutinize the feature disparities between regular 2D images and projected 2D images. The proposed approaches demonstrate superior accuracy and robustness against adversarial attacks. The generative model-based mapping algorithms yield regular 2D images, further minimizing the domain gap from regular 2D classification tasks. The source code is available at https://github.com/KaidongLi/pytorch-LatticePointClassifier.git.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.10901",
        "abstract url": "https://arxiv.org/abs/2410.10901",
        "title": "3DS: Decomposed Difficulty Data Selection's Case Study on LLM Medical Domain Adaptation",
        "rating": "-1",
        "keywords": [
            [
                "Medical",
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models(LLMs) excel in general tasks but struggle in specialized domains like healthcare due to limited domain-specific knowledge.Supervised Fine-Tuning(SFT) data construction for domain adaptation often relies on heuristic methods, such as GPT-4 annotation or manual data selection, with a data-centric focus on presumed diverse, high-quality datasets. However, these methods overlook the model's inherent knowledge distribution, introducing noise, redundancy, and irrelevant data, leading to a mismatch between the selected data and the model's learning task, resulting in suboptimal performance. To address this, we propose a two-stage model-centric data selection framework, Decomposed Difficulty Data Selection (3DS), which aligns data with the model's knowledge distribution for optimized adaptation. In Stage1, we apply Prompt-Driven Data Selection via Explicit Alignment, where the the model filters irrelevant or redundant data based on its internal knowledge. In Stage2, we perform Decomposed Difficulty Data Selection, where data selection is guided by our defined difficulty decomposition, using three metrics: Instruction Understanding, Response Confidence, and Response Correctness. Additionally, an attention-based importance weighting mechanism captures token importance for more accurate difficulty calibration. This two-stage approach ensures the selected data is not only aligned with the model's knowledge and preferences but also appropriately challenging for the model to learn, leading to more effective and targeted domain adaptation. In the case study of the medical domain, our extensive experiments on real-world healthcare datasets demonstrate the superiority of 3DS over exisiting methods in accuracy by over 5.29%. Our dataset and code will be open-sourced at https://anonymous.4open.science/r/3DS-E67F.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09385",
        "abstract url": "https://arxiv.org/abs/2410.09385",
        "title": "Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "This paper introduces Mamba4Cast, a zero-shot foundation model for time series forecasting. Based on the Mamba architecture and inspired by Prior-data Fitted Networks (PFNs), Mamba4Cast generalizes robustly across diverse time series tasks without the need for dataset specific fine-tuning. Mamba4Cast's key innovation lies in its ability to achieve strong zero-shot performance on real-world datasets while having much lower inference times than time series foundation models based on the transformer architecture. Trained solely on synthetic data, the model generates forecasts for entire horizons in a single pass, outpacing traditional auto-regressive approaches. Our experiments show that Mamba4Cast performs competitively against other state-of-the-art foundation models in various data sets while scaling significantly better with the prediction length. The source code can be accessed at https://github.com/automl/Mamba4Cast.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09388",
        "abstract url": "https://arxiv.org/abs/2410.09388",
        "title": "3-D Magnetotelluric Deep Learning Inversion Guided by Pseudo-Physical Information",
        "rating": "-1.5",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Magnetotelluric deep learning (DL) inversion methods based on joint data-driven and physics-driven have become a hot topic in recent years. When mapping observation data (or forward modeling data) to the resistivity model using neural networks (NNs), incorporating the error (loss) term of the inversion resistivity's forward modeling response--which introduces physical information about electromagnetic field propagation--can significantly enhance the inversion accuracy. To efficiently achieve data-physical dual-driven MT deep learning inversion for large-scale 3-D MT data, we propose using DL forward modeling networks to compute this portion of the loss. This approach introduces pseudo-physical information through the forward modeling of NN simulation, further guiding the inversion network fitting. Specifically, we first pre-train the forward modeling networks as fixed forward modeling operators, then transfer and integrate them into the inversion network training, and finally optimize the inversion network by minimizing the multinomial loss. Theoretical experimental results indicate that despite some simulation errors in DL forward modeling, the introduced pseudo-physical information still enhances inversion accuracy and significantly mitigates the overfitting problem during training. Additionally, we propose a new input mode that involves masking and adding noise to the data, simulating the field data environment of 3-D MT inversion, thereby making the method more flexible and effective for practical applications.",
        "subjects": [
            "physics.geo-ph",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09470",
        "abstract url": "https://arxiv.org/abs/2410.09470",
        "title": "Exploring Channel Distinguishability in Local Neighborhoods of the Model Space in Quantum Neural Networks",
        "rating": "-1.5",
        "keywords": [
            [
                "Quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "With the increasing interest in Quantum Machine Learning, Quantum Neural Networks (QNNs) have emerged and gained significant attention. These models have, however, been shown to be notoriously difficult to train, which we hypothesize is partially due to the architectures, called ansatzes, that are hardly studied at this point. Therefore, in this paper, we take a step back and analyze ansatzes. We initially consider their expressivity, i.e., the space of operations they are able to express, and show that the closeness to being a 2-design, the primarily used measure, fails at capturing this property. Hence, we look for alternative ways to characterize ansatzes by considering the local neighborhood of the model space, in particular, analyzing model distinguishability upon small perturbation of parameters. We derive an upper bound on their distinguishability, showcasing that QNNs with few parameters are hardly discriminable upon update. Our numerical experiments support our bounds and further indicate that there is a significant degree of variability, which stresses the need for warm-starting or clever initialization. Altogether, our work provides an ansatz-centric perspective on training dynamics and difficulties in QNNs, ultimately suggesting that iterative training of small quantum models may not be effective, which contrasts their initial motivation.",
        "subjects": [
            "quant-ph",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09491",
        "abstract url": "https://arxiv.org/abs/2410.09491",
        "title": "Dying Clusters Is All You Need -- Deep Clustering With an Unknown Number of Clusters",
        "rating": "-1.5",
        "keywords": [
            [
                "tabular"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Finding meaningful groups, i.e., clusters, in high-dimensional data such as images or texts without labeled data at hand is an important challenge in data mining. In recent years, deep clustering methods have achieved remarkable results in these tasks. However, most of these methods require the user to specify the number of clusters in advance. This is a major limitation since the number of clusters is typically unknown if labeled data is unavailable. Thus, an area of research has emerged that addresses this problem. Most of these approaches estimate the number of clusters separated from the clustering process. This results in a strong dependency of the clustering result on the quality of the initial embedding. Other approaches are tailored to specific clustering processes, making them hard to adapt to other scenarios. In this paper, we propose UNSEEN, a general framework that, starting from a given upper bound, is able to estimate the number of clusters. To the best of our knowledge, it is the first method that can be easily combined with various deep clustering algorithms. We demonstrate the applicability of our approach by combining UNSEEN with the popular deep clustering algorithms DCN, DEC, and DKM and verify its effectiveness through an extensive experimental evaluation on several image and tabular datasets. Moreover, we perform numerous ablations to analyze our approach and show the importance of its components. The code is available at: https://github.com/collinleiber/UNSEEN",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Acceppted at the Sixth ICDM Workshop on Deep Learning and Clustering"
    },
    {
        "paper id": "2410.09505",
        "abstract url": "https://arxiv.org/abs/2410.09505",
        "title": "HG2P: Hippocampus-inspired High-reward Graph and Model-Free Q-Gradient Penalty for Path Planning and Motion Control",
        "rating": "-1.5",
        "keywords": [
            [
                "robotic manipulation",
                "navigation"
            ],
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Goal-conditioned hierarchical reinforcement learning (HRL) decomposes complex reaching tasks into a sequence of simple subgoal-conditioned tasks, showing significant promise for addressing long-horizon planning in large-scale environments. This paper bridges the goal-conditioned HRL based on graph-based planning to brain mechanisms, proposing a hippocampus-striatum-like dual-controller hypothesis. Inspired by the brain mechanisms of organisms (i.e., the high-reward preferences observed in hippocampal replay) and instance-based theory, we propose a high-return sampling strategy for constructing memory graphs, improving sample efficiency. Additionally, we derive a model-free lower-level Q-function gradient penalty to resolve the model dependency issues present in prior work, improving the generalization of Lipschitz constraints in applications. Finally, we integrate these two extensions, High-reward Graph and model-free Gradient Penalty (HG2P), into the state-of-the-art framework ACLG, proposing a novel goal-conditioned HRL framework, HG2P+ACLG. Experimentally, the results demonstrate that our method outperforms state-of-the-art goal-conditioned HRL algorithms on a variety of long-horizon navigation tasks and robotic manipulation tasks.",
        "subjects": [
            "cs.LG",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09522",
        "abstract url": "https://arxiv.org/abs/2410.09522",
        "title": "Poverty mapping in Mongolia with AI-based Ger detection reveals urban slums persist after the COVID-19 pandemic",
        "rating": "-1.5",
        "keywords": [
            [
                "satellite"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "Mongolia is among the countries undergoing rapid urbanization, and its temporary nomadic dwellings-known as Ger-have expanded into urban areas. Ger settlements in cities are increasingly recognized as slums by their socio-economic deprivation. The distinctive circular, tent-like shape of gers enables their detection through very-high-resolution satellite imagery. We develop a computer vision algorithm to detect gers in Ulaanbaatar, the capital of Mongolia, utilizing satellite images collected from 2015 to 2023. Results reveal that ger settlements have been displaced towards the capital's peripheral areas. The predicted slum ratio based on our results exhibits a significant correlation (r = 0.84) with the World Bank's district-level poverty data. Our nationwide extrapolation suggests that slums may continue to take up one-fifth of the population after the COVID-19 pandemic, contrary to other official predictions that anticipated a decline. We discuss the potential of machine learning on satellite imagery in providing insights into urbanization patterns and monitoring the Sustainable Development Goals.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "20 pages"
    },
    {
        "paper id": "2410.09536",
        "abstract url": "https://arxiv.org/abs/2410.09536",
        "title": "TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "robot"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This work introduces Transformer-based Off-Policy Episodic Reinforcement Learning (TOP-ERL), a novel algorithm that enables off-policy updates in the ERL framework. In ERL, policies predict entire action trajectories over multiple time steps instead of single actions at every time step. These trajectories are typically parameterized by trajectory generators such as Movement Primitives (MP), allowing for smooth and efficient exploration over long horizons while capturing high-level temporal correlations. However, ERL methods are often constrained to on-policy frameworks due to the difficulty of evaluating state-action values for entire action sequences, limiting their sample efficiency and preventing the use of more efficient off-policy architectures. TOP-ERL addresses this shortcoming by segmenting long action sequences and estimating the state-action values for each segment using a transformer-based critic architecture alongside an n-step return estimation. These contributions result in efficient and stable training that is reflected in the empirical results conducted on sophisticated robot learning environments. TOP-ERL significantly outperforms state-of-the-art RL methods. Thorough ablation studies additionally show the impact of key design choices on the model performance.",
        "subjects": [
            "cs.LG",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09560",
        "abstract url": "https://arxiv.org/abs/2410.09560",
        "title": "Towards Scalable Semantic Representation for Recommendation",
        "rating": "-1.5",
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "With recent advances in large language models (LLMs), there has been emerging numbers of research in developing Semantic IDs based on LLMs to enhance the performance of recommendation systems. However, the dimension of these embeddings needs to match that of the ID embedding in recommendation, which is usually much smaller than the original length. Such dimension compression results in inevitable losses in discriminability and dimension robustness of the LLM embeddings, which motivates us to scale up the semantic representation. In this paper, we propose Mixture-of-Codes, which first constructs multiple independent codebooks for LLM representation in the indexing stage, and then utilizes the Semantic Representation along with a fusion module for the downstream recommendation stage. Extensive analysis and experiments demonstrate that our method achieves superior discriminability and dimension robustness scalability, leading to the best scale-up performance in recommendations.",
        "subjects": [
            "cs.IR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09579",
        "abstract url": "https://arxiv.org/abs/2410.09579",
        "title": "Structure of Artificial Neural Networks -- Empirical Investigations",
        "rating": "-1.5",
        "keywords": [
            [
                "architecture search"
            ],
            [
                "graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Within one decade, Deep Learning overtook the dominating solution methods of countless problems of artificial intelligence. ``Deep'' refers to the deep architectures with operations in manifolds of which there are no immediate observations. For these deep architectures some kind of structure is pre-defined -- but what is this structure? With a formal definition for structures of neural networks, neural architecture search problems and solution methods can be formulated under a common framework. Both practical and theoretical questions arise from closing the gap between applied neural architecture search and learning theory. Does structure make a difference or can it be chosen arbitrarily? This work is concerned with deep structures of artificial neural networks and examines automatic construction methods under empirical principles to shed light on to the so called ``black-box models''. Our contributions include a formulation of graph-induced neural networks that is used to pose optimisation problems for neural architecture. We analyse structural properties for different neural network objectives such as correctness, robustness or energy consumption and discuss how structure affects them. Selected automation methods for neural architecture optimisation problems are discussed and empirically analysed. With the insights gained from formalising graph-induced neural networks, analysing structural properties and comparing the applicability of neural architecture search methods qualitatively and quantitatively we advance these methods in two ways. First, new predictive models are presented for replacing computationally expensive evaluation schemes, and second, new generative models for informed sampling during neural architecture search are analysed and discussed.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ],
        "comment": "PhD thesis"
    },
    {
        "paper id": "2410.09621",
        "abstract url": "https://arxiv.org/abs/2410.09621",
        "title": "An Ensemble Scheme for Proactive Dominant Data Migration of Pervasive Tasks at the Edge",
        "rating": "-1.5",
        "keywords": [
            [
                "IoT"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Nowadays, a significant focus within the research community on the intelligent management of data at the confluence of the Internet of Things (IoT) and Edge Computing (EC) is observed. In this manuscript, we propose a scheme to be implemented by autonomous edge nodes concerning their identifications of the appropriate data to be migrated to particular locations within the infrastructure, thereby facilitating the effective processing of requests. Our objective is to equip nodes with the capability to comprehend the access patterns relating to offloaded data-driven tasks and to predict which data ought to be returned to the original nodes associated with those tasks. It is evident that these tasks depend on the processing of data that is absent from the original hosting nodes, thereby underscoring the essential data assets that necessitate access. To infer these data intervals, we utilize an ensemble approach that integrates a statistically oriented model and a machine learning framework. As a result, we are able to identify the dominant data assets in addition to detecting the density of the requests. A detailed analysis of the suggested method is provided by presenting the related formulations, which is also assessed and compared with models found in the relevant literature.",
        "subjects": [
            "cs.DC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09676",
        "abstract url": "https://arxiv.org/abs/2410.09676",
        "title": "Uncovering Attacks and Defenses in Secure Aggregation for Federated Deep Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "Federated learning"
            ],
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning enables the collaborative learning of a global model on diverse data, preserving data locality and eliminating the need to transfer user data to a central server. However, data privacy remains vulnerable, as attacks can target user training data by exploiting the updates sent by users during each learning iteration. Secure aggregation protocols are designed to mask/encrypt user updates and enable a central server to aggregate the masked information. MicroSecAgg (PoPETS 2024) proposes a single server secure aggregation protocol that aims to mitigate the high communication complexity of the existing approaches by enabling a one-time setup of the secret to be re-used in multiple training iterations. In this paper, we identify a security flaw in the MicroSecAgg that undermines its privacy guarantees. We detail the security flaw and our attack, demonstrating how an adversary can exploit predictable masking values to compromise user privacy. Our findings highlight the critical need for enhanced security measures in secure aggregation protocols, particularly the implementation of dynamic and unpredictable masking strategies. We propose potential countermeasures to mitigate these vulnerabilities and ensure robust privacy protection in the secure aggregation frameworks.",
        "subjects": [
            "cs.CR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09703",
        "abstract url": "https://arxiv.org/abs/2410.09703",
        "title": "Universal scaling laws in quantum-probabilistic machine learning by tensor network towards interpreting representation and generalization powers",
        "rating": "-1.5",
        "keywords": [
            [
                "quantum"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Interpreting the representation and generalization powers has been a long-standing issue in the field of machine learning (ML) and artificial intelligence. This work contributes to uncovering the emergence of universal scaling laws in quantum-probabilistic ML. We take the generative tensor network (GTN) in the form of a matrix product state as an example and show that with an untrained GTN (such as a random TN state), the negative logarithmic likelihood (NLL) $L$ generally increases linearly with the number of features $M$, i.e., $L \\simeq k M + const$. This is a consequence of the so-called ``catastrophe of orthogonality,'' which states that quantum many-body states tend to become exponentially orthogonal to each other as $M$ increases. We reveal that while gaining information through training, the linear scaling law is suppressed by a negative quadratic correction, leading to $L \\simeq \u03b2M - \u03b1M^2 + const$. The scaling coefficients exhibit logarithmic relationships with the number of training samples and the number of quantum channels $\u03c7$. The emergence of the quadratic correction term in NLL for the testing (training) set can be regarded as evidence of the generalization (representation) power of GTN. Over-parameterization can be identified by the deviation in the values of $\u03b1$ between training and testing sets while increasing $\u03c7$. We further investigate how orthogonality in the quantum feature map relates to the satisfaction of quantum probabilistic interpretation, as well as to the representation and generalization powers of GTN. The unveiling of universal scaling laws in quantum-probabilistic ML would be a valuable step toward establishing a white-box ML scheme interpreted within the quantum probabilistic framework.",
        "subjects": [
            "quant-ph",
            "cs.AI",
            "cs.IT",
            "cs.LG"
        ],
        "comment": "5 pages (main text) + 3 pages (appendices), 5 figures (main text) + 4 figures (appendices)"
    },
    {
        "paper id": "2410.09707",
        "abstract url": "https://arxiv.org/abs/2410.09707",
        "title": "Learning from the past: predicting critical transitions with machine learning trained on surrogates of historical data",
        "rating": "-1.5",
        "keywords": [
            [
                "biology"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Complex systems can undergo critical transitions, where slowly changing environmental conditions trigger a sudden shift to a new, potentially catastrophic state. Early warning signals for these events are crucial for decision-making in fields such as ecology, biology and climate science. Generic early warning signals motivated by dynamical systems theory have had mixed success on real noisy data. More recent studies found that deep learning classifiers trained on synthetic data could improve performance. However, neither of these methods take advantage of historical, system-specific data. Here, we introduce an approach that trains machine learning classifiers directly on surrogate data of past transitions, namely surrogate data-based machine learning (SDML). The approach provides early warning signals in empirical and experimental data from geology, climatology, sociology, and cardiology with higher sensitivity and specificity than two widely used generic early warning signals -- variance and lag-1 autocorrelation. Since the approach is trained directly on surrogates of historical data, it is not bound by the restricting assumption of a local bifurcation like previous methods. This system-specific approach can contribute to improved early warning signals to help humans better prepare for or avoid undesirable critical transitions.",
        "subjects": [
            "physics.data-an",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09718",
        "abstract url": "https://arxiv.org/abs/2410.09718",
        "title": "A Tidal Current Speed Forecasting Model based on Multiple Periodicity Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Tidal energy is one of the key components in increasing the penetration rate of renewable energy. The penetration of tidal energy in the electrical grid depends on the accuracy of tidal current speed forecasting. Modeling inaccuracies hinder forecast accuracy. Previous research has primarily used physical models to forecast tidal current speed. However, tidal current variations influenced by the orbital periods of celestial bodies make accurate physical modeling challenging. Researching the multiple periodicity of tides is crucial for accurately forecasting tidal current speed. In this article, we propose the Wavelet-Enhanced Convolutional Network (WCN) to learn multiple periodicity. The framework embeds intra-period and inter-period variations of one-dimensional tidal current data into the rows and columns of a two-dimensional tensor. Then, the two-dimensional variations of the sequence can be processed by convolutional kernels. We integrate a time-frequency analysis method into the framework to further address local periodic features. Additionally, to enhance the framework's stability, we optimize the framework's hyperparameters with the Tree-structured Parzen Estimator algorithm. The proposed framework avoids the lack of learning multiple periodicity. Compared with benchmarks, the proposed framework reduces the mean absolute error and mean square error in 10-step forecasting by, at most, 90.36% and 97.56%, respectively.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.10897",
        "abstract url": "https://arxiv.org/abs/2410.10897",
        "title": "EPi-cKANs: Elasto-Plasticity Informed Kolmogorov-Arnold Networks Using Chebyshev Polynomials",
        "rating": "-1.5",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Multilayer perceptron (MLP) networks are predominantly used to develop data-driven constitutive models for granular materials. They offer a compelling alternative to traditional physics-based constitutive models in predicting nonlinear responses of these materials, e.g., elasto-plasticity, under various loading conditions. To attain the necessary accuracy, MLPs often need to be sufficiently deep or wide, owing to the curse of dimensionality inherent in these problems. To overcome this limitation, we present an elasto-plasticity informed Chebyshev-based Kolmogorov-Arnold network (EPi-cKAN) in this study. This architecture leverages the benefits of KANs and augmented Chebyshev polynomials, as well as integrates physical principles within both the network structure and the loss function. The primary objective of EPi-cKAN is to provide an accurate and generalizable function approximation for non-linear stress-strain relationships, using fewer parameters compared to standard MLPs. To evaluate the efficiency, accuracy, and generalization capabilities of EPi-cKAN in modeling complex elasto-plastic behavior, we initially compare its performance with other cKAN-based models, which include purely data-driven parallel and serial architectures. Furthermore, to differentiate EPi-cKAN's distinct performance, we also compare it against purely data-driven and physics-informed MLP-based methods. Lastly, we test EPi-cKAN's ability to predict blind strain-controlled paths that extend beyond the training data distribution to gauge its generalization and predictive capabilities. Our findings indicate that, even with limited data and fewer parameters compared to other approaches, EPi-cKAN provides superior accuracy in predicting stress components and demonstrates better generalization when used to predict sand elasto-plastic behavior under blind triaxial axisymmetric strain-controlled loading paths.",
        "subjects": [
            "cond-mat.mtrl-sci",
            "cs.LG",
            "physics.comp-ph"
        ],
        "comment": "20 pages, 13 figures"
    },
    {
        "paper id": "2410.10899",
        "abstract url": "https://arxiv.org/abs/2410.10899",
        "title": "GPTON: Generative Pre-trained Transformers enhanced with Ontology Narration for accurate annotation of biological data",
        "rating": "-1.5",
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "By leveraging GPT-4 for ontology narration, we developed GPTON to infuse structured knowledge into LLMs through verbalized ontology terms, achieving accurate text and ontology annotations for over 68% of gene sets in the top five predictions. Manual evaluations confirm GPTON's robustness, highlighting its potential to harness LLMs and structured knowledge to significantly advance biomedical research beyond gene set annotation.",
        "subjects": [
            "q-bio.QM",
            "cs.AI"
        ],
        "comment": "25 pages, 6 figures"
    },
    {
        "paper id": "2410.09399",
        "abstract url": "https://arxiv.org/abs/2410.09399",
        "title": "Text Classification using Graph Convolutional Networks: A Comprehensive Survey",
        "rating": "-2",
        "keywords": [
            [
                "Graph"
            ],
            [
                "medical",
                "diagnosis"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Text classification is a quintessential and practical problem in natural language processing with applications in diverse domains such as sentiment analysis, fake news detection, medical diagnosis, and document classification. A sizable body of recent works exists where researchers have studied and tackled text classification from different angles with varying degrees of success. Graph convolution network (GCN)-based approaches have gained a lot of traction in this domain over the last decade with many implementations achieving state-of-the-art performance in more recent literature and thus, warranting the need for an updated survey. This work aims to summarize and categorize various GCN-based Text Classification approaches with regard to the architecture and mode of supervision. It identifies their strengths and limitations and compares their performance on various benchmark datasets. We also discuss future research directions and the challenges that exist in this domain.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09414",
        "abstract url": "https://arxiv.org/abs/2410.09414",
        "title": "Advancing Bug Detection in Fastjson2 with Large Language Models Driven Unit Test Generation",
        "rating": "-2",
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "Data-serialization libraries are essential tools in software development, responsible for converting between programmable data structures and data persistence formats. Among them, JSON is the most popular choice for exchanging data between different systems and programming languages, while JSON libraries serve as the programming toolkit for this task. Despite their widespread use, bugs in JSON libraries can cause severe issues such as data inconsistencies and security vulnerabilities. Unit test generation techniques are widely adopted to identify bugs in various libraries. However, there is limited systematic testing effort specifically for exposing bugs within JSON libraries in industrial practice. In this paper, we propose JSONTestGen, an approach leveraging large language models (LLMs) to generate unit tests for fastjson2, a popular open source JSON library from Alibaba. Pre-trained on billions of open-source text and code corpora, LLMs have demonstrated remarkable abilities in programming tasks. Based on historical bug-triggering unit tests, we utilize LLMs to generate more diverse test cases by incorporating JSON domain-specific mutation rules. To systematically and efficiently identify potential bugs, we adopt differential testing on the results of the generated unit tests. Our evaluation shows that JSONTestGen outperforms existing test generation tools in unknown defect detection. With JSONTestGen, we found 34 real bugs in fastjson2, 30 of which have already been fixed, including 12 non-crashing bugs. While manual inspection reveals that LLM-generated tests can be erroneous, particularly with self-contradictory assertions, we demonstrate that LLMs have the potential for classifying false-positive test failures. This suggests a promising direction for improved test oracle automation in the future.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09417",
        "abstract url": "https://arxiv.org/abs/2410.09417",
        "title": "Neurally Integrated Finite Elements for Differentiable Elasticity on Evolving Domains",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "physics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present an elastic simulator for domains defined as evolving implicit functions, which is efficient, robust, and differentiable with respect to both shape and material. This simulator is motivated by applications in 3D reconstruction: it is increasingly effective to recover geometry from observed images as implicit functions, but physical applications require accurately simulating and optimizing-for the behavior of such shapes under deformation, which has remained challenging. Our key technical innovation is to train a small neural network to fit quadrature points for robust numerical integration on implicit grid cells. When coupled with a Mixed Finite Element formulation, this yields a smooth, fully differentiable simulation model connecting the evolution of the underlying implicit surface to its elastic response. We demonstrate the efficacy of our approach on forward simulation of implicits, direct simulation of 3D shapes during editing, and novel physics-based shape and topology optimizations in conjunction with differentiable rendering.",
        "subjects": [
            "cs.GR",
            "cs.CV"
        ],
        "comment": "16 pages, 21 figures"
    },
    {
        "paper id": "2410.09422",
        "abstract url": "https://arxiv.org/abs/2410.09422",
        "title": "SimBrainNet: Evaluating Brain Network Similarity for Attention Disorders",
        "rating": "-2",
        "keywords": [
            [
                "EEG"
            ]
        ],
        "abstract": "Electroencephalography (EEG)-based attention disorder research seeks to understand brain activity patterns associated with attention. Previous studies have mainly focused on identifying brain regions involved in cognitive processes or classifying Attention-Deficit Hyperactivity Disorder (ADHD) and control subjects. However, analyzing effective brain connectivity networks for specific attentional processes and comparing them has not been explored. Therefore, in this study, we propose multivariate transfer entropy-based connectivity networks for cognitive events and introduce a new similarity measure, 'SimBrainNet', to assess these networks. A high similarity score suggests similar brain dynamics during cognitive events, indicating less attention variability. Our experiment involves 12 individuals with attention disorders (7 children and 5 adolescents). Noteworthy that child participants exhibit lower similarity scores compared to adolescents, indicating greater changes in attention. We found strong connectivity patterns in the left pre-frontal cortex for adolescent individuals compared to the child. Our study highlights the changes in attention levels across various cognitive events, offering insights into the underlying cognitive mechanisms, brain dynamics, and potential deficits in individuals with this disorder.",
        "subjects": [
            "q-bio.NC",
            "cs.HC"
        ],
        "comment": "10 pages, 2 figures, To appear in the Proceedings of 27th INTERNATIONAL CONFERENCE ON MEDICAL IMAGE COMPUTING AND COMPUTER ASSISTED INTERVENTION, 6-10 October 2024, MARRAKESH / MOROCCO"
    },
    {
        "paper id": "2410.09453",
        "abstract url": "https://arxiv.org/abs/2410.09453",
        "title": "MMAD: The First-Ever Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection",
        "rating": "-2",
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "Industrial"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "In the field of industrial inspection, Multimodal Large Language Models (MLLMs) have a high potential to renew the paradigms in practical applications due to their robust language capabilities and generalization abilities. However, despite their impressive problem-solving skills in many domains, MLLMs' ability in industrial anomaly detection has not been systematically studied. To bridge this gap, we present MMAD, the first-ever full-spectrum MLLMs benchmark in industrial Anomaly Detection. We defined seven key subtasks of MLLMs in industrial inspection and designed a novel pipeline to generate the MMAD dataset with 39,672 questions for 8,366 industrial images. With MMAD, we have conducted a comprehensive, quantitative evaluation of various state-of-the-art MLLMs. The commercial models performed the best, with the average accuracy of GPT-4o models reaching 74.9%. However, this result falls far short of industrial requirements. Our analysis reveals that current MLLMs still have significant room for improvement in answering questions related to industrial anomalies and defects. We further explore two training-free performance enhancement strategies to help models improve in industrial scenarios, highlighting their promising potential for future research.",
        "subjects": [
            "cs.AI",
            "cs.CV"
        ],
        "comment": "The code and data are available at https://github.com/jam-cc/MMAD"
    },
    {
        "paper id": "2410.09467",
        "abstract url": "https://arxiv.org/abs/2410.09467",
        "title": "Enhancing Single Image to 3D Generation using Gaussian Splatting and Hybrid Diffusion Priors",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "Diffusion"
            ],
            [
                "robotic manipulation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D object generation from a single image involves estimating the full 3D geometry and texture of unseen views from an unposed RGB image captured in the wild. Accurately reconstructing an object's complete 3D structure and texture has numerous applications in real-world scenarios, including robotic manipulation, grasping, 3D scene understanding, and AR/VR. Recent advancements in 3D object generation have introduced techniques that reconstruct an object's 3D shape and texture by optimizing the efficient representation of Gaussian Splatting, guided by pre-trained 2D or 3D diffusion models. However, a notable disparity exists between the training datasets of these models, leading to distinct differences in their outputs. While 2D models generate highly detailed visuals, they lack cross-view consistency in geometry and texture. In contrast, 3D models ensure consistency across different views but often result in overly smooth textures. We propose bridging the gap between 2D and 3D diffusion models to address this limitation by integrating a two-stage frequency-based distillation loss with Gaussian Splatting. Specifically, we leverage geometric priors in the low-frequency spectrum from a 3D diffusion model to maintain consistent geometry and use a 2D diffusion model to refine the fidelity and texture in the high-frequency spectrum of the generated 3D structure, resulting in more detailed and fine-grained outcomes. Our approach enhances geometric consistency and visual quality, outperforming the current SOTA. Additionally, we demonstrate the easy adaptability of our method for efficient object pose estimation and tracking.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09487",
        "abstract url": "https://arxiv.org/abs/2410.09487",
        "title": "Benchmarking Time Series Foundation Models for Short-Term Household Electricity Load Forecasting",
        "rating": "-2",
        "keywords": [
            [
                "Forecasting"
            ]
        ],
        "abstract": "Accurate household electricity short-term load forecasting (STLF) is key to future and sustainable energy systems. While various studies have analyzed statistical, machine learning, or deep learning approaches for household electricity STLF, recently proposed time series foundation models such as Chronos, TimesFM, or LagLlama have not yet been considered for household electricity STLF. These models are trained on a vast amount of time series data and are able to forecast time series without explicit task-specific training (zero-shot learning). In this study, we benchmark the forecasting capabilities of time series foundation models compared to Trained-from-Scratch (TFS) Transformer-based approaches. Our results suggest that foundation models perform comparably to TFS Transformer models, while the TimesFM foundation model outperforms all TFS models when the input size increases. At the same time, they require less effort, as they need no domain-specific training and only limited contextual data for inference.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09512",
        "abstract url": "https://arxiv.org/abs/2410.09512",
        "title": "The Indirect Method for Generating Libraries of Optimal Periodic Trajectories and Its Application to Economical Bipedal Walking",
        "rating": "-2",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "robotics"
            ]
        ],
        "abstract": "Trajectory optimization is an essential tool for generating efficient and dynamically consistent gaits in legged locomotion. This paper explores the indirect method of trajectory optimization, emphasizing its application in creating optimal periodic gaits for legged systems and contrasting it with the more commonly used direct method. While the direct method provides considerable flexibility in its implementation, it is limited by its input space parameterization. In contrast, the indirect method improves accuracy by defining control inputs as functions of the system's states and costates. We tackle the convergence challenges associated with indirect shooting methods, particularly through the systematic development of gait libraries by utilizing numerical continuation methods. Our contributions include: (1) the formalization of a general periodic trajectory optimization problem that extends existing first-order necessary conditions for a broader range of cost functions and operating conditions; (2) a methodology for efficiently generating libraries of optimal trajectories (gaits) utilizing a single shooting approach combined with numerical continuation methods, including a novel approach for reconstructing Lagrange multipliers and costates from passive gaits; and (3) a comparative analysis of the indirect and direct shooting methods using a compass-gait walker as a case study, demonstrating the former's superior accuracy in generating optimal gaits. The findings underscore the potential of the indirect method for generating families of optimal gaits, thereby advancing the field of trajectory optimization in legged robotics.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "submitted to the International Journal of Robotics Research (IJRR)"
    },
    {
        "paper id": "2410.09538",
        "abstract url": "https://arxiv.org/abs/2410.09538",
        "title": "Cybersecurity in Industry 5.0: Open Challenges and Future Directions",
        "rating": "-2",
        "keywords": [
            [
                "Industrial"
            ]
        ],
        "abstract": "Unlocking the potential of Industry 5.0 hinges on robust cybersecurity measures. This new Industrial Revolution prioritises human-centric values while addressing pressing societal issues such as resource conservation, climate change, and social stability. Recognising the heightened risk of cyberattacks due to the new enabling technologies in Industry 5.0, this paper analyses potential threats and corresponding countermeasures. Furthermore, it evaluates the existing industrial implementation frameworks, which reveals their inadequacy in ensuring a secure transition from Industry 4.0 to Industry 5.0. Consequently, the paper underscores the necessity of developing a new framework centred on cybersecurity to facilitate organisations' secure adoption of Industry 5.0 principles. The creation of such a framework is emphasised as a necessity for organisations.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "7 pages, 2 figures, Accepted for PST2024 : 21st Annual International Conference on Privacy, Security, and Trust"
    },
    {
        "paper id": "2410.09557",
        "abstract url": "https://arxiv.org/abs/2410.09557",
        "title": "Optimizing Quantitative Photoacoustic Imaging Systems: The Bayesian Cram\u00e9r-Rao Bound Approach",
        "rating": "-2",
        "keywords": [
            [
                "medical"
            ]
        ],
        "abstract": "Quantitative photoacoustic computed tomography (qPACT) is an emerging medical imaging modality that carries the promise of high-contrast, fine-resolution imaging of clinically relevant quantities like hemoglobin concentration and blood-oxygen saturation. However, qPACT image reconstruction is governed by a multiphysics, partial differential equation (PDE) based inverse problem that is highly non-linear and severely ill-posed. Compounding the difficulty of the problem is the lack of established design standards for qPACT imaging systems, as there is currently a proliferation of qPACT system designs for various applications and it is unknown which ones are optimal or how to best modify the systems under various design constraints. This work introduces a novel computational approach for the optimal experimental design (OED) of qPACT imaging systems based on the Bayesian Cram\u00e9r-Rao bound (CRB). Our approach incorporates several techniques to address challenges associated with forming the bound in the infinite-dimensional function space setting of qPACT, including priors with trace-class covariance operators and the use of the variational adjoint method to compute derivatives of the log-likelihood function needed in the bound computation. The resulting Bayesian CRB based design metric is computationally efficient and independent of the choice of estimator used to solve the inverse problem. The efficacy of the bound in guiding experimental design was demonstrated in a numerical study of qPACT design schemes under a stylized two-dimensional imaging geometry. To the best of our knowledge, this is the first work to propose Bayesian CRB based design for systems governed by PDEs.",
        "subjects": [
            "physics.med-ph",
            "cs.IT",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09582",
        "abstract url": "https://arxiv.org/abs/2410.09582",
        "title": "Improving 3D Finger Traits Recognition via Generalizable Neural Rendering",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "Depth",
                "NeRF",
                "radiance fields"
            ],
            [
                "biometric"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "3D biometric techniques on finger traits have become a new trend and have demonstrated a powerful ability for recognition and anti-counterfeiting. Existing methods follow an explicit 3D pipeline that reconstructs the models first and then extracts features from 3D models. However, these explicit 3D methods suffer from the following problems: 1) Inevitable information dropping during 3D reconstruction; 2) Tight coupling between specific hardware and algorithm for 3D reconstruction. It leads us to a question: Is it indispensable to reconstruct 3D information explicitly in recognition tasks? Hence, we consider this problem in an implicit manner, leaving the nerve-wracking 3D reconstruction problem for learnable neural networks with the help of neural radiance fields (NeRFs). We propose FingerNeRF, a novel generalizable NeRF for 3D finger biometrics. To handle the shape-radiance ambiguity problem that may result in incorrect 3D geometry, we aim to involve extra geometric priors based on the correspondence of binary finger traits like fingerprints or finger veins. First, we propose a novel Trait Guided Transformer (TGT) module to enhance the feature correspondence with the guidance of finger traits. Second, we involve extra geometric constraints on the volume rendering loss with the proposed Depth Distillation Loss and Trait Guided Rendering Loss. To evaluate the performance of the proposed method on different modalities, we collect two new datasets: SCUT-Finger-3D with finger images and SCUT-FingerVein-3D with finger vein images. Moreover, we also utilize the UNSW-3D dataset with fingerprint images for evaluation. In experiments, our FingerNeRF can achieve 4.37% EER on SCUT-Finger-3D dataset, 8.12% EER on SCUT-FingerVein-3D dataset, and 2.90% EER on UNSW-3D dataset, showing the superiority of the proposed implicit method in 3D finger biometrics.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "This paper is accepted in IJCV. For further information and access to the code, please visit our project page: https://scut-bip-lab.github.io/fingernerf/"
    },
    {
        "paper id": "2410.09588",
        "abstract url": "https://arxiv.org/abs/2410.09588",
        "title": "A Game-Theoretic Perspective for Efficient Modern Random Access",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "Modern random access mechanisms combine packet repetitions with multi-user detection mechanisms at the receiver to maximize the throughput and reliability in massive Internet of Things (IoT) scenarios. However, optimizing the access policy, which selects the number of repetitions, is a complicated problem, and failing to do so can lead to an inefficient use of resources and, potentially, to an increased congestion. In this paper, we follow a game-theoretic approach for optimizing the access policies of selfish users in modern random access mechanisms. Our goal is to find adequate values for the rewards given after a success to achieve a Nash equilibrium (NE) that optimizes the throughput of the system while considering the cost of transmission. Our results show that a mixed strategy, where repetitions are selected according to the irregular repetition slotted ALOHA (IRSA) protocol, attains a NE that maximizes the throughput in the special case with two users. In this scenario, our method increases the throughput by 30% when compared to framed ALOHA. Furthermore, we present three methods to attain a NE with near-optimal throughput for general modern random access scenarios, which exceed the throughput of framed ALOHA by up to 34%.",
        "subjects": [
            "cs.IT",
            "cs.GT"
        ],
        "comment": "Submitted for publication to IEEE WCNC 2025"
    },
    {
        "paper id": "2410.09591",
        "abstract url": "https://arxiv.org/abs/2410.09591",
        "title": "Unlearn and Burn: Adversarial Machine Unlearning Requests Destroy Model Accuracy",
        "rating": "-2",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "attack"
            ]
        ],
        "abstract": "Machine unlearning algorithms, designed for selective removal of training data from models, have emerged as a promising approach to growing privacy concerns. In this work, we expose a critical yet underexplored vulnerability in the deployment of unlearning systems: the assumption that the data requested for removal is always part of the original training set. We present a threat model where an attacker can degrade model accuracy by submitting adversarial unlearning requests for data not present in the training set. We propose white-box and black-box attack algorithms and evaluate them through a case study on image classification tasks using the CIFAR-10 and ImageNet datasets, targeting a family of widely used unlearning methods. Our results show extremely poor test accuracy following the attack: 3.6% on CIFAR-10 and 0.4% on ImageNet for white-box attacks, and 8.5% on CIFAR-10 and 1.3% on ImageNet for black-box attacks. Additionally, we evaluate various verification mechanisms to detect the legitimacy of unlearning requests and reveal the challenges in verification, as most of the mechanisms fail to detect stealthy attacks without severely impairing their ability to process valid requests. These findings underscore the urgent need for research on more robust request verification methods and unlearning protocols, should the deployment of machine unlearning systems become more prevalent in the future.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09641",
        "abstract url": "https://arxiv.org/abs/2410.09641",
        "title": "Soft Tester UE: A Novel Approach for Open RAN Security Testing",
        "rating": "-2",
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "With the rise of 5G and open radio access networks (O-RAN), there is a growing demand for customizable experimental platforms dedicated to security testing, as existing testbeds do not prioritize this area. Traditional, hardware-dependent testing methods pose challenges for smaller companies and research institutions. The growing wireless threat landscape highlights the critical need for proactive security testing, as 5G and O-RAN deployments are appealing targets for cybercriminals. To address these challenges, this article introduces the Soft Tester UE (soft T-UE), a software-defined test equipment designed to evaluate the security of 5G and O-RAN deployments via the Uu air interface between the user equipment (UE) and the network. The outcome is to deliver a free, open-source, and expandable test instrument to address the need for both standardized and customizable automated security testing. By extending beyond traditional security metrics, the soft T-UE promotes the development of new security measures and enhances the capability to anticipate and mitigate potential security breaches. The tool's automated testing capabilities are demonstrated through a scenario where the Radio Access Network (RAN) under test is evaluated when it receives fuzzed data when initiating a connection with an UE.",
        "subjects": [
            "cs.CR",
            "cs.NI",
            "eess.SY"
        ],
        "comment": "This article has been accepted for publication in the IEEE VTC Fall 2024--RitiRAN Workshop"
    },
    {
        "paper id": "2410.09664",
        "abstract url": "https://arxiv.org/abs/2410.09664",
        "title": "Tackling Coherent Noise in Quantum Computing via Cross-Layer Compiler Optimization",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum computing hardware is affected by quantum noise that undermine the quality of results of an executed quantum program. Amongst other quantum noises, coherent error that caused by parameter drifting and miscalibration, remains critical. While coherent error mitigation has been studied before, studies focused either on gate-level or pulse-level -- missing cross-level optimization opportunities; And most of them only target single-qubit gates -- while multi-qubit gates are also used in practice. To address above limitations, this work proposes a cross-layer approach for coherent error mitigation that considers program-level, gate-level, and pulse-level compiler optimizations, by leveraging the hidden inverse theory, and exploiting the structure inside different quantum programs, while also considering multi-qubit gates. We implemented our approach as compiler optimization passes, and integrated into IBM Qiskit framework. We tested our technique on real quantum computer (IBM-Brisbane), and demonstrated up to 92% fidelity improvements (45% on average), on several benchmarks.",
        "subjects": [
            "cs.AR",
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09680",
        "abstract url": "https://arxiv.org/abs/2410.09680",
        "title": "Integrating Reinforcement Learning and Large Language Models for Crop Production Process Management Optimization and Control through A New Knowledge-Based Deep Learning Paradigm",
        "rating": "-2",
        "keywords": [
            [
                "agricultural"
            ]
        ],
        "abstract": "Efficient and sustainable crop production process management is crucial to meet the growing global demand for food, fuel, and feed while minimizing environmental impacts. Traditional crop management practices, often developed through empirical experience, face significant challenges in adapting to the dynamic nature of modern agriculture, which is influenced by factors such as climate change, soil variability, and market conditions. Recently, reinforcement learning (RL) and large language models (LLMs) bring transformative potential, with RL providing adaptive methodologies to learn optimal strategies and LLMs offering vast, superhuman knowledge across agricultural domains, enabling informed, context-specific decision-making. This paper systematically examines how the integration of RL and LLMs into crop management decision support systems (DSSs) can drive advancements in agricultural practice. We explore recent advancements in RL and LLM algorithms, their application within crop management, and the use of crop management simulators to develop these technologies. The convergence of RL and LLMs with crop management DSSs presents new opportunities to optimize agricultural practices through data-driven, adaptive solutions that can address the uncertainties and complexities of crop production. However, this integration also brings challenges, particularly in real-world deployment. We discuss these challenges and propose potential solutions, including the use of offline RL and enhanced LLM integration, to maximize the effectiveness and sustainability of crop management. Our findings emphasize the need for continued research and innovation to unlock the full potential of these advanced tools in transforming agricultural systems into optimal and controllable ones.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "13 pages"
    },
    {
        "paper id": "2410.10900",
        "abstract url": "https://arxiv.org/abs/2410.10900",
        "title": "Oogway: Designing, Implementing, and Testing an AUV for RoboSub 2023",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Robotics",
                "robot"
            ]
        ],
        "abstract": "The Duke Robotics Club is proud to present our robot for the 2023 RoboSub Competition: Oogway. Oogway marks one of the largest design overhauls in club history. Beyond a revamped formfactor, some of Oogway's notable features include all-new computer vision software, advanced sonar integration, novel acoustics hardware processing, and upgraded stereoscopic cameras. Oogway was built on the principle of independent, well-integrated, and reliable subsystems. Individual components and subsystems were tested and designed separately. Oogway's most advanced capabilities are a result of the tight integration between these subsystems. Such examples include sonar-assisted computer vision algorithms and robot-agnostic controls configured in part through the robot's 3D model. The success of constructing and testing Oogway in under 2 year's time can be attributed to 20+ contributing club members, supporters within Duke's Pratt School of Engineering, and outside sponsors.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2410.09684"
    },
    {
        "paper id": "2410.09514",
        "abstract url": "https://arxiv.org/abs/2410.09514",
        "title": "Eco-Aware Graph Neural Networks for Sustainable Recommendations",
        "rating": "-2.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Recommender systems play a crucial role in alleviating information overload by providing personalized recommendations tailored to users' preferences and interests. Recently, Graph Neural Networks (GNNs) have emerged as a promising approach for recommender systems, leveraging their ability to effectively capture complex relationships and dependencies between users and items by representing them as nodes in a graph structure. In this study, we investigate the environmental impact of GNN-based recommender systems, an aspect that has been largely overlooked in the literature. Specifically, we conduct a comprehensive analysis of the carbon emissions associated with training and deploying GNN models for recommendation tasks. We evaluate the energy consumption and carbon footprint of different GNN architectures and configurations, considering factors such as model complexity, training duration, hardware specifications and embedding size. By addressing the environmental impact of resource-intensive algorithms in recommender systems, this study contributes to the ongoing efforts towards sustainable and responsible artificial intelligence, promoting the development of eco-friendly recommendation technologies that balance performance and environmental considerations. Code is available at: https://github.com/antoniopurificato/gnn_recommendation_and_environment.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": "9 pages, 2 tables, 3 figures, RecSoGood Workshop"
    },
    {
        "paper id": "2410.09567",
        "abstract url": "https://arxiv.org/abs/2410.09567",
        "title": "Timeseria: an object-oriented time series processing library",
        "rating": "-2.5",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Timeseria is an object-oriented time series processing library implemented in Python, which aims at making it easier to manipulate time series data and to build statistical and machine learning models on top of it. Unlike common data analysis frameworks, it builds up from well defined and reusable logical units (objects), which can be easily combined together in order to ensure a high level of consistency. Thanks to this approach, Timeseria can address by design several non-trivial issues often underestimated, such as handling data losses, non-uniform sampling rates, differences between aggregated data and punctual observations, time zones, daylight saving times, and more. Timeseria comes with a comprehensive set of base data structures, common data manipulation operations, and extensible models for data reconstruction, forecasting and anomaly detection. It also integrates a powerful plotting engine capable of handling even millions of data points.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09604",
        "abstract url": "https://arxiv.org/abs/2410.09604",
        "title": "EmbodiedCity: A Benchmark Platform for Embodied Agent in Real-world City Environment",
        "rating": "-2.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "vehicle"
            ],
            [
                "navigation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Embodied artificial intelligence emphasizes the role of an agent's body in generating human-like behaviors. The recent efforts on EmbodiedAI pay a lot of attention to building up machine learning models to possess perceiving, planning, and acting abilities, thereby enabling real-time interaction with the world. However, most works focus on bounded indoor environments, such as navigation in a room or manipulating a device, with limited exploration of embodying the agents in open-world scenarios. That is, embodied intelligence in the open and outdoor environment is less explored, for which one potential reason is the lack of high-quality simulators, benchmarks, and datasets. To address it, in this paper, we construct a benchmark platform for embodied intelligence evaluation in real-world city environments. Specifically, we first construct a highly realistic 3D simulation environment based on the real buildings, roads, and other elements in a real city. In this environment, we combine historically collected data and simulation algorithms to conduct simulations of pedestrian and vehicle flows with high fidelity. Further, we designed a set of evaluation tasks covering different EmbodiedAI abilities. Moreover, we provide a complete set of input and output interfaces for access, enabling embodied agents to easily take task requirements and current environmental observations as input and then make decisions and obtain performance evaluations. On the one hand, it expands the capability of existing embodied intelligence to higher levels. On the other hand, it has a higher practical value in the real world and can support more potential applications for artificial general intelligence. Based on this platform, we evaluate some popular large language models for embodied intelligence capabilities of different dimensions and difficulties.",
        "subjects": [
            "cs.AI",
            "cs.RO"
        ],
        "comment": "All of the software, Python library, codes, datasets, tutorials, and real-time online service are available on this website: https://embodied-city.fiblab.net"
    },
    {
        "paper id": "2410.09652",
        "abstract url": "https://arxiv.org/abs/2410.09652",
        "title": "Survival of the Safest: Towards Secure Prompt Optimization through Interleaved Multi-Objective Evolution",
        "rating": "-2.5",
        "keywords": [
            [
                "Survival"
            ],
            [
                "industrial"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities; however, the optimization of their prompts has historically prioritized performance metrics at the expense of crucial safety and security considerations. To overcome this shortcoming, we introduce \"Survival of the Safest\" (SoS), an innovative multi-objective prompt optimization framework that enhances both performance and security in LLMs simultaneously. SoS utilizes an interleaved multi-objective evolution strategy, integrating semantic, feedback, and crossover mutations to effectively traverse the prompt landscape. Differing from the computationally demanding Pareto front methods, SoS provides a scalable solution that expedites optimization in complex, high-dimensional discrete search spaces while keeping computational demands low. Our approach accommodates flexible weighting of objectives and generates a pool of optimized candidates, empowering users to select prompts that optimally meet their specific performance and security needs. Experimental evaluations across diverse benchmark datasets affirm SoS's efficacy in delivering high performance and notably enhancing safety and security compared to single-objective methods. This advancement marks a significant stride towards the deployment of LLM systems that are both high-performing and secure across varied industrial applications",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "cs.NE"
        ],
        "comment": "EMNLP 2024 Industry Track"
    },
    {
        "paper id": "2410.09656",
        "abstract url": "https://arxiv.org/abs/2410.09656",
        "title": "LSTM-Based Proactive Congestion Management for Internet of Vehicle Networks",
        "rating": "-2.5",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "forecasting"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Vehicle-to-everything (V2X) networks support a variety of safety, entertainment, and commercial applications. This is realized by applying the principles of the Internet of Vehicles (IoV) to facilitate connectivity among vehicles and between vehicles and roadside units (RSUs). Network congestion management is essential for IoVs and it represents a significant concern due to its impact on improving the efficiency of transportation systems and providing reliable communication among vehicles for the timely delivery of safety-critical packets. This paper introduces a framework for proactive congestion management for IoV networks. We generate congestion scenarios and a data set to predict the congestion using LSTM. We present the framework and the packet congestion dataset. Simulation results using SUMO with NS3 demonstrate the effectiveness of the framework for forecasting IoV network congestion and clustering/prioritizing packets employing recurrent neural networks.",
        "subjects": [
            "cs.NI",
            "cs.AI",
            "eess.SY"
        ],
        "comment": "This article has been accepted for publication in the IEEE VTC Fall 2024"
    },
    {
        "paper id": "2410.09698",
        "abstract url": "https://arxiv.org/abs/2410.09698",
        "title": "Incentivized Network Dynamics in Digital Job Recruitment",
        "rating": "-2.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "recommendation"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Online platforms have transformed the formal job market but continue to struggle with effectively engaging passive candidates-individuals not actively seeking employment but open to compelling opportunities. We introduce the Independent Halting Cascade (IHC) model, a novel framework that integrates complex network diffusion dynamics with economic game theory to address this challenge. Unlike traditional models that focus solely on information propagation, the IHC model empowers network agents to either disseminate a job posting or halt its spread by applying for the position themselves. By embedding economic incentives into agent decision-making processes, the model creates a dynamic interplay between maximizing information spread and promoting application. Our analysis uncovers distinct behavioral regimes within the IHC model, characterized by critical thresholds in recommendation and application probabilities. Extensive simulations on both synthetic and real-world network topologies demonstrate that the IHC model significantly outperforms traditional direct-recommendation systems in recruiting suitable passive candidates. Specifically, the model achieves up to a 30% higher hiring success rate compared to baseline methods. These findings offer strategic insights into leveraging economic incentives and network structures to enhance recruitment efficiency. The IHC model thus provides a robust framework for modernizing recruitment strategies, particularly in engaging the vast pool of passive candidates in the job market.",
        "subjects": [
            "cs.SI"
        ],
        "comment": "15 pages, 6 figures"
    },
    {
        "paper id": "2410.09443",
        "abstract url": "https://arxiv.org/abs/2410.09443",
        "title": "An Expeditious Spatial Mean Radiant Temperature Mapping Framework using Visual SLAM and Semantic Segmentation",
        "rating": "-3",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "SLAM"
            ],
            [
                "thermal"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Ensuring thermal comfort is essential for the well-being and productivity of individuals in built environments. Of the various thermal comfort indicators, the mean radiant temperature (MRT) is very challenging to measure. Most common measurement methodologies are time-consuming and not user-friendly. To address this issue, this paper proposes a novel MRT measurement framework that uses visual simultaneous localization and mapping (SLAM) and semantic segmentation techniques. The proposed approach follows the rule of thumb of the traditional MRT calculation method using surface temperature and view factors. However, it employs visual SLAM and creates a 3D thermal point cloud with enriched surface temperature information. The framework then implements Grounded SAM, a new object detection and segmentation tool to extract features with distinct temperature profiles on building surfaces. The detailed segmentation of thermal features not only reduces potential errors in the calculation of the MRT but also provides an efficient reconstruction of the spatial MRT distribution in the indoor environment. We also validate the calculation results with the reference measurement methodology. This data-driven framework offers faster and more efficient MRT measurements and spatial mapping than conventional methods. It can enable the direct engagement of researchers and practitioners in MRT measurements and contribute to research on thermal comfort and radiant cooling and heating systems.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": "Accepted by 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop"
    },
    {
        "paper id": "2410.09530",
        "abstract url": "https://arxiv.org/abs/2410.09530",
        "title": "Anomaly Detection and Inlet Pressure Prediction in Water Distribution Systems Using Machine Learning",
        "rating": "-3",
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "forecast"
            ]
        ],
        "abstract": "This study presents two models to optimize pressure management in water distribution networks. The first model forecasts pressure at distribution points and compares predictions with actual data to detect anomalies such as leaks and blockages. Early detection allows for timely interventions, minimizing economic losses and ensuring system sustainability. The second model estimates the necessary inlet pressure based on the influence of various distribution points, ensuring consistent water supply while reducing waste and optimizing resource management. Both models utilize modern machine learning algorithms to enhance the prediction process. The methodology includes the CNN-EMD model, which analyzes historical data collected every 15 minutes over two months to predict future pressures. The Empirical Mode Decomposition (EMD) method identifies fluctuations and anomalies, improving prediction accuracy. The second model combines CNN, EMD, and LSTM techniques to forecast required inlet pressure, emphasizing the impact of distribution points. Results show that the CNN-EMD and CNN-EMD-LSTM models enhance pressure management capabilities, with the first model achieving an anomaly detection accuracy of 85% to 95% and the second model predicting inlet pressure with an average accuracy of 93%. This enables flexible system adjustments and identifies critical factors affecting inlet pressure. In conclusion, advanced machine learning models like CNN-EMD and LSTM significantly improve pressure management in water distribution networks, facilitating early issue identification, ensuring efficient water supply, and optimizing resource management for future generations.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "13 pages, 14 figures"
    },
    {
        "paper id": "2410.09578",
        "abstract url": "https://arxiv.org/abs/2410.09578",
        "title": "Objective Measurements of Voice Quality",
        "rating": "-3",
        "keywords": [
            [
                "health",
                "physiological"
            ],
            [
                "music"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "The quality of human voice plays an important role across various fields like music, speech therapy, and communication, yet it lacks a universally accepted, objective definition. Instead, voice quality is referred to using subjective descriptors like \"rough\", \"breathy\" etc. Despite this subjectivity, extensive research across disciplines has linked these voice qualities to specific information about the speaker, such as health, physiological traits, and others. Current machine learning approaches for voice profiling rely on data-driven analysis without fully incorporating these established correlations, due to their qualitative nature. This paper aims to objectively quantify voice quality by synthesizing formulaic representations from past findings that correlate voice qualities to signal-processing metrics. We introduce formulae for 24 voice sub-qualities based on 25 signal properties, grounded in scientific literature. These formulae are tested against datasets with subjectively labeled voice qualities, demonstrating their validity.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09635",
        "abstract url": "https://arxiv.org/abs/2410.09635",
        "title": "Use of What-if Scenarios to Help Explain Artificial Intelligence Models for Neonatal Health",
        "rating": "-3.5",
        "keywords": [
            [
                "Health",
                "clinical"
            ],
            [
                "Tabular"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Early detection of intrapartum risk enables interventions to potentially prevent or mitigate adverse labor outcomes such as cerebral palsy. Currently, there is no accurate automated system to predict such events to assist with clinical decision-making. To fill this gap, we propose \"Artificial Intelligence (AI) for Modeling and Explaining Neonatal Health\" (AIMEN), a deep learning framework that not only predicts adverse labor outcomes from maternal, fetal, obstetrical, and intrapartum risk factors but also provides the model's reasoning behind the predictions made. The latter can provide insights into what modifications in the input variables of the model could have changed the predicted outcome. We address the challenges of imbalance and small datasets by synthesizing additional training data using Adaptive Synthetic Sampling (ADASYN) and Conditional Tabular Generative Adversarial Networks (CTGAN). AIMEN uses an ensemble of fully-connected neural networks as the backbone for its classification with the data augmentation supported by either ADASYN or CTGAN. AIMEN, supported by CTGAN, outperforms AIMEN supported by ADASYN in classification. AIMEN can predict a high risk for adverse labor outcomes with an average F1 score of 0.784. It also provides counterfactual explanations that can be achieved by changing 2 to 3 attributes on average. Resources available: https://github.com/ab9mamun/AIMEN.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "17 pages, 8 figures"
    },
    {
        "paper id": "2410.09643",
        "abstract url": "https://arxiv.org/abs/2410.09643",
        "title": "Multimodal Physical Activity Forecasting in Free-Living Clinical Settings: Hunting Opportunities for Just-in-Time Interventions",
        "rating": "-3.5",
        "keywords": [
            [
                "Clinical"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Objective: This research aims to develop a lifestyle intervention system, called MoveSense, that forecasts a patient's activity behavior to allow for early and personalized interventions in real-world clinical environments. Methods: We conducted two clinical studies involving 58 prediabetic veterans and 60 patients with obstructive sleep apnea to gather multimodal behavioral data using wearable devices. We develop multimodal long short-term memory (LSTM) network models, which are capable of forecasting the number of step counts of a patient up to 24 hours in advance by examining data from activity and engagement modalities. Furthermore, we design goal-based forecasting models to predict whether a person's next-day steps will be over a certain threshold. Results: Multimodal LSTM with early fusion achieves 33% and 37% lower mean absolute errors than linear regression and ARIMA respectively on the prediabetes dataset. LSTM also outperforms linear regression and ARIMA with a margin of 13% and 32% on the sleep dataset. Multimodal forecasting models also perform with 72% and 79% accuracy on the prediabetes dataset and sleep dataset respectively on goal-based forecasting. Conclusion: Our experiments conclude that multimodal LSTM models with early fusion are better than multimodal LSTM with late fusion and unimodal LSTM models and also than ARIMA and linear regression models. Significance: We address an important and challenging task of time-series forecasting in uncontrolled environments. Effective forecasting of a person's physical activity can aid in designing adaptive behavioral interventions to keep the user engaged and adherent to a prescribed routine.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "eess.SP"
        ],
        "comment": "9 pages, 5 figures"
    },
    {
        "paper id": "2410.09659",
        "abstract url": "https://arxiv.org/abs/2410.09659",
        "title": "Many-body Expansion Based Machine Learning Models for Octahedral Transition Metal Complexes",
        "rating": "-3.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "Graph"
            ],
            [
                "chemical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph-based machine learning models for materials properties show great potential to accelerate virtual high-throughput screening of large chemical spaces. However, in their simplest forms, graph-based models do not include any 3D information and are unable to distinguish stereoisomers such as those arising from different orderings of ligands around a metal center in coordination complexes. In this work we present a modification to revised autocorrelation descriptors, our molecular graph featurization method for machine learning various spin state dependent properties of octahedral transition metal complexes (TMCs). Inspired by analytical semi-empirical models for TMCs, the new modeling strategy is based on the many-body expansion (MBE) and allows one to tune the captured stereoisomer information by changing the truncation order of the MBE. We present the necessary modifications to include this approach in two commonly used machine learning methods, kernel ridge regression and feed-forward neural networks. On a test set composed of all possible isomers of binary transition metal complexes, the best MBE models achieve mean absolute errors of 2.75 kcal/mol on spin-splitting energies and 0.26 eV on frontier orbital energy gaps, a 30-40% reduction in error compared to models based on our previous approach. We also observe improved generalization to previously unseen ligands where the best-performing models exhibit mean absolute errors of 4.00 kcal/mol (i.e., a 0.73 kcal/mol reduction) on the spin-splitting energies and 0.53 eV (i.e., a 0.10 eV reduction) on the frontier orbital energy gaps. Because the new approach incorporates insights from electronic structure theory, such as ligand additivity relationships, these models exhibit systematic generalization from homoleptic to heteroleptic complexes, allowing for efficient screening of TMC search spaces.",
        "subjects": [
            "physics.chem-ph",
            "cond-mat.mtrl-sci",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09441",
        "abstract url": "https://arxiv.org/abs/2410.09441",
        "title": "From Text to Databases: attribute grammar as database meta-model",
        "rating": "-4",
        "keywords": [
            [
                "medical",
                "clinical"
            ],
            [
                "grammar"
            ]
        ],
        "abstract": "We present a general methodology for structuring textual data, represented as syntax trees enriched with semantic information, guided by a meta-model G defined as an attribute grammar. The method involves an evolution process where both the instance and its grammar evolve, with instance transformations guided by rewriting rules and a similarity measure. Each new instance generates a corresponding grammar, culminating in a target grammar GT that satisfies G. This methodology is applied to build a database populated from textual data. The process generates both a database schema and its instance, independent of specific database models. We demonstrate the approach using clinical medical cases, where trees represent database instances and grammars act as database schemas. Key contributions include the proposal of a general attribute grammar G, a formalization of grammar evolution, and a proof-of-concept implementation for database structuring.",
        "subjects": [
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09548",
        "abstract url": "https://arxiv.org/abs/2410.09548",
        "title": "Air-to-Ground Communications Beyond 5G: CoMP Handoff Management in UAV Network",
        "rating": "-4",
        "keywords": [
            [
                "5G"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "Air-to-ground (A2G) networks, using unmanned aerial vehicles (UAVs) as base stations to serve terrestrial user equipments (UEs), are promising for extending the spatial coverage capability in future communication systems. Coordinated transmission among multiple UAVs significantly improves network coverage and throughput compared to a single UAV transmission. However, implementing coordinated multi-point (CoMP) transmission for UAV mobility requires complex cooperation procedures, regardless of the handoff mechanism involved. This paper designs a novel CoMP transmission strategy that enables terrestrial UEs to achieve reliable and seamless connections with mobile UAVs. Specifically, a computationally efficient CoMP transmission method based on the theory of Poisson-Delaunay triangulation is developed, where an efficient subdivision search strategy for a CoMP UAV set is designed to minimize search overhead by a divide-and-conquer approach. For concrete performance evaluation, the cooperative handoff probability of the typical UE is analyzed, and the coverage probability with handoffs is derived. Simulation results demonstrate that the proposed scheme outperforms the conventional Voronoi scheme with the nearest serving UAV regarding coverage probabilities with handoffs. Moreover, each UE has a fixed and unique serving UAV set to avoid real-time dynamic UAV searching and achieve effective load balancing, significantly reducing system resource costs and enhancing network coverage performance.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "16 pages, 18 figures, 1 table"
    },
    {
        "paper id": "2410.09648",
        "abstract url": "https://arxiv.org/abs/2410.09648",
        "title": "Advancing Experimental Platforms for UAV Communications: Insights from AERPAW'S Digital Twin",
        "rating": "-4",
        "keywords": [
            [
                "5G"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "The rapid evolution of 5G and beyond has advanced space-air-terrestrial networks, with unmanned aerial vehicles (UAVs) offering enhanced coverage, flexible configurations, and cost efficiency. However, deploying UAV-based systems presents challenges including varying propagation conditions and hardware limitations. While simulators and theoretical models have been developed, real-world experimentation is critically important to validate the research. Digital twins, virtual replicas of physical systems, enable emulation that bridge theory and practice. This paper presents our experimental results from AERPAW's digital twin, showcasing its ability to simulate UAV communication scenarios and providing insights into system performance and reliability.",
        "subjects": [
            "eess.SY",
            "cs.AR",
            "cs.NI"
        ],
        "comment": "This article has been accepted for publication in the IEEE VTC Fall 2024--UAV Communication and Experimentation Workshop"
    },
    {
        "paper id": "2410.09606",
        "abstract url": "https://arxiv.org/abs/2410.09606",
        "title": "A Collaborative Team of UAV-Hexapod for an Autonomous Retrieval System in GNSS-Denied Maritime Environments",
        "rating": "-5",
        "keywords": [
            [
                "depth"
            ],
            [
                "LiDAR"
            ],
            [
                "robot"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "We present an integrated UAV-hexapod robotic system designed for GNSS-denied maritime operations, capable of autonomous deployment and retrieval of a hexapod robot via a winch mechanism installed on a UAV. This system is intended to address the challenges of localization, control, and mobility in dynamic maritime environments. Our solution leverages sensor fusion techniques, combining optical flow, LiDAR, and depth data for precise localization. Experimental results demonstrate the effectiveness of this system in real-world scenarios, validating its performance during field tests in both controlled and operational conditions in the MBZIRC 2023 Maritime Challenge.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09513",
        "abstract url": "https://arxiv.org/abs/2410.09513",
        "title": "Towards Design and Development of a Low-Cost Unmanned Surface Vehicle for Aquaculture Water Quality Monitoring in Shallow Water Environments",
        "rating": "-7",
        "keywords": [
            [
                "3D"
            ],
            [
                "Vehicle"
            ],
            [
                "Robot",
                "Navigation"
            ],
            [
                "health",
                "disease"
            ],
            [
                "Satellite"
            ]
        ],
        "abstract": "Unmanned surface vessels USVs are typically autonomous or remotely operated and are specifically designed for environmental monitoring in various aquatic environments Aquaculture requires constant monitoring and management of water quality for the health and productivity of aquaculture systems Poor water quality can lead to disease outbreaks reduced growth rates and even mass mortality of cultured species Many small aquaculture operations operate on tight budgets and in shallow water environments such as inland ponds coastal lagoons estuaries and shallow rivers particularly in developing regions This leads to the foremost manoeuvrability challenge underscoring the crucial need for agile cost effective USVs as efficient monitoring systems The paper proposes a low cost 3D printed twin hull catamaran style platform equipped with an Inertial Measurement Unit IMU and a Global Navigation Satellite System GNSS with a two layered control framework and a differential drive configuration developed using two high efficiency T200 thrusters The design utilizes the Robot Operating System ROS to create the control framework and incorporates Extended Kalman Filter EKF based sensor fusion techniques for localisation The paper evaluates the USVs autonomy through open water captive model experiments employing remote control methods to assess the vessels manoeuvrability and overall performance in shallow water conditions",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09381",
        "abstract url": "https://arxiv.org/abs/2410.09381",
        "title": "LLM-SmartAudit: Advanced Smart Contract Vulnerability Detection",
        "rating": "-10",
        "keywords": [],
        "abstract": "The immutable nature of blockchain technology, while revolutionary, introduces significant security challenges, particularly in smart contracts. These security issues can lead to substantial financial losses. Current tools and approaches often focus on specific types of vulnerabilities. However, a comprehensive tool capable of detecting a wide range of vulnerabilities with high accuracy is lacking. This paper introduces LLM-SmartAudit, a novel framework leveraging the advanced capabilities of Large Language Models (LLMs) to detect and analyze vulnerabilities in smart contracts. Using a multi-agent conversational approach, LLM-SmartAudit employs a collaborative system with specialized agents to enhance the audit process. To evaluate the effectiveness of LLM-SmartAudit, we compiled two distinct datasets: a labeled dataset for benchmarking against traditional tools and a real-world dataset for assessing practical applications. Experimental results indicate that our solution outperforms all traditional smart contract auditing tools, offering higher accuracy and greater efficiency. Furthermore, our framework can detect complex logic vulnerabilities that traditional tools have previously overlooked. Our findings demonstrate that leveraging LLM agents provides a highly effective method for automated smart contract auditing.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "12 pages, 5 figures, conference"
    },
    {
        "paper id": "2410.09425",
        "abstract url": "https://arxiv.org/abs/2410.09425",
        "title": "Computational complexity of the recoverable robust shortest path problem in acyclic digraphs",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, the recoverable robust shortest path problem in acyclic digraphs is considered. The interval budgeted uncertainty representation is used to model the uncertain second-stage costs. The computational complexity of this problem has been open to date. In this paper, we prove that the problem is strongly NP-hard even for the case of layered acyclic digraphs. We also show that for the discrete budgeted uncertainty, the problem is not approximable unless P=NP.",
        "subjects": [
            "cs.DS",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09427",
        "abstract url": "https://arxiv.org/abs/2410.09427",
        "title": "Meta-Learning for Hybrid Precoding in Millimeter Wave MIMO System",
        "rating": "-10",
        "keywords": [],
        "abstract": "The hybrid analog/digital architecture that connects a limited number of RF chains to multiple antennas through phase shifters could effectively address the energy consumption issues in massive multiple-input multiple-output (MIMO) systems. However, the main challenges in hybrid precoding lie in the coupling between analog and digital precoders and the constant modulus constraint. Generally, traditional optimization algorithms for this problem typically suffer from high computational complexity or suboptimal performance, while deep learning based solutions exhibit poor scalability and robustness. This paper proposes a plug and play, free of pre-training solution that leverages gradient guided meta learning (GGML) framework to maximize the spectral efficiency of MIMO systems through hybrid precoding. Specifically, GGML utilizes gradient information as network input to facilitate the sharing of gradient information flow. We retain the iterative process of traditional algorithms and leverage meta learning to alternately optimize the precoder. Simulation results show that this method outperforms existing methods, demonstrates robustness to variations in system parameters, and can even exceed the performance of fully digital weighted minimum mean square error (WMMSE) precoding with the same number of antennas.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "5pages, 6figures"
    },
    {
        "paper id": "2410.09436",
        "abstract url": "https://arxiv.org/abs/2410.09436",
        "title": "Sum Rate Maximization for Movable Antenna Enhanced Multiuser Covert Communications",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this letter, we propose to employ movable antenna (MA) to enhance covert communications with noise uncertainty, where the confidential data is transmitted from an MA-aided access point (AP) to multiple users with a warden attempting to detect the existence of the legal transmission. To maximize the sum rate of users under covertness constraint, we formulate an optimization problem to jointly design the transmit beamforming and the positions of MAs at the AP. To solve the formulated non-convex optimization problem, we develop a block successive upper-bound minimization (BSUM) based algorithm, where the proximal distance algorithm (PDA) and the successive convex approximation (SCA) are employed to optimize the transmit beamforming and the MAs' positions, respectively. Simulation results show that the proposed MAs-aided system can significantly increase the covert sum rate via antenna position optimization as compared to conventional systems with fixed-position antennas (FPAs).",
        "subjects": [
            "eess.SP"
        ],
        "comment": "5 pages, 4 figures (subfigures included), submitted to an IEEE journal for possible publication"
    },
    {
        "paper id": "2410.09460",
        "abstract url": "https://arxiv.org/abs/2410.09460",
        "title": "A Deep Learning Based Decoder for Concatenated Coding over Deletion Channels",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we introduce a deep learning-based decoder designed for concatenated coding schemes over a deletion/substitution channel. Specifically, we focus on serially concatenated codes, where the outer code is either a convolutional or a low-density parity-check (LDPC) code, and the inner code is a marker code. We utilize Bidirectional Gated Recurrent Units (BI-GRUs) as log-likelihood ratio (LLR) estimators and outer code decoders for estimating the message bits. Our results indicate that decoders powered by BI-GRUs perform comparably in terms of error rates with the MAP detection of the marker code. We also find that a single network can work well for a wide range of channel parameters. In addition, it is possible to use a single BI-GRU based network to estimate the message bits via one-shot decoding when the outer code is a convolutional code.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "6 pages, 8 figures"
    },
    {
        "paper id": "2410.09464",
        "abstract url": "https://arxiv.org/abs/2410.09464",
        "title": "Quantify Gas-to-Power Fault Propagation Speed:A Semi-Implicit Simulation Approach",
        "rating": "-10",
        "keywords": [],
        "abstract": "Relying heavily on the secure supply of natural gas, the modern clean electric power systems are prone to the gas disturbances induced by the inherent rupture and leakage faults. For the first time, this paper studies the cross-system propagation speed of these faults using a simulation-based approach. Firstly, we establish the differential algebraic equation models of the rupture and leakage faults respectively. The boundary conditions at the fault locations are derived using the method of characteristics. Secondly, we propose utilizing a semi-implicit approach to perform post-fault simulations. The approach, based on the stiffly-accurate Rosenbrock scheme, possesses the implicit numerical stability and explicit computation burdens. Therefore, the high-dimensional and multi-time-scale stiff models can be solved in an efficient and robust way. Thirdly, to accurately locate the simulation events, which can not be predicted a priori, we propose a critical-time-location strategy based on the continuous Runge-Kutta approach. In case studies, we verified the accuracy and the efficiency superiority of the proposed simulation approach. The impacts of gas faults on gas and power dynamics were investigated by simulation, where the critical events were identified accurately. We found that the fault propagation speed mainly depends on the fault position and is influenced by the pipe frictions. The bi-directional coupling between gas and power may lead to cascading failures.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09497",
        "abstract url": "https://arxiv.org/abs/2410.09497",
        "title": "Multigrid methods for the Stokes problem on GPU systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents a matrix-free multigrid method for solving the Stokes problem, discretized using $H^{\\text{div}}$-conforming discontinuous Galerkin methods. We employ a Schur complement method combined with the fast diagonalization method for the efficient evaluation of the local solver within the multiplicative Schwarz smoother. This approach operates directly on both the velocity and pressure spaces, eliminating the need for a global Schur complement approximation. By leveraging the tensor product structure of Raviart-Thomas elements and an optimized, conflict-free shared memory access pattern, the matrix-free operator evaluation demonstrates excellent performance numbers, reaching over one billion degrees of freedom per second on a single NVIDIA A100 GPU. Numerical results indicate efficiency comparable to that of the three-dimensional Poisson problem.",
        "subjects": [
            "math.NA",
            "cs.MS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09516",
        "abstract url": "https://arxiv.org/abs/2410.09516",
        "title": "Combining Causal Discovery and Machine Learning for Modeling Data Center Operations",
        "rating": "-10",
        "keywords": [],
        "abstract": "Data centers consume large amounts of energy, and their electricity demand is predicted to multiply in the coming years. To mitigate the growing environmental challenges associated with data center energy consumption, optimizing the operation of IT equipment (ITE) and Heating, Ventilation, and Air Conditioning (HVAC) systems is crucial. Yet, modeling such systems is challenging due to the complex and non-linear interactions between many factors, including server load, indoor climate, weather, and data center configurations. While physical simulations are capable of representing this complexity, they are often time-consuming to set up and run. Machine Learning (ML), in contrast, allows efficient data-driven modeling but typically does not consider a system's causal dynamics, lacks interpretability, and suffers from overfitting. This study addresses the limitations of ML-driven predictive modeling by employing Causal Discovery to select features that are causally related to the response variable. We use a simulated data center to generate time series of its operation and conduct experiments to compare ML models trained with all features, traditional feature selection methods, and causal feature selection. Our results show that causal feature selection leads to models with substantially fewer features and similar or better performance, especially when predicting the effects of interventions. In addition, our proposed methodology allows the interpretation of the causal mechanism and the integration of expert knowledge into the modeling process. Overall, our findings suggest that combining Causal Discovery with ML can be a promising alternative for feature selection methods and prediction of interactions in complex physical systems.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "11 pages, 11 figures and 5 tables"
    },
    {
        "paper id": "2410.09534",
        "abstract url": "https://arxiv.org/abs/2410.09534",
        "title": "SUS-Lib: An automated tool for usability evaluation based on the Software Usability Scale from user feedback",
        "rating": "-10",
        "keywords": [],
        "abstract": "Usability evaluation has received considerable attention from both the research and practice communities. While there are many evaluation tools available, the Software Usability Scale (SUS) is the most widely used. In this paper, we introduce and describe the SUS-Lib software package, which aims to compute SUS scores and generate graphical figures based on user input. SUS-Lib responds to the need for user-friendly software that requires only basic knowledge and skills of the Python environment and command line tools. By using open source solutions and low hardware resources, SUS-Lib is a cost-effective solution. In addition, due to its generic nature, SUS-Lib can also be used in different research setups and settings.",
        "subjects": [
            "cs.SE",
            "cs.DL"
        ],
        "comment": "18 pages, 5 tables, 5 figures, 28 references"
    },
    {
        "paper id": "2410.09562",
        "abstract url": "https://arxiv.org/abs/2410.09562",
        "title": "SituFont: A Just-in-Time Adaptive Intervention System for Enhancing Mobile Readability in Situational Visual Impairments",
        "rating": "-10",
        "keywords": [],
        "abstract": "Situational visual impairments (SVIs) significantly impact mobile readability, causing user discomfort and hindering information access. This paper introduces SituFont, a novel just-in-time adaptive intervention (JITAI) system designed to enhance mobile text readability by semi-automatically adjusting font parameters in response to real-time contextual changes. Leveraging smartphone sensors and a human-in-the-loop approach, SituFont personalizes the reading experience by adapting to individual user preferences, including personal factors such as fatigue and distraction level, and environmental factors like lighting, motion, and location. To inform the design of SituFont, we conducted formative interviews (N=15) to identify key SVI factors affecting readability and controlled experiments (N=18) to quantify the relationship between these factors and optimal text parameters. We then evaluated SituFont's effectiveness through a comparative user study under eight simulated SVI scenarios (N=12), demonstrating its ability to overcome SVIs. Our findings highlight the potential of JITAI systems like SituFont to mitigate the impact of SVIs and enhance mobile accessibility.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09618",
        "abstract url": "https://arxiv.org/abs/2410.09618",
        "title": "An Optical Method for Evaluating the Mechanical Properties of Wires Under Impact Tensile Load",
        "rating": "-10",
        "keywords": [],
        "abstract": "The dynamic properties of materials utilized in architecture or engineering applications can significantly affect their performance under dynamic or impact loading conditions. To evaluate such behavior, force transducers are commonly employed in testing. However, the calibration of force transducers is typically limited to static conditions and relies solely on gravitational forces exerted on standard masses. Thus, assessing the uncertainty in force measurements using force transducers during dynamic loading conditions remains a challenging task, presenting a significant obstacle in accurately characterizing the dynamic behavior of materials. In this work, an optical technique to evaluate the mechanical properties of wires subjected to impact tensile loads is presented. The wire under test is subjected to an impact tensile load by applying the inertial force of a rigid mass, which is supported by utilizing an aerostatic linear bearing with sufficiently small friction. The inertial force applied to the wire can be determined by multiplying the mass of the rigid mass by its acceleration, where the acceleration can be measured employing a Michelson type optical interferometer. The performance of the proposed method is demonstrated through experiments and analysis of the dynamic characteristics of a tungsten wire under impact tensile loading conditions.",
        "subjects": [
            "eess.SP",
            "cond-mat.mtrl-sci",
            "physics.app-ph",
            "physics.optics"
        ],
        "comment": "9 pages, 13 figures, submitted to IEEE Transactions on Instrumentation and Measurement"
    },
    {
        "paper id": "2410.09650",
        "abstract url": "https://arxiv.org/abs/2410.09650",
        "title": "Reducing Data Bottlenecks in Distributed, Heterogeneous Neural Networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "The rapid advancement of embedded multicore and many-core systems has revolutionized computing, enabling the development of high-performance, energy-efficient solutions for a wide range of applications. As models scale up in size, data movement is increasingly the bottleneck to performance. This movement of data can exist between processor and memory, or between cores and chips. This paper investigates the impact of bottleneck size, in terms of inter-chip data traffic, on the performance of deep learning models in embedded multicore and many-core systems. We conduct a systematic analysis of the relationship between bottleneck size, computational resource utilization, and model accuracy. We apply a hardware-software co-design methodology where data bottlenecks are replaced with extremely narrow layers to reduce the amount of data traffic. In effect, time-multiplexing of signals is replaced by learnable embeddings that reduce the demands on chip IOs. Our experiments on the CIFAR100 dataset demonstrate that the classification accuracy generally decreases as the bottleneck ratio increases, with shallower models experiencing a more significant drop compared to deeper models. Hardware-side evaluation reveals that higher bottleneck ratios lead to substantial reductions in data transfer volume across the layers of the neural network. Through this research, we can determine the trade-off between data transfer volume and model performance, enabling the identification of a balanced point that achieves good performance while minimizing data transfer volume. This characteristic allows for the development of efficient models that are well-suited for resource-constrained environments.",
        "subjects": [
            "cs.DC",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09662",
        "abstract url": "https://arxiv.org/abs/2410.09662",
        "title": "Exploring Demonstration Retrievers in RAG for Coding Tasks: Yeas and Nays!",
        "rating": "-10",
        "keywords": [],
        "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge bases, achieving state-of-the-art results in various coding tasks. The core of RAG is retrieving demonstration examples, which is essential to balance effectiveness (generation quality) and efficiency (retrieval time) for optimal performance. However, the high-dimensional nature of code representations and large knowledge bases often create efficiency bottlenecks, which are overlooked in previous research. This paper systematically evaluates the efficiency-effectiveness trade-off of retrievers across three coding tasks: Program Synthesis, Commit Message Generation, and Assertion Generation. We examined six retrievers: two sparse (BM25 and BM25L) and four dense retrievers, including one exhaustive dense retriever (SBERT's Semantic Search) and three approximate dense retrievers (ANNOY, LSH, and HNSW). Our findings show that while BM25 excels in effectiveness, it suffers in efficiency as the knowledge base grows beyond 1000 entries. In large-scale retrieval, efficiency differences become more pronounced, with approximate dense retrievers offering the greatest gains. For instance, in Commit Generation task, HNSW achieves a 44x speed up, while only with a 1.74% drop in RougeL compared with BM25. Our results also show that increasing the number of demonstrations in the prompt doesn't always improve the effectiveness and can increase latency and lead to incorrect outputs. Our findings provide valuable insights for practitioners aiming to build efficient and effective RAG systems for coding tasks.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "11 pages, 6 figures, 6 tables"
    },
    {
        "paper id": "2410.09663",
        "abstract url": "https://arxiv.org/abs/2410.09663",
        "title": "Optimal Inferential Control of Convolutional Neural Networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "Convolutional neural networks (CNNs) have achieved remarkable success in representing and simulating complex spatio-temporal dynamic systems within the burgeoning field of scientific machine learning. However, optimal control of CNNs poses a formidable challenge, because the ultra-high dimensionality and strong nonlinearity inherent in CNNs render them resistant to traditional gradient-based optimal control techniques. To tackle the challenge, we propose an optimal inferential control framework for CNNs that represent a complex spatio-temporal system, which sequentially infers the best control decisions based on the specified control objectives. This reformulation opens up the utilization of sequential Monte Carlo sampling, which is efficient in searching through high-dimensional spaces for nonlinear inference. We specifically leverage ensemble Kalman smoothing, a sequential Monte Carlo algorithm, to take advantage of its computational efficiency for nonlinear high-dimensional systems. Further, to harness graphics processing units (GPUs) to accelerate the computation, we develop a new sequential ensemble Kalman smoother based on matrix variate distributions. The smoother is capable of directly handling matrix-based inputs and outputs of CNNs without vectorization to fit with the parallelized computing architecture of GPUs. Numerical experiments show that the proposed approach is effective in controlling spatio-temporal systems with high-dimensional state and control spaces. All the code and data are available at https://github.com/Alivaziri/Optimal-Inferential-Control-of-CNNs.",
        "subjects": [
            "math.OC",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09668",
        "abstract url": "https://arxiv.org/abs/2410.09668",
        "title": "Automated Verification of Tree-Manipulating Programs Using Constrained Horn Clauses",
        "rating": "-10",
        "keywords": [],
        "abstract": "Verifying programs that manipulate tree data structures often requires complex, ad-hoc proofs that are hard to generalize and automate. This paper introduces an automatic technique for analyzing such programs. Our approach combines automata and logics to tackle the challenges posed by diverse tree data structures uniformly. At the core of our methodology is the knitted-tree encoding, which maps a program execution into a tree data structure encapsulating input, output, and intermediate configurations, within a single structure. By leveraging the compositional properties of knitted-trees, we characterize them using constrained Horn clauses (CHCs). This encoding reduces verification to solving CHC satisfiability, benefiting from ongoing advancements in CHC solvers. While we focus on the memory safety problem for illustration, our technique applies broadly to various verification tasks involving tree data structures.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09670",
        "abstract url": "https://arxiv.org/abs/2410.09670",
        "title": "Making Beshbarmak: Games for Central Asian Cultural Heritage",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper introduces \"Making Beshbarmak\", an interactive cooking game that celebrates the nomadic ancestry and cultural heritage of Central Asian communities worldwide. Designed to promote cultural appreciation and identity formation, the game invites players to learn and recreate the traditional dish Beshbarmak through an engaging step-by-step process, incorporating storytelling elements that explain the cultural significance of the meal. Our project contributes to digital cultural heritage and games research by offering an accessible, open-source prototype on p5.js, enabling users to connect with and explore Central Asian traditions. \"Making Beshbarmak\" serves as both an educational tool and a platform for cultural preservation, fostering a sense of belonging among Central Asian immigrant populations.",
        "subjects": [
            "cs.HC",
            "cs.MM"
        ],
        "comment": "5 pages, 2 figures, EAI ArtsIT Conference"
    },
    {
        "paper id": "2410.09710",
        "abstract url": "https://arxiv.org/abs/2410.09710",
        "title": "Parallelize Over Data Particle Advection: Participation, Ping Pong Particles, and Overhead",
        "rating": "-10",
        "keywords": [],
        "abstract": "Particle advection is one of the foundational algorithms for visualization and analysis and is central to understanding vector fields common to scientific simulations. Achieving efficient performance with large data in a distributed memory setting is notoriously difficult. Because of its simplicity and minimized movement of large vector field data, the Parallelize over Data (POD) algorithm has become a de facto standard. Despite its simplicity and ubiquitous usage, the scaling issues with the POD algorithm are known and have been described throughout the literature. In this paper, we describe a set of in-depth analyses of the POD algorithm that shed new light on the underlying causes for the poor performance of this algorithm. We designed a series of representative workloads to study the performance of the POD algorithm and executed them on a supercomputer while collecting timing and statistical data for analysis. We then performed two different types of analysis. In the first analysis, we introduce two novel metrics for measuring algorithmic efficiency over the course of a workload run. The second analysis was from the perspective of the particles being advected. Using particle centric analysis, we identify that the overheads associated with particle movement between processes (not the communication itself) have a dramatic impact on the overall execution time. In the first analysis, we introduce two novel metrics for measuring algorithmic efficiency over the course of a workload run. The second analysis was from the perspective of the particles being advected. Using particle-centric analysis, we identify that the overheads associated with particle movement between processes have a dramatic impact on the overall execution time. These overheads become particularly costly when flow features span multiple blocks, resulting in repeated particle circulation between blocks.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.09721",
        "abstract url": "https://arxiv.org/abs/2410.09721",
        "title": "\"I inherently just trust that it works\": Investigating Mental Models of Open-Source Libraries for Differential Privacy",
        "rating": "-10",
        "keywords": [],
        "abstract": "Differential privacy (DP) is a promising framework for privacy-preserving data science, but recent studies have exposed challenges in bringing this theoretical framework for privacy into practice. These tensions are particularly salient in the context of open-source software libraries for DP data analysis, which are emerging tools to help data stewards and analysts build privacy-preserving data pipelines for their applications. While there has been significant investment into such libraries, we need further inquiry into the role of these libraries in promoting understanding of and trust in DP, and in turn, the ways in which design of these open-source libraries can shed light on the challenges of creating trustworthy data infrastructures in practice. In this study, we use qualitative methods and mental models approaches to analyze the differences between conceptual models used to design open-source DP libraries and mental models of DP held by users. Through a two-stage study design involving formative interviews with 5 developers of open-source DP libraries and user studies with 17 data analysts, we find that DP libraries often struggle to bridge the gaps between developer and user mental models. In particular, we highlight the tension DP libraries face in maintaining rigorous DP implementations and facilitating user interaction. We conclude by offering practical recommendations for further development of DP libraries.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "39 Pages, 12 Figures. To be published in CSCW 2024"
    }
]