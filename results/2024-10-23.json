[
    {
        "paper id": "2410.17739",
        "abstract url": "https://arxiv.org/abs/2410.17739",
        "title": "Local Contrastive Editing of Gender Stereotypes",
        "rating": "2.5",
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "cs.CY",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Stereotypical bias encoded in language models (LMs) poses a threat to safe language technology, yet our understanding of how bias manifests in the parameters of LMs remains incomplete. We introduce local contrastive editing that enables the localization and editing of a subset of weights in a target model in relation to a reference model. We deploy this approach to identify and modify subsets of weights that are associated with gender stereotypes in LMs. Through a series of experiments, we demonstrate that local contrastive editing can precisely localize and control a small subset (< 0.5%) of weights that encode gender bias. Our work (i) advances our understanding of how stereotypical biases can manifest in the parameter space of LMs and (ii) opens up new avenues for developing parameter-efficient strategies for controlling model properties in a contrastive manner.",
        "subjects": [
            "cs.CL",
            "cs.CY"
        ],
        "comment": "Accepted at EMNLP 2024"
    },
    {
        "paper id": "2410.18035",
        "abstract url": "https://arxiv.org/abs/2410.18035",
        "title": "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning",
        "rating": "2.5",
        "keywords": [
            [
                "parameter-efficient",
                "PEFT",
                "efficient fine-tuning"
            ],
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are highly effective parameter-efficient fine-tuning (PEFT) methods. However, they introduce significant latency in multi-tenant settings due to the LoRA modules and MOE routers added to multiple linear modules in the Transformer layer. To address this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel and efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism. This mechanism calculates expert routing results once before generating the first new token and reuses these results for subsequent tokens, reducing latency. Extensive experiments and analysis on commonsense reasoning tasks, math reasoning tasks, and widely used LLM evaluation benchmarks demonstrate that MiLoRA consistently outperforms strong PEFT baselines with comparable tunable parameter budgets. Additionally, MiLoRA significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by EMNLP 2024 Findings. arXiv admin note: substantial text overlap with arXiv:2405.18203"
    },
    {
        "paper id": "2410.17637",
        "abstract url": "https://arxiv.org/abs/2410.17637",
        "title": "MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Visual preference alignment involves training Large Vision-Language Models (LVLMs) to predict human preferences between visual inputs. This is typically achieved by using labeled datasets of chosen/rejected pairs and employing optimization algorithms like direct preference optimization (DPO). Existing visual alignment methods, primarily designed for single-image scenarios, struggle to effectively handle the complexity of multi-image tasks due to the scarcity of diverse training data and the high cost of annotating chosen/rejected pairs. We present Multi-Image Augmented Direct Preference Optimization (MIA-DPO), a visual preference alignment approach that effectively handles multi-image inputs. MIA-DPO mitigates the scarcity of diverse multi-image training data by extending single-image data with unrelated images arranged in grid collages or pic-in-pic formats, significantly reducing the costs associated with multi-image data annotations. Our observation reveals that attention values of LVLMs vary considerably across different images. We use attention values to identify and filter out rejected responses the model may have mistakenly focused on. Our attention-aware selection for constructing the chosen/rejected pairs without relying on (i) human annotation, (ii) extra data, and (iii) external models or APIs. MIA-DPO is compatible with various architectures and outperforms existing methods on five multi-image benchmarks, achieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on the recent InternLM-XC2.5. Moreover, MIA-DPO has a minimal effect on the model's ability to understand single images.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Project URL: https://github.com/Liuziyu77/MIA-DPO"
    },
    {
        "paper id": "2410.17779",
        "abstract url": "https://arxiv.org/abs/2410.17779",
        "title": "ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in multimodal fusion have witnessed the remarkable success of vision-language (VL) models, which excel in various multimodal applications such as image captioning and visual question answering. However, building VL models requires substantial hardware resources, where efficiency is restricted by two key factors: the extended input sequence of the language model with vision features demands more computational operations, and a large number of additional learnable parameters increase memory complexity. These challenges significantly restrict the broader applicability of such models. To bridge this gap, we propose ADEM-VL, an efficient vision-language method that tunes VL models based on pretrained large language models (LLMs) by adopting a parameter-free cross-attention mechanism for similarity measurements in multimodal fusion. This approach only requires embedding vision features into the language space, significantly reducing the number of trainable parameters and accelerating both training and inference speeds. To enhance representation learning in fusion module, we introduce an efficient multiscale feature generation scheme that requires only a single forward pass through the vision encoder. Moreover, we propose an adaptive fusion scheme that dynamically discards less relevant visual information for each text token based on its attention score. This ensures that the fusion process prioritizes the most pertinent visual features. With experiments on various tasks including visual question answering, image captioning, and instruction-following, we demonstrate that our framework outperforms existing approaches. Specifically, our method surpasses existing methods by an average accuracy of 0.77% on ScienceQA dataset, with reduced training and inference latency, demonstrating the superiority of our framework. The code is available at https://github.com/Hao840/ADEM-VL.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17856",
        "abstract url": "https://arxiv.org/abs/2410.17856",
        "title": "ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting",
        "rating": "2",
        "keywords": [
            [
                "Vision-language",
                "VLMs"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. A key issue is the difficulty in smoothly connecting individual entities in low-level observations with abstract concepts required for planning. A common approach to address this problem is through the use of hierarchical agents, where VLMs serve as high-level reasoners that break down tasks into executable sub-tasks, typically specified using language and imagined observations. However, language often fails to effectively convey spatial information, while generating future images with sufficient accuracy remains challenging. To address these limitations, we propose visual-temporal context prompting, a novel communication protocol between VLMs and policy models. This protocol leverages object segmentation from both past and present observations to guide policy-environment interactions. Using this approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual observations and segmentation masks, with real-time object tracking provided by SAM-2. Our method unlocks the full potential of VLMs visual-language reasoning abilities, enabling them to solve complex creative tasks, especially those heavily reliant on spatial understanding. Experiments in Minecraft demonstrate that our approach allows agents to accomplish previously unattainable tasks, highlighting the effectiveness of visual-temporal context prompting in embodied decision-making. Codes and demos will be available on the project page: https://craftjarvis.github.io/ROCKET-1.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18325",
        "abstract url": "https://arxiv.org/abs/2410.18325",
        "title": "AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models",
        "rating": "2",
        "keywords": [
            [
                "Audio-Visual"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Following the success of Large Language Models (LLMs), expanding their boundaries to new modalities represents a significant paradigm shift in multimodal understanding. Human perception is inherently multimodal, relying not only on text but also on auditory and visual cues for a complete understanding of the world. In recognition of this fact, audio-visual LLMs have recently emerged. Despite promising developments, the lack of dedicated benchmarks poses challenges for understanding and evaluating models. In this work, we show that audio-visual LLMs struggle to discern subtle relationships between audio and visual signals, leading to hallucinations, underscoring the need for reliable benchmarks. To address this, we introduce AVHBench, the first comprehensive benchmark specifically designed to evaluate the perception and comprehension capabilities of audio-visual LLMs. Our benchmark includes tests for assessing hallucinations, as well as the cross-modal matching and reasoning abilities of these models. Our results reveal that most existing audio-visual LLMs struggle with hallucinations caused by cross-interactions between modalities, due to their limited capacity to perceive complex multimodal signals and their relationships. Additionally, we demonstrate that simple training with our AVHBench improves robustness of audio-visual LLMs against hallucinations.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "URL: https://github.com/AVHBench/AVHBench"
    },
    {
        "paper id": "2410.17661",
        "abstract url": "https://arxiv.org/abs/2410.17661",
        "title": "PETAH: Parameter Efficient Task Adaptation for Hybrid Transformers in a resource-limited Context",
        "rating": "1.5",
        "keywords": [
            [
                "Parameter Efficient"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Following their success in natural language processing (NLP), there has been a shift towards transformer models in computer vision. While transformers perform well and offer promising multi-tasking performance, due to their high compute requirements, many resource-constrained applications still rely on convolutional or hybrid models that combine the benefits of convolution and attention layers and achieve the best results in the sub 100M parameter range. Simultaneously, task adaptation techniques that allow for the use of one shared transformer backbone for multiple downstream tasks, resulting in great storage savings at negligible cost in performance, have not yet been adopted for hybrid transformers. In this work, we investigate how to achieve the best task-adaptation performance and introduce PETAH: Parameter Efficient Task Adaptation for Hybrid Transformers. We further combine PETAH adaptation with pruning to achieve highly performant and storage friendly models for multi-tasking. In our extensive evaluation on classification and other vision tasks, we demonstrate that our PETAH-adapted hybrid models outperform established task-adaptation techniques for ViTs while requiring fewer parameters and being more efficient on mobile hardware.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17676",
        "abstract url": "https://arxiv.org/abs/2410.17676",
        "title": "Towards a Similarity-adjusted Surprisal Theory",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Surprisal theory posits that the cognitive effort required to comprehend a word is determined by its contextual predictability, quantified as surprisal. Traditionally, surprisal theory treats words as distinct entities, overlooking any potential similarity between them. Giulianelli et al. (2023) address this limitation by introducing information value, a measure of predictability designed to account for similarities between communicative units. Our work leverages Ricotta and Szeidl's (2006) diversity index to extend surprisal into a metric that we term similarity-adjusted surprisal, exposing a mathematical relationship between surprisal and information value. Similarity-adjusted surprisal aligns with information value when considering graded similarities and reduces to standard surprisal when words are treated as distinct. Experimental results with reading time data indicate that similarity-adjusted surprisal adds predictive power beyond standard surprisal for certain datasets, suggesting it serves as a complementary measure of comprehension effort.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "EMNLP 2024 main conference proceedings"
    },
    {
        "paper id": "2410.17883",
        "abstract url": "https://arxiv.org/abs/2410.17883",
        "title": "Lightweight Neural App Control",
        "rating": "1.5",
        "keywords": [
            [
                "vision-language",
                "VLM"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper introduces a novel mobile phone control architecture, termed ``app agents\", for efficient interactions and controls across various Android apps. The proposed Lightweight Multi-modal App Control (LiMAC) takes as input a textual goal and a sequence of past mobile observations, such as screenshots and corresponding UI trees, to generate precise actions. To address the computational constraints inherent to smartphones, within LiMAC, we introduce a small Action Transformer (AcT) integrated with a fine-tuned vision-language model (VLM) for real-time decision-making and task execution. We evaluate LiMAC on two open-source mobile control datasets, demonstrating the superior performance of our small-form-factor approach against fine-tuned versions of open-source VLMs, such as Florence2 and Qwen2-VL. It also significantly outperforms prompt engineering baselines utilising closed-source foundation models like GPT-4o. More specifically, LiMAC increases the overall action accuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to prompt-engineering baselines.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17961",
        "abstract url": "https://arxiv.org/abs/2410.17961",
        "title": "Closed-form merging of parameter-efficient modules for Federated Continual Learning",
        "rating": "1.5",
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Model merging has emerged as a crucial technique in Deep Learning, enabling the integration of multiple models into a unified system while preserving performance and scalability. In this respect, the compositional properties of low-rank adaptation techniques (e.g., LoRA) have proven beneficial, as simple averaging LoRA modules yields a single model that mostly integrates the capabilities of all individual modules. Building on LoRA, we take a step further by imposing that the merged model matches the responses of all learned modules. Solving this objective in closed form yields an indeterminate system with A and B as unknown variables, indicating the existence of infinitely many closed-form solutions. To address this challenge, we introduce LoRM, an alternating optimization strategy that trains one LoRA matrix at a time. This allows solving for each unknown variable individually, thus finding a unique solution. We apply our proposed methodology to Federated Class-Incremental Learning (FCIL), ensuring alignment of model responses both between clients and across tasks. Our method demonstrates state-of-the-art performance across a range of FCIL scenarios.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17973",
        "abstract url": "https://arxiv.org/abs/2410.17973",
        "title": "Together We Can: Multilingual Automatic Post-Editing for Low-Resource Languages",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "This exploratory study investigates the potential of multilingual Automatic Post-Editing (APE) systems to enhance the quality of machine translations for low-resource Indo-Aryan languages. Focusing on two closely related language pairs, English-Marathi and English-Hindi, we exploit the linguistic similarities to develop a robust multilingual APE model. To facilitate cross-linguistic transfer, we generate synthetic Hindi-Marathi and Marathi-Hindi APE triplets. Additionally, we incorporate a Quality Estimation (QE)-APE multi-task learning framework. While the experimental results underline the complementary nature of APE and QE, we also observe that QE-APE multitask learning facilitates effective domain adaptation. Our experiments demonstrate that the multilingual APE models outperform their corresponding English-Hindi and English-Marathi single-pair models by $2.5$ and $2.39$ TER points, respectively, with further notable improvements over the multilingual APE model observed through multi-task learning ($+1.29$ and $+1.44$ TER points), data augmentation ($+0.53$ and $+0.45$ TER points) and domain adaptation ($+0.35$ and $+0.45$ TER points). We release the synthetic data, code, and models accrued during this study publicly at https://github.com/cfiltnlp/Multilingual-APE.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at Findings of EMNLP 2024"
    },
    {
        "paper id": "2410.18050",
        "abstract url": "https://arxiv.org/abs/2410.18050",
        "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Long-Context Question Answering (LCQA), a challenging task, aims to reason over long-context documents to yield accurate answers to questions. Existing long-context Large Language Models (LLMs) for LCQA often struggle with the \"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this issue by providing external factual evidence. However, its chunking strategy disrupts the global long-context information, and its low-quality retrieval in long contexts hinders LLMs from identifying effective factual details due to substantial noise. To this end, we propose LongRAG, a general, dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance RAG's understanding of complex long-context knowledge (i.e., global information and factual details). We design LongRAG as a plug-and-play paradigm, facilitating adaptation to various domains and LLMs. Extensive experiments on three multi-hop datasets demonstrate that LongRAG significantly outperforms long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG (up by 17.25%). Furthermore, we conduct quantitative ablation studies and multi-dimensional analyses, highlighting the effectiveness of the system's components and fine-tuning strategies. Data and code are available at https://github.com/QingFei1/LongRAG.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "EMNLP 2024 Main, Final"
    },
    {
        "paper id": "2410.18163",
        "abstract url": "https://arxiv.org/abs/2410.18163",
        "title": "Gazelle: An Instruction Dataset for Arabic Writing Assistance",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Writing has long been considered a hallmark of human intelligence and remains a pinnacle task for artificial intelligence (AI) due to the intricate cognitive processes involved. Recently, rapid advancements in generative AI, particularly through the development of Large Language Models (LLMs), have significantly transformed the landscape of writing assistance. However, underrepresented languages like Arabic encounter significant challenges in the development of advanced AI writing tools, largely due to the limited availability of data. This scarcity constrains the training of effective models, impeding the creation of sophisticated writing assistance technologies. To address these issues, we present Gazelle, a comprehensive dataset for Arabic writing assistance. In addition, we offer an evaluation framework designed to enhance Arabic writing assistance tools. Our human evaluation of leading LLMs, including GPT-4, GPT-4o, Cohere Command R+, and Gemini 1.5 Pro, highlights their respective strengths and limitations in addressing the challenges of Arabic writing. Our findings underscore the need for continuous model training and dataset enrichment to manage the complexities of Arabic language processing, paving the way for more effective AI-powered Arabic writing tools.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "EMNLP2024 Finding Camara-ready version"
    },
    {
        "paper id": "2410.18322",
        "abstract url": "https://arxiv.org/abs/2410.18322",
        "title": "Unified Microphone Conversion: Many-to-Many Device Mapping via Feature-wise Linear Modulation",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "In this study, we introduce Unified Microphone Conversion, a unified generative framework to enhance the resilience of sound event classification systems against device variability. Building on the limitations of previous works, we condition the generator network with frequency response information to achieve many-to-many device mapping. This approach overcomes the inherent limitation of CycleGAN, requiring separate models for each device pair. Our framework leverages the strengths of CycleGAN for unpaired training to simulate device characteristics in audio recordings and significantly extends its scalability by integrating frequency response related information via Feature-wise Linear Modulation. The experiment results show that our method outperforms the state-of-the-art method by 2.6% and reducing variability by 0.8% in macro-average F1 score.",
        "subjects": [
            "cs.SD",
            "cs.LG",
            "cs.MM",
            "eess.AS"
        ],
        "comment": "Currently under review for ICASSP 2025"
    },
    {
        "paper id": "2410.18351",
        "abstract url": "https://arxiv.org/abs/2410.18351",
        "title": "AdaEDL: Early Draft Stopping for Speculative Decoding of Large Language Models via an Entropy-based Lower Bound on Token Acceptance Probability",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Speculative decoding is a powerful technique that attempts to circumvent the autoregressive constraint of modern Large Language Models (LLMs). The aim of speculative decoding techniques is to improve the average inference time of a large, target model without sacrificing its accuracy, by using a more efficient draft model to propose draft tokens which are then verified in parallel. The number of draft tokens produced in each drafting round is referred to as the draft length and is often a static hyperparameter chosen based on the acceptance rate statistics of the draft tokens. However, setting a static draft length can negatively impact performance, especially in scenarios where drafting is expensive and there is a high variance in the number of tokens accepted. Adaptive Entropy-based Draft Length (AdaEDL) is a simple, training and parameter-free criteria which allows for early stopping of the token drafting process by approximating a lower bound on the expected acceptance probability of the drafted token based on the currently observed entropy of the drafted logits. We show that AdaEDL consistently outperforms static draft-length speculative decoding by 10%-57% as well as other training-free draft-stopping techniques by upto 10% in a variety of settings and datasets. At the same time, we show that AdaEDL is more robust than these techniques and preserves performance in high-sampling-temperature scenarios. Since it is training-free, in contrast to techniques that rely on the training of dataset-specific draft-stopping predictors, AdaEDL can seamlessly be integrated into a variety of pre-existing LLM systems.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Workshop on Efficient Natural Language and Signal Processing at NeurIPS 2024"
    },
    {
        "paper id": "2410.18398",
        "abstract url": "https://arxiv.org/abs/2410.18398",
        "title": "You Only Look Around: Learning Illumination Invariant Feature for Low-light Object Detection",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In this paper, we introduce YOLA, a novel framework for object detection in low-light scenarios. Unlike previous works, we propose to tackle this challenging problem from the perspective of feature learning. Specifically, we propose to learn illumination-invariant features through the Lambertian image formation model. We observe that, under the Lambertian assumption, it is feasible to approximate illumination-invariant feature maps by exploiting the interrelationships between neighboring color channels and spatially adjacent pixels. By incorporating additional constraints, these relationships can be characterized in the form of convolutional kernels, which can be trained in a detection-driven manner within a network. Towards this end, we introduce a novel module dedicated to the extraction of illumination-invariant features from low-light images, which can be easily integrated into existing object detection frameworks. Our empirical findings reveal significant improvements in low-light object detection tasks, as well as promising results in both well-lit and over-lit scenarios. Code is available at \\url{https://github.com/MingboHong/YOLA}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by NeurIPS2024"
    },
    {
        "paper id": "2410.17574",
        "abstract url": "https://arxiv.org/abs/2410.17574",
        "title": "Adversarial Domain Adaptation for Metal Cutting Sound Detection: Leveraging Abundant Lab Data for Scarce Industry Data",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Cutting state monitoring in the milling process is crucial for improving manufacturing efficiency and tool life. Cutting sound detection using machine learning (ML) models, inspired by experienced machinists, can be employed as a cost-effective and non-intrusive monitoring method in a complex manufacturing environment. However, labeling industry data for training is costly and time-consuming. Moreover, industry data is often scarce. In this study, we propose a novel adversarial domain adaptation (DA) approach to leverage abundant lab data to learn from scarce industry data, both labeled, for training a cutting-sound detection model. Rather than adapting the features from separate domains directly, we project them first into two separate latent spaces that jointly work as the feature space for learning domain-independent representations. We also analyze two different mechanisms for adversarial learning where the discriminator works as an adversary and a critic in separate settings, enabling our model to learn expressive domain-invariant and domain-ingrained features, respectively. We collected cutting sound data from multiple sensors in different locations, prepared datasets from lab and industry domain, and evaluated our learning models on them. Experiments showed that our models outperformed the multi-layer perceptron based vanilla domain adaptation models in labeling tasks on the curated datasets, achieving near 92%, 82% and 85% accuracy respectively for three different sensors installed in industry settings.",
        "subjects": [
            "cs.LG",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "8 pages, 3 figures, 3 tables, First two named Authors have equal contribution (Co-first author)"
    },
    {
        "paper id": "2410.17578",
        "abstract url": "https://arxiv.org/abs/2410.17578",
        "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are commonly used as evaluators in tasks (e.g., reward modeling, LLM-as-a-judge), where they act as proxies for human preferences or judgments. This leads to the need for meta-evaluation: evaluating the credibility of LLMs as evaluators. However, existing benchmarks primarily focus on English, offering limited insight into LLMs' effectiveness as evaluators in non-English contexts. To address this, we introduce MM-Eval, a multilingual meta-evaluation benchmark that covers 18 languages across six categories. MM-Eval evaluates various dimensions, including language-specific challenges like linguistics and language hallucinations. Evaluation results show that both proprietary and open-source language models have considerable room for improvement. Further analysis reveals a tendency for these models to assign middle-ground scores to low-resource languages. We publicly release our benchmark and code.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "work in progress"
    },
    {
        "paper id": "2410.17592",
        "abstract url": "https://arxiv.org/abs/2410.17592",
        "title": "A Kernel Perspective on Distillation-based Collaborative Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Over the past decade, there is a growing interest in collaborative learning that can enhance AI models of multiple parties. However, it is still challenging to enhance performance them without sharing private data and models from individual parties. One recent promising approach is to develop distillation-based algorithms that exploit unlabeled public data but the results are still unsatisfactory in both theory and practice. To tackle this problem, we rigorously analyze a representative distillation-based algorithm in the view of kernel regression. This work provides the first theoretical results to prove the (nearly) minimax optimality of the nonparametric collaborative learning algorithm that does not directly share local data or models in massively distributed statistically heterogeneous environments. Inspired by our theoretical results, we also propose a practical distillation-based collaborative learning algorithm based on neural network architecture. Our algorithm successfully bridges the gap between our theoretical assumptions and practical settings with neural networks through feature kernel matching. We simulate various regression tasks to verify our theory and demonstrate the practical feasibility of our proposed algorithm.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted to NeurIPS 2024"
    },
    {
        "paper id": "2410.17598",
        "abstract url": "https://arxiv.org/abs/2410.17598",
        "title": "PlantCamo: Plant Camouflage Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Camouflaged Object Detection (COD) aims to detect objects with camouflaged properties. Although previous studies have focused on natural (animals and insects) and unnatural (artistic and synthetic) camouflage detection, plant camouflage has been neglected. However, plant camouflage plays a vital role in natural camouflage. Therefore, this paper introduces a new challenging problem of Plant Camouflage Detection (PCD). To address this problem, we introduce the PlantCamo dataset, which comprises 1,250 images with camouflaged plants representing 58 object categories in various natural scenes. To investigate the current status of plant camouflage detection, we conduct a large-scale benchmark study using 20+ cutting-edge COD models on the proposed dataset. Due to the unique characteristics of plant camouflage, including holes and irregular borders, we developed a new framework, named PCNet, dedicated to PCD. Our PCNet surpasses performance thanks to its multi-scale global feature enhancement and refinement. Finally, we discuss the potential applications and insights, hoping this work fills the gap in fine-grained COD research and facilitates further intelligent ecology research. All resources will be available on https://github.com/yjybuaa/PlantCamo.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17632",
        "abstract url": "https://arxiv.org/abs/2410.17632",
        "title": "LMLPA: Language Model Linguistic Personality Assessment",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) are increasingly used in everyday life and research. One of the most common use cases is conversational interactions, enabled by the language generation capabilities of LLMs. Just as between two humans, a conversation between an LLM-powered entity and a human depends on the personality of the conversants. However, measuring the personality of a given LLM is currently a challenge. This paper introduces the Language Model Linguistic Personality Assessment (LMLPA), a system designed to evaluate the linguistic personalities of LLMs. Our system helps to understand LLMs' language generation capabilities by quantitatively assessing the distinct personality traits reflected in their linguistic outputs. Unlike traditional human-centric psychometrics, the LMLPA adapts a personality assessment questionnaire, specifically the Big Five Inventory, to align with the operational capabilities of LLMs, and also incorporates the findings from previous language-based personality measurement literature. To mitigate sensitivity to the order of options, our questionnaire is designed to be open-ended, resulting in textual answers. Thus, the AI rater is needed to transform ambiguous personality information from text responses into clear numerical indicators of personality traits. Utilising Principal Component Analysis and reliability validations, our findings demonstrate that LLMs possess distinct personality traits that can be effectively quantified by the LMLPA. This research contributes to Human-Computer Interaction and Human-Centered AI, providing a robust framework for future studies to refine AI personality assessments and expand their applications in multiple areas, including education and manufacturing.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17635",
        "abstract url": "https://arxiv.org/abs/2410.17635",
        "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Chain of Thought (CoT) of multi-step benefits from the logical structure of the reasoning steps and task-specific actions, significantly enhancing the mathematical reasoning capabilities of large language models. As the prevalence of long CoT, the number of reasoning steps exceeds manageable token limits and leads to higher computational demands. Inspired by the fundamental logic of human cognition, ``derive, then reduce'', we conceptualize the standard multi-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we consider the mathematical reasoning task, defining each reasoning step as text accompanied by a Python code snippet. To facilitate a longer reasoning path, self-correction is enabled through interactions with the code interpreter. Our MCoT aims to compress previous reasoning steps into a simplified question, enabling efficient next-step inference without relying on a lengthy KV cache. In our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the empirical results indicate that MCoT not only significantly enhances efficiency but also maintains comparable accuracy. While much remains to be explored, this work paves the way for exploring the long CoT reasoning abilities of LLMs.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2410.17670",
        "abstract url": "https://arxiv.org/abs/2410.17670",
        "title": "Quantifying the Risks of Tool-assisted Rephrasing to Linguistic Diversity",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Writing assistants and large language models see widespread use in the creation of text content. While their effectiveness for individual users has been evaluated in the literature, little is known about their proclivity to change language or reduce its richness when adopted by a large user base. In this paper, we take a first step towards quantifying this risk by measuring the semantic and vocabulary change enacted by the use of rephrasing tools on a multi-domain corpus of human-generated text.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17711",
        "abstract url": "https://arxiv.org/abs/2410.17711",
        "title": "Beware of Calibration Data for Pruning Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "As large language models (LLMs) are widely applied across various fields, model compression has become increasingly crucial for reducing costs and improving inference efficiency. Post-training pruning is a promising method that does not require resource-intensive iterative training and only needs a small amount of calibration data to assess the importance of parameters. Previous research has primarily focused on designing advanced pruning methods, while different calibration data's impact on pruning performance still lacks systematical exploration. We fill this blank and surprisingly observe that the effects of calibration data even value more than designing advanced pruning strategies, especially for high sparsity. Our preliminary exploration also discloses that using calibration data similar to the training data can yield better performance. As pre-training data is usually inaccessible for advanced LLMs, we further provide a self-generating calibration data synthesis strategy to construct feasible calibration data. We conduct experiments on the recent strong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that the proposed method outperforms commonly used calibration data and can effectively enhance strong pruning methods (e.g., Wanda, OWL).",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "under review"
    },
    {
        "paper id": "2410.17714",
        "abstract url": "https://arxiv.org/abs/2410.17714",
        "title": "CogSteer: Cognition-Inspired Selective Layer Intervention for Efficient Semantic Steering in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Despite their impressive capabilities, large language models (LLMs) often lack interpretability and can generate toxic content. While using LLMs as foundation models and applying semantic steering methods are widely practiced, we believe that efficient methods should be based on a thorough understanding of LLM behavior. To this end, we propose using eye movement measures to interpret LLM behavior across layers. We find that LLMs exhibit patterns similar to human gaze across layers and different layers function differently. Inspired by these findings, we introduce a heuristic steering layer selection and apply it to layer intervention methods via fine-tuning and inference. Using language toxification and detoxification as test beds, we demonstrate that our proposed CogSteer methods achieve better results in terms of toxicity scores while efficiently saving 97% of the computational resources and 60% of the training time. Our model-agnostic approach can be adopted into various LLMs, contributing to their interpretability and promoting trustworthiness for safe deployment.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17715",
        "abstract url": "https://arxiv.org/abs/2410.17715",
        "title": "Continual Learning on a Data Diet",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Continual Learning (CL) methods usually learn from all available data. However, this is not the case in human cognition which efficiently focuses on key experiences while disregarding the redundant information. Similarly, not all data points in a dataset have equal potential; some can be more informative than others. This disparity may significantly impact the performance, as both the quality and quantity of samples directly influence the model's generalizability and efficiency. Drawing inspiration from this, we explore the potential of learning from important samples and present an empirical study for evaluating coreset selection techniques in the context of CL to stimulate research in this unexplored area. We train different continual learners on increasing amounts of selected samples and investigate the learning-forgetting dynamics by shedding light on the underlying mechanisms driving their improved stability-plasticity balance. We present several significant observations: learning from selectively chosen samples (i) enhances incremental accuracy, (ii) improves knowledge retention of previous tasks, and (iii) refines learned representations. This analysis contributes to a deeper understanding of selective learning strategies in CL scenarios.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": "18 pages, 6 figures"
    },
    {
        "paper id": "2410.17725",
        "abstract url": "https://arxiv.org/abs/2410.17725",
        "title": "YOLOv11: An Overview of the Key Architectural Enhancements",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This study presents an architectural analysis of YOLOv11, the latest iteration in the YOLO (You Only Look Once) series of object detection models. We examine the models architectural innovations, including the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial Pyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) components, which contribute in improving the models performance in several ways such as enhanced feature extraction. The paper explores YOLOv11's expanded capabilities across various computer vision tasks, including object detection, instance segmentation, pose estimation, and oriented object detection (OBB). We review the model's performance improvements in terms of mean Average Precision (mAP) and computational efficiency compared to its predecessors, with a focus on the trade-off between parameter count and accuracy. Additionally, the study discusses YOLOv11's versatility across different model sizes, from nano to extra-large, catering to diverse application needs from edge devices to high-performance computing environments. Our research provides insights into YOLOv11's position within the broader landscape of object detection and its potential impact on real-time computer vision applications.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17728",
        "abstract url": "https://arxiv.org/abs/2410.17728",
        "title": "Dialectal and Low Resource Machine Translation for Aromanian",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We present a neural machine translation system that can translate between Romanian, English, and Aromanian (an endangered Eastern Romance language); the first of its kind. BLEU scores range from 17 to 32 depending on the direction and genre of the text. Alongside, we release the biggest known Aromanian-Romanian bilingual corpus, consisting of 79k cleaned sentence pairs. Additional tools such as an agnostic sentence embedder (used for both text mining and automatic evaluation) and a diacritics converter are also presented. We publicly release our findings and models. Finally, we describe the deployment of our quantized model at https://arotranslate.com.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "16 pages, 3 figures, 6 tables, submitted to COLING 2025"
    },
    {
        "paper id": "2410.17736",
        "abstract url": "https://arxiv.org/abs/2410.17736",
        "title": "MojoBench: Language Modeling and Benchmarks for Mojo",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The recently introduced Mojo programming language (PL) by Modular, has received significant attention in the scientific community due to its claimed significant speed boost over Python. Despite advancements in code Large Language Models (LLMs) across various PLs, Mojo remains unexplored in this context. To address this gap, we introduce MojoBench, the first framework for Mojo code generation. MojoBench includes HumanEval-Mojo, a benchmark dataset designed for evaluating code LLMs on Mojo, and Mojo-Coder, the first LLM pretrained and finetuned for Mojo code generation, which supports instructions in 5 natural languages (NLs). Our results show that Mojo-Coder achieves a 30-35% performance improvement over leading models like GPT-4o and Claude-3.5-Sonnet. Furthermore, we provide insights into LLM behavior with underrepresented and unseen PLs, offering potential strategies for enhancing model adaptability. MojoBench contributes to our understanding of LLM capabilities and limitations in emerging programming paradigms fostering more robust code generation systems.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17744",
        "abstract url": "https://arxiv.org/abs/2410.17744",
        "title": "Learning Versatile Skills with Curriculum Masking",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Masked prediction has emerged as a promising pretraining paradigm in offline reinforcement learning (RL) due to its versatile masking schemes, enabling flexible inference across various downstream tasks with a unified model. Despite the versatility of masked prediction, it remains unclear how to balance the learning of skills at different levels of complexity. To address this, we propose CurrMask, a curriculum masking pretraining paradigm for sequential decision making. Motivated by how humans learn by organizing knowledge in a curriculum, CurrMask adjusts its masking scheme during pretraining for learning versatile skills. Through extensive experiments, we show that CurrMask exhibits superior zero-shot performance on skill prompting tasks, goal-conditioned planning tasks, and competitive finetuning performance on offline RL tasks. Additionally, our analysis of training dynamics reveals that CurrMask gradually acquires skills of varying complexity by dynamically adjusting its masking scheme.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "NeurIPS 2024 poster, 21 pages, 8 figures"
    },
    {
        "paper id": "2410.17759",
        "abstract url": "https://arxiv.org/abs/2410.17759",
        "title": "Latent Structures of Intertextuality in French Fiction",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Intertextuality is a key concept in literary theory that challenges traditional notions of text, signification or authorship. It views texts as part of a vast intertextual network that is constantly evolving and being reconfigured. This paper argues that the field of computational literary studies is the ideal place to conduct a study of intertextuality since we have now the ability to systematically compare texts with each others. Specifically, we present a work on a corpus of more than 12.000 French fictions from the 18th, 19th and early 20th century. We focus on evaluating the underlying roles of two literary notions, sub-genres and the literary canon in the framing of textuality. The article attempts to operationalize intertextuality using state-of-the-art contextual language models to encode novels and capture features that go beyond simple lexical or thematic approaches. Previous research (Hughes, 2012) supports the existence of a literary \"style of a time\", and our findings further reinforce this concept. Our findings also suggest that both subgenres and canonicity play a significant role in shaping textual similarities within French fiction. These discoveries point to the importance of considering genre and canon as dynamic forces that influence the evolution and intertextual connections of literary works within specific historical contexts.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "13 pages, 6 figures. Computational Humanities Research Conference 2024"
    },
    {
        "paper id": "2410.17772",
        "abstract url": "https://arxiv.org/abs/2410.17772",
        "title": "Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models",
        "rating": "1",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "Robot"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "A central challenge towards developing robots that can relate human language to their perception and actions is the scarcity of natural language annotations in diverse robot datasets. Moreover, robot policies that follow natural language instructions are typically trained on either templated language or expensive human-labeled instructions, hindering their scalability. To this end, we introduce NILS: Natural language Instruction Labeling for Scalability. NILS automatically labels uncurated, long-horizon robot data at scale in a zero-shot manner without any human intervention. NILS combines pretrained vision-language foundation models in order to detect objects in a scene, detect object-centric changes, segment tasks from large datasets of unlabelled interaction data and ultimately label behavior datasets. Evaluations on BridgeV2, Fractal, and a kitchen play dataset show that NILS can autonomously annotate diverse robot demonstrations of unlabeled and unstructured datasets while alleviating several shortcomings of crowdsourced human annotations, such as low data quality and diversity. We use NILS to label over 115k trajectories obtained from over 430 hours of robot data. We open-source our auto-labeling code and generated annotations on our website: http://robottasklabeling.github.io.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Project Website at https://robottasklabeling.github.io/"
    },
    {
        "paper id": "2410.17783",
        "abstract url": "https://arxiv.org/abs/2410.17783",
        "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "While ongoing advancements in Large Language Models have demonstrated remarkable success across various NLP tasks, Retrieval Augmented Generation Model stands out to be highly effective on downstream applications like Question Answering. Recently, RAG-end2end model further optimized the architecture and achieved notable performance improvements on domain adaptation. However, the effectiveness of these RAG-based architectures remains relatively unexplored when fine-tuned on specialized domains such as customer service for building a reliable conversational AI system. Furthermore, a critical challenge persists in reducing the occurrence of hallucinations while maintaining high domain-specific accuracy. In this paper, we investigated the performance of diverse RAG and RAG-like architectures through domain adaptation and evaluated their ability to generate accurate and relevant response grounded in the contextual knowledge base. To facilitate the evaluation of the models, we constructed a novel dataset HotelConvQA, sourced from wide range of hotel-related conversations and fine-tuned all the models on our domain specific dataset. We also addressed a critical research gap on determining the impact of domain adaptation on reducing hallucinations across different RAG architectures, an aspect that was not properly measured in prior work. Our evaluation shows positive results in all metrics by employing domain adaptation, demonstrating strong performance on QA tasks and providing insights into their efficacy in reducing hallucinations. Our findings clearly indicate that domain adaptation not only enhances the models' performance on QA tasks but also significantly reduces hallucination across all evaluated RAG architectures.",
        "subjects": [
            "cs.CL",
            "cs.HC"
        ],
        "comment": "Initial Version fine-tuned on HotelConvQA"
    },
    {
        "paper id": "2410.17799",
        "abstract url": "https://arxiv.org/abs/2410.17799",
        "title": "OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Full-duplex spoken dialogue systems significantly advance over traditional turn-based dialogue systems, as they allow simultaneous bidirectional communication, closely mirroring human-human interactions. However, achieving low latency and natural interactions in full-duplex dialogue systems remains a significant challenge, especially considering human conversation dynamics such as interruptions, backchannels, and overlapping speech. In this paper, we introduce a novel End-to-End GPT-based model OmniFlatten for full-duplex conversation, capable of effectively modeling the complex behaviors inherent to natural conversations with low latency. To achieve full-duplex communication capabilities, we propose a multi-stage post-training scheme that progressively adapts a text-based large language model (LLM) backbone into a speech-text dialogue LLM, capable of generating text and speech in real time, without modifying the architecture of the backbone LLM. The training process comprises three stages: modality alignment, half-duplex dialogue learning, and full-duplex dialogue learning. Throughout all training stages, we standardize the data using a flattening operation, which allows us to unify the training methods and the model architecture across different modalities and tasks. Our approach offers a straightforward modeling technique and a promising research direction for developing efficient and natural end-to-end full-duplex spoken dialogue systems. Audio samples of dialogues generated by OmniFlatten can be found at this web site (https://omniflatten.github.io/).",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "Work in progress"
    },
    {
        "paper id": "2410.17810",
        "abstract url": "https://arxiv.org/abs/2410.17810",
        "title": "EntityCLIP: Entity-Centric Image-Text Matching via Multimodal Attentive Contrastive Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in image-text matching have been notable, yet prevailing models predominantly cater to broad queries and struggle with accommodating fine-grained query intention. In this paper, we work towards the \\textbf{E}ntity-centric \\textbf{I}mage-\\textbf{T}ext \\textbf{M}atching (EITM), a task that the text and image involve specific entity-related information. The challenge of this task mainly lies in the larger semantic gap in entity association modeling, comparing with the general image-text matching problem.To narrow the huge semantic gap between the entity-centric text and the images, we take the fundamental CLIP as the backbone and devise a multimodal attentive contrastive learning framework to tam CLIP to adapt EITM problem, developing a model named EntityCLIP. The key of our multimodal attentive contrastive learning is to generate interpretive explanation text using Large Language Models (LLMs) as the bridge clues. In specific, we proceed by extracting explanatory text from off-the-shelf LLMs. This explanation text, coupled with the image and text, is then input into our specially crafted Multimodal Attentive Experts (MMAE) module, which effectively integrates explanation texts to narrow the gap of the entity-related text and image in a shared semantic space. Building on the enriched features derived from MMAE, we further design an effective Gated Integrative Image-text Matching (GI-ITM) strategy. The GI-ITM employs an adaptive gating mechanism to aggregate MMAE's features, subsequently applying image-text matching constraints to steer the alignment between the text and the image. Extensive experiments are conducted on three social media news benchmarks including N24News, VisualNews, and GoodNews, the results shows that our method surpasses the competition methods with a clear margin.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17816",
        "abstract url": "https://arxiv.org/abs/2410.17816",
        "title": "Deep Learning for Active Region Classification: A Systematic Study from Convolutional Neural Networks to Vision Transformers",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "A solar active region can significantly disrupt the Sun Earth space environment, often leading to severe space weather events such as solar flares and coronal mass ejections. As a consequence, the automatic classification of active region groups is the crucial starting point for accurately and promptly predicting solar activity. This study presents our results concerned with the application of deep learning techniques to the classification of active region cutouts based on the Mount Wilson classification scheme. Specifically, we have explored the latest advancements in image classification architectures, from Convolutional Neural Networks to Vision Transformers, and reported on their performances for the active region classification task, showing that the crucial point for their effectiveness consists in a robust training process based on the latest advances in the field.",
        "subjects": [
            "astro-ph.SR",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17820",
        "abstract url": "https://arxiv.org/abs/2410.17820",
        "title": "Understanding When Tree of Thoughts Succeeds: Larger Models Excel in Generation, Not Discrimination",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Tree of Thoughts (ToT) is a reasoning strategy for Large Language Models (LLMs) that employs a generator to suggest reasoning steps and a discriminator to decide which steps to implement. ToT demonstrates strong performance on reasoning tasks, often surpassing simple methods such as Input-Output (IO) prompting and Chain-of-Thought (CoT) reasoning. However, ToT does not consistently outperform such simpler methods across all models, leaving large knowledge gaps on the conditions under which ToT is most beneficial. In this paper, we analyze the roles of the generator and discriminator separately to better understand the conditions when ToT is beneficial. We find that the generator plays a more critical role than the discriminator in driving the success of ToT. Scaling the generator leads to notable improvements in ToT performance, even when using a smaller model as the discriminator, whereas scaling the discriminator with a fixed generator yields only marginal gains. Our results show that models across different scales exhibit comparable discrimination capabilities, yet differ significantly in their generative performance for ToT.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Code: github.com/mainlp/tot-eval"
    },
    {
        "paper id": "2410.17832",
        "abstract url": "https://arxiv.org/abs/2410.17832",
        "title": "Exploiting Text-Image Latent Spaces for the Description of Visual Concepts",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Concept Activation Vectors (CAVs) offer insights into neural network decision-making by linking human friendly concepts to the model's internal feature extraction process. However, when a new set of CAVs is discovered, they must still be translated into a human understandable description. For image-based neural networks, this is typically done by visualizing the most relevant images of a CAV, while the determination of the concept is left to humans. In this work, we introduce an approach to aid the interpretation of newly discovered concept sets by suggesting textual descriptions for each CAV. This is done by mapping the most relevant images representing a CAV into a text-image embedding where a joint description of these relevant images can be computed. We propose utilizing the most relevant receptive fields instead of full images encoded. We demonstrate the capabilities of this approach in multiple experiments with and without given CAV labels, showing that the proposed approach provides accurate descriptions for the CAVs and reduces the challenge of concept interpretation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "19 pages, 7 figures, to be published in ICPR"
    },
    {
        "paper id": "2410.17858",
        "abstract url": "https://arxiv.org/abs/2410.17858",
        "title": "Blendify -- Python rendering framework for Blender",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the rapid growth of the volume of research fields like computer vision and computer graphics, researchers require effective and user-friendly rendering tools to visualize results. While advanced tools like Blender offer powerful capabilities, they also require a significant effort to master. This technical report introduces Blendify, a lightweight Python-based framework that seamlessly integrates with Blender, providing a high-level API for scene creation and rendering. Blendify reduces the complexity of working with Blender's native API by automating object creation, handling the colors and material linking, and implementing features such as shadow-catcher objects while maintaining support for high-quality ray-tracing rendering output. With a focus on usability Blendify enables efficient and flexible rendering workflow for rendering in common computer vision and computer graphics use cases. The code is available at https://github.com/ptrvilya/blendify",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "Project page: https://virtualhumans.mpi-inf.mpg.de/blendify/"
    },
    {
        "paper id": "2410.17875",
        "abstract url": "https://arxiv.org/abs/2410.17875",
        "title": "Understanding Layer Significance in LLM Alignment",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Aligning large language models (LLMs) through fine-tuning is essential for tailoring them to specific applications. Therefore, understanding what LLMs learn during the alignment process is crucial. Recent studies suggest that alignment primarily adjusts a model's presentation style rather than its foundational knowledge, indicating that only certain components of the model are significantly impacted. To delve deeper into LLM alignment, we propose to identify which layers within LLMs are most critical to the alignment process, thereby uncovering how alignment influences model behavior at a granular level. We propose a novel approach to identify the important layers for LLM alignment (ILA). It involves learning a binary mask for each incremental weight matrix in the LoRA algorithm, indicating the significance of each layer. ILA consistently identifies important layers across various alignment datasets, with nearly 90% overlap even with substantial dataset differences, highlighting fundamental patterns in LLM alignment. Experimental results indicate that freezing non-essential layers improves overall model performance, while selectively tuning the most critical layers significantly enhances fine-tuning efficiency with minimal performance loss.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17880",
        "abstract url": "https://arxiv.org/abs/2410.17880",
        "title": "A utility-based spatial analysis of residential street-level conditions; A case study of Rotterdam",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Residential location choices are traditionally modelled using factors related to accessibility and socioeconomic environments, neglecting the importance of local street-level conditions. Arguably, this neglect is due to data practices. Today, however, street-level images -- which are highly effective at encoding street-level conditions -- are widely available. Additionally, recent advances in discrete choice models incorporating computer vision capabilities offer opportunities to integrate street-level conditions into residential location choice analysis. This study leverages these developments to investigate the spatial distribution of utility derived from street-level conditions in residential location choices on a city-wide scale. In our case study of Rotterdam, the Netherlands, we find that the utility derived from street-level conditions varies significantly on a highly localised scale, with conditions rapidly changing even within neighbourhoods. Our results also reveal that the high real-estate prices in the city centre cannot be attributed to attractive street-level conditions. Furthermore, whereas the city centre is characterised by relatively unattractive residential street-level conditions, neighbourhoods in the southern part of the city -- often perceived as problematic -- exhibit surprisingly appealing street-level environments. The methodological contribution of this paper is that it advances the discrete choice models incorporating computer vision capabilities by introducing a semantic regularisation layer to the model. Thereby, it adds explainability and eliminates the need for a separate pipeline to extract information from images, streamlining the analysis. As such, this paper's findings and methodological advancements pave the way for further studies to explore integrating street-level conditions in urban planning.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17885",
        "abstract url": "https://arxiv.org/abs/2410.17885",
        "title": "R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric Reasoning in Large Multimodal Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Existing Large Multimodal Models (LMMs) struggle with mathematical geometric reasoning due to a lack of high-quality image-text paired data. Current geometric data generation approaches, which apply preset templates to generate geometric data or use Large Language Models (LLMs) to rephrase questions and answers (Q&A), unavoidably limit data accuracy and diversity. To synthesize higher-quality data, we propose a two-stage Reverse Chain-of-Thought (R-CoT) geometry problem generation pipeline. First, we introduce GeoChain to produce high-fidelity geometric images and corresponding descriptions highlighting relations among geometric elements. We then design a Reverse A&Q method that reasons step-by-step based on the descriptions and generates questions in reverse from the reasoning results. Experiments demonstrate that the proposed method brings significant and consistent improvements on multiple LMM baselines, achieving new performance records in the 2B, 7B, and 8B settings. Notably, R-CoT-8B significantly outperforms previous state-of-the-art open-source mathematical models by 16.6% on MathVista and 9.2% on GeoQA, while also surpassing the closed-source model GPT-4o by an average of 13% across both datasets. The code is available at https://github.com/dle666/R-CoT.",
        "subjects": [
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17886",
        "abstract url": "https://arxiv.org/abs/2410.17886",
        "title": "SpeakGer: A meta-data enriched speech corpus of German state and federal parliaments",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The application of natural language processing on political texts as well as speeches has become increasingly relevant in political sciences due to the ability to analyze large text corpora which cannot be read by a single person. But such text corpora often lack critical meta information, detailing for instance the party, age or constituency of the speaker, that can be used to provide an analysis tailored to more fine-grained research questions. To enable researchers to answer such questions with quantitative approaches such as natural language processing, we provide the SpeakGer data set, consisting of German parliament debates from all 16 federal states of Germany as well as the German Bundestag from 1947-2023, split into a total of 10,806,105 speeches. This data set includes rich meta data in form of information on both reactions from the audience towards the speech as well as information about the speaker's party, their age, their constituency and their party's political alignment, which enables a deeper analysis. We further provide three exploratory analyses, detailing topic shares of different parties throughout time, a descriptive analysis of the development of the age of an average speaker as well as a sentiment analysis of speeches of different parties with regards to the COVID-19 pandemic.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "10 pages, 3 figures"
    },
    {
        "paper id": "2410.17897",
        "abstract url": "https://arxiv.org/abs/2410.17897",
        "title": "Value Residual Learning For Alleviating Attention Concentration In Transformers",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earlier layers to be directly accessible to later layers. However, this approach is computationally expensive. To address this problem, we propose Transformer with residual value (ResFormer) which approximates cross-layer attention through adding a residual connection from the values of the the first layer to all subsequent layers. Based on this method, one variant is the Transformer with single layer value (SVFormer), where all layers share the same value embedding from first layer, reducing the KV cache by nearly 50%. Comprehensive empirical evidence demonstrates that ResFormer mitigates attention concentration problem in deeper layers and enhances representation across most layers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as downstream tasks. SVFormer trains significantly faster than the vanilla Transformer and performs better than other methods like GQA and CLA, with performance influenced by sequence length and cumulative learning rate.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17928",
        "abstract url": "https://arxiv.org/abs/2410.17928",
        "title": "SJMalloc: the security-conscious, fast, thread-safe and memory-efficient heap allocator",
        "rating": "1",
        "keywords": [
            [
                "memory-efficient"
            ]
        ],
        "abstract": "Heap-based exploits that leverage memory management errors continue to pose a significant threat to application security. The root cause of these vulnerabilities are the memory management errors within the applications, however various hardened allocator designs have been proposed as mitigation. A common feature of these designs is the strategic decision to store heap metadata separately from the application data in use, thereby reducing the risk of metadata corruption leading to security breaches. Despite their potential benefits, hardened allocators have not been widely adopted in real-world applications. The primary barrier to their adoption is the performance overheads they introduce. These overheads can negatively impact the efficiency and speed of applications, which is a critical consideration for developers and system administrators. Having learned from previous implementations, we developed SJMalloc, a general-purpose, high-performance allocator that addresses these concerns. SJMalloc stores its metadata out-of-band, away from the application's data on the heap. This design choice not only enhances security but also improves performance. Across a variety of real-world workloads, SJMalloc demonstrates a ~6% performance improvement compared to GLibcs allocator, while using only ~5% more memory. Furthermore, SJMalloc successfully passes the generic elements of the GLibc malloc testsuite and can thus be used as a drop-in replacement for the standard allocator, offering an easy upgrade path for enhanced security and performance without requiring changes to existing applications.",
        "subjects": [
            "cs.OS",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17952",
        "abstract url": "https://arxiv.org/abs/2410.17952",
        "title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Retrieval-augmented generation (RAG) enhances the question-answering (QA) abilities of large language models (LLMs) by integrating external knowledge. However, adapting general-purpose RAG systems to specialized fields such as science and medicine poses unique challenges due to distribution shifts and limited access to domain-specific data. To tackle this, we propose SimRAG, a self-training approach that equips the LLM with joint capabilities of question answering and question generation for domain adaptation. Our method first fine-tunes the LLM on instruction-following, question-answering, and search-related data. Then, it prompts the same LLM to generate diverse domain-relevant questions from unlabeled corpora, with an additional filtering strategy to retain high-quality synthetic examples. By leveraging these synthetic examples, the LLM can improve their performance on domain-specific RAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three domains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "comment": "Work in Progress"
    },
    {
        "paper id": "2410.17960",
        "abstract url": "https://arxiv.org/abs/2410.17960",
        "title": "Zeitenwenden: Detecting changes in the German political discourse",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "From a monarchy to a democracy, to a dictatorship and back to a democracy -- the German political landscape has been constantly changing ever since the first German national state was formed in 1871. After World War II, the Federal Republic of Germany was formed in 1949. Since then every plenary session of the German Bundestag was logged and even has been digitized over the course of the last few years. We analyze these texts using a time series variant of the topic model LDA to investigate which events had a lasting effect on the political discourse and how the political topics changed over time. This allows us to detect changes in word frequency (and thus key discussion points) in political discourse.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "7 pages, 6 figures"
    },
    {
        "paper id": "2410.17963",
        "abstract url": "https://arxiv.org/abs/2410.17963",
        "title": "A Time-Aware Approach to Early Detection of Anorexia: UNSL at eRisk 2024",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "The eRisk laboratory aims to address issues related to early risk detection on the Web. In this year's edition, three tasks were proposed, where Task 2 was about early detection of signs of anorexia. Early risk detection is a problem where precision and speed are two crucial objectives. Our research group solved Task 2 by defining a CPI+DMC approach, addressing both objectives independently, and a time-aware approach, where precision and speed are considered a combined single-objective. We implemented the last approach by explicitly integrating time during the learning process, considering the ERDE\u03b8 metric as the training objective. It also allowed us to incorporate temporal metrics to validate and select the optimal models. We achieved outstanding results for the ERDE50 metric and ranking-based metrics, demonstrating consistency in solving ERD problems.",
        "subjects": [
            "cs.CY",
            "cs.CL",
            "cs.LG"
        ],
        "comment": "In Conference and Labs of the Evaluation Forum (CLEF 2024), Grenoble, France"
    },
    {
        "paper id": "2410.17983",
        "abstract url": "https://arxiv.org/abs/2410.17983",
        "title": "Robust Two-View Geometry Estimation with Implicit Differentiation",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present a novel two-view geometry estimation framework which is based on a differentiable robust loss function fitting. We propose to treat the robust fundamental matrix estimation as an implicit layer, which allows us to avoid backpropagation through time and significantly improves the numerical stability. To take full advantage of the information from the feature matching stage we incorporate learnable weights that depend on the matching confidences. In this way our solution brings together feature extraction, matching and two-view geometry estimation in a unified end-to-end trainable pipeline. We evaluate our approach on the camera pose estimation task in both outdoor and indoor scenarios. The experiments on several datasets show that the proposed method outperforms both classic and learning-based state-of-the-art methods by a large margin. The project webpage is available at: https://github.com/VladPyatov/ihls",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "IROS 2024 Accepted"
    },
    {
        "paper id": "2410.17997",
        "abstract url": "https://arxiv.org/abs/2410.17997",
        "title": "Characterization of the multiplicity of solutions for camera pose given two vertically-aligned landmarks and accelerometer",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We consider the problem of recovering the position and orientation of a camera equipped with an accelerometer from sensor images of two labeled landmarks whose positions in a coordinate system aligned in a known way with gravity are known. This a variant on the much studied P$n$P problem of recovering camera position and orientation from $n$ points without any gravitational data. It is proved that in three types of singular cases there are infinitely many solutions, in another type of case there is one, and in a final type of case there are two. A precise characterization of each type of case. In particular, there is always a unique solution in the practically interesting case where the two landmarks are at the same altitude and the camera is at a different altitude. This case is studied by numerical simulation and an implementation on a consumer cellphone. It is also proved that if the two landmarks are unlabeled, then apart from the same singular cases, there are still always one or two solutions.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "32 pages, 8 figures"
    },
    {
        "paper id": "2410.18001",
        "abstract url": "https://arxiv.org/abs/2410.18001",
        "title": "Benchmarking Foundation Models on Exceptional Cases: Dataset Creation and Validation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Foundation models (FMs) have achieved significant success across various tasks, leading to research on benchmarks for reasoning abilities. However, there is a lack of studies on FMs performance in exceptional scenarios, which we define as out-of-distribution (OOD) reasoning tasks. This paper is the first to address these cases, developing a novel dataset for evaluation of FMs across multiple modalities, including graphic novels, calligraphy, news articles, and lyrics. It includes tasks for instance classification, character recognition, token prediction, and text generation. The paper also proposes prompt engineering techniques like Chain-of-Thought (CoT) and CoT+Few-Shot to enhance performance. Validation of FMs using various methods revealed improvements. The code repository is accessible at: https://github.com/MLAI-Yonsei/ExceptionalBenchmark",
        "subjects": [
            "cs.AI"
        ],
        "comment": "EMNLP 2024 Workshop Genbench(https://genbench.org/workshop_programme/)"
    },
    {
        "paper id": "2410.18027",
        "abstract url": "https://arxiv.org/abs/2410.18027",
        "title": "Cross-lingual Transfer of Reward Models in Multilingual Alignment",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Reinforcement learning with human feedback (RLHF) is shown to largely benefit from precise reward models (RMs). However, recent studies in reward modeling schemes are skewed towards English, limiting the applicability of RLHF in multilingual alignments. In this work, we investigate the cross-lingual transfer of RMs trained in diverse languages, primarily from English. Our experimental results demonstrate the strong cross-lingual transfer of English RMs, exceeding target language RMs by 3~4% average increase in Multilingual RewardBench. Furthermore, we analyze the cross-lingual transfer of RMs through the representation shifts. Finally, we perform multilingual alignment to exemplify how cross-lingual transfer in RM propagates to enhanced multilingual instruction-following capability, along with extensive analyses on off-the-shelf RMs. We release the code, model, and data.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18040",
        "abstract url": "https://arxiv.org/abs/2410.18040",
        "title": "Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for Russian Scientific Keyphrases",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Keyphrase selection is a challenging task in natural language processing that has a wide range of applications. Adapting existing supervised and unsupervised solutions for the Russian language faces several limitations due to the rich morphology of Russian and the limited number of training datasets available. Recent studies conducted on English texts show that large language models (LLMs) successfully address the task of generating keyphrases. LLMs allow achieving impressive results without task-specific fine-tuning, using text prompts instead. In this work, we access the performance of prompt-based methods for generating keyphrases for Russian scientific abstracts. First, we compare the performance of zero-shot and few-shot prompt-based methods, fine-tuned models, and unsupervised methods. Then we assess strategies for selecting keyphrase examples in a few-shot setting. We present the outcomes of human evaluation of the generated keyphrases and analyze the strengths and weaknesses of the models through expert assessment. Our results suggest that prompt-based methods can outperform common baselines even using simple text prompts.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "The 12th International Conference on Analysis of Images, Social Networks and Texts (AIST'2024)"
    },
    {
        "paper id": "2410.18051",
        "abstract url": "https://arxiv.org/abs/2410.18051",
        "title": "Real time anomalies detection on video",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Nowadays, many places use security cameras. Unfortunately, when an incident occurs, these technologies are used to show past events. So it can be considered as a deterrence tool than a detection tool. In this article, we will propose a deep learning approach trying to solve this problematic. This approach uses convolutional models (CNN) to extract relevant characteristics linked to the video images, theses characteristics will form times series to be analyzed by LSTM / GRU models.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18052",
        "abstract url": "https://arxiv.org/abs/2410.18052",
        "title": "In-Pixel Foreground and Contrast Enhancement Circuits with Customizable Mapping",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper presents an innovative in-pixel contrast enhancement circuit that performs image processing directly within the pixel circuit. The circuit can be tuned for different modes of operation. In foreground enhancement mode, it suppresses low-intensity background pixels to nearly zero, isolating the foreground for better object visibility. In contrast enhancement mode, it improves overall image contrast. The contrast enhancement function is customizable both during the design phase and in real-time, allowing the circuit to adapt to specific applications and varying lighting conditions. A model of the designed pixel circuit is developed and applied to a full pixel array, demonstrating significant improvements in image quality. Simulations performed in HSPICE show a nearly 6x increase in Michelson Contrast Ratio (CR) in the foreground enhancement mode. The simulation results indicate its potential for real-time, adaptive contrast enhancement across various imaging environments.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18071",
        "abstract url": "https://arxiv.org/abs/2410.18071",
        "title": "TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Recently, multimodal large language models (MLLMs) have received much attention for their impressive capabilities. The evaluation of MLLMs is becoming critical to analyzing attributes of MLLMs and providing valuable insights. However, current benchmarks overlook the problem of prompt sensitivity - minor prompt variations may lead to significant performance fluctuations. Thus, inappropriate prompts may obscure the models' capabilities, underestimating the models' performance. Moreover, different models have different preferences for different prompts, and thus, using the same prompt for all models will cause evaluation bias. This paper analyzes this deficiency in existing benchmarks and further introduces a new evaluation framework named TP-Eval, which introduces a prompt customization method to reduce evaluation biases and tap models' potential. TP-Eval will rewrite the original prompts to different customized prompts for different models. In particular, we propose some well-designed modules for prompt customization tailored to the scenario of MLLM evaluation. Extensive experiments demonstrate the effectiveness of our approach to uncovering models' capabilities, and TP-Eval should benefit the community in developing more comprehensive and convincing MLLM evaluation benchmarks.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18077",
        "abstract url": "https://arxiv.org/abs/2410.18077",
        "title": "ALTA: Compiler-Based Analysis of Transformers",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We propose a new programming language called ALTA and a compiler that can map ALTA programs to Transformer weights. ALTA is inspired by RASP, a language proposed by Weiss et al. (2021), and Tracr (Lindner et al., 2023), a compiler from RASP programs to Transformer weights. ALTA complements and extends this prior work, offering the ability to express loops and to compile programs to Universal Transformers, among other advantages. ALTA allows us to constructively show how Transformers can represent length-invariant algorithms for computing parity and addition, as well as a solution to the SCAN benchmark of compositional generalization tasks, without requiring intermediate scratchpad decoding steps. We also propose tools to analyze cases where the expressibility of an algorithm is established, but end-to-end training on a given training set fails to induce behavior consistent with the desired algorithm. To this end, we explore training from ALTA execution traces as a more fine-grained supervision signal. This enables additional experiments and theoretical analyses relating the learnability of various algorithms to data availability and modeling decisions, such as positional encodings. We make the ALTA framework -- language specification, symbolic interpreter, and weight compiler -- available to the community to enable further applications and insights.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18156",
        "abstract url": "https://arxiv.org/abs/2410.18156",
        "title": "Dreaming Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Incorporating novelties into deep learning systems remains a challenging problem. Introducing new information to a machine learning system can interfere with previously stored data and potentially alter the global model paradigm, especially when dealing with non-stationary sources. In such cases, traditional approaches based on validation error minimization offer limited advantages. To address this, we propose a training algorithm inspired by Stuart Kauffman's notion of the Adjacent Possible. This novel training methodology explores new data spaces during the learning phase. It predisposes the neural network to smoothly accept and integrate data sequences with different statistical characteristics than expected. The maximum distance compatible with such inclusion depends on a specific parameter: the sampling temperature used in the explorative phase of the present method. This algorithm, called Dreaming Learning, anticipates potential regime shifts over time, enhancing the neural network's responsiveness to non-stationary events that alter statistical properties. To assess the advantages of this approach, we apply this methodology to unexpected statistical changes in Markov chains and non-stationary dynamics in textual sequences. We demonstrated its ability to improve the auto-correlation of generated textual sequences by $\\sim 29\\%$ and enhance the velocity of loss convergence by $\\sim 100\\%$ in the case of a paradigm shift in Markov chains.",
        "subjects": [
            "cs.LG",
            "physics.data-an"
        ],
        "comment": "Accepted at the NeurIPS 2024 workshop on Intrinsically Motivated Open-ended Learning"
    },
    {
        "paper id": "2410.18160",
        "abstract url": "https://arxiv.org/abs/2410.18160",
        "title": "Future Token Prediction -- Causal Language Modelling with Per-Token Semantic State Vector for Multi-Token Prediction",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Causal decoder-only transformer models used for generative language modelling, such as Generative Pre-trained Transformers (GPT), are trained to predict the next token in a sequence based only on its previous tokens. Despite this simple training objective, they have proved to be powerful AI tools. However, only predicting the next token results in top layer embedding vectors that are highly token-focused. There may be benefits in generating embedding vectors at each token position that better capture the overall meaning of longer sequences of future text. Recent studies matching brain scans with deep language models suggest that humans also predict upcoming words when listening or reading but consider multiple future tokens rather than just one. This research investigates a new pretraining method called Future Token Prediction (FTP). In FTP, a large transformer encoder generates top layer embedding vectors for each token position, which, instead of being passed to a language head, are linearly and expansively projected to a pseudo-sequence, which is cross attended to by a small transformer decoder to predict the next N tokens forward from that position in the sequence. The top layer embedding vectors from FTP models exhibit distinct properties compared to those from standard GPT models, varying smoothly along a text sequence as measured by cosine similarity between adjacent tokens. Text generated by FTP models show improved topic coherence compared to standard GPT-like models trained with the same prediction perplexity for the next single token. The vectors are shown to better represent the topic of text based on the results of text classification examples. On a toy, but complex, coding problem, FTP networks produce significantly better results than GPT networks.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "15 pages, 7 figures, 3 tables"
    },
    {
        "paper id": "2410.18194",
        "abstract url": "https://arxiv.org/abs/2410.18194",
        "title": "ZIP-FIT: Embedding-Free Data Selection via Compression-Based Alignment",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Data selection is crucial for optimizing language model (LM) performance on specific tasks, yet most existing methods fail to effectively consider the target task distribution. Current approaches either ignore task-specific requirements entirely or rely on approximations that fail to capture the nuanced patterns needed for tasks like Autoformalization or code generation. Methods that do consider the target distribution often rely on simplistic, sometimes noisy, representations, like hashed n-gram features, which can lead to collisions and introduce noise. We introduce ZIP-FIT, a data selection framework that uses gzip compression to directly measure alignment between potential training data and the target task distribution. In extensive evaluations on Autoformalization and Python code generation, ZIP-FIT significantly outperforms leading baselines like DSIR and D4. Models trained on ZIP-FIT-selected data achieve their lowest cross-entropy loss up to 85.1\\% faster than baselines, demonstrating that better task alignment leads to more efficient learning. In addition, ZIP-FIT performs selection up to 65.8\\% faster than DSIR and two orders of magnitude faster than D4. Notably, ZIP-FIT shows that smaller, well-aligned datasets often outperform larger but less targeted ones, demonstrating that a small amount of higher quality data is superior to a large amount of lower quality data. Our results imply that task-aware data selection is crucial for efficient domain adaptation, and that compression offers a principled way to measure task alignment. By showing that targeted data selection can dramatically improve task-specific performance, our work provides new insights into the relationship between data quality, task alignment, and model learning efficiency.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18200",
        "abstract url": "https://arxiv.org/abs/2410.18200",
        "title": "Rethinking Positive Pairs in Contrastive Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Contrastive learning, a prominent approach to representation learning, traditionally assumes positive pairs are closely related samples (the same image or class) and negative pairs are distinct samples. We challenge this assumption by proposing to learn from arbitrary pairs, allowing any pair of samples to be positive within our framework.The primary challenge of the proposed approach lies in applying contrastive learning to disparate pairs which are semantically distant. Motivated by the discovery that SimCLR can separate given arbitrary pairs (e.g., garter snake and table lamp) in a subspace, we propose a feature filter in the condition of class pairs that creates the requisite subspaces by gate vectors selectively activating or deactivating dimensions. This filter can be optimized through gradient descent within a conventional contrastive learning mechanism. We present Hydra, a universal contrastive learning framework for visual representations that extends conventional contrastive learning to accommodate arbitrary pairs. Our approach is validated using IN1K, where 1K diverse classes compose 500,500 pairs, most of them being distinct. Surprisingly, Hydra achieves superior performance in this challenging setting. Additional benefits include the prevention of dimensional collapse and the discovery of class relationships. Our work highlights the value of learning common features of arbitrary pairs and potentially broadens the applicability of contrastive learning techniques on the sample pairs with weak relationships.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18209",
        "abstract url": "https://arxiv.org/abs/2410.18209",
        "title": "CorrectionLM: Self-Corrections with SLM for Dialogue State Tracking",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have demonstrated self-improvement capabilities via feedback and refinement, but current small language models (SLMs) have had limited success in this area. Existing correction approaches often rely on distilling knowledge from LLMs, which imposes significant computation demands. In this work, we introduce CORRECTIONLM, a novel correction framework that enables SLMs to self-correct using in-context exemplars without LLM involvement. Applied to two dialogue state tracking (DST) tasks in low-resource settings, CORRECTIONLM achieves results similar to a state-of-the-art LLM at a small fraction of the computation costs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18218",
        "abstract url": "https://arxiv.org/abs/2410.18218",
        "title": "Optimizing the role of human evaluation in LLM-based spoken document summarization systems",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "The emergence of powerful LLMs has led to a paradigm shift in abstractive summarization of spoken documents. The properties that make LLMs so valuable for this task -- creativity, ability to produce fluent speech, and ability to abstract information from large corpora -- also present new challenges to evaluating their content. Quick, cost-effective automatic evaluations such as ROUGE and BERTScore offer promise, but do not yet show competitive performance when compared to human evaluations. We draw on methodologies from the social sciences to propose an evaluation paradigm for spoken document summarization explicitly tailored for generative AI content. We provide detailed evaluation criteria and best practices guidelines to ensure robustness in the experimental design, replicability, and trustworthiness of human evaluation studies. We additionally include two case studies that show how these human-in-the-loop evaluation methods have been implemented at a major U.S. technology company.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18234",
        "abstract url": "https://arxiv.org/abs/2410.18234",
        "title": "Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model. Previous works have demonstrated that the optimal scheme (which maximizes the probability of accepting one of the input tokens) can be cast as a solution to a linear program. In this work we show that the optimal scheme can be decomposed into a two-step solution: in the first step an importance sampling (IS) type scheme is used to select one intermediate token; in the second step (single-draft) speculative sampling is applied to generate the output token. For the case of two identical draft models we further 1) establish a necessary and sufficient condition on the distributions of the target and draft models for the acceptance probability to equal one and 2) provide an explicit expression for the optimal acceptance probability. Our theoretical analysis also motives a new class of token-level selection scheme based on weighted importance sampling. Our experimental results demonstrate consistent improvements in the achievable block efficiency and token rates over baseline schemes in a number of scenarios.",
        "subjects": [
            "cs.CL",
            "cs.DC",
            "cs.IT",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18252",
        "abstract url": "https://arxiv.org/abs/2410.18252",
        "title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this paradigm is computationally inefficient. Inspired by classical deep RL literature, we propose separating generation and learning in RLHF. This enables asynchronous generation of new samples while simultaneously training on old samples, leading to faster training and more compute-optimal scaling. However, asynchronous training relies on an underexplored regime, online but off-policy RLHF: learning on samples from previous iterations of our model. To understand the challenges in this regime, we investigate a fundamental question: how much off-policyness can we tolerate for asynchronous training to speed up learning but maintain performance? Among several RLHF algorithms we tested, we find that online DPO is most robust to off-policy data, and robustness increases with the scale of the policy model. We study further compute optimizations for asynchronous RLHF but find that they come at a performance cost, giving rise to a trade-off. Finally, we verify the scalability of asynchronous RLHF by training LLaMA 3.1 8B on an instruction-following task 40% faster than a synchronous run while matching final performance.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "code at https://github.com/mnoukhov/async_rlhf"
    },
    {
        "paper id": "2410.18260",
        "abstract url": "https://arxiv.org/abs/2410.18260",
        "title": "Predicting total time to compress a video corpus using online inference systems",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Predicting the computational cost of compressing/transcoding clips in a video corpus is important for resource management of cloud services and VOD (Video On Demand) providers. Currently, customers of cloud video services are unaware of the cost of transcoding their files until the task is completed. Previous work concentrated on predicting perclip compression time, and thus estimating the cost of video compression. In this work, we propose new Machine Learning (ML) systems which predict cost for the entire corpus instead. This is a more appropriate goal since users are not interested in per-clip cost but instead the cost for the whole corpus. In this work, we evaluate our systems with respect to two video codecs (x264, x265) and a novel high-quality video corpus. We find that the accuracy of aggregate time prediction for a video corpus more than two times better than using per-clip predictions. Furthermore, we present an online inference framework in which we update the ML models as files are processed. A consideration of video compute overhead and appropriate choice of ML predictor for each fraction of corpus completed yields a prediction error of less than 5%. This is approximately two times better than previous work which proposed generalised predictors.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "Accepted by IEEE International Conference on Visual Communications and Image Processing (VCIP) 2024"
    },
    {
        "paper id": "2410.18277",
        "abstract url": "https://arxiv.org/abs/2410.18277",
        "title": "KhmerST: A Low-Resource Khmer Scene Text Detection and Recognition Benchmark",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Developing effective scene text detection and recognition models hinges on extensive training data, which can be both laborious and costly to obtain, especially for low-resourced languages. Conventional methods tailored for Latin characters often falter with non-Latin scripts due to challenges like character stacking, diacritics, and variable character widths without clear word boundaries. In this paper, we introduce the first Khmer scene-text dataset, featuring 1,544 expert-annotated images, including 997 indoor and 547 outdoor scenes. This diverse dataset includes flat text, raised text, poorly illuminated text, distant and partially obscured text. Annotations provide line-level text and polygonal bounding box coordinates for each scene. The benchmark includes baseline models for scene-text detection and recognition tasks, providing a robust starting point for future research endeavors. The KhmerST dataset is publicly accessible at https://gitlab.com/vannkinhnom123/khmerst.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at ACCV 2024"
    },
    {
        "paper id": "2410.18311",
        "abstract url": "https://arxiv.org/abs/2410.18311",
        "title": "CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) with billions of parameters have sparked a new wave of exciting AI applications. However, their high computational costs and memory demands during inference pose significant challenges. Adaptive sparse activation inference, which activates only a small number of neurons for each token, offers a novel way to accelerate model inference without degrading performance, showing great potential for resource-constrained hardware devices. Nevertheless, existing methods predict activated neurons based on individual tokens with additional MLP, which involve frequent changes in activation maps and resource calls, limiting the acceleration benefits of sparse activation. In this paper, we introduce CoreInfer, an MLP-free adaptive sparse activation inference method based on sentence-level prediction. Specifically, we propose the concept of sentence-wise core neurons, which refers to the subset of neurons most critical for a given sentence, and empirically demonstrate its effectiveness. To determine the core neurons, we explore the correlation between core neurons and the sentence's semantics. Remarkably, we discovered that core neurons exhibit both stability and similarity in relation to the sentence's semantics -- an insight overlooked by previous studies. Building on this finding, we further design two semantic-based methods for predicting core neurons to fit different input scenarios. In CoreInfer, the core neurons are determined during the pre-filling stage and fixed during the encoding stage, enabling zero-cost sparse inference. We evaluated the model generalization and task generalization of CoreInfer across various models and tasks. Notably, on an NVIDIA TITAN XP GPU, CoreInfer achieved a 10.33 times and 2.72 times speedup compared to the Huggingface implementation and PowerInfer, respectively.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": "Project page: https://wangqinsi1.github.io/coreinfer_page/"
    },
    {
        "paper id": "2410.18326",
        "abstract url": "https://arxiv.org/abs/2410.18326",
        "title": "Measuring individual semantic networks: A simulation study",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Accurately capturing individual differences in semantic networks is fundamental to advancing our mechanistic understanding of semantic memory. Past empirical attempts to construct individual-level semantic networks from behavioral paradigms may be limited by data constraints. To assess these limitations and propose improved designs for the measurement of individual semantic networks, we conducted a recovery simulation investigating the psychometric properties underlying estimates of individual semantic networks obtained from two different behavioral paradigms: free associations and relatedness judgment tasks. Our results show that successful inference of semantic networks is achievable, but they also highlight critical challenges. Estimates of absolute network characteristics are severely biased, such that comparisons between behavioral paradigms and different design configurations are often not meaningful. However, comparisons within a given paradigm and design configuration can be accurate and generalizable when based on designs with moderate numbers of cues, moderate numbers of responses, and cue sets including diverse words. Ultimately, our results provide insights that help evaluate past findings on the structure of semantic networks and design new studies capable of more reliably revealing individual differences in semantic networks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18336",
        "abstract url": "https://arxiv.org/abs/2410.18336",
        "title": "Assessing the Creativity of LLMs in Proposing Novel Solutions to Mathematical Problems",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The mathematical capabilities of AI systems are complex and multifaceted. Most existing research has predominantly focused on the correctness of AI-generated solutions to mathematical problems. In this work, we argue that beyond producing correct answers, AI systems should also be capable of, or assist humans in, developing novel solutions to mathematical challenges. This study explores the creative potential of Large Language Models (LLMs) in mathematical reasoning, an aspect that has received limited attention in prior research. We introduce a novel framework and benchmark, CreativeMath, which encompasses problems ranging from middle school curricula to Olympic-level competitions, designed to assess LLMs' ability to propose innovative solutions after some known solutions have been provided. Our experiments demonstrate that, while LLMs perform well on standard mathematical tasks, their capacity for creative problem-solving varies considerably. Notably, the Gemini-1.5-Pro model outperformed other LLMs in generating novel solutions. This research opens a new frontier in evaluating AI creativity, shedding light on both the strengths and limitations of LLMs in fostering mathematical innovation, and setting the stage for future developments in AI-assisted mathematical discovery.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18344",
        "abstract url": "https://arxiv.org/abs/2410.18344",
        "title": "Aggregated Knowledge Model: Enhancing Domain-Specific QA with Fine-Tuned and Retrieval-Augmented Generation Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces a novel approach to enhancing closed-domain Question Answering (QA) systems, focusing on the specific needs of the Lawrence Berkeley National Laboratory (LBL) Science Information Technology (ScienceIT) domain. Utilizing a rich dataset derived from the ScienceIT documentation, our study embarks on a detailed comparison of two fine-tuned large language models and five retrieval-augmented generation (RAG) models. Through data processing techniques, we transform the documentation into structured context-question-answer triples, leveraging the latest Large Language Models (AWS Bedrock, GCP PaLM2, Meta LLaMA2, OpenAI GPT-4, Google Gemini-Pro) for data-driven insights. Additionally, we introduce the Aggregated Knowledge Model (AKM), which synthesizes responses from the seven models mentioned above using K-means clustering to select the most representative answers. The evaluation of these models across multiple metrics offers a comprehensive look into their effectiveness and suitability for the LBL ScienceIT environment. The results demonstrate the potential benefits of integrating fine-tuning and retrieval-augmented strategies, highlighting significant performance improvements achieved with the AKM. The insights gained from this study can be applied to develop specialized QA systems tailored to specific domains.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18359",
        "abstract url": "https://arxiv.org/abs/2410.18359",
        "title": "Improving Model Factuality with Fine-grained Critique-based Evaluator",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Factuality evaluation aims to detect factual errors produced by language models (LMs) and hence guide the development of more factual models. Towards this goal, we train a factuality evaluator, FenCE, that provides LM generators with claim-level factuality feedback. We conduct data augmentation on a combination of public judgment datasets to train FenCE to (1) generate textual critiques along with scores and (2) make claim-level judgment based on diverse source documents obtained by various tools. We then present a framework that leverages FenCE to improve the factuality of LM generators by constructing training data. Specifically, we generate a set of candidate responses, leverage FenCE to revise and score each response without introducing lesser-known facts, and train the generator by preferring highly scored revised responses. Experiments show that our data augmentation methods improve the evaluator's accuracy by 2.9% on LLM-AggreFact. With FenCE, we improve Llama3-8B-chat's factuality rate by 14.45% on FActScore, outperforming state-of-the-art factuality finetuning methods by 6.96%.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18362",
        "abstract url": "https://arxiv.org/abs/2410.18362",
        "title": "WAFFLE: Multi-Modal Model for Automated Front-End Development",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Web development involves turning UI designs into functional webpages, which can be difficult for both beginners and experienced developers due to the complexity of HTML's hierarchical structures and styles. While Large Language Models (LLMs) have shown promise in generating source code, two major challenges persist in UI-to-HTML code generation: (1) effectively representing HTML's hierarchical structure for LLMs, and (2) bridging the gap between the visual nature of UI designs and the text-based format of HTML code. To tackle these challenges, we introduce Waffle, a new fine-tuning strategy that uses a structure-aware attention mechanism to improve LLMs' understanding of HTML's structure and a contrastive fine-tuning approach to align LLMs' understanding of UI images and HTML code. Models fine-tuned with Waffle show up to 9.00 pp (percentage point) higher HTML match, 0.0982 higher CW-SSIM, 32.99 higher CLIP, and 27.12 pp higher LLEM on our new benchmark WebSight-Test and an existing benchmark Design2Code, outperforming current fine-tuning methods.",
        "subjects": [
            "cs.SE",
            "cs.CL",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18363",
        "abstract url": "https://arxiv.org/abs/2410.18363",
        "title": "Contextual Biasing to Improve Domain-specific Custom Vocabulary Audio Transcription without Explicit Fine-Tuning of Whisper Model",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "OpenAI's Whisper Automated Speech Recognition model excels in generalizing across diverse datasets and domains. However, this broad adaptability can lead to diminished performance in tasks requiring recognition of specific vocabularies. Addressing this challenge typically involves fine-tuning the model, which demands extensive labeled audio data that is often difficult to acquire and unavailable for specific domains. In this study, we propose a method to enhance transcription accuracy without explicit fine-tuning or altering model parameters, using a relatively small training dataset. Our method leverages contextual biasing, to direct Whisper model's output towards a specific vocabulary by integrating a neural-symbolic prefix tree structure to guide the model's transcription output. To validate our approach, we conducted experiments using a validation dataset comprising maritime data collected within a simulated training environment. A comparison between the original Whisper models of varying parameter sizes and our biased model revealed a notable reduction in transcription word error rate and enhanced performance of downstream applications. Our findings suggest that this methodology holds promise for improving speech-to-text translation performance in domains characterized by limited vocabularies.",
        "subjects": [
            "cs.AI",
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18364",
        "abstract url": "https://arxiv.org/abs/2410.18364",
        "title": "Position-Aided Semantic Communication for Efficient Image Transmission: Design, Implementation, and Experimental Results",
        "rating": "1",
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "Semantic communication, augmented by knowledge bases (KBs), offers substantial reductions in transmission overhead and resilience to errors. However, existing methods predominantly rely on end-to-end training to construct KBs, often failing to fully capitalize on the rich information available at communication devices. Motivated by the growing convergence of sensing and communication, we introduce a novel Position-Aided Semantic Communication (PASC) framework, which integrates localization into semantic transmission. This framework is particularly designed for position-based image communication, such as real-time uploading of outdoor camera-view images. By utilizing the position, the framework retrieves corresponding maps, and then an advanced foundation model (FM)-driven view generator is employed to synthesize images closely resembling the target images. The PASC framework further leverages the FM to fuse the synthesized image with deviations from the real one, enhancing semantic reconstruction. Notably, the framework is highly flexible, capable of adapting to dynamic content and fluctuating channel conditions through a novel FM-based parameter optimization strategy. Additionally, the challenges of real-time deployment are addressed, with the development of a hardware testbed to validate the framework. Simulations and real-world tests demonstrate that the proposed PASC approach not only significantly boosts transmission efficiency, but also remains robust in diverse and evolving transmission scenarios.",
        "subjects": [
            "eess.IV",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18371",
        "abstract url": "https://arxiv.org/abs/2410.18371",
        "title": "Gibberish is All You Need for Membership Inference Detection in Contrastive Language-Audio Pretraining",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Audio can disclose PII, particularly when combined with related text data. Therefore, it is essential to develop tools to detect privacy leakage in Contrastive Language-Audio Pretraining(CLAP). Existing MIAs need audio as input, risking exposure of voiceprint and requiring costly shadow models. We first propose PRMID, a membership inference detector based probability ranking given by CLAP, which does not require training shadow models but still requires both audio and text of the individual as input. To address these limitations, we then propose USMID, a textual unimodal speaker-level membership inference detector, querying the target model using only text data. We randomly generate textual gibberish that are clearly not in training dataset. Then we extract feature vectors from these texts using the CLAP model and train a set of anomaly detectors on them. During inference, the feature vector of each test text is input into the anomaly detector to determine if the speaker is in the training set (anomalous) or not (normal). If available, USMID can further enhance detection by integrating real audio of the tested speaker. Extensive experiments on various CLAP model architectures and datasets demonstrate that USMID outperforms baseline methods using only text data.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18390",
        "abstract url": "https://arxiv.org/abs/2410.18390",
        "title": "Monolingual and Multilingual Misinformation Detection for Low-Resource Languages: A Comprehensive Survey",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In today's global digital landscape, misinformation transcends linguistic boundaries, posing a significant challenge for moderation systems. While significant advances have been made in misinformation detection, the focus remains largely on monolingual high-resource contexts, with low-resource languages often overlooked. This survey aims to bridge that gap by providing a comprehensive overview of the current research on low-resource language misinformation detection in both monolingual and multilingual settings. We review the existing datasets, methodologies, and tools used in these domains, identifying key challenges related to: data resources, model development, cultural and linguistic context, real-world applications, and research efforts. We also examine emerging approaches, such as language-agnostic models and multi-modal techniques, while emphasizing the need for improved data collection practices, interdisciplinary collaboration, and stronger incentives for socially responsible AI research. Our findings underscore the need for robust, inclusive systems capable of addressing misinformation across diverse linguistic and cultural contexts.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18391",
        "abstract url": "https://arxiv.org/abs/2410.18391",
        "title": "Faster Algorithms for User-Level Private Stochastic Convex Optimization",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We study private stochastic convex optimization (SCO) under user-level differential privacy (DP) constraints. In this setting, there are $n$ users (e.g., cell phones), each possessing $m$ data items (e.g., text messages), and we need to protect the privacy of each user's entire collection of data items. Existing algorithms for user-level DP SCO are impractical in many large-scale machine learning scenarios because: (i) they make restrictive assumptions on the smoothness parameter of the loss function and require the number of users to grow polynomially with the dimension of the parameter space; or (ii) they are prohibitively slow, requiring at least $(mn)^{3/2}$ gradient computations for smooth losses and $(mn)^3$ computations for non-smooth losses. To address these limitations, we provide novel user-level DP algorithms with state-of-the-art excess risk and runtime guarantees, without stringent assumptions. First, we develop a linear-time algorithm with state-of-the-art excess risk (for a non-trivial linear-time algorithm) under a mild smoothness assumption. Our second algorithm applies to arbitrary smooth losses and achieves optimal excess risk in $\\approx (mn)^{9/8}$ gradient computations. Third, for non-smooth loss functions, we obtain optimal excess risk in $n^{11/8} m^{5/4}$ gradient computations. Moreover, our algorithms do not require the number of users to grow polynomially with the dimension.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "math.OC"
        ],
        "comment": "NeurIPS 2024"
    },
    {
        "paper id": "2410.18399",
        "abstract url": "https://arxiv.org/abs/2410.18399",
        "title": "CloudEye: A New Paradigm of Video Analysis System for Mobile Visual Scenarios",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Mobile deep vision systems play a vital role in numerous scenarios. However, deep learning applications in mobile vision scenarios face problems such as tight computing resources. With the development of edge computing, the architecture of edge clouds has mitigated some of the issues related to limited computing resources. However, it has introduced increased latency. To address these challenges, we designed CloudEye which consists of Fast Inference Module, Feature Mining Module and Quality Encode Module. CloudEye is a real-time, efficient mobile visual perception system that leverages content information mining on edge servers in a mobile vision system environment equipped with edge servers and coordinated with cloud servers. Proven by sufficient experiments, we develop a prototype system that reduces network bandwidth usage by 69.50%, increases inference speed by 24.55%, and improves detection accuracy by 67.30%",
        "subjects": [
            "cs.CV",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18417",
        "abstract url": "https://arxiv.org/abs/2410.18417",
        "title": "Large Language Models Reflect the Ideology of their Creators",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are trained on vast amounts of data to generate natural language, enabling them to perform tasks like text summarization and question answering. These models have become popular in artificial intelligence (AI) assistants like ChatGPT and already play an influential role in how humans access information. However, the behavior of LLMs varies depending on their design, training, and use. In this paper, we uncover notable diversity in the ideological stance exhibited across different LLMs and languages in which they are accessed. We do this by prompting a diverse panel of popular LLMs to describe a large number of prominent and controversial personalities from recent world history, both in English and in Chinese. By identifying and analyzing moral assessments reflected in the generated descriptions, we find consistent normative differences between how the same LLM responds in Chinese compared to English. Similarly, we identify normative disagreements between Western and non-Western LLMs about prominent actors in geopolitical conflicts. Furthermore, popularly hypothesized disparities in political goals among Western models are reflected in significant normative differences related to inclusion, social inequality, and political scandals. Our results show that the ideological stance of an LLM often reflects the worldview of its creators. This raises important concerns around technological and regulatory efforts with the stated aim of making LLMs ideologically `unbiased', and it poses risks for political instrumentalization.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18430",
        "abstract url": "https://arxiv.org/abs/2410.18430",
        "title": "Building Dialogue Understanding Models for Low-resource Language Indonesian from Scratch",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Making use of off-the-shelf resources of resource-rich languages to transfer knowledge for low-resource languages raises much attention recently. The requirements of enabling the model to reach the reliable performance lack well guided, such as the scale of required annotated data or the effective framework. To investigate the first question, we empirically investigate the cost-effectiveness of several methods to train the intent classification and slot-filling models for Indonesia (ID) from scratch by utilizing the English data. Confronting the second challenge, we propose a Bi-Confidence-Frequency Cross-Lingual transfer framework (BiCF), composed by ``BiCF Mixing'', ``Latent Space Refinement'' and ``Joint Decoder'', respectively, to tackle the obstacle of lacking low-resource language dialogue data. Extensive experiments demonstrate our framework performs reliably and cost-efficiently on different scales of manually annotated Indonesian data. We release a large-scale fine-labeled dialogue dataset (ID-WOZ) and ID-BERT of Indonesian for further research.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.19862",
        "abstract url": "https://arxiv.org/abs/2410.19862",
        "title": "Real-Time Weapon Detection Using YOLOv8 for Enhanced Safety",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "This research paper presents the development of an AI model utilizing YOLOv8 for real-time weapon detection, aimed at enhancing safety in public spaces such as schools, airports, and public transportation systems. As incidents of violence continue to rise globally, there is an urgent need for effective surveillance technologies that can quickly identify potential threats. Our approach focuses on leveraging advanced deep learning techniques to create a highly accurate and efficient system capable of detecting weapons in real-time video streams. The model was trained on a comprehensive dataset containing thousands of images depicting various types of firearms and edged weapons, ensuring a robust learning process. We evaluated the model's performance using key metrics such as precision, recall, F1-score, and mean Average Precision (mAP) across multiple Intersection over Union (IoU) thresholds, revealing a significant capability to differentiate between weapon and non-weapon classes with minimal error. Furthermore, we assessed the system's operational efficiency, demonstrating that it can process frames at high speeds suitable for real-time applications. The findings indicate that our YOLOv8-based weapon detection model not only contributes to the existing body of knowledge in computer vision but also addresses critical societal needs for improved safety measures in vulnerable environments. By harnessing the power of artificial intelligence, this research lays the groundwork for developing practical solutions that can be deployed in security settings, ultimately enhancing the protective capabilities of law enforcement and public safety agencies.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "21 pages, 5 figures"
    },
    {
        "paper id": "2410.19870",
        "abstract url": "https://arxiv.org/abs/2410.19870",
        "title": "Causal Order Discovery based on Monotonic SCMs",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In this paper, we consider the problem of causal order discovery within the framework of monotonic Structural Causal Models (SCMs), which have gained attention for their potential to enable causal inference and causal discovery from observational data. While existing approaches either assume prior knowledge about the causal order or use complex optimization techniques to impose sparsity in the Jacobian of Triangular Monotonic Increasing maps, our work introduces a novel sequential procedure that directly identifies the causal order by iteratively detecting the root variable. This method eliminates the need for sparsity assumptions and the associated optimization challenges, enabling the identification of a unique SCM without the need for multiple independence tests to break the Markov equivalence class. We demonstrate the effectiveness of our approach in sequentially finding the root variable, comparing it to methods that maximize Jacobian sparsity.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "Accepted to the NeurIPS 2024 Workshop on Causal Representation Learning"
    },
    {
        "paper id": "2411.00809",
        "abstract url": "https://arxiv.org/abs/2411.00809",
        "title": "Adaptive Dense Reward: Understanding the Gap Between Action and Reward Space in Alignment",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) has proven highly effective in aligning Large Language Models (LLMs) with human preferences. However, the original RLHF typically optimizes under an overall reward, which can lead to a suboptimal learning process. This limitation stems from RLHF's lack of awareness regarding which specific tokens should be reinforced or suppressed. Moreover, conflicts in supervision can arise, for instance, when a chosen response includes erroneous tokens, while a rejected response contains accurate elements. To rectify these shortcomings, increasing dense reward methods, such as step-wise and token-wise RLHF, have been proposed. However, these existing methods are limited to specific tasks (like mathematics). In this paper, we propose the ``Adaptive Message-wise RLHF'' method, which robustly applies to various tasks. By defining pivot tokens as key indicators, our approach adaptively identifies essential information and converts sample-level supervision into fine-grained, subsequence-level supervision. This aligns the density of rewards and action spaces more closely with the information density of the input. Experiments demonstrate that our method can be integrated into various training methods, significantly mitigating hallucinations and catastrophic forgetting problems while outperforming other methods on multiple evaluation metrics. Our method improves the success rate on adversarial samples by 10\\% compared to the sample-wise approach and achieves a 1.3\\% improvement on evaluation benchmarks such as MMLU, GSM8K, and HumanEval et al.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03334",
        "abstract url": "https://arxiv.org/abs/2411.03334",
        "title": "Neural Network Prediction of Strong Lensing Systems with Domain Adaptation and Uncertainty Quantification",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Modeling strong gravitational lenses is computationally expensive for the complex data from modern and next-generation cosmic surveys. Deep learning has emerged as a promising approach for finding lenses and predicting lensing parameters, such as the Einstein radius. Mean-variance Estimators (MVEs) are a common approach for obtaining aleatoric (data) uncertainties from a neural network prediction. However, neural networks have not been demonstrated to perform well on out-of-domain target data successfully - e.g., when trained on simulated data and applied to real, observational data. In this work, we perform the first study of the efficacy of MVEs in combination with unsupervised domain adaptation (UDA) on strong lensing data. The source domain data is noiseless, and the target domain data has noise mimicking modern cosmology surveys. We find that adding UDA to MVE increases the accuracy on the target data by a factor of about two over an MVE model without UDA. Including UDA also permits much more well-calibrated aleatoric uncertainty predictions. Advancements in this approach may enable future applications of MVE models to real observational data.",
        "subjects": [
            "astro-ph.IM",
            "astro-ph.CO",
            "astro-ph.GA",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted to the Machine Learning for Physical Sciences workshop at NeurIPS 2024; 24 pages, 2 figures, 4 tables"
    },
    {
        "paper id": "2410.17594",
        "abstract url": "https://arxiv.org/abs/2410.17594",
        "title": "How to Continually Adapt Text-to-Image Diffusion Models for Flexible Customization?",
        "rating": "0.5",
        "keywords": [
            [
                "Diffusion",
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Custom diffusion models (CDMs) have attracted widespread attention due to their astonishing generative ability for personalized concepts. However, most existing CDMs unreasonably assume that personalized concepts are fixed and cannot change over time. Moreover, they heavily suffer from catastrophic forgetting and concept neglect on old personalized concepts when continually learning a series of new concepts. To address these challenges, we propose a novel Concept-Incremental text-to-image Diffusion Model (CIDM), which can resolve catastrophic forgetting and concept neglect to learn new customization tasks in a concept-incremental manner. Specifically, to surmount the catastrophic forgetting of old concepts, we develop a concept consolidation loss and an elastic weight aggregation module. They can explore task-specific and task-shared knowledge during training, and aggregate all low-rank weights of old concepts based on their contributions during inference. Moreover, in order to address concept neglect, we devise a context-controllable synthesis strategy that leverages expressive region features and noise estimation to control the contexts of generated images according to user conditions. Experiments validate that our CIDM surpasses existing custom diffusion models. The source codes are available at https://github.com/JiahuaDong/CIFC.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to NeurIPS2024"
    },
    {
        "paper id": "2410.17599",
        "abstract url": "https://arxiv.org/abs/2410.17599",
        "title": "Cross-model Control: Improving Multiple Large Language Models in One-time Training",
        "rating": "0.5",
        "keywords": [
            [
                "unlearning"
            ],
            [
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "The number of large language models (LLMs) with varying parameter scales and vocabularies is increasing. While they deliver powerful performance, they also face a set of common optimization needs to meet specific requirements or standards, such as instruction following or avoiding the output of sensitive information from the real world. However, how to reuse the fine-tuning outcomes of one model to other models to reduce training costs remains a challenge. To bridge this gap, we introduce Cross-model Control (CMC), a method that improves multiple LLMs in one-time training with a portable tiny language model. Specifically, we have observed that the logit shift before and after fine-tuning is remarkably similar across different models. Based on this insight, we incorporate a tiny language model with a minimal number of parameters. By training alongside a frozen template LLM, the tiny model gains the capability to alter the logits output by the LLMs. To make this tiny language model applicable to models with different vocabularies, we propose a novel token mapping strategy named PM-MinED. We have conducted extensive experiments on instruction tuning and unlearning tasks, demonstrating the effectiveness of CMC. Our code is available at https://github.com/wujwyi/CMC.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by NeurIPS 2024"
    },
    {
        "paper id": "2410.17614",
        "abstract url": "https://arxiv.org/abs/2410.17614",
        "title": "Extending and Applying Automated HERMES Software Publication Workflows",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Research software is an import output of research and must be published according to the FAIR Principles for Research Software. This can be achieved by publishing software with metadata under a persistent identifier. HERMES is a tool that leverages continuous integration to automate the publication of software with rich metadata. In this work, we describe the HERMES workflow itself, and how to extend it to meet the needs of specific research software metadata or infrastructure. We introduce the HERMES plugin architecture and provide the example of creating a new HERMES plugin that harvests metadata from a metadata source in source code repositories. We show how to use HERMES as an end user, both via the command line interface, and as a step in a continuous integration pipeline. Finally, we report three informal case studies whose results provide a preliminary evaluation of the feasibility and applicability of HERMES workflows, and the extensibility of the hermes software package.",
        "subjects": [
            "cs.SE",
            "cs.CY",
            "cs.DL",
            "cs.IR"
        ],
        "comment": "17 pages, 2 figures, 2 tables, submitted to a special issue of Electronic Communications of the EASST collecting submissions of deRSE24, Conference for Research Software Engineers"
    },
    {
        "paper id": "2410.17619",
        "abstract url": "https://arxiv.org/abs/2410.17619",
        "title": "From PDFs to Structured Data: Utilizing LLM Analysis in Sports Database Management",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "This study investigates the effectiveness of Large Language Models (LLMs) in processing semi-structured data from PDF documents into structured formats, specifically examining their application in updating the Finnish Sports Clubs Database. Through action research methodology, we developed and evaluated an AI-assisted approach utilizing OpenAI's GPT-4 and Anthropic's Claude 3 Opus models to process data from 72 sports federation membership reports. The system achieved a 90% success rate in automated processing, successfully handling 65 of 72 files without errors and converting over 7,900 rows of data. While the initial development time was comparable to traditional manual processing (three months), the implemented system shows potential for reducing future processing time by approximately 90%. Key challenges included handling multilingual content, processing multi-page datasets, and managing extraneous information. The findings suggest that while LLMs demonstrate significant potential for automating semi-structured data processing tasks, optimal results are achieved through a hybrid approach combining AI automation with selective human oversight. This research contributes to the growing body of literature on practical LLM applications in organizational data management and provides insights into the transformation of traditional data processing workflows.",
        "subjects": [
            "cs.CE",
            "cs.AI"
        ],
        "comment": "11 pages, 1 figure; corrected the corresponding authors e-mail"
    },
    {
        "paper id": "2410.17621",
        "abstract url": "https://arxiv.org/abs/2410.17621",
        "title": "Process Supervision-Guided Policy Optimization for Code Generation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Reinforcement Learning (RL) with unit test feedback has enhanced large language models (LLMs) code generation, but relies on sparse rewards provided only after complete code evaluation, limiting learning efficiency and incremental improvements. When generated code fails all unit tests, no learning signal is received, hindering progress on complex tasks. To address this, we propose a Process Reward Model (PRM) that delivers dense, line-level feedback on code correctness during generation, mimicking human code refinement and providing immediate guidance. We explore various strategies for training PRMs and integrating them into the RL framework, finding that using PRMs both as dense rewards and for value function initialization significantly boosts performance. Our approach increases our in-house LLM's pass rate from 28.2% to 29.8% on LiveCodeBench and from 31.8% to 35.8% on our internal benchmark. Our experimental results highlight the effectiveness of PRMs in enhancing RL-driven code generation, especially for long-horizon scenarios.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "14 pages, 5 figures"
    },
    {
        "paper id": "2410.17624",
        "abstract url": "https://arxiv.org/abs/2410.17624",
        "title": "Incremental Learning of Affordances using Markov Logic Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Affordances enable robots to have a semantic understanding of their surroundings. This allows them to have more acting flexibility when completing a given task. Capturing object affordances in a machine learning model is a difficult task, because of their dependence on contextual information. Markov Logic Networks (MLN) combine probabilistic reasoning with logic that is able to capture such context. Mobile robots operate in partially known environments wherein unseen object affordances can be observed. This new information must be incorporated into the existing knowledge, without having to retrain the MLN from scratch. We introduce the MLN Cumulative Learning Algorithm (MLN-CLA). MLN-CLA learns new relations in various knowledge domains by retaining knowledge and only updating the changed knowledge, for which the MLN is retrained. We show that MLN-CLA is effective for accumulative learning and zero-shot affordance inference, outperforming strong baselines.",
        "subjects": [
            "cs.LG",
            "cs.RO"
        ],
        "comment": "accepted at IEEE IRC 2024"
    },
    {
        "paper id": "2410.17628",
        "abstract url": "https://arxiv.org/abs/2410.17628",
        "title": "Feature Learning in Attention Mechanisms Is More Compact and Stable Than in Convolution",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Attention and convolution are fundamental techniques in machine learning. While they use different approaches to learn features - attention mechanisms capture both global and local data relathionships, while convolutional layers focus on local patterns - both methods are effective for various tasks. Although the feature learning of both models is well-studied individually, there has not been a direct comparison of their feature learning dynamics. In this paper, we compare their Lipschitz continuity with respect to the Wasserstein distance and covering numbers under similar settings. We demonstrate that attention processes data in a more compact and stable manner. Compactness refers to the lower variance and intrinsic dimensionality of the activation outputs, while stability refers to the changes between inputs and outputs. We validate our findings through experiments using topological data analysis, measuring the 1-, 2-, and infinity-Wasserstein distances between the outputs of each layer from both models. Furthermore, we extend our comparison to Vision Transformers (ViTs) and ResNets, showing that while ViTs have higher output variance, their feature learning is more stable than that of ResNets.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17647",
        "abstract url": "https://arxiv.org/abs/2410.17647",
        "title": "Entity-based Reinforcement Learning for Autonomous Cyber Defence",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "A significant challenge for autonomous cyber defence is ensuring a defensive agent's ability to generalise across diverse network topologies and configurations. This capability is necessary for agents to remain effective when deployed in dynamically changing environments, such as an enterprise network where devices may frequently join and leave. Standard approaches to deep reinforcement learning, where policies are parameterised using a fixed-input multi-layer perceptron (MLP) expect fixed-size observation and action spaces. In autonomous cyber defence, this makes it hard to develop agents that generalise to environments with network topologies different from those trained on, as the number of nodes affects the natural size of the observation and action spaces. To overcome this limitation, we reframe the problem of autonomous network defence using entity-based reinforcement learning, where the observation and action space of an agent are decomposed into a collection of discrete entities. This framework enables the use of policy parameterisations specialised in compositional generalisation. Namely, we train a Transformer-based policy on the Yawning Titan cyber-security simulation environment and test its generalisation capabilities across various network topologies. We demonstrate that this approach significantly outperforms an MLP-based policy on fixed networks, and has the ability for zero-shot generalisation to networks of a different size to those seen in training. These findings highlight the potential for entity-based reinforcement learning to advance the field of autonomous cyber defence by providing more generalisable policies capable of handling variations in real-world network environments.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": "Material to appear in the proceedings of the 1st International Workshop on Autonomous Cybersecurity at ACM CCS 2024"
    },
    {
        "paper id": "2410.17656",
        "abstract url": "https://arxiv.org/abs/2410.17656",
        "title": "AutoRNet: Automatically Optimizing Heuristics for Robust Network Design via Large Language Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Achieving robust networks is a challenging problem due to its NP-hard nature and complex solution space. Current methods, from handcrafted feature extraction to deep learning, have made progress but remain rigid, requiring manual design and large labeled datasets. To address these issues, we propose AutoRNet, a framework that integrates large language models (LLMs) with evolutionary algorithms to generate heuristics for robust network design. We design network optimization strategies to provide domain-specific prompts for LLMs, utilizing domain knowledge to generate advanced heuristics. Additionally, we introduce an adaptive fitness function to balance convergence and diversity while maintaining degree distributions. AutoRNet is evaluated on sparse and dense scale-free networks, outperforming current methods by reducing the need for manual design and large datasets.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17696",
        "abstract url": "https://arxiv.org/abs/2410.17696",
        "title": "Optimizing Load Scheduling in Power Grids Using Reinforcement Learning and Markov Decision Processes",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Power grid load scheduling is a critical task that ensures the balance between electricity generation and consumption while minimizing operational costs and maintaining grid stability. Traditional optimization methods often struggle with the dynamic and stochastic nature of power systems, especially when faced with renewable energy sources and fluctuating demand. This paper proposes a reinforcement learning (RL) approach using a Markov Decision Process (MDP) framework to address the challenges of dynamic load scheduling. The MDP is defined by a state space representing grid conditions, an action space covering control operations like generator adjustments and storage management, and a reward function balancing economic efficiency and system reliability. We investigate the application of various RL algorithms, from basic Q-Learning to more advanced Deep Q-Networks (DQN) and Actor-Critic methods, to determine optimal scheduling policies. The proposed approach is evaluated through a simulated power grid environment, demonstrating its potential to improve scheduling efficiency and adapt to variable demand patterns. Our results show that the RL-based method provides a robust and scalable solution for real-time load scheduling, contributing to the efficient management of modern power grids.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17732",
        "abstract url": "https://arxiv.org/abs/2410.17732",
        "title": "FuzzWiz -- Fuzzing Framework for Efficient Hardware Coverage",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Ever-increasing design complexity of System-on-Chips (SoCs) led to significant verification challenges. Unlike software, bugs in hardware design are vigorous and eternal i.e., once the hardware is fabricated, it cannot be repaired with any patch. Despite being one of the powerful techniques used in verification, the dynamic random approach cannot give confidence to complex Register Transfer Leve (RTL) designs during the pre-silicon design phase. In particular, achieving coverage targets and exposing bugs is a complicated task with random simulations. In this paper, we leverage an existing testing solution available in the software world known as fuzzing and apply it to hardware verification in order to achieve coverage targets in quick time. We created an automated hardware fuzzing framework FuzzWiz using metamodeling and Python to achieve coverage goals faster. It includes parsing the RTL design module, converting it into C/C++ models, creating generic testbench with assertions, fuzzer-specific compilation, linking, and fuzzing. Furthermore, it is configurable and provides the debug flow if any crash is detected during the fuzzing process. The proposed framework is applied on four IP blocks from Google's OpenTitan chip with various fuzzing engines to show its scalability and compatibility. Our benchmarking results show that we could achieve around 90% of the coverage 10 times faster than traditional simulation regression based approach.",
        "subjects": [
            "cs.AR",
            "cs.AI",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17748",
        "abstract url": "https://arxiv.org/abs/2410.17748",
        "title": "Can Uncertainty Quantification Enable Better Learning-based Index Tuning?",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Index tuning is crucial for optimizing database performance by selecting optimal indexes based on workload. The key to this process lies in an accurate and efficient benefit estimator. Traditional methods relying on what-if tools often suffer from inefficiency and inaccuracy. In contrast, learning-based models provide a promising alternative but face challenges such as instability, lack of interpretability, and complex management. To overcome these limitations, we adopt a novel approach: quantifying the uncertainty in learning-based models' results, thereby combining the strengths of both traditional and learning-based methods for reliable index tuning. We propose Beauty, the first uncertainty-aware framework that enhances learning-based models with uncertainty quantification and uses what-if tools as a complementary mechanism to improve reliability and reduce management complexity. Specifically, we introduce a novel method that combines AutoEncoder and Monte Carlo Dropout to jointly quantify uncertainty, tailored to the characteristics of benefit estimation tasks. In experiments involving sixteen models, our approach outperformed existing uncertainty quantification methods in the majority of cases. We also conducted index tuning tests on six datasets. By applying the Beauty framework, we eliminated worst-case scenarios and more than tripled the occurrence of best-case scenarios.",
        "subjects": [
            "cs.DB",
            "cs.LG"
        ],
        "comment": "14 pages, 11 figures"
    },
    {
        "paper id": "2410.17752",
        "abstract url": "https://arxiv.org/abs/2410.17752",
        "title": "AdaDiffSR: Adaptive Region-aware Dynamic Acceleration Diffusion Model for Real-World Image Super-Resolution",
        "rating": "0.5",
        "keywords": [
            [
                "Diffusion",
                "Super-Resolution"
            ],
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Diffusion models (DMs) have shown promising results on single-image super-resolution and other image-to-image translation tasks. Benefiting from more computational resources and longer inference times, they are able to yield more realistic images. Existing DMs-based super-resolution methods try to achieve an overall average recovery over all regions via iterative refinement, ignoring the consideration that different input image regions require different timesteps to reconstruct. In this work, we notice that previous DMs-based super-resolution methods suffer from wasting computational resources to reconstruct invisible details. To further improve the utilization of computational resources, we propose AdaDiffSR, a DMs-based SR pipeline with dynamic timesteps sampling strategy (DTSS). Specifically, by introducing the multi-metrics latent entropy module (MMLE), we can achieve dynamic perception of the latent spatial information gain during the denoising process, thereby guiding the dynamic selection of the timesteps. In addition, we adopt a progressive feature injection module (PFJ), which dynamically injects the original image features into the denoising process based on the current information gain, so as to generate images with both fidelity and realism. Experiments show that our AdaDiffSR achieves comparable performance over current state-of-the-art DMs-based SR methods while consuming less computational resources and inference time on both synthetic and real-world datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "18 pages, 6 figures, ECCV2024 accepted"
    },
    {
        "paper id": "2410.17765",
        "abstract url": "https://arxiv.org/abs/2410.17765",
        "title": "Faster Language Models with Better Multi-Token Prediction Using Tensor Decomposition",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose a new model for multi-token prediction in transformers, aiming to enhance sampling efficiency without compromising accuracy. Motivated by recent work that predicts the probabilities of subsequent tokens using multiple heads, we connect this approach to rank-$1$ canonical tensor decomposition. By generalizing it to a rank-$r$ canonical probability decomposition, we develop an improved model that predicts multiple tokens simultaneously. This model can also be interpreted as a mixture of experts, allowing us to leverage successful techniques from that domain for efficient and robust training. Importantly, the overall overhead for training and sampling remains low. Our method demonstrates significant improvements in inference speed for both text and code generation tasks, proving particularly beneficial within the self-speculative decoding paradigm. It maintains its effectiveness across various model sizes and training epochs, highlighting its robustness and scalability.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17770",
        "abstract url": "https://arxiv.org/abs/2410.17770",
        "title": "Locating Information in Large Language Models via Random Matrix Theory",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "As large language models (LLMs) become central to AI applications, gaining a deeper understanding of their inner workings is increasingly important. In this work, we analyze the weight matrices of pretrained transformer models -- specifically BERT and Llama -- using random matrix theory (RMT) as a zero-information hypothesis. While randomly initialized weights perfectly agree with RMT predictions, deviations emerge after training, allowing us to locate learned structures within the models. We identify layer-type specific behaviors that are consistent across all blocks and architectures considered. By pinpointing regions that deviate from RMT predictions, we highlight areas of feature learning and confirm this through comparisons with the activation covariance matrices of the corresponding layers. Our method provides a diagnostic tool for identifying relevant regions in transformer weights using only the trained matrices. Additionally, we address the ongoing debate regarding the significance of small singular values in the context of fine-tuning and alignment in LLMs. Our findings reveal that, after fine-tuning, small singular values play a crucial role in the models' capabilities, suggesting that removing them in an already aligned transformer can be detrimental, as it may compromise model alignment.",
        "subjects": [
            "cs.LG",
            "cond-mat.dis-nn"
        ],
        "comment": "17 pages, 14 figures"
    },
    {
        "paper id": "2410.17784",
        "abstract url": "https://arxiv.org/abs/2410.17784",
        "title": "Holon Programming Model -- A Software-Defined Approach for System of Systems",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "As Systems of Systems evolve into increasingly complex networks, harnessing their collective potential becomes paramount. Traditional SoS engineering approaches lack the necessary programmability to develop third party SoS level behaviors. To address this challenge, we propose a software defined approach to enable flexible and adaptive programming of SoS. We introduce the Holon Programming Model, a software-defined framework designed to meet these needs. The Holon Programming Model empowers developers to design and orchestrate complex system behaviors effectively, as illustrated in our disaster management scenario. This research outlines the Holon Programming Model theoretical underpinnings and practical applications, with the aim of driving further exploration and advancement in the field of software defined SoS",
        "subjects": [
            "cs.AI",
            "cs.ET",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17796",
        "abstract url": "https://arxiv.org/abs/2410.17796",
        "title": "A Comprehensive Analysis on the Learning Curve in Kernel Ridge Regression",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper conducts a comprehensive study of the learning curves of kernel ridge regression (KRR) under minimal assumptions. Our contributions are three-fold: 1) we analyze the role of key properties of the kernel, such as its spectral eigen-decay, the characteristics of the eigenfunctions, and the smoothness of the kernel; 2) we demonstrate the validity of the Gaussian Equivalent Property (GEP), which states that the generalization performance of KRR remains the same when the whitened features are replaced by standard Gaussian vectors, thereby shedding light on the success of previous analyzes under the Gaussian Design Assumption; 3) we derive novel bounds that improve over existing bounds across a broad range of setting such as (in)dependent feature vectors and various combinations of eigen-decay rates in the over/underparameterized regimes.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17835",
        "abstract url": "https://arxiv.org/abs/2410.17835",
        "title": "Optimal Streaming Algorithms for Multi-Armed Bandits",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper studies two variants of the best arm identification (BAI) problem under the streaming model, where we have a stream of $n$ arms with reward distributions supported on $[0,1]$ with unknown means. The arms in the stream are arriving one by one, and the algorithm cannot access an arm unless it is stored in a limited size memory. We first study the streaming \\eps-$top$-$k$ arms identification problem, which asks for $k$ arms whose reward means are lower than that of the $k$-th best arm by at most $\\eps$ with probability at least $1-\u03b4$. For general $\\eps \\in (0,1)$, the existing solution for this problem assumes $k = 1$ and achieves the optimal sample complexity $O(\\frac{n}{\\eps^2} \\log \\frac{1}\u03b4)$ using $O(\\log^*(n))$ ($\\log^*(n)$ equals the number of times that we need to apply the logarithm function on $n$ before the results is no more than 1.) memory and a single pass of the stream. We propose an algorithm that works for any $k$ and achieves the optimal sample complexity $O(\\frac{n}{\\eps^2} \\log\\frac{k}\u03b4)$ using a single-arm memory and a single pass of the stream. Second, we study the streaming BAI problem, where the objective is to identify the arm with the maximum reward mean with at least $1-\u03b4$ probability, using a single-arm memory and as few passes of the input stream as possible. We present a single-arm-memory algorithm that achieves a near instance-dependent optimal sample complexity within $O(\\log \u0394_2^{-1})$ passes, where $\u0394_2$ is the gap between the mean of the best arm and that of the second best arm.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "24pages"
    },
    {
        "paper id": "2410.17839",
        "abstract url": "https://arxiv.org/abs/2410.17839",
        "title": "Few-shot NeRF by Adaptive Rendering Loss Regularization",
        "rating": "0.5",
        "keywords": [
            [
                "NeRF"
            ],
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Novel view synthesis with sparse inputs poses great challenges to Neural Radiance Field (NeRF). Recent works demonstrate that the frequency regularization of Positional Encoding (PE) can achieve promising results for few-shot NeRF. In this work, we reveal that there exists an inconsistency between the frequency regularization of PE and rendering loss. This prevents few-shot NeRF from synthesizing higher-quality novel views. To mitigate this inconsistency, we propose Adaptive Rendering loss regularization for few-shot NeRF, dubbed AR-NeRF. Specifically, we present a two-phase rendering supervision and an adaptive rendering loss weight learning strategy to align the frequency relationship between PE and 2D-pixel supervision. In this way, AR-NeRF can learn global structures better in the early training phase and adaptively learn local details throughout the training process. Extensive experiments show that our AR-NeRF achieves state-of-the-art performance on different datasets, including object-level and complex scenes.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by ECCV2024"
    },
    {
        "paper id": "2410.17840",
        "abstract url": "https://arxiv.org/abs/2410.17840",
        "title": "Is the GPU Half-Empty or Half-Full? Practical Scheduling Techniques for LLMs",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Serving systems for Large Language Models (LLMs) improve throughput by processing several requests concurrently. However, multiplexing hardware resources between concurrent requests involves non-trivial scheduling decisions. Practical serving systems typically implement these decisions at two levels: First, a load balancer routes requests to different servers which each hold a replica of the LLM. Then, on each server, an engine-level scheduler decides when to run a request, or when to queue or preempt it. Improved scheduling policies may benefit a wide range of LLM deployments and can often be implemented as \"drop-in replacements\" to a system's current policy. In this work, we survey scheduling techniques from the literature and from practical serving systems. We find that schedulers from the literature often achieve good performance but introduce significant complexity. In contrast, schedulers in practical deployments often leave easy performance gains on the table but are easy to implement, deploy and configure. This finding motivates us to introduce two new scheduling techniques, which are both easy to implement, and outperform current techniques on production workload traces.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "12 pages, 11 figures"
    },
    {
        "paper id": "2410.17851",
        "abstract url": "https://arxiv.org/abs/2410.17851",
        "title": "The Probabilistic Tsetlin Machine: A Novel Approach to Uncertainty Quantification",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Tsetlin Machines (TMs) have emerged as a compelling alternative to conventional deep learning methods, offering notable advantages such as smaller memory footprint, faster inference, fault-tolerant properties, and interpretability. Although various adaptations of TMs have expanded their applicability across diverse domains, a fundamental gap remains in understanding how TMs quantify uncertainty in their predictions. In response, this paper introduces the Probabilistic Tsetlin Machine (PTM) framework, aimed at providing a robust, reliable, and interpretable approach for uncertainty quantification. Unlike the original TM, the PTM learns the probability of staying on each state of each Tsetlin Automaton (TA) across all clauses. These probabilities are updated using the feedback tables that are part of the TM framework: Type I and Type II feedback. During inference, TAs decide their actions by sampling states based on learned probability distributions, akin to Bayesian neural networks when generating weight values. In our experimental analysis, we first illustrate the spread of the probabilities across TA states for the noisy-XOR dataset. Then we evaluate the PTM alongside benchmark models using both simulated and real-world datasets. The experiments on the simulated dataset reveal the PTM's effectiveness in uncertainty quantification, particularly in delineating decision boundaries and identifying regions of high uncertainty. Moreover, when applied to multiclass classification tasks using the Iris dataset, the PTM demonstrates competitive performance in terms of predictive entropy and expected calibration error, showcasing its potential as a reliable tool for uncertainty estimation. Our findings underscore the importance of selecting appropriate models for accurate uncertainty quantification in predictive tasks, with the PTM offering a particularly interpretable and effective solution.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "12 pages, 5 figures, 6 tables, accepted and presented at ICAAI 2024, London"
    },
    {
        "paper id": "2410.17882",
        "abstract url": "https://arxiv.org/abs/2410.17882",
        "title": "Identifiable Representation and Model Learning for Latent Dynamic Systems",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Learning identifiable representations and models from low-level observations is useful for an intelligent spacecraft to reliability finish downstream tasks. For temporal observations, to ensure that the data generating process is provably inverted, most existing works either assume the noise variables in the dynamic mechanisms are (conditionally) independent, or require interventions which can directly affect each latent variable. However, in practice, the relationship between the exogenous inputs/interventions and the latent variables may follow some complex deterministic mechanisms. In this work, we study the problem of identifiable representation and model learning for latent dynamic systems. The key idea is that we use an inductive bias inspired by controllable canonical forms, which is invariant, sparse, and input dependent by definition. We prove that, for linear or affine nonlinear latent dynamic systems, it is possible to identify the representations up to scaling and determine the models up to some simple transformations. The results have potential to provide some theoretical guarantees for developing more trustworthy decision-making and control methods for intelligent spacecrafts.",
        "subjects": [
            "cs.LG",
            "eess.SY",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17904",
        "abstract url": "https://arxiv.org/abs/2410.17904",
        "title": "Reinforcement Learning under Latent Dynamics: Toward Statistical and Algorithmic Modularity",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Real-world applications of reinforcement learning often involve environments where agents operate on complex, high-dimensional observations, but the underlying (''latent'') dynamics are comparatively simple. However, outside of restrictive settings such as small latent spaces, the fundamental statistical requirements and algorithmic principles for reinforcement learning under latent dynamics are poorly understood. This paper addresses the question of reinforcement learning under $\\textit{general}$ latent dynamics from a statistical and algorithmic perspective. On the statistical side, our main negative result shows that most well-studied settings for reinforcement learning with function approximation become intractable when composed with rich observations; we complement this with a positive result, identifying latent pushforward coverability as a general condition that enables statistical tractability. Algorithmically, we develop provably efficient observable-to-latent reductions -- that is, reductions that transform an arbitrary algorithm for the latent MDP into an algorithm that can operate on rich observations -- in two settings: one where the agent has access to hindsight observations of the latent dynamics [LADZ23], and one where the agent can estimate self-predictive latent models [SAGHCB20]. Together, our results serve as a first step toward a unified statistical and algorithmic theory for reinforcement learning under latent dynamics.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "math.OC",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17935",
        "abstract url": "https://arxiv.org/abs/2410.17935",
        "title": "Semi-Implicit Functional Gradient Flow",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Particle-based variational inference methods (ParVIs) use non-parametric variational families represented by particles to approximate the target distribution according to the kernelized Wasserstein gradient flow for the Kullback-Leibler (KL) divergence. Recent works introduce functional gradient flows to substitute the kernel for better flexibility. However, the deterministic updating mechanism may suffer from limited exploration and require expensive repetitive runs for new samples. In this paper, we propose Semi-Implicit Functional Gradient flow (SIFG), a functional gradient ParVI method that uses perturbed particles as the approximation family. The corresponding functional gradient flow, which can be estimated via denoising score matching, exhibits strong theoretical convergence guarantee. We also present an adaptive version of our method to automatically choose the suitable noise magnitude. Extensive experiments demonstrate the effectiveness and efficiency of the proposed framework on both simulated and real data problems.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "31 pages, 12 figures"
    },
    {
        "paper id": "2410.17945",
        "abstract url": "https://arxiv.org/abs/2410.17945",
        "title": "Theoretically Grounded Pruning of Large Ground Sets for Constrained, Discrete Optimization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Modern instances of combinatorial optimization problems often exhibit billion-scale ground sets, which have many uninformative or redundant elements. In this work, we develop light-weight pruning algorithms to quickly discard elements that are unlikely to be part of an optimal solution. Under mild assumptions on the instance, we prove theoretical guarantees on the fraction of the optimal value retained and the size of the resulting pruned ground set. Through extensive experiments on real-world datasets for various applications, we demonstrate that our algorithm, QuickPrune, efficiently prunes over 90% of the ground set and outperforms state-of-the-art classical and machine learning heuristics for pruning.",
        "subjects": [
            "cs.DS",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17948",
        "abstract url": "https://arxiv.org/abs/2410.17948",
        "title": "Generalized Resubstitution for Regression Error Estimation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose generalized resubstitution error estimators for regression, a broad family of estimators, each corresponding to a choice of empirical probability measures and loss function. The usual sum of squares criterion is a special case corresponding to the standard empirical probability measure and the quadratic loss. Other choices of empirical probability measure lead to more general estimators with superior bias and variance properties. We prove that these error estimators are consistent under broad assumptions. In addition, procedures for choosing the empirical measure based on the method of moments and maximum pseudo-likelihood are proposed and investigated. Detailed experimental results using polynomial regression demonstrate empirically the superior finite-sample bias and variance properties of the proposed estimators. The R code for the experiments is provided.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17950",
        "abstract url": "https://arxiv.org/abs/2410.17950",
        "title": "Benchmarking Floworks against OpenAI & Anthropic: A Novel Framework for Enhanced LLM Function Calling",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in various domains, yet their economic impact has been limited by challenges in tool use and function calling. This paper introduces ThorV2, a novel architecture that significantly enhances LLMs' function calling abilities. We develop a comprehensive benchmark focused on HubSpot CRM operations to evaluate ThorV2 against leading models from OpenAI and Anthropic. Our results demonstrate that ThorV2 outperforms existing models in accuracy, reliability, latency, and cost efficiency for both single and multi-API calling tasks. We also show that ThorV2 is far more reliable and scales better to multistep tasks compared to traditional models. Our work offers the tantalizing possibility of more accurate function-calling compared to today's best-performing models using significantly smaller LLMs. These advancements have significant implications for the development of more capable AI assistants and the broader application of LLMs in real-world scenarios.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "15 pages for main paper, 21 pages in total including references and appendix, 10 figures"
    },
    {
        "paper id": "2410.17957",
        "abstract url": "https://arxiv.org/abs/2410.17957",
        "title": "MCUBERT: Memory-Efficient BERT Inference on Commodity Microcontrollers",
        "rating": "0.5",
        "keywords": [
            [
                "Memory-Efficient"
            ],
            [
                "architecture search"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we propose MCUBERT to enable language models like BERT on tiny microcontroller units (MCUs) through network and scheduling co-optimization. We observe the embedding table contributes to the major storage bottleneck for tiny BERT models. Hence, at the network level, we propose an MCU-aware two-stage neural architecture search algorithm based on clustered low-rank approximation for embedding compression. To reduce the inference memory requirements, we further propose a novel fine-grained MCU-friendly scheduling strategy. Through careful computation tiling and re-ordering as well as kernel design, we drastically increase the input sequence lengths supported on MCUs without any latency or accuracy penalty. MCUBERT reduces the parameter size of BERT-tiny and BERT-mini by 5.7$\\times$ and 3.0$\\times$ and the execution memory by 3.5$\\times$ and 4.3$\\times$, respectively. MCUBERT also achieves 1.5$\\times$ latency reduction. For the first time, MCUBERT enables lightweight BERT models on commodity MCUs and processing more than 512 tokens with less than 256KB of memory.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "ICCAD 2024"
    },
    {
        "paper id": "2410.17972",
        "abstract url": "https://arxiv.org/abs/2410.17972",
        "title": "Dependency Graph Parsing as Sequence Labeling",
        "rating": "0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Various linearizations have been proposed to cast syntactic dependency parsing as sequence labeling. However, these approaches do not support more complex graph-based representations, such as semantic dependencies or enhanced universal dependencies, as they cannot handle reentrancy or cycles. By extending them, we define a range of unbounded and bounded linearizations that can be used to cast graph parsing as a tagging task, enlarging the toolbox of problems that can be solved under this paradigm. Experimental results on semantic dependency and enhanced UD parsing show that with a good choice of encoding, sequence-labeling dependency graph parsers combine high efficiency with accuracies close to the state of the art, in spite of their simplicity.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted at EMNLP-2024"
    },
    {
        "paper id": "2410.17998",
        "abstract url": "https://arxiv.org/abs/2410.17998",
        "title": "Estimating the Spectral Moments of the Kernel Integral Operator from Finite Sample Matrices",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Analyzing the structure of sampled features from an input data distribution is challenging when constrained by limited measurements in both the number of inputs and features. Traditional approaches often rely on the eigenvalue spectrum of the sample covariance matrix derived from finite measurement matrices; however, these spectra are sensitive to the size of the measurement matrix, leading to biased insights. In this paper, we introduce a novel algorithm that provides unbiased estimates of the spectral moments of the kernel integral operator in the limit of infinite inputs and features from finitely sampled measurement matrices. Our method, based on dynamic programming, is efficient and capable of estimating the moments of the operator spectrum. We demonstrate the accuracy of our estimator on radial basis function (RBF) kernels, highlighting its consistency with the theoretical spectra. Furthermore, we showcase the practical utility and robustness of our method in understanding the geometry of learned representations in neural networks.",
        "subjects": [
            "cs.LG",
            "math.SP",
            "math.ST",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18012",
        "abstract url": "https://arxiv.org/abs/2410.18012",
        "title": "MiniFed : Integrating LLM-based Agentic-Workflow for Simulating FOMC Meeting",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "The Federal Funds rate in the United States plays a significant role in both domestic and international financial markets. However, research has predominantly focused on the effects of adjustments to the Federal Funds rate rather than on the decision-making process itself. Recent advancements in large language models(LLMs) offer a potential method for reconstructing the original FOMC meetings, which are responsible for setting the Federal Funds rate. In this paper, we propose a five-stage FOMC meeting simulation framework, MiniFed, which employs LLM agents to simulate real-world FOMC meeting members and optimize the FOMC structure. This framework effectively revitalizes the FOMC meeting process and facilitates projections of the Federal Funds rate. Experimental results demonstrate that our proposed MiniFed framework achieves both high accuracy in Federal Funds rate projections and behavioral alignment with the agents' real-world counterparts. Given that few studies have focused on employing LLM agents to simulate large-scale real-world conferences, our work can serve as a benchmark for future developments.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18038",
        "abstract url": "https://arxiv.org/abs/2410.18038",
        "title": "POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Each request in LLM inference goes through two phases: compute-bound prefill and memory-bandwidth-bound decode. To improve GPU utilization, recent systems use hybrid batching that combines the prefill and decode phases of different requests into the same batch. Hybrid batching works well for linear operations as it amortizes the cost of loading model weights from HBM. However, attention computation in hybrid batches remains inefficient because existing attention kernels are optimized for either prefill or decode. In this paper, we present POD-Attention -- the first GPU kernel that efficiently computes attention for hybrid batches. POD-Attention aims to maximize the utilization of both compute and memory bandwidth by carefully allocating the GPU's resources such that prefill and decode operations happen concurrently on the same multiprocessor. We integrate POD-Attention in a state-of-the-art LLM inference scheduler Sarathi-Serve. POD-Attention speeds up attention computation by up to 75% (mean 28%) and increases LLM serving throughput by up to 22% in offline inference. In online inference, POD-Attention enables lower time-to-first-token (TTFT), time-between-tokens (TBT), and request execution latency versus Sarathi-Serve.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18066",
        "abstract url": "https://arxiv.org/abs/2410.18066",
        "title": "The Double-Edged Sword of Behavioral Responses in Strategic Classification: Theory and User Studies",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "When humans are subject to an algorithmic decision system, they can strategically adjust their behavior accordingly (``game'' the system). While a growing line of literature on strategic classification has used game-theoretic modeling to understand and mitigate such gaming, these existing works consider standard models of fully rational agents. In this paper, we propose a strategic classification model that considers behavioral biases in human responses to algorithms. We show how misperceptions of a classifier (specifically, of its feature weights) can lead to different types of discrepancies between biased and rational agents' responses, and identify when behavioral agents over- or under-invest in different features. We also show that strategic agents with behavioral biases can benefit or (perhaps, unexpectedly) harm the firm compared to fully rational strategic agents. We complement our analytical results with user studies, which support our hypothesis of behavioral biases in human responses to the algorithm. Together, our findings highlight the need to account for human (cognitive) biases when designing AI systems, and providing explanations of them, to strategic human in the loop.",
        "subjects": [
            "cs.LG",
            "cs.GT",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18067",
        "abstract url": "https://arxiv.org/abs/2410.18067",
        "title": "Beyond position: how rotary embeddings shape representations and memory in autoregressive transfomers",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Rotary Positional Embeddings (RoPE) enhance positional encoding in Transformer models, yet their full impact on model dynamics remains underexplored. This paper studies how RoPE introduces position-dependent rotations, causing phase shifts in token embeddings that influence higher-frequency components within the model's internal representations. Through spectral analysis, we demonstrate that RoPE's rotation matrices induce oscillatory behaviors in embeddings, affecting information retention across layers and shaping temporal modeling capabilities. We show that activation functions in feed-forward networks interact with RoPE-modulated embeddings to generate harmonics, leading to constructive or destructive interference based on phase alignment. Our findings reveal that phase alignment amplifies activations and sharpens attention, while misalignment weakens activations and disrupts focus on positional patterns. This study underscores the importance of frequency components as intrinsic elements of model behavior, offering new insights beyond traditional analyses.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18162",
        "abstract url": "https://arxiv.org/abs/2410.18162",
        "title": "Stochastic gradient descent in high dimensions for multi-spiked tensor PCA",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study the dynamics in high dimensions of online stochastic gradient descent for the multi-spiked tensor model. This multi-index model arises from the tensor principal component analysis (PCA) problem with multiple spikes, where the goal is to estimate $r$ unknown signal vectors within the $N$-dimensional unit sphere through maximum likelihood estimation from noisy observations of a $p$-tensor. We determine the number of samples and the conditions on the signal-to-noise ratios (SNRs) required to efficiently recover the unknown spikes from natural random initializations. We show that full recovery of all spikes is possible provided a number of sample scaling as $N^{p-2}$, matching the algorithmic threshold identified in the rank-one case [Ben Arous, Gheissari, Jagannath 2020, 2021]. Our results are obtained through a detailed analysis of a low-dimensional system that describes the evolution of the correlations between the estimators and the spikes, while controlling the noise in the dynamics. We find that the spikes are recovered sequentially in a process we term \"sequential elimination\": once a correlation exceeds a critical threshold, all correlations sharing a row or column index become sufficiently small, allowing the next correlation to grow and become macroscopic. The order in which correlations become macroscopic depends on their initial values and the corresponding SNRs, leading to either exact recovery or recovery of a permutation of the spikes. In the matrix case, when $p=2$, if the SNRs are sufficiently separated, we achieve exact recovery of the spikes, whereas equal SNRs lead to recovery of the subspace spanned by the spikes.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.PR",
            "math.ST"
        ],
        "comment": "58 pages, 10 figures. This is part of our manuscript arXiv:2408.06401"
    },
    {
        "paper id": "2410.18202",
        "abstract url": "https://arxiv.org/abs/2410.18202",
        "title": "PyTSC: A Unified Platform for Multi-Agent Reinforcement Learning in Traffic Signal Control",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Multi-Agent Reinforcement Learning (MARL) presents a promising approach for addressing the complexity of Traffic Signal Control (TSC) in urban environments. However, existing platforms for MARL-based TSC research face challenges such as slow simulation speeds and convoluted, difficult-to-maintain codebases. To address these limitations, we introduce PyTSC, a robust and flexible simulation environment that facilitates the training and evaluation of MARL algorithms for TSC. PyTSC integrates multiple simulators, such as SUMO and CityFlow, and offers a streamlined API, empowering researchers to explore a broad spectrum of MARL approaches efficiently. PyTSC accelerates experimentation and provides new opportunities for advancing intelligent traffic management systems in real-world applications.",
        "subjects": [
            "cs.MA",
            "cs.AI"
        ],
        "comment": "13 pages"
    },
    {
        "paper id": "2410.18216",
        "abstract url": "https://arxiv.org/abs/2410.18216",
        "title": "Neural Cover Selection for Image Steganography",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In steganography, selecting an optimal cover image, referred to as cover selection, is pivotal for effective message concealment. Traditional methods have typically employed exhaustive searches to identify images that conform to specific perceptual or complexity metrics. However, the relationship between these metrics and the actual message hiding efficacy of an image is unclear, often yielding less-than-ideal steganographic outcomes. Inspired by recent advancements in generative models, we introduce a novel cover selection framework, which involves optimizing within the latent space of pretrained generative models to identify the most suitable cover images, distinguishing itself from traditional exhaustive search methods. Our method shows significant advantages in message recovery and image quality. We also conduct an information-theoretic analysis of the generated cover images, revealing that message hiding predominantly occurs in low-variance pixels, reflecting the waterfilling algorithm's principles in parallel Gaussian channels. Our code can be found at: https://github.com/karlchahine/Neural-Cover-Selection-for-Image-Steganography.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18221",
        "abstract url": "https://arxiv.org/abs/2410.18221",
        "title": "Data Augmentation for Automated Adaptive Rodent Training",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Fully optimized automation of behavioral training protocols for lab animals like rodents has long been a coveted goal for researchers. It is an otherwise labor-intensive and time-consuming process that demands close interaction between the animal and the researcher. In this work, we used a data-driven approach to optimize the way rodents are trained in labs. In pursuit of our goal, we looked at data augmentation, a technique that scales well in data-poor environments. Using data augmentation, we built several artificial rodent models, which in turn would be used to build an efficient and automatic trainer. Then we developed a novel similarity metric based on the action probability distribution to measure the behavioral resemblance of our models to that of real rodents.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "5 pages, 3 figures"
    },
    {
        "paper id": "2410.18241",
        "abstract url": "https://arxiv.org/abs/2410.18241",
        "title": "Characterising Open Source Co-opetition in Company-hosted Open Source Software Projects: The Cases of PyTorch, TensorFlow, and Transformers",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "Companies, including market rivals, have long collaborated on the development of open source software (OSS), resulting in a tangle of co-operation and competition known as \"open source co-opetition\". While prior work investigates open source co-opetition in OSS projects that are hosted by vendor-neutral foundations, we have a limited understanding thereof in OSS projects that are hosted and governed by one company. Given their prevalence, it is timely to investigate open source co-opetition in such contexts. Towards this end, we conduct a mixed-methods analysis of three company-hosted OSS projects in the artificial intelligence (AI) industry: Meta's PyTorch (prior to its donation to the Linux Foundation), Google's TensorFlow, and Hugging Face's Transformers. We contribute three key findings. First, while the projects exhibit similar code authorship patterns between host and external companies (80%/20% of commits), collaborations are structured differently (e.g., decentralised vs. hub-and-spoke networks). Second, host and external companies engage in strategic, non-strategic, and contractual collaborations, with varying incentives and collaboration practices. Some of the observed collaborations are specific to the AI industry (e.g., hardware-software optimizations or AI model integrations), while others are typical of the broader software industry (e.g., bug fixing or task outsourcing). Third, single-vendor governance creates a power imbalance that influences open source co-opetition practices and possibilities, from the host company's singular decision-making power (e.g., the risk of license change) to their community involvement strategy (e.g., from over-control to over-delegation). We conclude with recommendations for future research.",
        "subjects": [
            "cs.SE",
            "cs.AI",
            "cs.CY"
        ],
        "comment": "26 pages, 2 figures, 9 tables"
    },
    {
        "paper id": "2410.18242",
        "abstract url": "https://arxiv.org/abs/2410.18242",
        "title": "Human-Agent Coordination in Games under Incomplete Information via Multi-Step Intent",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Strategic coordination between autonomous agents and human partners under incomplete information can be modeled as turn-based cooperative games. We extend a turn-based game under incomplete information, the shared-control game, to allow players to take multiple actions per turn rather than a single action. The extension enables the use of multi-step intent, which we hypothesize will improve performance in long-horizon tasks. To synthesize cooperative policies for the agent in this extended game, we propose an approach featuring a memory module for a running probabilistic belief of the environment dynamics and an online planning algorithm called IntentMCTS. This algorithm strategically selects the next action by leveraging any communicated multi-step intent via reward augmentation while considering the current belief. Agent-to-agent simulations in the Gnomes at Night testbed demonstrate that IntentMCTS requires fewer steps and control switches than baseline methods. A human-agent user study corroborates these findings, showing an 18.52% higher success rate compared to the heuristic baseline and a 5.56% improvement over the single-step prior work. Participants also report lower cognitive load, frustration, and higher satisfaction with the IntentMCTS agent partner.",
        "subjects": [
            "cs.AI",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18248",
        "abstract url": "https://arxiv.org/abs/2410.18248",
        "title": "Fast Inference for Augmented Large Language Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone LLMs by integrating external data sources through API calls. In interactive LLM applications, efficient scheduling is crucial for maintaining low request completion times, directly impacting user engagement. However, these augmentations introduce scheduling challenges due to the need to manage limited memory for cached information (KV caches). As a result, traditional size-based scheduling algorithms, such as Shortest Job First (SJF), become less effective at minimizing completion times. Existing work focuses only on handling requests during API calls by preserving, discarding, or swapping memory without considering how to schedule requests with API calls. In this paper, we propose LAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes request completion time through a unified scheduling approach that considers the total length of requests and their handling strategies during API calls. Recognizing that LLM inference is memory-bound, our approach ranks requests based on their consumption of memory over time, which depends on both the output sizes and how a request is managed during its API calls. To implement our scheduling, LAMPS predicts the strategy that minimizes memory waste of a request during its API calls, aligning with but improving upon existing approaches. We also propose starvation prevention techniques and optimizations to mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM and evaluate its performance against baseline LLM inference systems, demonstrating improvements in end-to-end latency by 27%-85% and reductions in TTFT by 4%-96% compared to the existing augmented-LLM system, with even greater gains over vLLM.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18253",
        "abstract url": "https://arxiv.org/abs/2410.18253",
        "title": "Exploring Network Structure with the Density of States",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Community detection, as well as the identification of other structures like core periphery and disassortative patterns, is an important topic in network analysis. While most methods seek to find the best partition of the network according to some criteria, there is a body of results that suggest that a single network can have many good but distinct partitions. In this paper we introduce the density of states as a tool for studying the space of all possible network partitions. We demonstrate how to use the well known Wang-Landau method to compute a network's density of states. We show that, even using modularity to measure quality, the density of states can still rule out spurious structure in random networks and overcome resolution limits. We demonstrate how these methods can be used to find `building blocks', groups of nodes which are consistently found together in detected communities. This suggests an approach to partitioning based on exploration of the network's structure landscape rather than optimisation.",
        "subjects": [
            "cs.SI",
            "physics.soc-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18268",
        "abstract url": "https://arxiv.org/abs/2410.18268",
        "title": "Stabilizing black-box model selection with the inflated argmax",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Model selection is the process of choosing from a class of candidate models given data. For instance, methods such as the LASSO and sparse identification of nonlinear dynamics (SINDy) formulate model selection as finding a sparse solution to a linear system of equations determined by training data. However, absent strong assumptions, such methods are highly unstable: if a single data point is removed from the training set, a different model may be selected. This paper presents a new approach to stabilizing model selection that leverages a combination of bagging and an \"inflated\" argmax operation. Our method selects a small collection of models that all fit the data, and it is stable in that, with high probability, the removal of any training point will result in a collection of selected models that overlaps with the original collection. In addition to developing theoretical guarantees, we illustrate this method in (a) a simulation in which strongly correlated covariates make standard LASSO model selection highly unstable and (b) a Lotka-Volterra model selection problem focused on identifying how competition in an ecosystem influences species' abundances. In both settings, the proposed method yields stable and compact collections of selected models, outperforming a variety of benchmarks.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "stat.ME"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18283",
        "abstract url": "https://arxiv.org/abs/2410.18283",
        "title": "Augmenting Training Data with Vector-Quantized Variational Autoencoder for Classifying RF Signals",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Radio frequency (RF) communication has been an important part of civil and military communication for decades. With the increasing complexity of wireless environments and the growing number of devices sharing the spectrum, it has become critical to efficiently manage and classify the signals that populate these frequencies. In such scenarios, the accurate classification of wireless signals is essential for effective spectrum management, signal interception, and interference mitigation. However, the classification of wireless RF signals often faces challenges due to the limited availability of labeled training data, especially under low signal-to-noise ratio (SNR) conditions. To address these challenges, this paper proposes the use of a Vector-Quantized Variational Autoencoder (VQ-VAE) to augment training data, thereby enhancing the performance of a baseline wireless classifier. The VQ-VAE model generates high-fidelity synthetic RF signals, increasing the diversity and fidelity of the training dataset by capturing the complex variations inherent in RF communication signals. Our experimental results show that incorporating VQ-VAE-generated data significantly improves the classification accuracy of the baseline model, particularly in low SNR conditions. This augmentation leads to better generalization and robustness of the classifier, overcoming the constraints imposed by limited real-world data. By improving RF signal classification, the proposed approach enhances the efficacy of wireless communication in both civil and tactical settings, ensuring reliable and secure operations. This advancement supports critical decision-making and operational readiness in environments where communication fidelity is essential.",
        "subjects": [
            "cs.LG",
            "cs.NI",
            "eess.SP"
        ],
        "comment": "IEEE Milcom 2024"
    },
    {
        "paper id": "2410.18293",
        "abstract url": "https://arxiv.org/abs/2410.18293",
        "title": "1-2-3-Go! Policy Synthesis for Parameterized Markov Decision Processes via Decision-Tree Learning and Generalization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Despite the advances in probabilistic model checking, the scalability of the verification methods remains limited. In particular, the state space often becomes extremely large when instantiating parameterized Markov decision processes (MDPs) even with moderate values. Synthesizing policies for such \\emph{huge} MDPs is beyond the reach of available tools. We propose a learning-based approach to obtain a reasonable policy for such huge MDPs. The idea is to generalize optimal policies obtained by model-checking small instances to larger ones using decision-tree learning. Consequently, our method bypasses the need for explicit state-space exploration of large models, providing a practical solution to the state-space explosion problem. We demonstrate the efficacy of our approach by performing extensive experimentation on the relevant models from the quantitative verification benchmark set. The experimental results indicate that our policies perform well, even when the size of the model is orders of magnitude beyond the reach of state-of-the-art analysis tools.",
        "subjects": [
            "cs.AI",
            "cs.LG",
            "cs.LO",
            "eess.SY"
        ],
        "comment": "Preprint. Under review"
    },
    {
        "paper id": "2410.18294",
        "abstract url": "https://arxiv.org/abs/2410.18294",
        "title": "NexusIndex: Integrating Advanced Vector Indexing and Multi-Model Embeddings for Robust Fake News Detection",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The proliferation of fake news on digital platforms has underscored the need for robust and scalable detection mechanisms. Traditional methods often fall short in handling large and diverse datasets due to limitations in scalability and accuracy. In this paper, we propose NexusIndex, a novel framework and model that enhances fake news detection by integrating advanced language models, an innovative FAISSNexusIndex layer, and attention mechanisms. Our approach leverages multi-model embeddings to capture rich contextual and semantic nuances, significantly improving text interpretation and classification accuracy. By transforming articles into high-dimensional embeddings and indexing them efficiently, NexusIndex facilitates rapid similarity searches across extensive collections of news articles. The FAISSNexusIndex layer further optimizes this process, enabling real-time detection and enhancing the system's scalability and performance. Our experimental results demonstrate that NexusIndex outperforms state-of-the-art methods in efficiency and accuracy across diverse datasets.",
        "subjects": [
            "cs.IR",
            "cs.DB",
            "cs.LG",
            "cs.NE"
        ],
        "comment": "9 pages, 3 figures"
    },
    {
        "paper id": "2410.18355",
        "abstract url": "https://arxiv.org/abs/2410.18355",
        "title": "Real-time 3D-aware Portrait Video Relighting",
        "rating": "0.5",
        "keywords": [
            [
                "3D",
                "NeRF",
                "Radiance Fields"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Synthesizing realistic videos of talking faces under custom lighting conditions and viewing angles benefits various downstream applications like video conferencing. However, most existing relighting methods are either time-consuming or unable to adjust the viewpoints. In this paper, we present the first real-time 3D-aware method for relighting in-the-wild videos of talking faces based on Neural Radiance Fields (NeRF). Given an input portrait video, our method can synthesize talking faces under both novel views and novel lighting conditions with a photo-realistic and disentangled 3D representation. Specifically, we infer an albedo tri-plane, as well as a shading tri-plane based on a desired lighting condition for each video frame with fast dual-encoders. We also leverage a temporal consistency network to ensure smooth transitions and reduce flickering artifacts. Our method runs at 32.98 fps on consumer-level hardware and achieves state-of-the-art results in terms of reconstruction quality, lighting error, lighting instability, temporal consistency and inference speed. We demonstrate the effectiveness and interactivity of our method on various portrait videos with diverse lighting and viewing conditions.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "Accepted to CVPR 2024 (Highlight). Project page: http://geometrylearning.com/VideoRelighting"
    },
    {
        "paper id": "2410.18368",
        "abstract url": "https://arxiv.org/abs/2410.18368",
        "title": "Multi-objective Optimization in CPU Design Space Exploration: Attention is All You Need",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Design space exploration (DSE) enables architects to systematically evaluate various design options, guiding decisions on the most suitable configurations to meet specific objectives such as optimizing performance, power, and area. However, the growing complexity of modern CPUs has dramatically increased the number of micro-architectural parameters and expanded the overall design space, making DSE more challenging and time-consuming. Existing DSE frameworks struggle in large-scale design spaces due to inaccurate models and limited insights into parameter impact, hindering efficient identification of optimal micro-architectures within tight timeframes. In this work, we introduce AttentionDSE. Its key idea is to use the attention mechanism to establish a direct mapping of micro-architectural parameters to their contributions to predicted performance. This approach enhances both the prediction accuracy and interpretability of the performance model. Furthermore, the weights are dynamically adjusted, enabling the model to respond to design changes and effectively pinpoint the key micro-architectural parameters/components responsible for performance bottlenecks. Thus, AttentionDSE accurately, purposefully, and rapidly discovers optimal designs. Experiments on SPEC 2017 demonstrate that AttentionDSE significantly reduces exploration time by over 80\\% and achieves 3.9\\% improvement in Pareto Hypervolume compared to state-of-the-art DSE frameworks while maintaining superior prediction accuracy and efficiency with an increasing number of parameters.",
        "subjects": [
            "cs.LG",
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18378",
        "abstract url": "https://arxiv.org/abs/2410.18378",
        "title": "Delta: A Cloud-assisted Data Enrichment Framework for On-Device Continual Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In modern mobile applications, users frequently encounter various new contexts, necessitating on-device continual learning (CL) to ensure consistent model performance. While existing research predominantly focused on developing lightweight CL frameworks, we identify that data scarcity is a critical bottleneck for on-device CL. In this work, we explore the potential of leveraging abundant cloud-side data to enrich scarce on-device data, and propose a private, efficient and effective data enrichment framework Delta. Specifically, Delta first introduces a directory dataset to decompose the data enrichment problem into device-side and cloud-side sub-problems without sharing sensitive data. Next, Delta proposes a soft data matching strategy to effectively solve the device-side sub-problem with sparse user data, and an optimal data sampling scheme for cloud server to retrieve the most suitable dataset for enrichment with low computational complexity. Further, Delta refines the data sampling scheme by jointly considering the impact of enriched data on both new and past contexts, mitigating the catastrophic forgetting issue from a new aspect. Comprehensive experiments across four typical mobile computing tasks with varied data modalities demonstrate that Delta could enhance the overall model accuracy by an average of 15.1%, 12.4%, 1.1% and 5.6% for visual, IMU, audio and textual tasks compared with few-shot CL, and consistently reduce the communication costs by over 90% compared to federated CL.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18379",
        "abstract url": "https://arxiv.org/abs/2410.18379",
        "title": "Identifying Factors Contributing to Bad Days for Software Developers: A Mixed Methods Study",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Software development is a dynamic activity that requires engineers to work effectively with tools, processes, and collaborative teams. As a result, the presence of friction can significantly hinder productivity, increase frustration, and contribute to low morale among developers. By contrast, higher satisfaction levels are positively correlated with higher levels of perceived productivity. Hence, understanding the factors that cause bad experiences for developers is critical for fostering a positive and productive engineering environment. In this research, we employed a mixed-method approach, including interviews, surveys, diary studies, and analysis of developer telemetry data to uncover and triangulate common factors that cause \"bad days\" for developers. The interviews involved 22 developers across different levels and roles. The survey captured the perception of 214 developers about factors that cause them to have \"bad days,\" their frequency, and their impact on job satisfaction. The daily diary study engaged 79 developers for 30 days to document factors that caused \"bad days\" in the moment. We examined the telemetry signals of 131 consenting participants to validate the impact of bad developer experience using system data. Findings from our research revealed factors that cause \"bad days\" for developers and significantly impact their work and well-being. We discuss the implications of these findings and suggest future work.",
        "subjects": [
            "cs.SE",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18402",
        "abstract url": "https://arxiv.org/abs/2410.18402",
        "title": "Low-Rank Tensor Learning by Generalized Nonconvex Regularization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we study the problem of low-rank tensor learning, where only a few of training samples are observed and the underlying tensor has a low-rank structure. The existing methods are based on the sum of nuclear norms of unfolding matrices of a tensor, which may be suboptimal. In order to explore the low-rankness of the underlying tensor effectively, we propose a nonconvex model based on transformed tensor nuclear norm for low-rank tensor learning. Specifically, a family of nonconvex functions are employed onto the singular values of all frontal slices of a tensor in the transformed domain to characterize the low-rankness of the underlying tensor. An error bound between the stationary point of the nonconvex model and the underlying tensor is established under restricted strong convexity on the loss function (such as least squares loss and logistic regression) and suitable regularity conditions on the nonconvex penalty function. By reformulating the nonconvex function into the difference of two convex functions, a proximal majorization-minimization (PMM) algorithm is designed to solve the resulting model. Then the global convergence and convergence rate of PMM are established under very mild conditions. Numerical experiments are conducted on tensor completion and binary classification to demonstrate the effectiveness of the proposed method over other state-of-the-art methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18404",
        "abstract url": "https://arxiv.org/abs/2410.18404",
        "title": "Enhancing Feature-Specific Data Protection via Bayesian Coordinate Differential Privacy",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Local Differential Privacy (LDP) offers strong privacy guarantees without requiring users to trust external parties. However, LDP applies uniform protection to all data features, including less sensitive ones, which degrades performance of downstream tasks. To overcome this limitation, we propose a Bayesian framework, Bayesian Coordinate Differential Privacy (BCDP), that enables feature-specific privacy quantification. This more nuanced approach complements LDP by adjusting privacy protection according to the sensitivity of each feature, enabling improved performance of downstream tasks without compromising privacy. We characterize the properties of BCDP and articulate its connections with standard non-Bayesian privacy frameworks. We further apply our BCDP framework to the problems of private mean estimation and ordinary least-squares regression. The BCDP-based approach obtains improved accuracy compared to a purely LDP-based approach, without compromising on privacy.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.19865",
        "abstract url": "https://arxiv.org/abs/2410.19865",
        "title": "Evaluating Deep Learning Approaches for Predictions in Unmonitored Basins with Continental-scale Stream Temperature Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The prediction of streamflows and other environmental variables in unmonitored basins is a grand challenge in hydrology. Recent machine learning (ML) models can harness vast datasets for accurate predictions at large spatial scales. However, there are open questions regarding model design and data needed for inputs and training to improve performance. This study explores these questions while demonstrating the ability of deep learning models to make accurate stream temperature predictions in unmonitored basins across the conterminous United States. First, we compare top-down models that utilize data from a large number of basins with bottom-up methods that transfer ML models built on local sites, reflecting traditional regionalization techniques. We also evaluate an intermediary grouped modeling approach that categorizes sites based on regional co-location or similarity of catchment characteristics. Second, we evaluate trade-offs between model complexity, prediction accuracy, and applicability for more target locations by systematically removing inputs. We then examine model performance when additional training data becomes available due to reductions in input requirements. Our results suggest that top-down models significantly outperform bottom-up and grouped models. Moreover, it is possible to get acceptable accuracy by reducing both dynamic and static inputs enabling predictions for more sites with lower model complexity and computational needs. From detailed error analysis, we determined that the models are more accurate for sites primarily controlled by air temperatures compared to locations impacted by groundwater and dams. By addressing these questions, this research offers a comprehensive perspective on optimizing ML model design for accurate predictions in unmonitored regions.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "physics.ao-ph"
        ],
        "comment": "47 pages, 12 figures, 7 tables, submitted to Water Resources Research"
    },
    {
        "paper id": "2410.17600",
        "abstract url": "https://arxiv.org/abs/2410.17600",
        "title": "Graphusion: A RAG Framework for Knowledge Graph Construction with a Global Perspective",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Knowledge Graphs (KGs) are crucial in the field of artificial intelligence and are widely used in downstream tasks, such as question-answering (QA). The construction of KGs typically requires significant effort from domain experts. Large Language Models (LLMs) have recently been used for Knowledge Graph Construction (KGC). However, most existing approaches focus on a local perspective, extracting knowledge triplets from individual sentences or documents, missing a fusion process to combine the knowledge in a global KG. This work introduces Graphusion, a zero-shot KGC framework from free text. It contains three steps: in Step 1, we extract a list of seed entities using topic modeling to guide the final KG includes the most relevant entities; in Step 2, we conduct candidate triplet extraction using LLMs; in Step 3, we design the novel fusion module that provides a global view of the extracted knowledge, incorporating entity merging, conflict resolution, and novel triplet discovery. Results show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for entity extraction and relation recognition, respectively. Moreover, we showcase how Graphusion could be applied to the Natural Language Processing (NLP) domain and validate it in an educational scenario. Specifically, we introduce TutorQA, a new expert-verified benchmark for QA, comprising six tasks and a total of 1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant improvement on the benchmark, for example, a 9.2% accuracy improvement on sub-graph completion.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.DB"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2407.10794"
    },
    {
        "paper id": "2410.17606",
        "abstract url": "https://arxiv.org/abs/2410.17606",
        "title": "Towards Effective Data-Free Knowledge Distillation via Diverse Diffusion Augmentation",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Data-free knowledge distillation (DFKD) has emerged as a pivotal technique in the domain of model compression, substantially reducing the dependency on the original training data. Nonetheless, conventional DFKD methods that employ synthesized training data are prone to the limitations of inadequate diversity and discrepancies in distribution between the synthesized and original datasets. To address these challenges, this paper introduces an innovative approach to DFKD through diverse diffusion augmentation (DDA). Specifically, we revise the paradigm of common data synthesis in DFKD to a composite process through leveraging diffusion models subsequent to data synthesis for self-supervised augmentation, which generates a spectrum of data samples with similar distributions while retaining controlled variations. Furthermore, to mitigate excessive deviation in the embedding space, we introduce an image filtering technique grounded in cosine similarity to maintain fidelity during the knowledge distillation process. Comprehensive experiments conducted on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets showcase the superior performance of our method across various teacher-student network configurations, outperforming the contemporary state-of-the-art DFKD methods. Code will be available at:https://github.com/SLGSP/DDA.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17694",
        "abstract url": "https://arxiv.org/abs/2410.17694",
        "title": "An Adaptive Framework for Generating Systematic Explanatory Answer in Online Q&A Platforms",
        "rating": "0",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Question Answering (QA) systems face challenges in handling complex questions that require multi-domain knowledge synthesis. The naive RAG models, although effective in information retrieval, struggle with complex questions that require comprehensive and in-depth answers. The pioneering task is defined as explanatory answer generation, which entails handling identified challenges such as the requirement for comprehensive information and logical coherence within the generated context. To address these issues, we refer to systematic thinking theory and propose SynthRAG, an innovative framework designed to enhance QA performance. SynthRAG improves on conventional models by employing adaptive outlines for dynamic content structuring, generating systematic information to ensure detailed coverage, and producing customized answers tailored to specific user inquiries. This structured approach guarantees logical coherence and thorough integration of information, yielding responses that are both insightful and methodically organized. Empirical evaluations underscore SynthRAG's effectiveness, demonstrating its superiority in handling complex questions, overcoming the limitations of naive RAG models, and significantly improving answer quality and depth. Furthermore, an online deployment on the Zhihu platform revealed that SynthRAG's answers achieved notable user engagement, with each response averaging 5.73 upvotes and surpassing the performance of 79.8% of human contributors, highlighting the practical relevance and impact of the proposed framework. Our code is available at https://github.com/czy1999/SynthRAG .",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "10 pages, 6 figures"
    },
    {
        "paper id": "2410.17741",
        "abstract url": "https://arxiv.org/abs/2410.17741",
        "title": "Efficient Neural Implicit Representation for 3D Human Reconstruction",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "NeRF"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "High-fidelity digital human representations are increasingly in demand in the digital world, particularly for interactive telepresence, AR/VR, 3D graphics, and the rapidly evolving metaverse. Even though they work well in small spaces, conventional methods for reconstructing 3D human motion frequently require the use of expensive hardware and have high processing costs. This study presents HumanAvatar, an innovative approach that efficiently reconstructs precise human avatars from monocular video sources. At the core of our methodology, we integrate the pre-trained HuMoR, a model celebrated for its proficiency in human motion estimation. This is adeptly fused with the cutting-edge neural radiance field technology, Instant-NGP, and the state-of-the-art articulated model, Fast-SNARF, to enhance the reconstruction fidelity and speed. By combining these two technologies, a system is created that can render quickly and effectively while also providing estimation of human pose parameters that are unmatched in accuracy. We have enhanced our system with an advanced posture-sensitive space reduction technique, which optimally balances rendering quality with computational efficiency. In our detailed experimental analysis using both artificial and real-world monocular videos, we establish the advanced performance of our approach. HumanAvatar consistently equals or surpasses contemporary leading-edge reconstruction techniques in quality. Furthermore, it achieves these complex reconstructions in minutes, a fraction of the time typically required by existing methods. Our models achieve a training speed that is 110X faster than that of State-of-The-Art (SoTA) NeRF-based models. Our technique performs noticeably better than SoTA dynamic human NeRF methods if given an identical runtime limit. HumanAvatar can provide effective visuals after only 30 seconds of training.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17774",
        "abstract url": "https://arxiv.org/abs/2410.17774",
        "title": "Quasi-Medial Distance Field (Q-MDF): A Robust Method for Approximating and Discretizing Neural Medial Axis",
        "rating": "0",
        "keywords": [
            [
                "signed distance field",
                "SDF"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The medial axis, a lower-dimensional shape descriptor, plays an important role in the field of digital geometry processing. Despite its importance, robust computation of the medial axis transform from diverse inputs, especially point clouds with defects, remains a significant challenge. In this paper, we tackle the challenge by proposing a new implicit method that diverges from mainstream explicit medial axis computation techniques. Our key technical insight is the difference between the signed distance field (SDF) and the medial field (MF) of a solid shape is the unsigned distance field (UDF) of the shape's medial axis. This allows for formulating medial axis computation as an implicit reconstruction problem. Utilizing a modified double covering method, we extract the medial axis as the zero level-set of the UDF. Extensive experiments show that our method has enhanced accuracy and robustness in learning compact medial axis transform from thorny meshes and point clouds compared to existing methods.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17802",
        "abstract url": "https://arxiv.org/abs/2410.17802",
        "title": "GenUDC: High Quality 3D Mesh Generation with Unsigned Dual Contouring Representation",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generating high-quality meshes with complex structures and realistic surfaces is the primary goal of 3D generative models. Existing methods typically employ sequence data or deformable tetrahedral grids for mesh generation. However, sequence-based methods have difficulty producing complex structures with many faces due to memory limits. The deformable tetrahedral grid-based method MeshDiffusion fails to recover realistic surfaces due to the inherent ambiguity in deformable grids. We propose the GenUDC framework to address these challenges by leveraging the Unsigned Dual Contouring (UDC) as the mesh representation. UDC discretizes a mesh in a regular grid and divides it into the face and vertex parts, recovering both complex structures and fine details. As a result, the one-to-one mapping between UDC and mesh resolves the ambiguity problem. In addition, GenUDC adopts a two-stage, coarse-to-fine generative process for 3D mesh generation. It first generates the face part as a rough shape and then the vertex part to craft a detailed shape. Extensive evaluations demonstrate the superiority of UDC as a mesh representation and the favorable performance of GenUDC in mesh generation. The code and trained models are available at https://github.com/TrepangCat/GenUDC.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "ACMMM 2024, code:https://github.com/TrepangCat/GenUDC"
    },
    {
        "paper id": "2410.17809",
        "abstract url": "https://arxiv.org/abs/2410.17809",
        "title": "An Intelligent Agentic System for Complex Image Restoration Problems",
        "rating": "0",
        "keywords": [
            [
                "vision-language",
                "VLMs"
            ],
            [
                "Image Restoration"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Real-world image restoration (IR) is inherently complex and often requires combining multiple specialized models to address diverse degradations. Inspired by human problem-solving, we propose AgenticIR, an agentic system that mimics the human approach to image processing by following five key stages: Perception, Scheduling, Execution, Reflection, and Rescheduling. AgenticIR leverages large language models (LLMs) and vision-language models (VLMs) that interact via text generation to dynamically operate a toolbox of IR models. We fine-tune VLMs for image quality analysis and employ LLMs for reasoning, guiding the system step by step. To compensate for LLMs' lack of specific IR knowledge and experience, we introduce a self-exploration method, allowing the LLM to observe and summarize restoration results into referenceable documents. Experiments demonstrate AgenticIR's potential in handling complex IR tasks, representing a promising path toward achieving general intelligence in visual processing.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17823",
        "abstract url": "https://arxiv.org/abs/2410.17823",
        "title": "Att2CPC: Attention-Guided Lossy Attribute Compression of Point Clouds",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "With the great progress of 3D sensing and acquisition technology, the volume of point cloud data has grown dramatically, which urges the development of efficient point cloud compression methods. In this paper, we focus on the task of learned lossy point cloud attribute compression (PCAC). We propose an efficient attention-based method for lossy compression of point cloud attributes leveraging on an autoencoder architecture. Specifically, at the encoding side, we conduct multiple downsampling to best exploit the local attribute patterns, in which effective External Cross Attention (ECA) is devised to hierarchically aggregate features by intergrating attributes and geometry contexts. At the decoding side, the attributes of the point cloud are progressively reconstructed based on the multi-scale representation and the zero-padding upsampling tactic. To the best of our knowledge, this is the first approach to introduce attention mechanism to point-based lossy PCAC task. We verify the compression efficiency of our model on various sequences, including human body frames, sparse objects, and large-scale point cloud scenes. Experiments show that our method achieves an average improvement of 1.15 dB and 2.13 dB in BD-PSNR of Y channel and YUV channel, respectively, when comparing with the state-of-the-art point-based method Deep-PCAC. Codes of this paper are available at https://github.com/I2-Multimedia-Lab/Att2CPC.",
        "subjects": [
            "cs.LG",
            "cs.CV",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17855",
        "abstract url": "https://arxiv.org/abs/2410.17855",
        "title": "TAGE: Trustworthy Attribute Group Editing for Stable Few-shot Image Generation",
        "rating": "0",
        "keywords": [
            [
                "GAN",
                "image editing"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Generative Adversarial Networks (GANs) have emerged as a prominent research focus for image editing tasks, leveraging the powerful image generation capabilities of the GAN framework to produce remarkable results.However, prevailing approaches are contingent upon extensive training datasets and explicit supervision, presenting a significant challenge in manipulating the diverse attributes of new image classes with limited sample availability. To surmount this hurdle, we introduce TAGE, an innovative image generation network comprising three integral modules: the Codebook Learning Module (CLM), the Code Prediction Module (CPM) and the Prompt-driven Semantic Module (PSM). The CPM module delves into the semantic dimensions of category-agnostic attributes, encapsulating them within a discrete codebook. This module is predicated on the concept that images are assemblages of attributes, and thus, by editing these category-independent attributes, it is theoretically possible to generate images from unseen categories. Subsequently, the CPM module facilitates naturalistic image editing by predicting indices of category-independent attribute vectors within the codebook. Additionally, the PSM module generates semantic cues that are seamlessly integrated into the Transformer architecture of the CPM, enhancing the model's comprehension of the targeted attributes for editing. With these semantic cues, the model can generate images that accentuate desired attributes more prominently while maintaining the integrity of the original category, even with a limited number of samples. We have conducted extensive experiments utilizing the Animal Faces, Flowers, and VGGFaces datasets. The results of these experiments demonstrate that our proposed method not only achieves superior performance but also exhibits a high degree of stability when compared to other few-shot image generation techniques.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Accepted by International Conference on Signal Processing Systems Conference"
    },
    {
        "paper id": "2410.17863",
        "abstract url": "https://arxiv.org/abs/2410.17863",
        "title": "CASCRNet: An Atrous Spatial Pyramid Pooling and Shared Channel Residual based Network for Capsule Endoscopy",
        "rating": "0",
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "disease"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "This manuscript summarizes work on the Capsule Vision Challenge 2024 by MISAHUB. To address the multi-class disease classification task, which is challenging due to the complexity and imbalance in the Capsule Vision challenge dataset, this paper proposes CASCRNet (Capsule endoscopy-Aspp-SCR-Network), a parameter-efficient and novel model that uses Shared Channel Residual (SCR) blocks and Atrous Spatial Pyramid Pooling (ASPP) blocks. Further, the performance of the proposed model is compared with other well-known approaches. The experimental results yield that proposed model provides better disease classification results. The proposed model was successful in classifying diseases with an F1 Score of 78.5% and a Mean AUC of 98.3%, which is promising given its compact architecture.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "8 pages, 4 figures"
    },
    {
        "paper id": "2410.17891",
        "abstract url": "https://arxiv.org/abs/2410.17891",
        "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, we propose adapting these models to build text diffusion models. We demonstrate connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. We release a suite of DLMs (with 127M, 355M, and 7B parameters) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions \\url{https://github.com/HKUNLP/DiffuLLaMA}.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "25 pages. Code: https://github.com/HKUNLP/DiffuLLaMA"
    },
    {
        "paper id": "2410.17932",
        "abstract url": "https://arxiv.org/abs/2410.17932",
        "title": "VR-Splatting: Foveated Radiance Field Rendering via 3D Gaussian Splatting and Neural Points",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "NeRF",
                "radiance fields"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advances in novel view synthesis (NVS), particularly neural radiance fields (NeRF) and Gaussian splatting (3DGS), have demonstrated impressive results in photorealistic scene rendering. These techniques hold great potential for applications in virtual tourism and teleportation, where immersive realism is crucial. However, the high-performance demands of virtual reality (VR) systems present challenges in directly utilizing even such fast-to-render scene representations like 3DGS due to latency and computational constraints. In this paper, we propose foveated rendering as a promising solution to these obstacles. We analyze state-of-the-art NVS methods with respect to their rendering performance and compatibility with the human visual system. Our approach introduces a novel foveated rendering approach for Virtual Reality, that leverages the sharp, detailed output of neural point rendering for the foveal region, fused with a smooth rendering of 3DGS for the peripheral vision. Our evaluation confirms that perceived sharpness and detail-richness are increased by our approach compared to a standard VR-ready 3DGS configuration. Our system meets the necessary performance requirements for real-time VR interactions, ultimately enhancing the user's immersive experience. Project page: https://lfranke.github.io/vr_splatting",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17941",
        "abstract url": "https://arxiv.org/abs/2410.17941",
        "title": "Spiking Graph Neural Network on Riemannian Manifolds",
        "rating": "0",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Graph neural networks (GNNs) have become the dominant solution for learning on graphs, the typical non-Euclidean structures. Conventional GNNs, constructed with the Artificial Neuron Network (ANN), have achieved impressive performance at the cost of high computation and energy consumption. In parallel, spiking GNNs with brain-like spiking neurons are drawing increasing research attention owing to the energy efficiency. So far, existing spiking GNNs consider graphs in Euclidean space, ignoring the structural geometry, and suffer from the high latency issue due to Back-Propagation-Through-Time (BPTT) with the surrogate gradient. In light of the aforementioned issues, we are devoted to exploring spiking GNN on Riemannian manifolds, and present a Manifold-valued Spiking GNN (MSG). In particular, we design a new spiking neuron on geodesically complete manifolds with the diffeomorphism, so that BPTT regarding the spikes is replaced by the proposed differentiation via manifold. Theoretically, we show that MSG approximates a solver of the manifold ordinary differential equation. Extensive experiments on common graphs show the proposed MSG achieves superior performance to previous spiking GNNs and energy efficiency to conventional GNNs.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted by NeurIPS 2024, 30 pages"
    },
    {
        "paper id": "2410.17954",
        "abstract url": "https://arxiv.org/abs/2410.17954",
        "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for Efficient Mixture-of-Experts Inference",
        "rating": "0",
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "forecast"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large Language Models (LLMs) in terms of performance, face significant deployment challenges during inference due to their high memory demands. Existing offloading techniques, which involve swapping activated and idle experts between the GPU and CPU, often suffer from rigid expert caching mechanisms. These mechanisms fail to adapt to dynamic routing, leading to inefficient cache utilization, or incur prohibitive costs for prediction training. To tackle these inference-specific challenges, we introduce ExpertFlow, a comprehensive system specifically designed to enhance inference efficiency by accommodating flexible routing and enabling efficient expert scheduling between CPU and GPU. This reduces overhead and boosts system performance. Central to our approach is a predictive routing path-based offloading mechanism that utilizes a lightweight predictor to accurately forecast routing paths before computation begins. This proactive strategy allows for real-time error correction in expert caching, significantly increasing cache hit ratios and reducing the frequency of expert transfers, thereby minimizing I/O overhead. Additionally, we implement a dynamic token scheduling strategy that optimizes MoE inference by rearranging input tokens across different batches. This method not only reduces the number of activated experts per batch but also improves computational efficiency. Our extensive experiments demonstrate that ExpertFlow achieves up to 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times compared to baseline methods, highlighting its effectiveness and utility as a robust solution for resource-constrained inference scenarios.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": "Mixture-of-Experts, Inference, Offloading"
    },
    {
        "paper id": "2410.17966",
        "abstract url": "https://arxiv.org/abs/2410.17966",
        "title": "A Wavelet Diffusion GAN for Image Super-Resolution",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "GAN",
                "text-to-image",
                "Super-Resolution"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "In recent years, diffusion models have emerged as a superior alternative to generative adversarial networks (GANs) for high-fidelity image generation, with wide applications in text-to-image generation, image-to-image translation, and super-resolution. However, their real-time feasibility is hindered by slow training and inference speeds. This study addresses this challenge by proposing a wavelet-based conditional Diffusion GAN scheme for Single-Image Super-Resolution (SISR). Our approach utilizes the diffusion GAN paradigm to reduce the timesteps required by the reverse diffusion process and the Discrete Wavelet Transform (DWT) to achieve dimensionality reduction, decreasing training and inference times significantly. The results of an experimental validation on the CelebA-HQ dataset confirm the effectiveness of our proposed scheme. Our approach outperforms other state-of-the-art methodologies successfully ensuring high-fidelity output while overcoming inherent drawbacks associated with diffusion models in time-sensitive applications.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "The paper has been accepted at Italian Workshop on Neural Networks (WIRN) 2024"
    },
    {
        "paper id": "2410.18013",
        "abstract url": "https://arxiv.org/abs/2410.18013",
        "title": "Scalable Ranked Preference Optimization for Text-to-Image Generation",
        "rating": "0",
        "keywords": [
            [
                "Text-to-Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Direct Preference Optimization (DPO) has emerged as a powerful approach to align text-to-image (T2I) models with human feedback. Unfortunately, successful application of DPO to T2I models requires a huge amount of resources to collect and label large-scale datasets, e.g., millions of generated paired images annotated with human preferences. In addition, these human preference datasets can get outdated quickly as the rapid improvements of T2I models lead to higher quality images. In this work, we investigate a scalable approach for collecting large-scale and fully synthetic datasets for DPO training. Specifically, the preferences for paired images are generated using a pre-trained reward function, eliminating the need for involving humans in the annotation process, greatly improving the dataset collection efficiency. Moreover, we demonstrate that such datasets allow averaging predictions across multiple models and collecting ranked preferences as opposed to pairwise preferences. Furthermore, we introduce RankDPO to enhance DPO-based methods using the ranking feedback. Applying RankDPO on SDXL and SD3-Medium models with our synthetically generated preference dataset \"Syn-Pic\" improves both prompt-following (on benchmarks like T2I-Compbench, GenEval, and DPG-Bench) and visual quality (through user studies). This pipeline presents a practical and scalable solution to develop better preference datasets to enhance the performance of text-to-image models.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://snap-research.github.io/RankDPO/"
    },
    {
        "paper id": "2410.18032",
        "abstract url": "https://arxiv.org/abs/2410.18032",
        "title": "GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration",
        "rating": "0",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Graphs are widely used for modeling relational data in real-world scenarios, such as social networks and urban computing. Existing LLM-based graph analysis approaches either integrate graph neural networks (GNNs) for specific machine learning tasks, limiting their transferability, or rely solely on LLMs' internal reasoning ability, resulting in suboptimal performance. To address these limitations, we take advantage of recent advances in LLM-based agents, which have shown capabilities of utilizing external knowledge or tools for problem solving. By simulating human problem-solving strategies such as analogy and collaboration, we propose a multi-agent system based on LLMs named GraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from three modules, and the agents with different specialities can collaborate with each other to address complex problems. Specifically, (1) input-output normalization module: the question agent extracts and refines four key arguments from the original question, facilitating the problem understanding, and the answer agent organizes the results to meet the output requirement; (2) external knowledge retrieval module: we first build a knowledge base consisting of relevant documentation and experience information, and then the search agent retrieves the most relevant entries for each question. (3) problem-solving module: given the retrieved information from search agent, the coding agent uses established algorithms via programming to generate solutions, and in case the coding agent does not work, the reasoning agent will directly compute the results without programming. Extensive experiments on six graph analysis benchmarks demonstrate that GraphTeam achieves state-of-the-art performance with an average 25.85% improvement over the best baseline in terms of accuracy. The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18057",
        "abstract url": "https://arxiv.org/abs/2410.18057",
        "title": "CLEAR: Character Unlearning in Textual and Visual Modalities",
        "rating": "0",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Machine Unlearning (MU) is critical for enhancing privacy and security in deep learning models, particularly in large multimodal language models (MLLMs), by removing specific private or hazardous information. While MU has made significant progress in textual and visual modalities, multimodal unlearning (MMU) remains significantly underexplored, partially due to the absence of a suitable open-source benchmark. To address this, we introduce CLEAR, a new benchmark designed to evaluate MMU methods. CLEAR contains 200 fictitious individuals and 3,700 images linked with corresponding question-answer pairs, enabling a thorough evaluation across modalities. We assess 10 MU methods, adapting them for MMU, and highlight new challenges specific to multimodal forgetting. We also demonstrate that simple $\\ell_1$ regularization on LoRA weights significantly mitigates catastrophic forgetting, preserving model performance on retained data. The dataset is available at https://huggingface.co/datasets/therem/CLEAR",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18065",
        "abstract url": "https://arxiv.org/abs/2410.18065",
        "title": "SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for Long-Horizon Manipulation",
        "rating": "0",
        "keywords": [
            [
                "Robot"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Robot learning has proven to be a general and effective technique for programming manipulators. Imitation learning is able to teach robots solely from human demonstrations but is bottlenecked by the capabilities of the demonstrations. Reinforcement learning uses exploration to discover better behaviors; however, the space of possible improvements can be too large to start from scratch. And for both techniques, the learning difficulty increases proportional to the length of the manipulation task. Accounting for this, we propose SPIRE, a system that first uses Task and Motion Planning (TAMP) to decompose tasks into smaller learning subproblems and second combines imitation and reinforcement learning to maximize their strengths. We develop novel strategies to train learning agents when deployed in the context of a planning system. We evaluate SPIRE on a suite of long-horizon and contact-rich robot manipulation problems. We find that SPIRE outperforms prior approaches that integrate imitation learning, reinforcement learning, and planning by 35% to 50% in average task performance, is 6 times more data efficient in the number of human demonstrations needed to train proficient agents, and learns to complete tasks nearly twice as efficiently. View https://sites.google.com/view/spire-corl-2024 for more details.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Conference on Robot Learning (CoRL) 2024"
    },
    {
        "paper id": "2410.18072",
        "abstract url": "https://arxiv.org/abs/2410.18072",
        "title": "WorldSimBench: Towards Video Generation Models as World Simulators",
        "rating": "0",
        "keywords": [
            [
                "Robot"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in predictive models have demonstrated exceptional capabilities in predicting the future state of objects and scenes. However, the lack of categorization based on inherent characteristics continues to hinder the progress of predictive model development. Additionally, existing benchmarks are unable to effectively evaluate higher-capability, highly embodied predictive models from an embodied perspective. In this work, we classify the functionalities of predictive models into a hierarchy and take the first step in evaluating World Simulators by proposing a dual evaluation framework called WorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation and Implicit Manipulative Evaluation, encompassing human preference assessments from the visual perspective and action-level evaluations in embodied tasks, covering three representative embodied scenarios: Open-Ended Embodied Environment, Autonomous, Driving, and Robot Manipulation. In the Explicit Perceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment dataset based on fine-grained human feedback, which we use to train a Human Preference Evaluator that aligns with human perception and explicitly assesses the visual fidelity of World Simulators. In the Implicit Manipulative Evaluation, we assess the video-action consistency of World Simulators by evaluating whether the generated situation-aware video can be accurately translated into the correct control signals in dynamic environments. Our comprehensive evaluation offers key insights that can drive further innovation in video generation models, positioning World Simulators as a pivotal advancement toward embodied artificial intelligence.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18074",
        "abstract url": "https://arxiv.org/abs/2410.18074",
        "title": "UnCLe: Unsupervised Continual Learning of Depth Completion",
        "rating": "0",
        "keywords": [
            [
                "Depth"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We propose UnCLe, a standardized benchmark for Unsupervised Continual Learning of a multimodal depth estimation task: Depth completion aims to infer a dense depth map from a pair of synchronized RGB image and sparse depth map. We benchmark depth completion models under the practical scenario of unsupervised learning over continuous streams of data. Existing methods are typically trained on a static, or stationary, dataset. However, when adapting to novel non-stationary distributions, they \"catastrophically forget\" previously learned information. UnCLe simulates these non-stationary distributions by adapting depth completion models to sequences of datasets containing diverse scenes captured from distinct domains using different visual and range sensors. We adopt representative methods from continual learning paradigms and translate them to enable unsupervised continual learning of depth completion. We benchmark these models for indoor and outdoor and investigate the degree of catastrophic forgetting through standard quantitative metrics. Furthermore, we introduce model inversion quality as an additional measure of forgetting. We find that unsupervised continual learning of depth completion is an open problem, and we invite researchers to leverage UnCLe as a development platform.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Preprint"
    },
    {
        "paper id": "2410.18083",
        "abstract url": "https://arxiv.org/abs/2410.18083",
        "title": "FIPER: Generalizable Factorized Fields for Joint Image Compression and Super-Resolution",
        "rating": "0",
        "keywords": [
            [
                "Super-Resolution"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "In this work, we propose a unified representation for Super-Resolution (SR) and Image Compression, termed **Factorized Fields**, motivated by the shared principles between these two tasks. Both SISR and Image Compression require recovering and preserving fine image details--whether by enhancing resolution or reconstructing compressed data. Unlike previous methods that mainly focus on network architecture, our proposed approach utilizes a basis-coefficient decomposition to explicitly capture multi-scale visual features and structural components in images, addressing the core challenges of both tasks. We first derive our SR model, which includes a Coefficient Backbone and Basis Swin Transformer for generalizable Factorized Fields. Then, to further unify these two tasks, we leverage the strong information-recovery capabilities of the trained SR modules as priors in the compression pipeline, improving both compression efficiency and detail reconstruction. Additionally, we introduce a merged-basis compression branch that consolidates shared structures, further optimizing the compression process. Extensive experiments show that our unified representation delivers state-of-the-art performance, achieving an average relative improvement of 204.4% in PSNR over the baseline in Super-Resolution (SR) and 9.35% BD-rate reduction in Image Compression compared to the previous SOTA.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "Project page: https://jayisaking.github.io/FIPER/"
    },
    {
        "paper id": "2410.18210",
        "abstract url": "https://arxiv.org/abs/2410.18210",
        "title": "Towards Understanding the Fragility of Multilingual LLMs against Fine-Tuning Attacks",
        "rating": "0",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in Large Language Models (LLMs) have sparked widespread concerns about their safety. Recent work demonstrates that safety alignment of LLMs can be easily removed by fine-tuning with a few adversarially chosen instruction-following examples, i.e., fine-tuning attacks. We take a further step to understand fine-tuning attacks in multilingual LLMs. We first discover cross-lingual generalization of fine-tuning attacks: using a few adversarially chosen instruction-following examples in one language, multilingual LLMs can also be easily compromised (e.g., multilingual LLMs fail to refuse harmful prompts in other languages). Motivated by this finding, we hypothesize that safety-related information is language-agnostic and propose a new method termed Safety Information Localization (SIL) to identify the safety-related information in the model parameter space. Through SIL, we validate this hypothesis and find that only changing 20% of weight parameters in fine-tuning attacks can break safety alignment across all languages. Furthermore, we provide evidence to the alternative pathways hypothesis for why freezing safety-related parameters does not prevent fine-tuning attacks, and we demonstrate that our attack vector can still jailbreak LLMs adapted to new languages.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CR",
            "cs.LG"
        ],
        "comment": "14 pages, 6 figures, 7 tables"
    },
    {
        "paper id": "2410.18215",
        "abstract url": "https://arxiv.org/abs/2410.18215",
        "title": "Advancing NLP Security by Leveraging LLMs as Adversarial Engines",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "This position paper proposes a novel approach to advancing NLP security by leveraging Large Language Models (LLMs) as engines for generating diverse adversarial attacks. Building upon recent work demonstrating LLMs' effectiveness in creating word-level adversarial examples, we argue for expanding this concept to encompass a broader range of attack types, including adversarial patches, universal perturbations, and targeted attacks. We posit that LLMs' sophisticated language understanding and generation capabilities can produce more effective, semantically coherent, and human-like adversarial examples across various domains and classifier architectures. This paradigm shift in adversarial NLP has far-reaching implications, potentially enhancing model robustness, uncovering new vulnerabilities, and driving innovation in defense mechanisms. By exploring this new frontier, we aim to contribute to the development of more secure, reliable, and trustworthy NLP systems for critical applications.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": "5 pages"
    },
    {
        "paper id": "2410.18238",
        "abstract url": "https://arxiv.org/abs/2410.18238",
        "title": "CARLA2Real: a tool for reducing the sim2real gap in CARLA simulator",
        "rating": "0",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Simulators are indispensable for research in autonomous systems such as self-driving cars, autonomous robots and drones. Despite significant progress in various simulation aspects, such as graphical realism, an evident gap persists between the virtual and real-world environments. Since the ultimate goal is to deploy the autonomous systems in the real world, closing the sim2real gap is of utmost importance. In this paper, we employ a state-of-the-art approach to enhance the photorealism of simulated data, aligning them with the visual characteristics of real-world datasets. Based on this, we developed CARLA2Real, an easy-to-use, publicly available tool (plug-in) for the widely used and open-source CARLA simulator. This tool enhances the output of CARLA in near real-time, achieving a frame rate of 13 FPS, translating it to the visual style and realism of real-world datasets such as Cityscapes, KITTI, and Mapillary Vistas. By employing the proposed tool, we generated synthetic datasets from both the simulator and the enhancement model outputs, including their corresponding ground truth annotations for tasks related to autonomous driving. Then, we performed a number of experiments to evaluate the impact of the proposed approach on feature extraction and semantic segmentation methods when trained on the enhanced synthetic data. The results demonstrate that the sim2real gap is significant and can indeed be reduced by the introduced approach.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "22 pages"
    },
    {
        "paper id": "2410.18287",
        "abstract url": "https://arxiv.org/abs/2410.18287",
        "title": "LEGO: Language Model Building Blocks",
        "rating": "0",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are essential in natural language processing (NLP) but are costly in data collection, pre-training, fine-tuning, and inference. Task-specific small language models (SLMs) offer a cheaper alternative but lack robustness and generalization. This paper proposes LEGO, a novel technique to extract SLMs from an LLM and recombine them. Using state-of-the-art LLM pruning strategies, we can create task- and user-specific SLM building blocks that are efficient for fine-tuning and inference while also preserving user data privacy. LEGO utilizes Federated Learning and a novel aggregation scheme for the LLM reconstruction, maintaining robustness without high costs and preserving user data privacy. We experimentally demonstrate the versatility of LEGO, showing its ability to enable model heterogeneity and mitigate the effects of data heterogeneity while maintaining LLM robustness.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18387",
        "abstract url": "https://arxiv.org/abs/2410.18387",
        "title": "Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks",
        "rating": "0",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "Biomedical",
                "medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Several medical Multimodal Large Languange Models (MLLMs) have been developed to address tasks involving visual images with textual instructions across various medical modalities, achieving impressive results. Most current medical generalist models are region-agnostic, treating the entire image as a holistic representation. However, they struggle to identify which specific regions they are focusing on when generating a sentence. To mimic the behavior of doctors, who typically begin by reviewing the entire image before concentrating on specific regions for a thorough evaluation, we aim to enhance the capability of medical MLLMs in understanding anatomical regions within entire medical scans. To achieve it, we first formulate Region-Centric tasks and construct a large-scale dataset, MedRegInstruct, to incorporate regional information into training. Combining our collected dataset with other medical multimodal corpora for training, we propose a Region-Aware medical MLLM, MedRegA, which is the first bilingual generalist medical AI system to simultaneously handle image-level and region-level medical vision-language tasks across a broad range of modalities. Our MedRegA not only enables three region-centric tasks, but also achieves the best performance for visual question answering, report generation and medical image classification over 8 modalities, showcasing significant versatility. Experiments demonstrate that our model can not only accomplish powerful performance across various medical vision-language tasks in bilingual settings, but also recognize and detect structures in multimodal medical scans, boosting the interpretability and user interactivity of medical MLLMs. Our project page is https://medrega.github.io.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Technical Report"
    },
    {
        "paper id": "2410.18410",
        "abstract url": "https://arxiv.org/abs/2410.18410",
        "title": "FreCaS: Efficient Higher-Resolution Image Generation via Frequency-aware Cascaded Sampling",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "While image generation with diffusion models has achieved a great success, generating images of higher resolution than the training size remains a challenging task due to the high computational cost. Current methods typically perform the entire sampling process at full resolution and process all frequency components simultaneously, contradicting with the inherent coarse-to-fine nature of latent diffusion models and wasting computations on processing premature high-frequency details at early diffusion stages. To address this issue, we introduce an efficient $\\textbf{Fre}$quency-aware $\\textbf{Ca}$scaded $\\textbf{S}$ampling framework, $\\textbf{FreCaS}$ in short, for higher-resolution image generation. FreCaS decomposes the sampling process into cascaded stages with gradually increased resolutions, progressively expanding frequency bands and refining the corresponding details. We propose an innovative frequency-aware classifier-free guidance (FA-CFG) strategy to assign different guidance strengths for different frequency components, directing the diffusion model to add new details in the expanded frequency domain of each stage. Additionally, we fuse the cross-attention maps of previous and current stages to avoid synthesizing unfaithful layouts. Experiments demonstrate that FreCaS significantly outperforms state-of-the-art methods in image quality and generation speed. In particular, FreCaS is about 2.86$\\times$ and 6.07$\\times$ faster than ScaleCrafter and DemoFusion in generating a 2048$\\times$2048 image using a pre-trained SDXL model and achieves an FID$_b$ improvement of 11.6 and 3.7, respectively. FreCaS can be easily extended to more complex models such as SD3. The source code of FreCaS can be found at $\\href{\\text{https://github.com/xtudbxk/FreCaS}}{https://github.com/xtudbxk/FreCaS}$.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18415",
        "abstract url": "https://arxiv.org/abs/2410.18415",
        "title": "Decoding on Graphs: Faithful and Sound Reasoning on Knowledge Graphs through Generation of Well-Formed Chains",
        "rating": "0",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Knowledge Graphs (KGs) can serve as reliable knowledge sources for question answering (QA) due to their structured representation of knowledge. Existing research on the utilization of KG for large language models (LLMs) prevalently relies on subgraph retriever or iterative prompting, overlooking the potential synergy of LLMs' step-wise reasoning capabilities and KGs' structural nature. In this paper, we present DoG (Decoding on Graphs), a novel framework that facilitates a deep synergy between LLMs and KGs. We first define a concept, well-formed chain, which consists of a sequence of interrelated fact triplets on the KGs, starting from question entities and leading to answers. We argue that this concept can serve as a principle for making faithful and sound reasoning for KGQA. To enable LLMs to generate well-formed chains, we propose graph-aware constrained decoding, in which a constraint derived from the topology of the KG regulates the decoding process of the LLMs. This constrained decoding method ensures the generation of well-formed chains while making full use of the step-wise reasoning capabilities of LLMs. Based on the above, DoG, a training-free approach, is able to provide faithful and sound reasoning trajectories grounded on the KGs. Experiments across various KGQA tasks with different background KGs demonstrate that DoG achieves superior and robust performance. DoG also shows general applicability with various open-source LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.19863",
        "abstract url": "https://arxiv.org/abs/2410.19863",
        "title": "Breaking the Illusion: Real-world Challenges for Adversarial Patches in Object Detection",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Adversarial attacks pose a significant threat to the robustness and reliability of machine learning systems, particularly in computer vision applications. This study investigates the performance of adversarial patches for the YOLO object detection network in the physical world. Two attacks were tested: a patch designed to be placed anywhere within the scene - global patch, and another patch intended to partially overlap with specific object targeted for removal from detection - local patch. Various factors such as patch size, position, rotation, brightness, and hue were analyzed to understand their impact on the effectiveness of the adversarial patches. The results reveal a notable dependency on these parameters, highlighting the challenges in maintaining attack efficacy in real-world conditions. Learning to align digitally applied transformation parameters with those measured in the real world still results in up to a 64\\% discrepancy in patch performance. These findings underscore the importance of understanding environmental influences on adversarial attacks, which can inform the development of more robust defenses for practical machine learning applications.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "- 21 pages, 17 figures, 7 tables - accepted in 1st Workshop on Enabling Machine Learning Operations for next-Gen Embedded Wireless Networked Devices (EMERGE), 2024"
    },
    {
        "paper id": "2410.17576",
        "abstract url": "https://arxiv.org/abs/2410.17576",
        "title": "Real-time Vehicle-to-Vehicle Communication Based Network Cooperative Control System through Distributed Database and Multimodal Perception: Demonstrated in Crossroads",
        "rating": "-0.5",
        "keywords": [
            [
                "autonomous driving",
                "radar",
                "Vehicle"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The autonomous driving industry is rapidly advancing, with Vehicle-to-Vehicle (V2V) communication systems highlighting as a key component of enhanced road safety and traffic efficiency. This paper introduces a novel Real-time Vehicle-to-Vehicle Communication Based Network Cooperative Control System (VVCCS), designed to revolutionize macro-scope traffic planning and collision avoidance in autonomous driving. Implemented on Quanser Car (Qcar) hardware platform, our system integrates the distributed databases into individual autonomous vehicles and an optional central server. We also developed a comprehensive multi-modal perception system with multi-objective tracking and radar sensing. Through a demonstration within a physical crossroad environment, our system showcases its potential to be applied in congested and complex urban environments.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "eess.SY"
        ],
        "comment": "ICICT 2024, 18 pages"
    },
    {
        "paper id": "2410.17579",
        "abstract url": "https://arxiv.org/abs/2410.17579",
        "title": "Bonsai: Gradient-free Graph Distillation for Node Classification",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Graph distillation has emerged as a promising avenue to enable scalable training of GNNs by compressing the training dataset while preserving essential graph characteristics. Our study uncovers significant shortcomings in current graph distillation techniques. First, the majority of the algorithms paradoxically require training on the full dataset to perform distillation. Second, due to their gradient-emulating approach, these methods require fresh distillation for any change in hyperparameters or GNN architecture, limiting their flexibility and reusability. Finally, they fail to achieve substantial size reduction due to synthesizing fully-connected, edge-weighted graphs. To address these challenges, we present Bonsai, a novel graph distillation method empowered by the observation that \\textit{computation trees} form the fundamental processing units of message-passing GNNs. Bonsai distills datasets by encoding a careful selection of \\textit{exemplar} trees that maximize the representation of all computation trees in the training set. This unique approach imparts Bonsai as the first linear-time, model-agnostic graph distillation algorithm for node classification that outperforms existing baselines across $6$ real-world datasets on accuracy, while being $22$ times faster on average. Bonsai is grounded in rigorous mathematical guarantees on the adopted approximation strategies making it robust to GNN architectures, datasets, and parameters.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17589",
        "abstract url": "https://arxiv.org/abs/2410.17589",
        "title": "Challenge on Sound Scene Synthesis: Evaluating Text-to-Audio Generation",
        "rating": "-0.5",
        "keywords": [
            [
                "Text-to-Audio"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Despite significant advancements in neural text-to-audio generation, challenges persist in controllability and evaluation. This paper addresses these issues through the Sound Scene Synthesis challenge held as part of the Detection and Classification of Acoustic Scenes and Events 2024. We present an evaluation protocol combining objective metric, namely Fr\u00e9chet Audio Distance, with perceptual assessments, utilizing a structured prompt format to enable diverse captions and effective evaluation. Our analysis reveals varying performance across sound categories and model architectures, with larger models generally excelling but innovative lightweight approaches also showing promise. The strong correlation between objective metrics and human ratings validates our evaluation approach. We discuss outcomes in terms of audio quality, controllability, and architectural considerations for text-to-audio synthesizers, providing direction for future research.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "cs.LG",
            "cs.MM",
            "eess.AS"
        ],
        "comment": "accepted to NeurIPS 2024 Workshop: Audio Imagination"
    },
    {
        "paper id": "2410.17617",
        "abstract url": "https://arxiv.org/abs/2410.17617",
        "title": "Self-Supervised Graph Neural Networks for Enhanced Feature Extraction in Heterogeneous Information Networks",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper explores the applications and challenges of graph neural networks (GNNs) in processing complex graph data brought about by the rapid development of the Internet. Given the heterogeneity and redundancy problems that graph data often have, traditional GNN methods may be overly dependent on the initial structure and attribute information of the graph, which limits their ability to accurately simulate more complex relationships and patterns in the graph. Therefore, this study proposes a graph neural network model under a self-supervised learning framework, which can flexibly combine different types of additional information of the attribute graph and its nodes, so as to better mine the deep features in the graph data. By introducing a self-supervisory mechanism, it is expected to improve the adaptability of existing models to the diversity and complexity of graph data and improve the overall performance of the model.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17629",
        "abstract url": "https://arxiv.org/abs/2410.17629",
        "title": "Graph Signal Adaptive Message Passing",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper proposes Graph Signal Adaptive Message Passing (GSAMP), a novel message passing method that simultaneously conducts online prediction, missing data imputation, and noise removal on time-varying graph signals. Unlike conventional Graph Signal Processing methods that apply the same filter to the entire graph, the spatiotemporal updates of GSAMP employ a distinct approach that utilizes localized computations at each node. This update is based on an adaptive solution obtained from an optimization problem designed to minimize the discrepancy between observed and estimated values. GSAMP effectively processes real-world, time-varying graph signals under Gaussian and impulsive noise conditions.",
        "subjects": [
            "eess.SP",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17631",
        "abstract url": "https://arxiv.org/abs/2410.17631",
        "title": "Exploring structure diversity in atomic resolution microscopy with graph neural networks",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The emergence of deep learning (DL) has provided great opportunities for the high-throughput analysis of atomic-resolution micrographs. However, the DL models trained by image patches in fixed size generally lack efficiency and flexibility when processing micrographs containing diversified atomic configurations. Herein, inspired by the similarity between the atomic structures and graphs, we describe a few-shot learning framework based on an equivariant graph neural network (EGNN) to analyze a library of atomic structures (e.g., vacancies, phases, grain boundaries, doping, etc.), showing significantly promoted robustness and three orders of magnitude reduced computing parameters compared to the image-driven DL models, which is especially evident for those aggregated vacancy lines with flexible lattice distortion. Besides, the intuitiveness of graphs enables quantitative and straightforward extraction of the atomic-scale structural features in batches, thus statistically unveiling the self-assembly dynamics of vacancy lines under electron beam irradiation. A versatile model toolkit is established by integrating EGNN sub-models for single structure recognition to process images involving varied configurations in the form of a task chain, leading to the discovery of novel doping configurations with superior electrocatalytic properties for hydrogen evolution reactions. This work provides a powerful tool to explore structure diversity in a fast, accurate, and intelligent manner.",
        "subjects": [
            "cond-mat.mtrl-sci",
            "cond-mat.mes-hall",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17648",
        "abstract url": "https://arxiv.org/abs/2410.17648",
        "title": "Towards Active Participant-Centric Vertical Federated Learning: Some Representations May Be All You Need",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Vertical Federated Learning (VFL) enables collaborative model training across different participants with distinct features and common samples, while preserving data privacy. Existing VFL methodologies often struggle with realistic data partitions, typically incurring high communication costs and significant operational complexity. In this work, we introduce a novel simplified approach to VFL, Active Participant-Centric VFL (APC-VFL), that, to the best of our knowledge, is the first to require only a single communication round between participants, and allows the active participant to do inference in a non collaborative fashion. This method integrates unsupervised representation learning with knowledge distillation to achieve comparable accuracy to traditional VFL methods based on vertical split learning in classical settings, reducing required communication rounds by up to $4200\\times$, while being more flexible. Our approach also shows improvements compared to non-federated local models, as well as a comparable VFL proposal, VFedTrans, offering an efficient and flexible solution for collaborative learning.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17700",
        "abstract url": "https://arxiv.org/abs/2410.17700",
        "title": "Scalable Random Feature Latent Variable Models",
        "rating": "-0.5",
        "keywords": [
            [
                "kernel learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Random feature latent variable models (RFLVMs) represent the state-of-the-art in latent variable models, capable of handling non-Gaussian likelihoods and effectively uncovering patterns in high-dimensional data. However, their heavy reliance on Monte Carlo sampling results in scalability issues which makes it difficult to use these models for datasets with a massive number of observations. To scale up RFLVMs, we turn to the optimization-based variational Bayesian inference (VBI) algorithm which is known for its scalability compared to sampling-based methods. However, implementing VBI for RFLVMs poses challenges, such as the lack of explicit probability distribution functions (PDFs) for the Dirichlet process (DP) in the kernel learning component, and the incompatibility of existing VBI algorithms with RFLVMs. To address these issues, we introduce a stick-breaking construction for DP to obtain an explicit PDF and a novel VBI algorithm called ``block coordinate descent variational inference\" (BCD-VI). This enables the development of a scalable version of RFLVMs, or in short, SRFLVM. Our proposed method shows scalability, computational efficiency, superior performance in generating informative latent representations and the ability of imputing missing data across various real-world datasets, outperforming state-of-the-art competitors.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17712",
        "abstract url": "https://arxiv.org/abs/2410.17712",
        "title": "A Data-Driven Odyssey in Solar Vehicles",
        "rating": "-0.5",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Solar vehicles, which simultaneously produce and consume energy, require meticulous energy management. However, potential users often feel uncertain about their operation compared to conventional vehicles. This study presents a simulator designed to help users understand long-distance travel in solar vehicles and recognize the importance of proper energy management. By utilizing Google Maps data and weather information, the simulator replicates real-world driving conditions and provides a dashboard displaying vehicle status, updated hourly based on user-inputted speed. Users can explore various speed policy scenarios and receive recommendations for optimal driving strategies. The simulator's effectiveness was validated using the route of the World Solar Challenge (WSC). This research enables users to monitor energy dynamics before a journey, enhancing their understanding of energy management and informing appropriate speed decisions.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17760",
        "abstract url": "https://arxiv.org/abs/2410.17760",
        "title": "Topology meets Machine Learning: An Introduction using the Euler Characteristic Transform",
        "rating": "-0.5",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This overview article makes the case for how topological concepts can enrich research in machine learning. Using the Euler Characteristic Transform (ECT), a geometrical-topological invariant, as a running example, I present different use cases that result in more efficient models for analyzing point clouds, graphs, and meshes. Moreover, I outline a vision for how topological concepts could be used in the future, comprising (1) the learning of functions on topological spaces, (2) the building of hybrid models that imbue neural networks with knowledge about the topological information in data, and (3) the analysis of qualitative properties of neural networks. With current research already addressing some of these aspects, this article thus serves as an introduction and invitation to this nascent area of research.",
        "subjects": [
            "cs.LG",
            "math.AT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17762",
        "abstract url": "https://arxiv.org/abs/2410.17762",
        "title": "Anomaly Resilient Temporal QoS Prediction using Hypergraph Convoluted Transformer Network",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Quality-of-Service (QoS) prediction is a critical task in the service lifecycle, enabling precise and adaptive service recommendations by anticipating performance variations over time in response to evolving network uncertainties and user preferences. However, contemporary QoS prediction methods frequently encounter data sparsity and cold-start issues, which hinder accurate QoS predictions and limit the ability to capture diverse user preferences. Additionally, these methods often assume QoS data reliability, neglecting potential credibility issues such as outliers and the presence of greysheep users and services with atypical invocation patterns. Furthermore, traditional approaches fail to leverage diverse features, including domain-specific knowledge and complex higher-order patterns, essential for accurate QoS predictions. In this paper, we introduce a real-time, trust-aware framework for temporal QoS prediction to address the aforementioned challenges, featuring an end-to-end deep architecture called the Hypergraph Convoluted Transformer Network (HCTN). HCTN combines a hypergraph structure with graph convolution over hyper-edges to effectively address high-sparsity issues by capturing complex, high-order correlations. Complementing this, the transformer network utilizes multi-head attention along with parallel 1D convolutional layers and fully connected dense blocks to capture both fine-grained and coarse-grained dynamic patterns. Additionally, our approach includes a sparsity-resilient solution for detecting greysheep users and services, incorporating their unique characteristics to improve prediction accuracy. Trained with a robust loss function resistant to outliers, HCTN demonstrated state-of-the-art performance on the large-scale WSDREAM-2 datasets for response time and throughput.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "16 pages, 12 figures"
    },
    {
        "paper id": "2410.17792",
        "abstract url": "https://arxiv.org/abs/2410.17792",
        "title": "Enhancing Federated Learning Convergence with Dynamic Data Queue and Data Entropy-driven Participant Selection",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL) is a decentralized approach for collaborative model training on edge devices. This distributed method of model training offers advantages in privacy, security, regulatory compliance, and cost-efficiency. Our emphasis in this research lies in addressing statistical complexity in FL, especially when the data stored locally across devices is not identically and independently distributed (non-IID). We have observed an accuracy reduction of up to approximately 10\\% to 30\\%, particularly in skewed scenarios where each edge device trains with only 1 class of data. This reduction is attributed to weight divergence, quantified using the Euclidean distance between device-level class distributions and the population distribution, resulting in a bias term (\\(\u03b4_k\\)). As a solution, we present a method to improve convergence in FL by creating a global subset of data on the server and dynamically distributing it across devices using a Dynamic Data queue-driven Federated Learning (DDFL). Next, we leverage Data Entropy metrics to observe the process during each training round and enable reasonable device selection for aggregation. Furthermore, we provide a convergence analysis of our proposed DDFL to justify their viability in practical FL scenarios, aiming for better device selection, a non-sub-optimal global model, and faster convergence. We observe that our approach results in a substantial accuracy boost of approximately 5\\% for the MNIST dataset, around 18\\% for CIFAR-10, and 20\\% for CIFAR-100 with a 10\\% global subset of data, outperforming the state-of-the-art (SOTA) aggregation algorithms.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CR"
        ],
        "comment": "The Journal is submitted to IEEE Transactions in the Internet of Things"
    },
    {
        "paper id": "2410.17881",
        "abstract url": "https://arxiv.org/abs/2410.17881",
        "title": "AdaRankGrad: Adaptive Gradient-Rank and Moments for Memory-Efficient LLMs Training and Fine-Tuning",
        "rating": "-0.5",
        "keywords": [
            [
                "Memory-Efficient"
            ],
            [
                "biological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Training and fine-tuning large language models (LLMs) come with challenges related to memory and computational requirements due to the increasing size of the model weights and the optimizer states. Various techniques have been developed to tackle these challenges, such as low-rank adaptation (LoRA), which involves introducing a parallel trainable low-rank matrix to the fixed pre-trained weights at each layer. However, these methods often fall short compared to the full-rank weight training approach, as they restrict the parameter search to a low-rank subspace. This limitation can disrupt training dynamics and require a full-rank warm start to mitigate the impact. In this paper, we introduce a new method inspired by a phenomenon we formally prove: as training progresses, the rank of the estimated layer gradients gradually decreases, and asymptotically approaches rank one. Leveraging this, our approach involves adaptively reducing the rank of the gradients during Adam optimization steps, using an efficient online-updating low-rank projections rule. We further present a randomized SVD scheme for efficiently finding the projection matrix. Our technique enables full-parameter fine-tuning with adaptive low-rank gradient updates, significantly reducing overall memory requirements during training compared to state-of-the-art methods while improving model performance in both pretraining and fine-tuning. Finally, we provide a convergence analysis of our method and demonstrate its merits for training and fine-tuning language and biological foundation models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17898",
        "abstract url": "https://arxiv.org/abs/2410.17898",
        "title": "Scalable Offline Reinforcement Learning for Mean Field Games",
        "rating": "-0.5",
        "keywords": [
            [
                "navigation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Reinforcement learning algorithms for mean-field games offer a scalable framework for optimizing policies in large populations of interacting agents. Existing methods often depend on online interactions or access to system dynamics, limiting their practicality in real-world scenarios where such interactions are infeasible or difficult to model. In this paper, we present Offline Munchausen Mirror Descent (Off-MMD), a novel mean-field RL algorithm that approximates equilibrium policies in mean-field games using purely offline data. By leveraging iterative mirror descent and importance sampling techniques, Off-MMD estimates the mean-field distribution from static datasets without relying on simulation or environment dynamics. Additionally, we incorporate techniques from offline reinforcement learning to address common issues like Q-value overestimation, ensuring robust policy learning even with limited data coverage. Our algorithm scales to complex environments and demonstrates strong performance on benchmark tasks like crowd exploration or navigation, highlighting its applicability to real-world multi-agent systems where online experimentation is infeasible. We empirically demonstrate the robustness of Off-MMD to low-quality datasets and conduct experiments to investigate its sensitivity to hyperparameter choices.",
        "subjects": [
            "cs.LG",
            "cs.MA"
        ],
        "comment": "Submitted to AAMAS"
    },
    {
        "paper id": "2410.17920",
        "abstract url": "https://arxiv.org/abs/2410.17920",
        "title": "Gaze-Assisted Medical Image Segmentation",
        "rating": "-0.5",
        "keywords": [
            [
                "Medical",
                "CT",
                "clinical"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "The annotation of patient organs is a crucial part of various diagnostic and treatment procedures, such as radiotherapy planning. Manual annotation is extremely time-consuming, while its automation using modern image analysis techniques has not yet reached levels sufficient for clinical adoption. This paper investigates the idea of semi-supervised medical image segmentation using human gaze as interactive input for segmentation correction. In particular, we fine-tuned the Segment Anything Model in Medical Images (MedSAM), a public solution that uses various prompt types as additional input for semi-automated segmentation correction. We used human gaze data from reading abdominal images as a prompt for fine-tuning MedSAM. The model was validated on a public WORD database, which consists of 120 CT scans of 16 abdominal organs. The results of the gaze-assisted MedSAM were shown to be superior to the results of the state-of-the-art segmentation models. In particular, the average Dice coefficient for 16 abdominal organs was 85.8%, 86.7%, 81.7%, and 90.5% for nnUNetV2, ResUNet, original MedSAM, and our gaze-assisted MedSAM model, respectively.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "16 pages, 4 figures, Accepted to AIM-FM Workshop @ NeurIPS'24"
    },
    {
        "paper id": "2410.17967",
        "abstract url": "https://arxiv.org/abs/2410.17967",
        "title": "POMDP-Driven Cognitive Massive MIMO Radar: Joint Target Detection-Tracking In Unknown Disturbances",
        "rating": "-0.5",
        "keywords": [
            [
                "Radar"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The joint detection and tracking of a moving target embedded in an unknown disturbance represents a key feature that motivates the development of the cognitive radar paradigm. Building upon recent advancements in robust target detection with multiple-input multiple-output (MIMO) radars, this work explores the application of a Partially Observable Markov Decision Process (POMDP) framework to enhance the tracking and detection tasks in a statistically unknown environment. In the POMDP setup, the radar system is considered as an intelligent agent that continuously senses the surrounding environment, optimizing its actions to maximize the probability of detection $(P_D)$ and improve the target position and velocity estimation, all this while keeping a constant probability of false alarm $(P_{FA})$. The proposed approach employs an online algorithm that does not require any apriori knowledge of the noise statistics, and it relies on a much more general observation model than the traditional range-azimuth-elevation model employed by conventional tracking algorithms. Simulation results clearly show substantial performance improvement of the POMDP-based algorithm compared to the State-Action-Reward-State-Action (SARSA)-based one that has been recently investigated in the context of massive MIMO (MMIMO) radar systems.",
        "subjects": [
            "cs.LG",
            "eess.SP",
            "stat.AP"
        ],
        "comment": "The paper has been submitted to ieee Transactions on radar systems"
    },
    {
        "paper id": "2410.17986",
        "abstract url": "https://arxiv.org/abs/2410.17986",
        "title": "Federated Transformer: Multi-Party Vertical Federated Learning on Practical Fuzzily Linked Data",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL) is an evolving paradigm that enables multiple parties to collaboratively train models without sharing raw data. Among its variants, Vertical Federated Learning (VFL) is particularly relevant in real-world, cross-organizational collaborations, where distinct features of a shared instance group are contributed by different parties. In these scenarios, parties are often linked using fuzzy identifiers, leading to a common practice termed as multi-party fuzzy VFL. Existing models generally address either multi-party VFL or fuzzy VFL between two parties. Extending these models to practical multi-party fuzzy VFL typically results in significant performance degradation and increased costs for maintaining privacy. To overcome these limitations, we introduce the Federated Transformer (FeT), a novel framework that supports multi-party VFL with fuzzy identifiers. FeT innovatively encodes these identifiers into data representations and employs a transformer architecture distributed across different parties, incorporating three new techniques to enhance performance. Furthermore, we have developed a multi-party privacy framework for VFL that integrates differential privacy with secure multi-party computation, effectively protecting local representations while minimizing associated utility costs. Our experiments demonstrate that the FeT surpasses the baseline models by up to 46\\% in terms of accuracy when scaled to 50 parties. Additionally, in two-party fuzzy VFL settings, FeT also shows improved performance and privacy over cutting-edge VFL models.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18070",
        "abstract url": "https://arxiv.org/abs/2410.18070",
        "title": "Training Free Guided Flow Matching with Optimal Control",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Controlled generation with pre-trained Diffusion and Flow Matching models has vast applications. One strategy for guiding ODE-based generative models is through optimizing a target loss $R(x_1)$ while staying close to the prior distribution. Along this line, some recent work showed the effectiveness of guiding flow model by differentiating through its ODE sampling process. Despite the superior performance, the theoretical understanding of this line of methods is still preliminary, leaving space for algorithm improvement. Moreover, existing methods predominately focus on Euclidean data manifold, and there is a compelling need for guided flow methods on complex geometries such as SO(3), which prevails in high-stake scientific applications like protein design. We present OC-Flow, a general and theoretically grounded training-free framework for guided flow matching using optimal control. Building upon advances in optimal control theory, we develop effective and practical algorithms for solving optimal control in guided ODE-based generation and provide a systematic theoretical analysis of the convergence guarantee in both Euclidean and SO(3). We show that existing backprop-through-ODE methods can be interpreted as special cases of Euclidean OC-Flow. OC-Flow achieved superior performance in extensive experiments on text-guided image manipulation, conditional molecule generation, and all-atom peptide design.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18075",
        "abstract url": "https://arxiv.org/abs/2410.18075",
        "title": "ProFL: Performative Robust Optimal Federated Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Performative prediction (PP) is a framework that captures distribution shifts that occur during the training of machine learning models due to their deployment. As the trained model is used, its generated data could cause the model to evolve, leading to deviations from the original data distribution. The impact of such model-induced distribution shifts in the federated learning (FL) setup remains unexplored despite being increasingly likely to transpire in real-life use cases. Although Jin et al. (2024) recently extended PP to FL in a straightforward manner, the resulting model only converges to a performative stable point, which may be far from optimal. The methods in Izzo et al. (2021); Miller et al. (2021) can find a performative optimal point in centralized settings, but they require the performative risk to be convex and the training data to be noiseless, assumptions often violated in realistic FL systems. This paper overcomes all of these shortcomings and proposes Performative robust optimal Federated Learning (ProFL), an algorithm that finds performative optimal points in FL from noisy and contaminated data. We present the convergence analysis under the Polyak-Lojasiewicz condition, which applies to non-convex objectives. Extensive experiments on multiple datasets validate our proposed algorithms' efficiency.",
        "subjects": [
            "cs.LG",
            "cs.IT"
        ],
        "comment": "27 pages with Appendix, 18 figures. The paper has been submitted and is currently under review"
    },
    {
        "paper id": "2410.18076",
        "abstract url": "https://arxiv.org/abs/2410.18076",
        "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. In this work, we study how unlabeled prior trajectory data can be leveraged to learn efficient exploration strategies. While prior data can be used to pretrain a set of low-level skills, or as additional off-policy data for online RL, it has been unclear how to combine these ideas effectively for online exploration. Our method SUPE (Skills from Unlabeled Prior data for Exploration) demonstrates that a careful combination of these ideas compounds their benefits. Our method first extracts low-level skills using a variational autoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an optimistic reward model, transforming prior data into high-level, task-relevant examples. Finally, SUPE uses these transformed examples as additional off-policy data for online RL to learn a high-level policy that composes pretrained low-level skills to explore efficiently. We empirically show that SUPE reliably outperforms prior strategies, successfully solving a suite of long-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "23 pages, 10 figures"
    },
    {
        "paper id": "2410.18082",
        "abstract url": "https://arxiv.org/abs/2410.18082",
        "title": "Prioritized Generative Replay",
        "rating": "-0.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Sample-efficient online reinforcement learning often uses replay buffers to store experience for reuse when updating the value function. However, uniform replay is inefficient, since certain classes of transitions can be more relevant to learning. While prioritization of more useful samples is helpful, this strategy can also lead to overfitting, as useful samples are likely to be more rare. In this work, we instead propose a prioritized, parametric version of an agent's memory, using generative models to capture online experience. This paradigm enables (1) densification of past experience, with new generations that benefit from the generative model's generalization capacity and (2) guidance via a family of \"relevance functions\" that push these generations towards more useful parts of an agent's acquired history. We show this recipe can be instantiated using conditional diffusion models and simple relevance functions such as curiosity- or value-based metrics. Our approach consistently improves performance and sample efficiency in both state- and pixel-based domains. We expose the mechanisms underlying these gains, showing how guidance promotes diversity in our generated transitions and reduces overfitting. We also showcase how our approach can train policies with even higher update-to-data ratios than before, opening up avenues to better scale online RL agents.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18195",
        "abstract url": "https://arxiv.org/abs/2410.18195",
        "title": "Personalized Instance-based Navigation Toward User-Specific Objects in Realistic Environments",
        "rating": "-0.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "Navigation"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In the last years, the research interest in visual navigation towards objects in indoor environments has grown significantly. This growth can be attributed to the recent availability of large navigation datasets in photo-realistic simulated environments, like Gibson and Matterport3D. However, the navigation tasks supported by these datasets are often restricted to the objects present in the environment at acquisition time. Also, they fail to account for the realistic scenario in which the target object is a user-specific instance that can be easily confused with similar objects and may be found in multiple locations within the environment. To address these limitations, we propose a new task denominated Personalized Instance-based Navigation (PIN), in which an embodied agent is tasked with locating and reaching a specific personal object by distinguishing it among multiple instances of the same category. The task is accompanied by PInNED, a dedicated new dataset composed of photo-realistic scenes augmented with additional 3D objects. In each episode, the target object is presented to the agent using two modalities: a set of visual reference images on a neutral background and manually annotated textual descriptions. Through comprehensive evaluations and analyses, we showcase the challenges of the PIN task as well as the performance and shortcomings of currently available methods designed for object-driven navigation, considering modular and end-to-end agents.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "NeurIPS 2024 Datasets and Benchmarks Track. Project page: https://aimagelab.github.io/pin/"
    },
    {
        "paper id": "2410.18275",
        "abstract url": "https://arxiv.org/abs/2410.18275",
        "title": "Screw Geometry Meets Bandits: Incremental Acquisition of Demonstrations to Generate Manipulation Plans",
        "rating": "-0.5",
        "keywords": [
            [
                "robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In this paper, we study the problem of methodically obtaining a sufficient set of kinesthetic demonstrations, one at a time, such that a robot can be confident of its ability to perform a complex manipulation task in a given region of its workspace. Although Learning from Demonstrations has been an active area of research, the problems of checking whether a set of demonstrations is sufficient, and systematically seeking additional demonstrations have remained open. We present a novel approach to address these open problems using (i) a screw geometric representation to generate manipulation plans from demonstrations, which makes the sufficiency of a set of demonstrations measurable; (ii) a sampling strategy based on PAC-learning from multi-armed bandit optimization to evaluate the robot's ability to generate manipulation plans in a subregion of its task space; and (iii) a heuristic to seek additional demonstration from areas of weakness. Thus, we present an approach for the robot to incrementally and actively ask for new demonstration examples until the robot can assess with high confidence that it can perform the task successfully. We present experimental results on two example manipulation tasks, namely, pouring and scooping, to illustrate our approach. A short video on the method: https://youtu.be/R-qICICdEos",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "8 pages, 6 figures, under review in IEEE Robotics and Automation Letters"
    },
    {
        "paper id": "2410.18312",
        "abstract url": "https://arxiv.org/abs/2410.18312",
        "title": "Countering Autonomous Cyber Threats",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "With the capability to write convincing and fluent natural language and generate code, Foundation Models present dual-use concerns broadly and within the cyber domain specifically. Generative AI has already begun to impact cyberspace through a broad illicit marketplace for assisting malware development and social engineering attacks through hundreds of malicious-AI-as-a-services tools. More alarming is that recent research has shown the potential for these advanced models to inform or independently execute offensive cyberspace operations. However, these previous investigations primarily focused on the threats posed by proprietary models due to the until recent lack of strong open-weight model and additionally leave the impacts of network defenses or potential countermeasures unexplored. Critically, understanding the aptitude of downloadable models to function as offensive cyber agents is vital given that they are far more difficult to govern and prevent their misuse. As such, this work evaluates several state-of-the-art FMs on their ability to compromise machines in an isolated network and investigates defensive mechanisms to defeat such AI-powered attacks. Using target machines from a commercial provider, the most recently released downloadable models are found to be on par with a leading proprietary model at conducting simple cyber attacks with common hacking tools against known vulnerabilities. To mitigate such LLM-powered threats, defensive prompt injection (DPI) payloads for disrupting the malicious cyber agent's workflow are demonstrated to be effective. From these results, the implications for AI safety and governance with respect to cybersecurity is analyzed.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.CY"
        ],
        "comment": "76 pages, MPhil Thesis"
    },
    {
        "paper id": "2410.18333",
        "abstract url": "https://arxiv.org/abs/2410.18333",
        "title": "Search-Based Path Planning among Movable Obstacles",
        "rating": "-0.5",
        "keywords": [
            [
                "robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper investigates Path planning Among Movable Obstacles (PAMO), which seeks a minimum cost collision-free path among static obstacles from start to goal while allowing the robot to push away movable obstacles (i.e., objects) along its path when needed. To develop planners that are complete and optimal for PAMO, the planner has to search a giant state space involving both the location of the robot as well as the locations of the objects, which grows exponentially with respect to the number of objects. The main idea in this paper is that, only a small fraction of this giant state space needs to be explored during planning as guided by a heuristic, and most of the objects far away from the robot are intact, which thus leads to runtime efficient algorithms. Based on this idea, this paper introduces two PAMO formulations, i.e., bi-objective and resource constrained problems in an occupancy grid, and develops PAMO*, a search method with completeness and solution optimality guarantees, to solve the two problems. We then further extend PAMO* to hybrid-state PAMO* to plan in continuous spaces with high-fidelity interaction between the robot and the objects. Our results show that, PAMO* can often find optimal solutions within a second in cluttered environments with up to 400 objects.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18345",
        "abstract url": "https://arxiv.org/abs/2410.18345",
        "title": "Geometric Feature Enhanced Knowledge Graph Embedding and Spatial Reasoning",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Geospatial Knowledge Graphs (GeoKGs) model geoentities (e.g., places and natural features) and spatial relationships in an interconnected manner, providing strong knowledge support for geographic applications, including data retrieval, question-answering, and spatial reasoning. However, existing methods for mining and reasoning from GeoKGs, such as popular knowledge graph embedding (KGE) techniques, lack geographic awareness. This study aims to enhance general-purpose KGE by developing new strategies and integrating geometric features of spatial relations, including topology, direction, and distance, to infuse the embedding process with geographic intuition. The new model is tested on downstream link prediction tasks, and the results show that the inclusion of geometric features, particularly topology and direction, improves prediction accuracy for both geoentities and spatial relations. Our research offers new perspectives for integrating spatial concepts and principles into the GeoKG mining process, providing customized GeoAI solutions for geospatial challenges.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "4 pages, 1 figure, Accepted for the 7th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery"
    },
    {
        "paper id": "2410.18352",
        "abstract url": "https://arxiv.org/abs/2410.18352",
        "title": "FedBaF: Federated Learning Aggregation Biased by a Foundation Model",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Foundation models are now a major focus of leading technology organizations due to their ability to generalize across diverse tasks. Existing approaches for adapting foundation models to new applications often rely on Federated Learning (FL) and disclose the foundation model weights to clients when using it to initialize the global model. While these methods ensure client data privacy, they compromise model and information security. In this paper, we introduce Federated Learning Aggregation Biased by a Foundation Model (FedBaF), a novel method for dynamically integrating pre-trained foundation model weights during the FL aggregation phase. Unlike conventional methods, FedBaF preserves the confidentiality of the foundation model while still leveraging its power to train more accurate models, especially in non-IID and adversarial scenarios. Our comprehensive experiments use Pre-ResNet and foundation models like Vision Transformer to demonstrate that FedBaF not only matches, but often surpasses the test accuracy of traditional weight initialization methods by up to 11.4\\% in IID and up to 15.8\\% in non-IID settings. Additionally, FedBaF applied to a Transformer-based language model significantly reduced perplexity by up to 39.2\\%.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18374",
        "abstract url": "https://arxiv.org/abs/2410.18374",
        "title": "Integrating Canonical Neural Units and Multi-Scale Training for Handwritten Text Recognition",
        "rating": "-0.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The segmentation-free research efforts for addressing handwritten text recognition can be divided into three categories: connectionist temporal classification (CTC), hidden Markov model and encoder-decoder methods. In this paper, inspired by the above three modeling methods, we propose a new recognition network by using a novel three-dimensional (3D) attention module and global-local context information. Based on the feature maps of the last convolutional layer, a series of 3D blocks with different resolutions are split. Then, these 3D blocks are fed into the 3D attention module to generate sequential visual features. Finally, by integrating the visual features and the corresponding global-local context features, a well-designed representation can be obtained. Main canonical neural units including attention mechanisms, fully-connected layer, recurrent unit and convolutional layer are efficiently organized into a network and can be jointly trained by the CTC loss and the cross-entropy loss. Experiments on the latest Chinese handwritten text datasets (the SCUT-HCCDoc and the SCUT-EPT) and one English handwritten text dataset (the IAM) show that the proposed method can make a new milestone.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18393",
        "abstract url": "https://arxiv.org/abs/2410.18393",
        "title": "SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness",
        "rating": "-0.5",
        "keywords": [
            [
                "disease"
            ],
            [
                "cs.SI",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Social media is often the first place where communities discuss the latest societal trends. Prior works have utilized this platform to extract epidemic-related information (e.g. infections, preventive measures) to provide early warnings for epidemic prediction. However, these works only focused on English posts, while epidemics can occur anywhere in the world, and early discussions are often in the local, non-English languages. In this work, we introduce the first multilingual Event Extraction (EE) framework SPEED++ for extracting epidemic event information for a wide range of diseases and languages. To this end, we extend a previous epidemic ontology with 20 argument roles; and curate our multilingual EE dataset SPEED++ comprising 5.1K tweets in four languages for four diseases. Annotating data in every language is infeasible; thus we develop zero-shot cross-lingual cross-disease models (i.e., training only on English COVID data) utilizing multilingual pre-training and show their efficacy in extracting epidemic-related events for 65 diverse languages across different diseases. Experiments demonstrate that our framework can provide epidemic warnings for COVID-19 in its earliest stages in Dec 2019 (3 weeks before global discussions) from Chinese Weibo posts without any training in Chinese. Furthermore, we exploit our framework's argument extraction capabilities to aggregate community epidemic discussions like symptoms and cure measures, aiding misinformation detection and public attention monitoring. Overall, we lay a strong foundation for multilingual epidemic preparedness.",
        "subjects": [
            "cs.CL",
            "cs.SI"
        ],
        "comment": "Accepted at EMNLP 2024"
    },
    {
        "paper id": "2410.18396",
        "abstract url": "https://arxiv.org/abs/2410.18396",
        "title": "Revisiting Differentiable Structure Learning: Inconsistency of $\\ell_1$ Penalty and Beyond",
        "rating": "-0.5",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent advances in differentiable structure learning have framed the combinatorial problem of learning directed acyclic graphs as a continuous optimization problem. Various aspects, including data standardization, have been studied to identify factors that influence the empirical performance of these methods. In this work, we investigate critical limitations in differentiable structure learning methods, focusing on settings where the true structure can be identified up to Markov equivalence classes, particularly in the linear Gaussian case. While Ng et al. (2024) highlighted potential non-convexity issues in this setting, we demonstrate and explain why the use of $\\ell_1$-penalized likelihood in such cases is fundamentally inconsistent, even if the global optimum of the optimization problem can be found. To resolve this limitation, we develop a hybrid differentiable structure learning method based on $\\ell_0$-penalized likelihood with hard acyclicity constraint, where the $\\ell_0$ penalty can be approximated by different techniques including Gumbel-Softmax. Specifically, we first estimate the underlying moral graph, and use it to restrict the search space of the optimization problem, which helps alleviate the non-convexity issue. Experimental results show that the proposed method enhances empirical performance both before and after data standardization, providing a more reliable path for future advancements in differentiable structure learning, especially for learning Markov equivalence classes.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18416",
        "abstract url": "https://arxiv.org/abs/2410.18416",
        "title": "SkiLD: Unsupervised Skill Discovery Guided by Factor Interactions",
        "rating": "-0.5",
        "keywords": [
            [
                "robot"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Unsupervised skill discovery carries the promise that an intelligent agent can learn reusable skills through autonomous, reward-free environment interaction. Existing unsupervised skill discovery methods learn skills by encouraging distinguishable behaviors that cover diverse states. However, in complex environments with many state factors (e.g., household environments with many objects), learning skills that cover all possible states is impossible, and naively encouraging state diversity often leads to simple skills that are not ideal for solving downstream tasks. This work introduces Skill Discovery from Local Dependencies (Skild), which leverages state factorization as a natural inductive bias to guide the skill learning process. The key intuition guiding Skild is that skills that induce <b>diverse interactions</b> between state factors are often more valuable for solving downstream tasks. To this end, Skild develops a novel skill learning objective that explicitly encourages the mastering of skills that effectively induce different interactions within an environment. We evaluate Skild in several domains with challenging, long-horizon sparse reward tasks including a realistic simulated household robot domain, where Skild successfully learns skills with clear semantic meaning and shows superior performance compared to existing unsupervised reinforcement learning methods that only maximize state coverage.",
        "subjects": [
            "cs.LG",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03331",
        "abstract url": "https://arxiv.org/abs/2411.03331",
        "title": "Hypergraphs as Weighted Directed Self-Looped Graphs: Spectral Properties, Clustering, Cheeger Inequality",
        "rating": "-0.5",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Hypergraphs naturally arise when studying group relations and have been widely used in the field of machine learning. There has not been a unified formulation of hypergraphs, yet the recently proposed edge-dependent vertex weights (EDVW) modeling is one of the most generalized modeling methods of hypergraphs, i.e., most existing hypergraphs can be formulated as EDVW hypergraphs without any information loss to the best of our knowledge. However, the relevant algorithmic developments on EDVW hypergraphs remain nascent: compared to spectral graph theories, the formulations are incomplete, the spectral clustering algorithms are not well-developed, and one result regarding hypergraph Cheeger Inequality is even incorrect. To this end, deriving a unified random walk-based formulation, we propose our definitions of hypergraph Rayleigh Quotient, NCut, boundary/cut, volume, and conductance, which are consistent with the corresponding definitions on graphs. Then, we prove that the normalized hypergraph Laplacian is associated with the NCut value, which inspires our HyperClus-G algorithm for spectral clustering on EDVW hypergraphs. Finally, we prove that HyperClus-G can always find an approximately linearly optimal partitioning in terms of Both NCut and conductance. Additionally, we provide extensive experiments to validate our theoretical findings from an empirical perspective.",
        "subjects": [
            "cs.SI",
            "cs.DM",
            "cs.DS",
            "cs.LG"
        ],
        "comment": "Preprint, 31 pages"
    },
    {
        "paper id": "2411.03335",
        "abstract url": "https://arxiv.org/abs/2411.03335",
        "title": "Asymmetric Weighted Cascade Model for Competitive Influence Maximization",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "We introduce a modified Weighted Cascade model that integrates asymmetric budgets and product scores, providing new insights into the Generalized Asymmetric Influence Maximization problem, which we establish as NP-hard. Our simulations demonstrate that players with higher budgets possess a distinct advantage in networks characterized by larger diameters, whereas players with superior product scores exhibit a significant advantage in networks with smaller diameters. Moreover, we identify a robust linear relationship between graph size and the magnitude of influenced nodes. In densely connected networks we derive bounds for the probabilities of influence that are independent of network size. Our examination of Nash equilibria in this domain underscores the absence of a guaranteed pure Nash equilibrium, suggesting that the strategic enhancement of budgets or product scores may yield more substantial benefits than the pursuit of an optimal strategy in this context.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17563",
        "abstract url": "https://arxiv.org/abs/2410.17563",
        "title": "Efficiently Scheduling Parallel DAG Tasks on Identical Multiprocessors",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "Parallel real-time embedded applications can be modelled as directed acyclic graphs (DAGs) whose nodes model subtasks and whose edges model precedence constraints among subtasks. Efficiently scheduling such parallel tasks can be challenging in itself, particularly in hard real-time systems where it must be ensured offline that the deadlines of the parallel applications will be met at run time. In this paper, we tackle the problem of scheduling DAG tasks on identical multiprocessor systems efficiently, in terms of processor utilisation. We propose a new algorithm that attempts to use dedicated processor clusters for high-utilisation tasks, as in federated scheduling, but is also capable of reclaiming the processing capacity lost to fragmentation, by splitting the execution of parallel tasks over different existing clusters, in a manner inspired by semi-partitioned C=D scheduling (originally devised for non-parallel tasks). In the experiments with synthetic DAG task sets, our Segmented-Flattened-and-Split scheduling approach achieves a significantly higher scheduling success ratio than federated scheduling.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "Version submitted to RTNS 2024, on 16/08/2024 (with some typos fixed)"
    },
    {
        "paper id": "2410.17565",
        "abstract url": "https://arxiv.org/abs/2410.17565",
        "title": "Double Banking on Knowledge: Customized Modulation and Prototypes for Multi-Modality Semi-supervised Medical Image Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "Medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Multi-modality (MM) semi-supervised learning (SSL) based medical image segmentation has recently gained increasing attention for its ability to utilize MM data and reduce reliance on labeled images. However, current methods face several challenges: (1) Complex network designs hinder scalability to scenarios with more than two modalities. (2) Focusing solely on modality-invariant representation while neglecting modality-specific features, leads to incomplete MM learning. (3) Leveraging unlabeled data with generative methods can be unreliable for SSL. To address these problems, we propose Double Bank Dual Consistency (DBDC), a novel MM-SSL approach for medical image segmentation. To address challenge (1), we propose a modality all-in-one segmentation network that accommodates data from any number of modalities, removing the limitation on modality count. To address challenge (2), we design two learnable plug-in banks, Modality-Level Modulation bank (MLMB) and Modality-Level Prototype (MLPB) bank, to capture both modality-invariant and modality-specific knowledge. These banks are updated using our proposed Modality Prototype Contrastive Learning (MPCL). Additionally, we design Modality Adaptive Weighting (MAW) to dynamically adjust learning weights for each modality, ensuring balanced MM learning as different modalities learn at different rates. Finally, to address challenge (3), we introduce a Dual Consistency (DC) strategy that enforces consistency at both the image and feature levels without relying on generative methods. We evaluate our method on a 2-to-4 modality segmentation task using three open-source datasets, and extensive experiments show that our method outperforms state-of-the-art approaches.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17566",
        "abstract url": "https://arxiv.org/abs/2410.17566",
        "title": "Differentially Private Learning Needs Better Model Initialization and Self-Distillation",
        "rating": "-1",
        "keywords": [
            [
                "grammar"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Differentially private SGD (DPSGD) enables privacy-preserving training of language models, but often reduces utility, diversity, and linguistic quality. We introduce DPRefine, a three-phase method that initializes a model using data synthesis from a small pre-trained LM with rigorous filtering, applies DP finetuning on private data, and performs self-distillation to refine outputs. This approach significantly outperforms vanilla DPSGD, with AlpacaEval preferring DPRefine's generations in 78.4% of cases across all datasets. Our analysis reveals that DPRefine reduces linguistic errors in generated text by 84.0%, mitigating grammar and spelling errors, commonly associated with DPSGD. It also reduces inconsistencies of non-private models, such as hallucinated details and misattributed quotes. We find that small models like GPT-2 can be effective for initialization and distillation, highlighting their potential in enabling scalable and efficient deployment of privacy-preserving language.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CR"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2410.17584",
        "abstract url": "https://arxiv.org/abs/2410.17584",
        "title": "Exploring Tokenization Methods for Multitrack Sheet Music Generation",
        "rating": "-1",
        "keywords": [
            [
                "Music"
            ],
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "This study explores the tokenization of multitrack sheet music in ABC notation, introducing two methods--bar-stream and line-stream patching. We compare these methods against existing techniques, including bar patching, byte patching, and Byte Pair Encoding (BPE). In terms of both computational efficiency and the musicality of the generated compositions, experimental results show that bar-stream patching performs best overall compared to the others, which makes it a promising tokenization strategy for sheet music generation.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "eess.AS"
        ],
        "comment": "3 pages, 1 figure, 1 table"
    },
    {
        "paper id": "2410.17610",
        "abstract url": "https://arxiv.org/abs/2410.17610",
        "title": "ImDy: Human Inverse Dynamics from Imitated Observations",
        "rating": "-1",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Inverse dynamics (ID), which aims at reproducing the driven torques from human kinematic observations, has been a critical tool for gait analysis. However, it is hindered from wider application to general motion due to its limited scalability. Conventional optimization-based ID requires expensive laboratory setups, restricting its availability. To alleviate this problem, we propose to exploit the recently progressive human motion imitation algorithms to learn human inverse dynamics in a data-driven manner. The key insight is that the human ID knowledge is implicitly possessed by motion imitators, though not directly applicable. In light of this, we devise an efficient data collection pipeline with state-of-the-art motion imitation algorithms and physics simulators, resulting in a large-scale human inverse dynamics benchmark as Imitated Dynamics (ImDy). ImDy contains over 150 hours of motion with joint torque and full-body ground reaction force data. With ImDy, we train a data-driven human inverse dynamics solver ImDyS(olver) in a fully supervised manner, which conducts ID and ground reaction force estimation simultaneously. Experiments on ImDy and real-world data demonstrate the impressive competency of ImDyS in human inverse dynamics and ground reaction force estimation. Moreover, the potential of ImDy(-S) as a fundamental motion analysis tool is exhibited with downstream applications. The project page is https://foruck.github.io/ImDy/.",
        "subjects": [
            "cs.AI",
            "cs.CV",
            "cs.GR",
            "cs.RO"
        ],
        "comment": "Yong-Lu Li and Cewu Lu are the corresponding authors"
    },
    {
        "paper id": "2410.17618",
        "abstract url": "https://arxiv.org/abs/2410.17618",
        "title": "V2V Path Loss Modeling at 26 GHz Based on Real-Traffic Measurements",
        "rating": "-1",
        "keywords": [
            [
                "vehicle"
            ]
        ],
        "abstract": "In this letter, we investigate single-slope path loss models complemented with shadowing effects in the context of vehicular communications. We present several models obtained based on extensive measurement campaigns with inter-vehicle transmission conducted at 26.555 GHz in real-traffic experiments, mainly along high-speed roads. Particular attention has been put on the impact of aerial characteristics (omnidirectional versus directional), surrounding environment (e.g., urban versus rural), and their mounting point on cars (at the rooftop, on the bumper, and below the car chassis). Finally, the effect of signal ducting and of the number of blocking cars has been analyzed and the decorrelation time has been discussed",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17622",
        "abstract url": "https://arxiv.org/abs/2410.17622",
        "title": "Bridging the Gaps: Utilizing Unlabeled Face Recognition Datasets to Boost Semi-Supervised Facial Expression Recognition",
        "rating": "-1",
        "keywords": [
            [
                "Facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, Facial Expression Recognition (FER) has gained increasing attention. Most current work focuses on supervised learning, which requires a large amount of labeled and diverse images, while FER suffers from the scarcity of large, diverse datasets and annotation difficulty. To address these problems, we focus on utilizing large unlabeled Face Recognition (FR) datasets to boost semi-supervised FER. Specifically, we first perform face reconstruction pre-training on large-scale facial images without annotations to learn features of facial geometry and expression regions, followed by two-stage fine-tuning on FER datasets with limited labels. In addition, to further alleviate the scarcity of labeled and diverse images, we propose a Mixup-based data augmentation strategy tailored for facial images, and the loss weights of real and virtual images are determined according to the intersection-over-union (IoU) of the faces in the two images. Experiments on RAF-DB, AffectNet, and FERPlus show that our method outperforms existing semi-supervised FER methods and achieves new state-of-the-art performance. Remarkably, with only 5%, 25% training sets,our method achieves 64.02% on AffectNet,and 88.23% on RAF-DB, which is comparable to fully supervised state-of-the-art methods. Codes will be made publicly available at https://github.com/zhelishisongjie/SSFER.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17641",
        "abstract url": "https://arxiv.org/abs/2410.17641",
        "title": "Fixed-Parameter Tractability of Hedge Cut",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "In the Hedge Cut problem, the edges of a graph are partitioned into groups called hedges, and the question is what is the minimum number of hedges to delete to disconnect the graph. Ghaffari, Karger, and Panigrahi [SODA 2017] showed that Hedge Cut can be solved in quasipolynomial-time, raising the hope for a polynomial time algorithm. Jaffke, Lima, Masar\u00edk, Pilipczuk, and Souza [SODA 2023] complemented this result by showing that assuming the Exponential Time Hypothesis (ETH), no polynomial-time algorithm exists. In this paper, we show that Hedge Cut is fixed-parameter tractable parameterized by the solution size $\\ell$ by providing an algorithm with running time $\\binom{O(\\log n) + \\ell}{\\ell} \\cdot m^{O(1)}$, which can be upper bounded by $c^{\\ell} \\cdot (n+m)^{O(1)}$ for any constant $c>1$. This running time captures at the same time the fact that the problem is quasipolynomial-time solvable, and that it is fixed-parameter tractable parameterized by $\\ell$. We further generalize this algorithm to an algorithm with running time $\\binom{O(k \\log n) + \\ell}{\\ell} \\cdot n^{O(k)} \\cdot m^{O(1)}$ for Hedge $k$-Cut.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "12 pages, 1 figure, to appear in SODA 2025"
    },
    {
        "paper id": "2410.17642",
        "abstract url": "https://arxiv.org/abs/2410.17642",
        "title": "Surgical Scene Segmentation by Transformer With Asymmetric Feature Enhancement",
        "rating": "-1",
        "keywords": [
            [
                "Surgical",
                "surgery"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Surgical scene segmentation is a fundamental task for robotic-assisted laparoscopic surgery understanding. It often contains various anatomical structures and surgical instruments, where similar local textures and fine-grained structures make the segmentation a difficult task. Vision-specific transformer method is a promising way for surgical scene understanding. However, there are still two main challenges. Firstly, the absence of inner-patch information fusion leads to poor segmentation performance. Secondly, the specific characteristics of anatomy and instruments are not specifically modeled. To tackle the above challenges, we propose a novel Transformer-based framework with an Asymmetric Feature Enhancement module (TAFE), which enhances local information and then actively fuses the improved feature pyramid into the embeddings from transformer encoders by a multi-scale interaction attention strategy. The proposed method outperforms the SOTA methods in several different surgical segmentation tasks and additionally proves its ability of fine-grained structure recognition. Code is available at https://github.com/cyuan-sjtu/ViT-asym.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17657",
        "abstract url": "https://arxiv.org/abs/2410.17657",
        "title": "ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "Clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have shown promising potential in the medical domain, assisting with tasks like clinical note generation and patient communication. However, current LLMs are limited to text-based communication, hindering their ability to interact with diverse forms of information in clinical environments. Despite clinical agents succeeding in diverse signal interaction, they are oriented to a single clinical scenario and hence fail for broader applications. To evaluate clinical agents holistically, we propose ClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting of 18 tasks across five key realistic clinical dimensions. Building on this, we introduce ReflecTool, a novel framework that excels at utilizing domain-specific tools within two stages. The first optimization stage progressively enlarges a long-term memory by saving successful solving processes and tool-wise experience of agents in a tiny pre-defined training set. In the following inference stage, ReflecTool can search for supportive successful demonstrations from already built long-term memory to guide the tool selection strategy, and a verifier improves the tool usage according to the tool-wise experience with two verification methods--iterative refinement and candidate selection. Extensive experiments on ClinicalAgent Benchmark demonstrate that ReflecTool surpasses the pure LLMs with more than 10 points and the well-established agent-based methods with 3 points, highlighting its adaptability and effectiveness in solving complex clinical tasks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "20 pages"
    },
    {
        "paper id": "2410.17678",
        "abstract url": "https://arxiv.org/abs/2410.17678",
        "title": "$k$-Hyperopic Cops and Robber",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "A generalization of hyperopic cops and robber, analogous to the $k$-visibility cops and robber, is introduced in this paper. For a positive integer $k$ the $k$-hyperopic game of cops and robber is defined similarly as the usual cops and robber game, but with the robber being omniscient and invisible to the cops that are at distance at most $k$ away from the robber. The cops win the game if, after a finite number of rounds, a cop occupies the same vertex as robber. Otherwise, robber wins. The minimum number of cops needed to win the game on a graph $G$ is the $k$-hyperopic cop number $c_{H,k}(G)$ of $G$. In addition to basic properties of the new invariant, cop-win graphs are characterized and a general upper bound in terms of the matching number of the graph is given. The invariant is also studied on trees where the upper bounds mostly depend on the relation between $k$ and the diameter of the tree. It is also proven that the 2-hyperopic cop number of outerplanar graphs is at most 2 and an upper bound in terms of the number of vertices of the graph is presented for $k \\geq 3$.",
        "subjects": [
            "math.CO",
            "cs.DM"
        ],
        "comment": "20 pages, 4 figures"
    },
    {
        "paper id": "2410.17690",
        "abstract url": "https://arxiv.org/abs/2410.17690",
        "title": "Markov Potential Game with Final-time Reach-Avoid Objectives",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "We formulate a Markov potential game with final-time reach-avoid objectives by integrating potential game theory with stochastic reach-avoid control. Our focus is on multi-player trajectory planning where players maximize the same multi-player reach-avoid objective: the probability of all participants reaching their designated target states by a specified time, while avoiding collisions with one another. Existing approaches require centralized computation of actions via a global policy, which may have prohibitively expensive communication costs. Instead, we focus on approximations of the global policy via local state feedback policies. First, we adapt the recursive single player reach-avoid value iteration to the multi-player framework with local policies, and show that the same recursion holds on the joint state space. To find each player's optimal local policy, the multi-player reach-avoid value function is projected from the joint state to the local state using the other players' occupancy measures. Then, we propose an iterative best response scheme for the multi-player value iteration to converge to a pure Nash equilibrium. We demonstrate the utility of our approach in finding collision-free policies for multi-player motion planning in simulation.",
        "subjects": [
            "eess.SY",
            "cs.GT",
            "cs.MA",
            "cs.RO"
        ],
        "comment": "8 pages, 2 figures"
    },
    {
        "paper id": "2410.17735",
        "abstract url": "https://arxiv.org/abs/2410.17735",
        "title": "New Insight in Cervical Cancer Diagnosis Using Convolution Neural Network Architecture",
        "rating": "-1",
        "keywords": [
            [
                "Diagnosis",
                "Cancer"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The Pap smear is a screening method for early cervical cancer diagnosis. The selection of the right optimizer in the convolutional neural network (CNN) model is key to the success of the CNN in image classification, including the classification of cervical cancer Pap smear images. In this study, stochastic gradient descent (SGD), RMSprop, Adam, AdaGrad, AdaDelta, Adamax, and Nadam optimizers were used to classify cervical cancer Pap smear images from the SipakMed dataset. Resnet-18, Resnet-34, and VGG-16 are the CNN architectures used in this study, and each architecture uses a transfer-learning model. Based on the test results, we conclude that the transfer learning model performs better on all CNNs and optimization techniques and that in the transfer learning model, the optimization has little influence on the training of the model. Adamax, with accuracy values of 72.8% and 66.8%, had the best accuracy for the VGG-16 and Resnet-18 architectures, respectively. Resnet-34 had 54.0%. This is 0.034% lower than Nadam. Overall, Adamax is a suitable optimizer for CNN in cervical cancer classification on Resnet-18, Resnet-34, and VGG-16 architectures. This study provides new insights into the configuration of CNN models for Pap smear image analysis.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17740",
        "abstract url": "https://arxiv.org/abs/2410.17740",
        "title": "Emotion Recognition with Facial Attention and Objective Activation Functions",
        "rating": "-1",
        "keywords": [
            [
                "Facial"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we study the effect of introducing channel and spatial attention mechanisms, namely SEN-Net, ECA-Net, and CBAM, to existing CNN vision-based models such as VGGNet, ResNet, and ResNetV2 to perform the Facial Emotion Recognition task. We show that not only attention can significantly improve the performance of these models but also that combining them with a different activation function can further help increase the performance of these models.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17777",
        "abstract url": "https://arxiv.org/abs/2410.17777",
        "title": "Tight Bounds for Online Balanced Partitioning in the Generalized Learning Model",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Resource allocation in distributed and networked systems such as the Cloud is becoming increasingly flexible, allowing these systems to dynamically adjust toward the workloads they serve, in a demand-aware manner. Online balanced partitioning is a fundamental optimization problem underlying such self-adjusting systems. We are given a set of $\\ell$ servers. On each server we can schedule up to $k$ processes simultaneously. The demand is described as a sequence of requests $\u03c3_t=\\{p_i, p_{j}\\}$, which means that the two processes $p_i,p_{j}$ communicate. A process can be migrated from one server to another which costs 1 unit per process move. If the communicating processes are on different servers, it further incurs a communication cost of 1 unit for this request. The objective is to minimize the competitive ratio: the cost of serving such a request sequence compared to the cost incurred by an optimal offline algorithm. Henzinger et al. (at SIGMETRICS 2019) introduced a learning variant of this problem where the cost of an online algorithm is compared to the cost of a static offline algorithm that does not perform any communication, but which simply learns the communication graph and keeps the discovered connected components together. This problem variant was recently also studied at SODA 2021. In this paper, we consider a more general learning model (i.e., stronger adversary), where the offline algorithm is not restricted to keep connected components together. Our main contribution are tight bounds for this problem. In particular, we present two deterministic online algorithms: (1) an online algorithm with competitive ratio $O(\\max(\\sqrt{k\\ell \\log k}, \\ell \\log k))$ and augmentation $1+\u03b5$; (2) an online algorithm with competitive ratio $O(\\sqrt{k})$ and augmentation $2+\u03b5$. We further present lower bounds showing optimality of these bounds.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17790",
        "abstract url": "https://arxiv.org/abs/2410.17790",
        "title": "Regularized autoregressive modeling and its application to audio signal declipping",
        "rating": "-1",
        "keywords": [
            [
                "patent"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Autoregressive (AR) modeling is invaluable in signal processing, in particular in speech and audio fields. Attempts in the literature can be found that regularize or constrain either the time-domain signal values or the AR coefficients, which is done for various reasons, including the incorporation of prior information or numerical stabilization. Although these attempts are appealing, an encompassing and generic modeling framework is still missing. We propose such a framework and the related optimization problem and algorithm. We discuss the computational demands of the algorithm and explore the effects of various improvements on its convergence speed. In the experimental part, we demonstrate the usefulness of our approach on the audio declipping problem. We compare its performance against the state-of-the-art methods and demonstrate the competitiveness of the proposed method, especially for mildly clipped signals. The evaluation is extended by considering a heuristic algorithm of generalized linear prediction (GLP), a strong competitor which has only been presented as a patent and is new in the scientific community.",
        "subjects": [
            "eess.AS",
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17827",
        "abstract url": "https://arxiv.org/abs/2410.17827",
        "title": "RE-tune: Incremental Fine Tuning of Biomedical Vision-Language Models for Multi-label Chest X-ray Classification",
        "rating": "-1",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "trajectory"
            ],
            [
                "Biomedical",
                "healthcare",
                "diagnosis",
                "X-ray",
                "disease"
            ],
            [
                "cs.AI"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In this paper we introduce RE-tune, a novel approach for fine-tuning pre-trained Multimodal Biomedical Vision-Language models (VLMs) in Incremental Learning scenarios for multi-label chest disease diagnosis. RE-tune freezes the backbones and only trains simple adaptors on top of the Image and Text encoders of the VLM. By engineering positive and negative text prompts for diseases, we leverage the ability of Large Language Models to steer the training trajectory. We evaluate RE-tune in three realistic incremental learning scenarios: class-incremental, label-incremental, and data-incremental. Our results demonstrate that Biomedical VLMs are natural continual learners and prevent catastrophic forgetting. RE-tune not only achieves accurate multi-label classification results, but also prioritizes patient privacy and it distinguishes itself through exceptional computational efficiency, rendering it highly suitable for broad adoption in real-world healthcare settings.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "Accepted for publication at Medical Imaging meets NeurIPS (NeurIPS23)"
    },
    {
        "paper id": "2410.17936",
        "abstract url": "https://arxiv.org/abs/2410.17936",
        "title": "Reconfigurable Hydrostatics: Toward Multifunctional and Powerful Wearable Robotics",
        "rating": "-1",
        "keywords": [
            [
                "Robotics",
                "robot"
            ]
        ],
        "abstract": "Wearable and locomotive robot designers face multiple challenges when choosing actuation. Traditional fully actuated designs using electric motors are multifunctional but oversized and inefficient for bearing conservative loads and for being backdrivable. Alternatively, quasi-passive and underactuated designs reduce the size of motorization and energy storage, but are often designed for specific tasks. Designers of versatile and stronger wearable robots will face these challenges unless future actuators become very torque-dense, backdrivable and efficient. This paper explores a design paradigm for addressing this issue: reconfigurable hydrostatics. We show that a hydrostatic actuator can integrate a passive force mechanism and a sharing mechanism in the fluid domain and still be multifunctional. First, an analytical study compares how these two mechanisms can relax the motorization requirements in the context of a load-bearing exoskeleton. Then, the hydrostatic concept integrating these two mechanisms using hydraulic components is presented. A case study analysis shows the mass/efficiency/inertia benefits of the concept over a fully actuated one. Then, the feasibility of the concept is partially validated with a proof-of-concept that actuates the knees of an exoskeleton. The experiments show that it can track the vertical ground reaction force (GRF) profiles of walking, running, squatting, and jumping, and that the energy consumption is 6x lower. The transient force behaviors due to switching from one leg to the other are also analyzed along with some mitigation to improve them.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17980",
        "abstract url": "https://arxiv.org/abs/2410.17980",
        "title": "Stick-breaking Attention",
        "rating": "-1",
        "keywords": [
            [
                "grammar"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The self-attention mechanism traditionally relies on the softmax operator, necessitating positional embeddings like RoPE, or position biases to account for token order. But current methods using still face length generalisation challenges. We propose an alternative attention mechanism based on the stick-breaking process: For each token before the current, we determine a break point $\u03b2_{i,j}$, which represents the proportion of the remaining stick to allocate to the current token. We repeat the process until the stick is fully allocated, resulting in a sequence of attention weights. This process naturally incorporates recency bias, which has linguistic motivations for grammar parsing (Shen et. al., 2017). We study the implications of replacing the conventional softmax-based attention mechanism with stick-breaking attention. We then discuss implementation of numerically stable stick-breaking attention and adapt Flash Attention to accommodate this mechanism. When used as a drop-in replacement for current softmax+RoPE attention systems, we find that stick-breaking attention performs competitively with current methods on length generalisation and downstream tasks. Stick-breaking also performs well at length generalisation, allowing a model trained with $2^{11}$ context window to perform well at $2^{14}$ with perplexity improvements.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17988",
        "abstract url": "https://arxiv.org/abs/2410.17988",
        "title": "A Pipeline for Segmenting and Structuring RGB-D Data for Robotics Applications",
        "rating": "-1",
        "keywords": [
            [
                "RGB-D",
                "depth"
            ],
            [
                "Robotics",
                "navigation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We introduce a novel pipeline for segmenting and structuring color and depth (RGB-D) data. Existing processing pipelines for RGB-D data have focused on extracting geometric information alone. This approach precludes the development of more advanced robotic navigation and manipulation algorithms, which benefit from a semantic understanding of their environment. Our pipeline can segment RGB-D data into accurate semantic masks. These masks are then used to fuse raw captured point clouds into semantically separated point clouds. We store this information using the Universal Scene Description (USD) file format, a format suitable for easy querying by downstream robotics algorithms, human-friendly visualization, and robotics simulation.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18007",
        "abstract url": "https://arxiv.org/abs/2410.18007",
        "title": "Effective Finite Time Stability Control for Human-Machine Shared Vehicle Following System",
        "rating": "-1",
        "keywords": [
            [
                "Vehicle"
            ]
        ],
        "abstract": "With the development of intelligent connected vehicle technology, human-machine shared control has gained popularity in vehicle following due to its effectiveness in driver assistance. However, traditional vehicle following systems struggle to maintain stability when driver reaction time fluctuates, as these variations require different levels of system intervention. To address this issue, the proposed human-machine shared vehicle following assistance system (HM-VFAS) integrates driver outputs under various states with the assistance system. The system employs an intelligent driver model that accounts for reaction time delays, simulating time-varying driver outputs. A control authority allocation strategy is designed to dynamically adjust the level of intervention based on real-time driver state assessment. To handle instability from driver authority switching, the proposed solution includes a two-layer adaptive finite time sliding mode controller (A-FTSMC). The first layer is an integral sliding mode adaptive controller that ensures robustness by compensating for uncertainties in the driver output. The second layer is a fast non-singular terminal sliding mode controller designed to accelerate convergence for rapid stabilization. Using real driver videos as inputs, the performance of the HM-VFAS was evaluated. Results show that the proposed control strategy maintains a safe distance under time-varying driver states, with the actual acceleration error relative to the target acceleration maintained within 0.5m/s~2 and the maximum acceleration error reduced by 1.2m/s~2. Compared to traditional controllers, the A-FTSMC controller offers faster convergence and less vibration, reducing the stabilization time by 27.3%.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18079",
        "abstract url": "https://arxiv.org/abs/2410.18079",
        "title": "FreeVS: Generative View Synthesis on Free Driving Trajectory",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "Trajectory",
                "vehicle"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Existing reconstruction-based novel view synthesis methods for driving scenes focus on synthesizing camera views along the recorded trajectory of the ego vehicle. Their image rendering performance will severely degrade on viewpoints falling out of the recorded trajectory, where camera rays are untrained. We propose FreeVS, a novel fully generative approach that can synthesize camera views on free new trajectories in real driving scenes. To control the generation results to be 3D consistent with the real scenes and accurate in viewpoint pose, we propose the pseudo-image representation of view priors to control the generation process. Viewpoint transformation simulation is applied on pseudo-images to simulate camera movement in each direction. Once trained, FreeVS can be applied to any validation sequences without reconstruction process and synthesis views on novel trajectories. Moreover, we propose two new challenging benchmarks tailored to driving scenes, which are novel camera synthesis and novel trajectory synthesis, emphasizing the freedom of viewpoints. Given that no ground truth images are available on novel trajectories, we also propose to evaluate the consistency of images synthesized on novel trajectories with 3D perception models. Experiments on the Waymo Open Dataset show that FreeVS has a strong image synthesis performance on both the recorded trajectories and novel trajectories. Project Page: https://freevs24.github.io/",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://freevs24.github.io/"
    },
    {
        "paper id": "2410.18084",
        "abstract url": "https://arxiv.org/abs/2410.18084",
        "title": "DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes",
        "rating": "-1",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "3D"
            ],
            [
                "diffusion",
                "inpainting"
            ],
            [
                "trajectory",
                "LiDAR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "LiDAR scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D LiDAR generation framework capable of generating large-scale, high-quality LiDAR scenes that capture the temporal evolution of dynamic environments. DynamicCity mainly consists of two key models. 1) A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novel Projection Module to effectively compress 4D LiDAR features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, we utilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06x training speedup, and 70.84% memory reduction). 2) A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, a Padded Rollout Operation is proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting versatile 4D generation applications, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D LiDAR generation methods across multiple metrics. The code will be released to facilitate future research.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "Preprint; 29 pages, 15 figures, 7 tables; Project Page at https://dynamic-city.github.io/"
    },
    {
        "paper id": "2410.18153",
        "abstract url": "https://arxiv.org/abs/2410.18153",
        "title": "Physics-informed Neural Networks for Functional Differential Equations: Cylindrical Approximation and Its Convergence Guarantees",
        "rating": "-1",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.AI"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We propose the first learning scheme for functional differential equations (FDEs). FDEs play a fundamental role in physics, mathematics, and optimal control. However, the numerical analysis of FDEs has faced challenges due to its unrealistic computational costs and has been a long standing problem over decades. Thus, numerical approximations of FDEs have been developed, but they often oversimplify the solutions. To tackle these two issues, we propose a hybrid approach combining physics-informed neural networks (PINNs) with the \\textit{cylindrical approximation}. The cylindrical approximation expands functions and functional derivatives with an orthonormal basis and transforms FDEs into high-dimensional PDEs. To validate the reliability of the cylindrical approximation for FDE applications, we prove the convergence theorems of approximated functional derivatives and solutions. Then, the derived high-dimensional PDEs are numerically solved with PINNs. Through the capabilities of PINNs, our approach can handle a broader class of functional derivatives more efficiently than conventional discretization-based methods, improving the scalability of the cylindrical approximation. As a proof of concept, we conduct experiments on two FDEs and demonstrate that our model can successfully achieve typical $L^1$ relative error orders of PINNs $\\sim 10^{-3}$. Overall, our work provides a strong backbone for physicists, mathematicians, and machine learning experts to analyze previously challenging FDEs, thereby democratizing their numerical analysis, which has received limited attention. Code is available at \\url{https://github.com/TaikiMiyagawa/FunctionalPINN}.",
        "subjects": [
            "math.NA",
            "cond-mat.dis-nn",
            "cs.AI",
            "hep-th",
            "stat.ML"
        ],
        "comment": "Accepted at NeurIPS 2024. Both authors contributed equally. Some contents are omitted due to arXiv's storage limit. Please refer to the full paper at OpenReview (NeurIPS 2024) or https://github.com/TaikiMiyagawa/FunctionalPINN"
    },
    {
        "paper id": "2410.18203",
        "abstract url": "https://arxiv.org/abs/2410.18203",
        "title": "Vocal Melody Construction for Persian Lyrics Using LSTM Recurrent Neural Networks",
        "rating": "-1",
        "keywords": [
            [
                "song",
                "music"
            ],
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "The present paper investigated automatic melody construction for Persian lyrics as an input. It was assumed that there is a phonological correlation between the lyric syllables and the melody in a song. A seq2seq neural network was developed to investigate this assumption, trained on parallel syllable and note sequences in Persian songs to suggest a pleasant melody for a new sequence of syllables. More than 100 pieces of Persian music were collected and converted from the printed version to the digital format due to the lack of a dataset on Persian digital music. Finally, 14 new lyrics were given to the model as input, and the suggested melodies were performed and recorded by music experts to evaluate the trained model. The evaluation was conducted using an audio questionnaire, which more than 170 persons answered. According to the answers about the pleasantness of melody, the system outputs scored an average of 3.005 from 5, while the human-made melodies for the same lyrics obtained an average score of 4.078.",
        "subjects": [
            "cs.SD",
            "cs.LG",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18217",
        "abstract url": "https://arxiv.org/abs/2410.18217",
        "title": "A Methodology for Transformer Ratio Adjustment in Small-Size Rotary Transformers",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "This study addresses a neglected challenge that has been hidden in the Rotary Transformer (RT) field: the possibility of a discrepancy between transformer ratio and turn number ratio in small-size transformers. Previous investigations have shown that in the geometry design of RTs, as well as their resonant circuit design, the transformer ratio has been regarded as the same as the turn number ratio. This estimation is logical and true when a large-size RT is investigated. However, in small-size RTs, the magnitudes of leakage and magnetization inductances are significantly close, which leads to a difference between transformer ratio and turn number ratio. Accordingly, the absence of an exact methodology for transformer ratio calculation brought us to conduct this investigation. In this regard, a transformer ratio adjustment is suggested after proposing a low-error magnetic model. Its accuracy is high enough to consider different air gaps and subsequently calculate inductance with reference to 3D finite element analysis (3D-FEA). Finally, we take advantage of a test bench to show the exactness and proficiency of the suggested transformer ratio adjustment.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18223",
        "abstract url": "https://arxiv.org/abs/2410.18223",
        "title": "Locally seeded embeddings, and Ramsey numbers of bipartite graphs with sublinear bandwidth",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "A seminal result of Lee asserts that the Ramsey number of any bipartite $d$-degenerate graph $H$ satisfies $\\log r(H) = \\log n + O(d)$. In particular, this bound applies to every bipartite graph of maximal degree $\u0394$. It remains a compelling challenge to identify conditions that guarantee that an $n$-vertex graph $H$ has Ramsey number linear in $n$, independently of $\u0394$. Our contribution is a characterization of bipartite graphs with linear-size Ramsey numbers in terms of graph bandwidth, a notion of local connectivity. We prove that for any $n$-vertex bipartite graph $H$ with maximal degree at most $\u0394$ and bandwidth $b(H)$ at most $\\exp(-C\u0394\\log\u0394)\\,n$, we have $\\log r(H) = \\log n + O(1)$. This characterization is nearly optimal: for every $\u0394$ there exists an $n$-vertex bipartite graph $H$ of degree at most $\u0394$ and $b(H) \\leq \\exp(-c\u0394)\\,n$, such that $\\log r(H) = \\log n + \u03a9(\u0394)$. We also provide bounds interpolating between these two bandwidth regimes.",
        "subjects": [
            "math.CO",
            "cs.DM",
            "cs.DS"
        ],
        "comment": "Main result of the paper is known (due to Choongbum Lee, arXiv:1504.06285)"
    },
    {
        "paper id": "2410.18225",
        "abstract url": "https://arxiv.org/abs/2410.18225",
        "title": "Generalizations across filler-gap dependencies in neural language models",
        "rating": "-1",
        "keywords": [
            [
                "grammatical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Humans develop their grammars by making structural generalizations from finite input. We ask how filler-gap dependencies, which share a structural generalization despite diverse surface forms, might arise from the input. We explicitly control the input to a neural language model (NLM) to uncover whether the model posits a shared representation for filler-gap dependencies. We show that while NLMs do have success differentiating grammatical from ungrammatical filler-gap dependencies, they rely on superficial properties of the input, rather than on a shared generalization. Our work highlights the need for specific linguistic inductive biases to model language acquisition.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "accepted at CoNLL 2024"
    },
    {
        "paper id": "2410.18228",
        "abstract url": "https://arxiv.org/abs/2410.18228",
        "title": "MsMorph: An Unsupervised pyramid learning network for brain image registration",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "MRI"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In the field of medical image analysis, image registration is a crucial technique. Despite the numerous registration models that have been proposed, existing methods still fall short in terms of accuracy and interpretability. In this paper, we present MsMorph, a deep learning-based image registration framework aimed at mimicking the manual process of registering image pairs to achieve more similar deformations, where the registered image pairs exhibit consistency or similarity in features. By extracting the feature differences between image pairs across various as-pects using gradients, the framework decodes semantic information at different scales and continuously compen-sates for the predicted deformation field, driving the optimization of parameters to significantly improve registration accuracy. The proposed method simulates the manual approach to registration, focusing on different regions of the image pairs and their neighborhoods to predict the deformation field between the two images, which provides strong interpretability. We compared several existing registration methods on two public brain MRI datasets, including LPBA and Mindboggle. The experimental results show that our method consistently outperforms state of the art in terms of metrics such as Dice score, Hausdorff distance, average symmetric surface distance, and non-Jacobian. The source code is publicly available at https://github.com/GaodengFan/MsMorph",
        "subjects": [
            "cs.CV"
        ],
        "comment": "18 pages, 10 figures"
    },
    {
        "paper id": "2410.18239",
        "abstract url": "https://arxiv.org/abs/2410.18239",
        "title": "E2E-Swin-Unet++: An Enhanced End-to-End Swin-Unet Architecture With Dual Decoders For PTMC Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "surgery",
                "clinical"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Efficiently managing papillary thyroid microcarcinoma (PTMC) while minimizing patient discomfort poses a significant clinical challenge. Radiofrequency ablation (RFA) offers a less invasive alternative to surgery and radiation therapy for PTMC treatment, characterized by shorter recovery times and reduced pain. As an image-guided procedure, RFA generates localized heat by delivering high-frequency electrical currents through electrodes to the targeted area under ultrasound imaging guidance. However, the precision and skill required by operators for accurate guidance using current ultrasound B-mode imaging technologies remain significant challenges. To address these challenges, we develop a novel AI segmentation model, E2E-Swin-Unet++. This model enhances ultrasound B-mode imaging by enabling real-time identification and segmentation of PTMC tumors and monitoring of the region of interest for precise targeting during treatment. E2E-Swin- Unet++ is an advanced end-to-end extension of the Swin-Unet architecture, incorporating thyroid region information to minimize the risk of false PTMC segmentation while providing fast inference capabilities. Experimental results on a real clinical RFA dataset demonstrate the superior performance of E2E-Swin-Unet++ compared to related models. Our proposed solution significantly improves the precision and control of RFA ablation treatment by enabling real-time identification and segmentation of PTMC margins during the procedure.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18270",
        "abstract url": "https://arxiv.org/abs/2410.18270",
        "title": "Multilingual Hallucination Gaps in Large Language Models",
        "rating": "-1",
        "keywords": [
            [
                "biographies"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are increasingly used as alternatives to traditional search engines given their capacity to generate text that resembles human language. However, this shift is concerning, as LLMs often generate hallucinations, misleading or false information that appears highly credible. In this study, we explore the phenomenon of hallucinations across multiple languages in freeform text generation, focusing on what we call multilingual hallucination gaps. These gaps reflect differences in the frequency of hallucinated answers depending on the prompt and language used. To quantify such hallucinations, we used the FactScore metric and extended its framework to a multilingual setting. We conducted experiments using LLMs from the LLaMA, Qwen, and Aya families, generating biographies in 19 languages and comparing the results to Wikipedia pages. Our results reveal variations in hallucination rates, especially between high and low resource languages, raising important questions about LLM multilingual performance and the challenges in evaluating hallucinations in multilingual freeform text generation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18295",
        "abstract url": "https://arxiv.org/abs/2410.18295",
        "title": "Kenyan Sign Language (KSL) Dataset: Using Artificial Intelligence (AI) in Bridging Communication Barrier among the Deaf Learners",
        "rating": "-1",
        "keywords": [
            [
                "Sign Language"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Kenyan Sign Language (KSL) is the primary language used by the deaf community in Kenya. It is the medium of instruction from Pre-primary 1 to university among deaf learners, facilitating their education and academic achievement. Kenyan Sign Language is used for social interaction, expression of needs, making requests and general communication among persons who are deaf in Kenya. However, there exists a language barrier between the deaf and the hearing people in Kenya. Thus, the innovation on AI4KSL is key in eliminating the communication barrier. Artificial intelligence for KSL is a two-year research project (2023-2024) that aims to create a digital open-access AI of spontaneous and elicited data from a representative sample of the Kenyan deaf community. The purpose of this study is to develop AI assistive technology dataset that translates English to KSL as a way of fostering inclusion and bridging language barriers among deaf learners in Kenya. Specific objectives are: Build KSL dataset for spoken English and video recorded Kenyan Sign Language and to build transcriptions of the KSL signs to a phonetic-level interface of the sign language. In this paper, the methodology for building the dataset is described. Data was collected from 48 teachers and tutors of the deaf learners and 400 learners who are Deaf. Participants engaged mainly in sign language elicitation tasks through reading and singing. Findings of the dataset consisted of about 14,000 English sentences with corresponding KSL Gloss derived from a pool of about 4000 words and about 20,000 signed KSL videos that are either signed words or sentences. The second level of data outcomes consisted of 10,000 split and segmented KSL videos. The third outcome of the dataset consists of 4,000 transcribed words into five articulatory parameters according to HamNoSys system.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": "14 pages, to be published in 3rd International Conference on Artificial Intelligence and Robotics (MIRG-ICAIR 2023)"
    },
    {
        "paper id": "2410.18298",
        "abstract url": "https://arxiv.org/abs/2410.18298",
        "title": "Robust and Explainable Depression Identification from Speech Using Vowel-Based Ensemble Learning Approaches",
        "rating": "-1",
        "keywords": [
            [
                "Health",
                "diagnosis"
            ],
            [
                "cs.LG",
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "This study investigates explainable machine learning algorithms for identifying depression from speech. Grounded in evidence from speech production that depression affects motor control and vowel generation, pre-trained vowel-based embeddings, that integrate semantically meaningful linguistic units, are used. Following that, an ensemble learning approach decomposes the problem into constituent parts characterized by specific depression symptoms and severity levels. Two methods are explored: a \"bottom-up\" approach with 8 models predicting individual Patient Health Questionnaire-8 (PHQ-8) item scores, and a \"top-down\" approach using a Mixture of Experts (MoE) with a router module for assessing depression severity. Both methods depict performance comparable to state-of-the-art baselines, demonstrating robustness and reduced susceptibility to dataset mean/median values. System explainability benefits are discussed highlighting their potential to assist clinicians in depression diagnosis and screening.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "accepted at the IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI 2024)"
    },
    {
        "paper id": "2410.18321",
        "abstract url": "https://arxiv.org/abs/2410.18321",
        "title": "Calibrating Deep Neural Network using Euclidean Distance",
        "rating": "-1",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Uncertainty is a fundamental aspect of real-world scenarios, where perfect information is rarely available. Humans naturally develop complex internal models to navigate incomplete data and effectively respond to unforeseen or partially observed events. In machine learning, Focal Loss is commonly used to reduce misclassification rates by emphasizing hard-to-classify samples. However, it does not guarantee well-calibrated predicted probabilities and may result in models that are overconfident or underconfident. High calibration error indicates a misalignment between predicted probabilities and actual outcomes, affecting model reliability. This research introduces a novel loss function called Focal Calibration Loss (FCL), designed to improve probability calibration while retaining the advantages of Focal Loss in handling difficult samples. By minimizing the Euclidean norm through a strictly proper loss, FCL penalizes the instance-wise calibration error and constrains bounds. We provide theoretical validation for proposed method and apply it to calibrate CheXNet for potential deployment in web-based health-care systems. Extensive evaluations on various models and datasets demonstrate that our method achieves SOTA performance in both calibration and accuracy metrics.",
        "subjects": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18373",
        "abstract url": "https://arxiv.org/abs/2410.18373",
        "title": "UGotMe: An Embodied System for Affective Human-Robot Interaction",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "Equipping humanoid robots with the capability to understand emotional states of human interactants and express emotions appropriately according to situations is essential for affective human-robot interaction. However, enabling current vision-aware multimodal emotion recognition models for affective human-robot interaction in the real-world raises embodiment challenges: addressing the environmental noise issue and meeting real-time requirements. First, in multiparty conversation scenarios, the noises inherited in the visual observation of the robot, which may come from either 1) distracting objects in the scene or 2) inactive speakers appearing in the field of view of the robot, hinder the models from extracting emotional cues from vision inputs. Secondly, realtime response, a desired feature for an interactive system, is also challenging to achieve. To tackle both challenges, we introduce an affective human-robot interaction system called UGotMe designed specifically for multiparty conversations. Two denoising strategies are proposed and incorporated into the system to solve the first issue. Specifically, to filter out distracting objects in the scene, we propose extracting face images of the speakers from the raw images and introduce a customized active face extraction strategy to rule out inactive speakers. As for the second issue, we employ efficient data transmission from the robot to the local server to improve realtime response capability. We deploy UGotMe on a human robot named Ameca to validate its real-time inference capabilities in practical scenarios. Videos demonstrating real-world deployment are available at https://pi3-141592653.github.io/UGotMe/.",
        "subjects": [
            "cs.RO",
            "cs.HC"
        ],
        "comment": "7 pages, 5 figures"
    },
    {
        "paper id": "2410.18385",
        "abstract url": "https://arxiv.org/abs/2410.18385",
        "title": "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval",
        "rating": "-1",
        "keywords": [
            [
                "named entity recognition"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Despite the recent advancements in information retrieval (IR), zero-shot IR remains a significant challenge, especially when dealing with new domains, languages, and newly-released use cases that lack historical query traffic from existing users. For such cases, it is common to use query augmentations followed by fine-tuning pre-trained models on the document data paired with synthetic queries. In this work, we propose a novel Universal Document Linking (UDL) algorithm, which links similar documents to enhance synthetic query generation across multiple datasets with different characteristics. UDL leverages entropy for the choice of similarity models and named entity recognition (NER) for the link decision of documents using similarity scores. Our empirical studies demonstrate the effectiveness and universality of the UDL across diverse datasets and IR models, surpassing state-of-the-art methods in zero-shot cases. The developed code for reproducibility is included in https://github.com/eoduself/UDL",
        "subjects": [
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "comment": "Accepted for publication at EMNLP 2024 Main Conference"
    },
    {
        "paper id": "2410.18395",
        "abstract url": "https://arxiv.org/abs/2410.18395",
        "title": "A contrastive-learning approach for auditory attention detection",
        "rating": "-1",
        "keywords": [
            [
                "EEG"
            ],
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Carrying conversations in multi-sound environments is one of the more challenging tasks, since the sounds overlap across time and frequency making it difficult to understand a single sound source. One proposed approach to help isolate an attended speech source is through decoding the electroencephalogram (EEG) and identifying the attended audio source using statistical or machine learning techniques. However, the limited amount of data in comparison to other machine learning problems and the distributional shift between different EEG recordings emphasizes the need for a self supervised approach that works with limited data to achieve a more robust solution. In this paper, we propose a method based on self supervised learning to minimize the difference between the latent representations of an attended speech signal and the corresponding EEG signal. This network is further finetuned for the auditory attention classification task. We compare our results with previously published methods and achieve state-of-the-art performance on the validation set.",
        "subjects": [
            "cs.LG",
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18400",
        "abstract url": "https://arxiv.org/abs/2410.18400",
        "title": "DMVC: Multi-Camera Video Compression Network aimed at Improving Deep Learning Accuracy",
        "rating": "-1",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "navigation"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "We introduce a cutting-edge video compression framework tailored for the age of ubiquitous video data, uniquely designed to serve machine learning applications. Unlike traditional compression methods that prioritize human visual perception, our innovative approach focuses on preserving semantic information critical for deep learning accuracy, while efficiently reducing data size. The framework operates on a batch basis, capable of handling multiple video streams simultaneously, thereby enhancing scalability and processing efficiency. It features a dual reconstruction mode: lightweight for real-time applications requiring swift responses, and high-precision for scenarios where accuracy is crucial. Based on a designed deep learning algorithms, it adeptly segregates essential information from redundancy, ensuring machine learning tasks are fed with data of the highest relevance. Our experimental results, derived from diverse datasets including urban surveillance and autonomous vehicle navigation, showcase DMVC's superiority in maintaining or improving machine learning task accuracy, while achieving significant data compression. This breakthrough paves the way for smarter, scalable video analysis systems, promising immense potential across various applications from smart city infrastructure to autonomous systems, establishing a new benchmark for integrating video compression with machine learning.",
        "subjects": [
            "cs.CV",
            "cs.DC",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18406",
        "abstract url": "https://arxiv.org/abs/2410.18406",
        "title": "MoMQ: Mixture-of-Experts Enhances Multi-Dialect Query Generation across Relational and Non-Relational Databases",
        "rating": "-1",
        "keywords": [
            [
                "SQL"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The improvement in translating natural language to structured query language (SQL) can be attributed to the advancements in large language models (LLMs). Open-source LLMs, tailored for specific database dialects such as MySQL, have shown great performance. However, cloud service providers are looking for a unified database manager service (e.g., Cosmos DB from Azure, Amazon Aurora from AWS, Lindorm from AlibabaCloud) that can support multiple dialects. This requirement has led to the concept of multi-dialect query generation, which presents challenges to LLMs. These challenges include syntactic differences among dialects and imbalanced data distribution across multiple dialects. To tackle these challenges, we propose MoMQ, a novel Mixture-of-Experts-based multi-dialect query generation framework across both relational and non-relational databases. MoMQ employs a dialect expert group for each dialect and a multi-level routing strategy to handle dialect-specific knowledge, reducing interference during query generation. Additionally, a shared expert group is introduced to address data imbalance, facilitating the transfer of common knowledge from high-resource dialects to low-resource ones. Furthermore, we have developed a high-quality multi-dialect query generation benchmark that covers relational and non-relational databases such as MySQL, PostgreSQL, Cypher for Neo4j, and nGQL for NebulaGraph. Extensive experiments have shown that MoMQ performs effectively and robustly even in resource-imbalanced scenarios.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.DB",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18408",
        "abstract url": "https://arxiv.org/abs/2410.18408",
        "title": "Scale Propagation Network for Generalizable Depth Completion",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Depth"
            ],
            [
                "LiDAR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Depth completion, inferring dense depth maps from sparse measurements, is crucial for robust 3D perception. Although deep learning based methods have made tremendous progress in this problem, these models cannot generalize well across different scenes that are unobserved in training, posing a fundamental limitation that yet to be overcome. A careful analysis of existing deep neural network architectures for depth completion, which are largely borrowing from successful backbones for image analysis tasks, reveals that a key design bottleneck actually resides in the conventional normalization layers. These normalization layers are designed, on one hand, to make training more stable, on the other hand, to build more visual invariance across scene scales. However, in depth completion, the scale is actually what we want to robustly estimate in order to better generalize to unseen scenes. To mitigate, we propose a novel scale propagation normalization (SP-Norm) method to propagate scales from input to output, and simultaneously preserve the normalization operator for easy convergence. More specifically, we rescale the input using learned features of a single-layer perceptron from the normalized input, rather than directly normalizing the input as conventional normalization layers. We then develop a new network architecture based on SP-Norm and the ConvNeXt V2 backbone. We explore the composition of various basic blocks and architectures to achieve superior performance and efficient inference for generalizable depth completion. Extensive experiments are conducted on six unseen datasets with various types of sparse depth maps, i.e., randomly sampled 0.1\\%/1\\%/10\\% valid pixels, 4/8/16/32/64-line LiDAR points, and holes from Structured-Light. Our model consistently achieves the best accuracy with faster speed and lower memory when compared to state-of-the-art methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Major revision in IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
        "paper id": "2410.18418",
        "abstract url": "https://arxiv.org/abs/2410.18418",
        "title": "Knowledge-Assisted Privacy Preserving in Semantic Communication",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Semantic communication (SC) offers promising advancements in data transmission efficiency and reliability by focusing on delivering true meaning rather than solely binary bits of messages. However, privacy concerns in SC might become outstanding. Eavesdroppers equipped with advanced semantic coding models and extensive knowledge could be capable of correctly decoding and reasoning sensitive semantics from just a few stolen bits. To this end, this article explores utilizing knowledge to enhance data privacy in SC networks. Specifically, we first identify the potential attacks in SC based on the analysis of knowledge. Then, we propose a knowledge-assisted privacy preserving SC framework, which consists of a data transmission layer for precisely encoding and decoding source messages, and a knowledge management layer responsible for injecting appropriate knowledge into the transmission pair. Moreover, we elaborate on the transceiver design in the proposed SC framework to explain how knowledge should be utilized properly. Finally, some challenges of the proposed SC framework are discussed to expedite the practical implementation.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18429",
        "abstract url": "https://arxiv.org/abs/2410.18429",
        "title": "GPU Accelerated 3D P-wave Source Free Adaptive Wavefield Reconstruction Inversion with an application to experimental VSP physical modeling data",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "Wavefield reconstruction inversion (WRI) has been considered a potential solution to the issue of local minima inherent in conventional full waveform inversion (FWI) methods. However, most current WRI research has been confined to 2D problems due to the computational challenges posed by solving augmented systems for optimal data-fitting wavefields. This constraint limits WRI applicability to realistic 3D scenarios. This study introduces a GPU-accelerated 3D source-free adaptive WRI (GPU-SF-AWRI) method that overcomes these computational barriers by adaptively controlling the computational accuracy of wavefield simulation and optimizing GPU utilization, thus enhancing its suitability for 3D applications. The inclusion of an on-the-fly source estimation technique further boosts its performance on realistic problems. Numerical experiments reveal that the proposed GPU-accelerated method achieves a 195-fold speedup compared to CPU-based approaches. By incorporating adaptive accuracy and total variation regularization, we attain a 2-fold speedup while maintaining inversion accuracy. We applied the GPU-SF-AWRI method to numerical and actual Vertical Seismic Profiling (VSP) physical modeling P-wave data, confirming its efficacy in addressing real data challenges and mitigating local minima associated with conventional FWI.",
        "subjects": [
            "physics.geo-ph",
            "eess.SP",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.19869",
        "abstract url": "https://arxiv.org/abs/2410.19869",
        "title": "Comparing YOLO11 and YOLOv8 for instance segmentation of occluded and non-occluded immature green fruits in complex orchard environment",
        "rating": "-1",
        "keywords": [
            [
                "agricultural"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This study conducted a comprehensive performance evaluation on YOLO11 and YOLOv8, the latest in the \"You Only Look Once\" (YOLO) series, focusing on their instance segmentation capabilities for immature green apples in orchard environments. YOLO11n-seg achieved the highest mask precision across all categories with a notable score of 0.831, highlighting its effectiveness in fruit detection. YOLO11m-seg and YOLO11l-seg excelled in non-occluded and occluded fruitlet segmentation with scores of 0.851 and 0.829, respectively. Additionally, YOLO11x-seg led in mask recall for all categories, achieving a score of 0.815, with YOLO11m-seg performing best for non-occluded immature green fruitlets at 0.858 and YOLOv8x-seg leading the occluded category with 0.800. In terms of mean average precision at a 50\\% intersection over union (mAP@50), YOLO11m-seg consistently outperformed, registering the highest scores for both box and mask segmentation, at 0.876 and 0.860 for the \"All\" class and 0.908 and 0.909 for non-occluded immature fruitlets, respectively. YOLO11l-seg and YOLOv8l-seg shared the top box mAP@50 for occluded immature fruitlets at 0.847, while YOLO11m-seg achieved the highest mask mAP@50 of 0.810. Despite the advancements in YOLO11, YOLOv8n surpassed its counterparts in image processing speed, with an impressive inference speed of 3.3 milliseconds, compared to the fastest YOLO11 series model at 4.8 milliseconds, underscoring its suitability for real-time agricultural applications related to complex green fruit environments.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "16 Pages, 10 Figures, 3 Tables"
    },
    {
        "paper id": "2410.17573",
        "abstract url": "https://arxiv.org/abs/2410.17573",
        "title": "Securing Federated Learning Against Novel and Classic Backdoor Threats During Foundation Model Integration",
        "rating": "-1.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "attack"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated learning (FL) enables decentralized model training while preserving privacy. Recently, integrating Foundation Models (FMs) into FL has boosted performance but also introduced a novel backdoor attack mechanism. Attackers can exploit the FM's capabilities to embed backdoors into synthetic data generated by FMs used for model fusion, subsequently infecting all client models through knowledge sharing without involvement in the long-lasting FL process. These novel attacks render existing FL backdoor defenses ineffective, as they primarily detect anomalies among client updates, which may appear uniformly malicious under this attack. Our work proposes a novel data-free defense strategy by constraining abnormal activations in the hidden feature space during model aggregation on the server. The activation constraints, optimized using synthetic data alongside FL training, mitigate the attack while barely affecting model performance, as the parameters remain untouched. Extensive experiments demonstrate its effectiveness against both novel and classic backdoor attacks, outperforming existing defenses while maintaining model performance.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17587",
        "abstract url": "https://arxiv.org/abs/2410.17587",
        "title": "Predicting Company Growth by Econophysics informed Machine Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Predicting company growth is crucial for strategic adjustment, operational decision-making, risk assessment, and loan eligibility reviews. Traditional models for company growth often focus too much on theory, overlooking practical forecasting, or they rely solely on time series forecasting techniques, ignoring interpretability and the inherent mechanisms of company growth. In this paper, we propose a machine learning-based prediction framework that incorporates an econophysics model for company growth. Our model captures both the intrinsic growth mechanisms of companies led by scaling laws and the fluctuations influenced by random factors and individual decisions, demonstrating superior predictive performance compared with methods that use time series techniques alone. Its advantages are more pronounced in long-range prediction tasks. By explicitly modeling the baseline growth and volatility components, our model is more interpretable.",
        "subjects": [
            "cs.CE",
            "cs.LG",
            "econ.GN",
            "physics.soc-ph"
        ],
        "comment": "18 pages, 12 figures"
    },
    {
        "paper id": "2410.17602",
        "abstract url": "https://arxiv.org/abs/2410.17602",
        "title": "Integrating Large Language Models for UAV Control in Simulated Environments: A Modular Interaction Approach",
        "rating": "-1.5",
        "keywords": [
            [
                "UAV"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The intersection of LLMs (Large Language Models) and UAV (Unoccupied Aerial Vehicles) technology represents a promising field of research with the potential to enhance UAV capabilities significantly. This study explores the application of LLMs in UAV control, focusing on the opportunities for integrating advanced natural language processing into autonomous aerial systems. By enabling UAVs to interpret and respond to natural language commands, LLMs simplify the UAV control and usage, making them accessible to a broader user base and facilitating more intuitive human-machine interactions. The paper discusses several key areas where LLMs can impact UAV technology, including autonomous decision-making, dynamic mission planning, enhanced situational awareness, and improved safety protocols. Through a comprehensive review of current developments and potential future directions, this study aims to highlight how LLMs can transform UAV operations, making them more adaptable, responsive, and efficient in complex environments. A template development framework for integrating LLMs in UAV control is also described. Proof of Concept results that integrate existing LLM models and popular robotic simulation platforms are demonstrated. The findings suggest that while there are substantial technical and ethical challenges to address, integrating LLMs into UAV control holds promising implications for advancing autonomous aerial systems.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17764",
        "abstract url": "https://arxiv.org/abs/2410.17764",
        "title": "Beyond Backpropagation: Optimization with Multi-Tangent Forward Gradients",
        "rating": "-1.5",
        "keywords": [
            [
                "biologically"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The gradients used to train neural networks are typically computed using backpropagation. While an efficient way to obtain exact gradients, backpropagation is computationally expensive, hinders parallelization, and is biologically implausible. Forward gradients are an approach to approximate the gradients from directional derivatives along random tangents computed by forward-mode automatic differentiation. So far, research has focused on using a single tangent per step. This paper provides an in-depth analysis of multi-tangent forward gradients and introduces an improved approach to combining the forward gradients from multiple tangents based on orthogonal projections. We demonstrate that increasing the number of tangents improves both approximation quality and optimization performance across various tasks.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17781",
        "abstract url": "https://arxiv.org/abs/2410.17781",
        "title": "Evaluating Explanations Through LLMs: Beyond Traditional User Studies",
        "rating": "-1.5",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "As AI becomes fundamental in sectors like healthcare, explainable AI (XAI) tools are essential for trust and transparency. However, traditional user studies used to evaluate these tools are often costly, time consuming, and difficult to scale. In this paper, we explore the use of Large Language Models (LLMs) to replicate human participants to help streamline XAI evaluation. We reproduce a user study comparing counterfactual and causal explanations, replicating human participants with seven LLMs under various settings. Our results show that (i) LLMs can replicate most conclusions from the original study, (ii) different LLMs yield varying levels of alignment in the results, and (iii) experimental factors such as LLM memory and output variability affect alignment with human responses. These initial findings suggest that LLMs could provide a scalable and cost-effective way to simplify qualitative XAI evaluation.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17787",
        "abstract url": "https://arxiv.org/abs/2410.17787",
        "title": "Large Language Models Engineer Too Many Simple Features For Tabular Data",
        "rating": "-1.5",
        "keywords": [
            [
                "Tabular"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Tabular machine learning problems often require time-consuming and labor-intensive feature engineering. Recent efforts have focused on using large language models (LLMs) to capitalize on their potential domain knowledge. At the same time, researchers have observed ethically concerning negative biases in other LLM-related use cases, such as text generation. These developments motivated us to investigate whether LLMs exhibit a bias that negatively impacts the performance of feature engineering. While not ethically concerning, such a bias could hinder practitioners from fully utilizing LLMs for automated data science. Therefore, we propose a method to detect potential biases by detecting anomalies in the frequency of operators (e.g., adding two features) suggested by LLMs when engineering new features. Our experiments evaluate the bias of four LLMs, two big frontier and two small open-source models, across 27 tabular datasets. Our results indicate that LLMs are biased toward simple operators, such as addition, and can fail to utilize more complex operators, such as grouping followed by aggregations. Furthermore, the bias can negatively impact the predictive performance when using LLM-generated features. Our results call for mitigating bias when using LLMs for feature engineering.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Preprint"
    },
    {
        "paper id": "2410.17865",
        "abstract url": "https://arxiv.org/abs/2410.17865",
        "title": "Population stratification for prediction of mortality in post-AKI patients",
        "rating": "-1.5",
        "keywords": [
            [
                "healthcare",
                "clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Acute kidney injury (AKI) is a serious clinical condition that affects up to 20% of hospitalised patients. AKI is associated with short term unplanned hospital readmission and post-discharge mortality risk. Patient risk and healthcare expenditures can be minimised by followup planning grounded on predictive models and machine learning. Since AKI is multi-factorial, predictive models specialised in different categories of patients can increase accuracy of predictions. In the present article we present some results following this approach.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17906",
        "abstract url": "https://arxiv.org/abs/2410.17906",
        "title": "Leveraging Deep Learning for Time Series Extrinsic Regression in predicting photometric metallicity of Fundamental-mode RR Lyrae Stars",
        "rating": "-1.5",
        "keywords": [
            [
                "Astronomy"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Astronomy is entering an unprecedented era of Big Data science, driven by missions like the ESA's Gaia telescope, which aims to map the Milky Way in three dimensions. Gaia's vast dataset presents a monumental challenge for traditional analysis methods. The sheer scale of this data exceeds the capabilities of manual exploration, necessitating the utilization of advanced computational techniques. In response to this challenge, we developed a novel approach leveraging deep learning to estimate the metallicity of fundamental mode (ab-type) RR Lyrae stars from their light curves in the Gaia optical G-band. Our study explores applying deep learning techniques, particularly advanced neural network architectures, in predicting photometric metallicity from time-series data. Our deep learning models demonstrated notable predictive performance, with a low mean absolute error (MAE) of 0.0565, the root mean square error (RMSE) achieved is 0.0765 and a high $R^2$ regression performance of 0.9401 measured by cross-validation. The weighted mean absolute error (wMAE) is 0.0563, while the weighted root mean square error (wRMSE) is 0.0763. These results showcase the effectiveness of our approach in accurately estimating metallicity values. Our work underscores the importance of deep learning in astronomical research, particularly with large datasets from missions like Gaia. By harnessing the power of deep learning methods, we can provide precision in analyzing vast datasets, contributing to more precise and comprehensive insights into complex astronomical phenomena.",
        "subjects": [
            "cs.AI",
            "astro-ph.IM"
        ],
        "comment": "Sensors 2024, 24(16), 5203; (23 pages)"
    },
    {
        "paper id": "2410.17913",
        "abstract url": "https://arxiv.org/abs/2410.17913",
        "title": "Deep learning for model correction of dynamical systems with data scarcity",
        "rating": "-1.5",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present a deep learning framework for correcting existing dynamical system models utilizing only a scarce high-fidelity data set. In many practical situations, one has a low-fidelity model that can capture the dynamics reasonably well but lacks high resolution, due to the inherent limitation of the model and the complexity of the underlying physics. When high resolution data become available, it is natural to seek model correction to improve the resolution of the model predictions. We focus on the case when the amount of high-fidelity data is so small that most of the existing data driven modeling methods cannot be applied. In this paper, we address these challenges with a model-correction method which only requires a scarce high-fidelity data set. Our method first seeks a deep neural network (DNN) model to approximate the existing low-fidelity model. By using the scarce high-fidelity data, the method then corrects the DNN model via transfer learning (TL). After TL, an improved DNN model with high prediction accuracy to the underlying dynamics is obtained. One distinct feature of the propose method is that it does not assume a specific form of the model correction terms. Instead, it offers an inherent correction to the low-fidelity model via TL. A set of numerical examples are presented to demonstrate the effectiveness of the proposed method.",
        "subjects": [
            "cs.LG",
            "math.DS",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17918",
        "abstract url": "https://arxiv.org/abs/2410.17918",
        "title": "Addressing Asynchronicity in Clinical Multimodal Fusion via Individualized Chest X-ray Generation",
        "rating": "-1.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "health",
                "X-ray",
                "disease",
                "Clinical"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Integrating multi-modal clinical data, such as electronic health records (EHR) and chest X-ray images (CXR), is particularly beneficial for clinical prediction tasks. However, in a temporal setting, multi-modal data are often inherently asynchronous. EHR can be continuously collected but CXR is generally taken with a much longer interval due to its high cost and radiation dose. When clinical prediction is needed, the last available CXR image might have been outdated, leading to suboptimal predictions. To address this challenge, we propose DDL-CXR, a method that dynamically generates an up-to-date latent representation of the individualized CXR images. Our approach leverages latent diffusion models for patient-specific generation strategically conditioned on a previous CXR image and EHR time series, providing information regarding anatomical structures and disease progressions, respectively. In this way, the interaction across modalities could be better captured by the latent CXR generation process, ultimately improving the prediction performance. Experiments using MIMIC datasets show that the proposed model could effectively address asynchronicity in multimodal fusion and consistently outperform existing methods.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted by NeurIPS-24"
    },
    {
        "paper id": "2410.17943",
        "abstract url": "https://arxiv.org/abs/2410.17943",
        "title": "Optimizing Travel Itineraries with AI Algorithms in a Microservices Architecture: Balancing Cost, Time, Preferences, and Sustainability",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "The objective of this research is how an implementation of AI algorithms in the microservices architecture enhances travel itineraries by cost, time, user preferences, and environmental sustainability. It uses machine learning models for both cost forecasting and personalization, genetic algorithm for optimization of the itinerary, and heuristics for sustainability checking. Primary evaluated parameters consist of latency, ability to satisfy user preferences, cost and environmental concern. The experimental results demonstrate an average of 4.5 seconds of response time on 1000 concurrent users and 92% of user preferences accuracy. The cost efficiency is proved, with 95% of provided trips being within the limits of the budget declared by the user. The system also implements some measures to alleviate negative externalities related to travel and 60% of offered travel plans had green options incorporated, resulting in the average 15% lower carbon emissions than the traditional travel plans offered. The genetic algorithm with time complexity O(g.p.f) provides the optimal solution in 100 generations. Every iteration improves the quality of the solution by 5%, thus enabling its effective use in optimization problems where time is measured in seconds. Finally, the system is designed to be fault-tolerant with functional 99.9% availability which allows the provision of services even when requirements are exceeded. Travel optimization platform is turned dynamic and efficient by this microservices based architecture which provides enhanced scaling, allows asynchronous communication and real time changes. Because of the incorporation of Ai, cost control and eco-friendliness approaches, the system addresses the different user needs in the present days travel business.",
        "subjects": [
            "cs.SE",
            "cs.AI",
            "cs.CY",
            "cs.LG"
        ],
        "comment": "18 pages, 6 figures"
    },
    {
        "paper id": "2410.17971",
        "abstract url": "https://arxiv.org/abs/2410.17971",
        "title": "Dynamic Spectrum Access for Ambient Backscatter Communication-assisted D2D Systems with Quantum Reinforcement Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "Quantum"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Spectrum access is an essential problem in device-to-device (D2D) communications. However, with the recent growth in the number of mobile devices, the wireless spectrum is becoming scarce, resulting in low spectral efficiency for D2D communications. To address this problem, this paper aims to integrate the ambient backscatter communication technology into D2D devices to allow them to backscatter ambient RF signals to transmit their data when the shared spectrum is occupied by mobile users. To obtain the optimal spectrum access policy, i.e., stay idle or access the shared spectrum and perform active transmissions or backscattering ambient RF signals for transmissions, to maximize the average throughput for D2D users, deep reinforcement learning (DRL) can be adopted. However, DRL-based solutions may require long training time due to the curse of dimensionality issue as well as complex deep neural network architectures. For that, we develop a novel quantum reinforcement learning (RL) algorithm that can achieve a faster convergence rate with fewer training parameters compared to DRL thanks to the quantum superposition and quantum entanglement principles. Specifically, instead of using conventional deep neural networks, the proposed quantum RL algorithm uses a parametrized quantum circuit to approximate an optimal policy. Extensive simulations then demonstrate that the proposed solution not only can significantly improve the average throughput of D2D devices when the shared spectrum is busy but also can achieve much better performance in terms of convergence rate and learning complexity compared to existing DRL-based methods.",
        "subjects": [
            "cs.NI",
            "cs.AI"
        ],
        "comment": "12 pages, 7 figures"
    },
    {
        "paper id": "2410.17976",
        "abstract url": "https://arxiv.org/abs/2410.17976",
        "title": "metasnf: Meta Clustering with Similarity Network Fusion in R",
        "rating": "-1.5",
        "keywords": [
            [
                "biomedical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "metasnf is an R package that enables users to apply meta clustering, a method for efficiently searching a broad space of cluster solutions by clustering the solutions themselves, to clustering workflows based on similarity network fusion (SNF). SNF is a multi-modal data integration algorithm commonly used for biomedical subtype discovery. The package also contains functions to assist with cluster visualization, characterization, and validation. This package can help researchers identify SNF-derived cluster solutions that are guided by context-specific utility over context-agnostic measures of quality.",
        "subjects": [
            "stat.CO",
            "cs.LG"
        ],
        "comment": "72 pages, 22 figures, submitted to Journal of Statistical Software"
    },
    {
        "paper id": "2410.17991",
        "abstract url": "https://arxiv.org/abs/2410.17991",
        "title": "AI driven health recommender",
        "rating": "-1.5",
        "keywords": [
            [
                "health",
                "disease"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "As AI emerged as highest valued technology, We used that to create a web application that makes a patient work easier .It detects the disease name based on the symptoms given by the patient and recommends medication for respective disease, precautions to take, diet to follow and workouts to do, so the disease can be minimized. The web application is made with clean and Realtime data by using Machine learning as root. We used flask to create a user-friendly platform.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18003",
        "abstract url": "https://arxiv.org/abs/2410.18003",
        "title": "Inferring stability properties of chaotic systems on autoencoders' latent spaces",
        "rating": "-1.5",
        "keywords": [
            [
                "forecast"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The data-driven learning of solutions of partial differential equations can be based on a divide-and-conquer strategy. First, the high dimensional data is compressed to a latent space with an autoencoder; and, second, the temporal dynamics are inferred on the latent space with a form of recurrent neural network. In chaotic systems and turbulence, convolutional autoencoders and echo state networks (CAE-ESN) successfully forecast the dynamics, but little is known about whether the stability properties can also be inferred. We show that the CAE-ESN model infers the invariant stability properties and the geometry of the tangent space in the low-dimensional manifold (i.e. the latent space) through Lyapunov exponents and covariant Lyapunov vectors. This work opens up new opportunities for inferring the stability of high-dimensional chaotic systems in latent spaces.",
        "subjects": [
            "cs.LG",
            "nlin.CD"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18164",
        "abstract url": "https://arxiv.org/abs/2410.18164",
        "title": "TabDPT: Scaling Tabular Foundation Models",
        "rating": "-1.5",
        "keywords": [
            [
                "Tabular"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The challenges faced by neural networks on tabular data are well-documented and have hampered the progress of tabular foundation models. Techniques leveraging in-context learning (ICL) have shown promise here, allowing for dynamic adaptation to unseen data. ICL can provide predictions for entirely new datasets without further training or hyperparameter tuning, therefore providing very fast inference when encountering a novel task. However, scaling ICL for tabular data remains an issue: approaches based on large language models cannot efficiently process numeric tables, and tabular-specific techniques have not been able to effectively harness the power of real data to improve performance and generalization. We are able to overcome these challenges by training tabular-specific ICL-based architectures on real data with self-supervised learning and retrieval, combining the best of both worlds. Our resulting model -- the Tabular Discriminative Pre-trained Transformer (TabDPT) -- achieves state-of-the-art performance on the CC18 (classification) and CTR23 (regression) benchmarks with no task-specific fine-tuning, demonstrating the adapatability and speed of ICL once the model is pre-trained. TabDPT also demonstrates strong scaling as both model size and amount of available data increase, pointing towards future improvements simply through the curation of larger tabular pre-training datasets and training larger models.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "Minimal TabDPT interface to provide predictions on new datasets available at the following link: https://github.com/layer6ai-labs/TabDPT"
    },
    {
        "paper id": "2410.18230",
        "abstract url": "https://arxiv.org/abs/2410.18230",
        "title": "Assessment of Developmental Dysgraphia Utilising a Display Tablet",
        "rating": "-1.5",
        "keywords": [
            [
                "diagnosis"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Even though the computerised assessment of developmental dysgraphia (DD) based on online handwriting processing has increasing popularity, most of the solutions are based on a setup, where a child writes on a paper fixed to a digitizing tablet that is connected to a computer. Although this approach enables the standard way of writing using an inking pen, it is difficult to be administered by children themselves. The main goal of this study is thus to explore, whether the quantitative analysis of online handwriting recorded via a display screen tablet could sufficiently support the assessment of DD as well. For the purpose of this study, we enrolled 144 children (attending the 3rd and 4th class of a primary school), whose handwriting proficiency was assessed by a special education counsellor, and who assessed themselves by the Handwriting Proficiency Screening Questionnaires for Children (HPSQ C). Using machine learning models based on a gradient-boosting algorithm, we were able to support the DD diagnosis with up to 83.6% accuracy. The HPSQ C total score was estimated with a minimum error equal to 10.34 %. Children with DD spent significantly higher time in-air, they had a higher number of pen elevations, a bigger height of on-surface strokes, a lower in-air tempo, and a higher variation in the angular velocity. Although this study shows a promising impact of DD assessment via display tablets, it also accents the fact that modelling of subjective scores is challenging and a complex and data-driven quantification of DD manifestations is needed.",
        "subjects": [
            "cs.HC",
            "cs.LG"
        ],
        "comment": "16 pages"
    },
    {
        "paper id": "2410.18237",
        "abstract url": "https://arxiv.org/abs/2410.18237",
        "title": "Bayesian optimization for robust robotic grasping using a sensorized compliant hand",
        "rating": "-1.5",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "One of the first tasks we learn as children is to grasp objects based on our tactile perception. Incorporating such skill in robots will enable multiple applications, such as increasing flexibility in industrial processes or providing assistance to people with physical disabilities. However, the difficulty lies in adapting the grasping strategies to a large variety of tasks and objects, which can often be unknown. The brute-force solution is to learn new grasps by trial and error, which is inefficient and ineffective. In contrast, Bayesian optimization applies active learning by adding information to the approximation of an optimal grasp. This paper proposes the use of Bayesian optimization techniques to safely perform robotic grasping. We analyze different grasp metrics to provide realistic grasp optimization in a real system including tactile sensors. An experimental evaluation in the robotic system shows the usefulness of the method for performing unknown object grasping even in the presence of noise and uncertainty inherent to a real-world environment.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18318",
        "abstract url": "https://arxiv.org/abs/2410.18318",
        "title": "Self-Supervised Learning for Time Series: A Review & Critique of FITS",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Accurate time series forecasting is a highly valuable endeavour with applications across many industries. Despite recent deep learning advancements, increased model complexity, and larger model sizes, many state-of-the-art models often perform worse or on par with simpler models. One of those cases is a recently proposed model, FITS, claiming competitive performance with significantly reduced parameter counts. By training a one-layer neural network in the complex frequency domain, we are able to replicate these results. Our experiments on a wide range of real-world datasets further reveal that FITS especially excels at capturing periodic and seasonal patterns, but struggles with trending, non-periodic, or random-resembling behavior. With our two novel hybrid approaches, where we attempt to remedy the weaknesses of FITS by combining it with DLinear, we achieve the best results of any known open-source model on multivariate regression and promising results in multiple/linear regression on price datasets, on top of vastly improving upon what FITS achieves as a standalone model.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "arXiv:2307.03756v3 45 pages, 36 figures"
    },
    {
        "paper id": "2410.18353",
        "abstract url": "https://arxiv.org/abs/2410.18353",
        "title": "Precision Soil Quality Analysis Using Transformer-based Data Fusion Strategies: A Systematic Review",
        "rating": "-1.5",
        "keywords": [
            [
                "remote sensing",
                "agricultural"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This review explores the most recent advancements in transformer-based data fusion techniques in agricultural remote sensing (RS), with a particular focus on soil analysis. Utilizing a systematic, data-driven approach, we demonstrate that transformers have significantly outperformed conventional deep learning and machine learning methods since 2022, achieving prediction performance between 92% and 97%. The review is specifically focused on soil analysis, due to the importance of soil condition in optimizing crop productivity and ensuring sustainable farming practices. Transformer-based models have shown remarkable capabilities in handling complex multivariate soil data, improving the accuracy of soil moisture prediction, soil element analysis, and other soil-related applications. This systematic review primarily focuses on 1) analysing research trends and patterns in the literature, both chronologically and technically, and 2) conducting a comparative analysis of data fusion approaches, considering factors such as data types, techniques, and RS applications. Finally, we propose a roadmap for implementing data fusion methods in agricultural RS.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "14 pages, 9 figures, 4 tables, Journal"
    },
    {
        "paper id": "2410.18354",
        "abstract url": "https://arxiv.org/abs/2410.18354",
        "title": "Assessing Alcohol Use Disorder: Insights from Lifestyle, Background, and Family History with Machine Learning Techniques",
        "rating": "-1.5",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This study explored how lifestyle, personal background, and family history contribute to the risk of developing Alcohol Use Disorder (AUD). Survey data from the All of Us Program was utilized to extract information on AUD status, lifestyle, personal background, and family history for 6,016 participants. Key determinants of AUD were identified using decision trees including annual income, recreational drug use, length of residence, sex/gender, marital status, education level, and family history of AUD. Data visualization and Chi-Square Tests of Independence were then used to assess associations between identified factors and AUD. Afterwards, machine learning techniques including decision trees, random forests, and Naive Bayes were applied to predict an individual's likelihood of developing AUD. Random forests were found to achieve the highest accuracy (82%), compared to Decision Trees and Naive Bayes. Findings from this study can offer insights that help parents, healthcare professionals, and educators develop strategies to reduce AUD risk, enabling early intervention and targeted prevention efforts.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18380",
        "abstract url": "https://arxiv.org/abs/2410.18380",
        "title": "Harnessing PU Learning for Enhanced Cloud-based DDoS Detection: A Comparative Analysis",
        "rating": "-1.5",
        "keywords": [
            [
                "Support Vector Machine"
            ],
            [
                "anomaly detection"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper explores the application of Positive-Unlabeled (PU) learning for enhanced Distributed Denial-of-Service (DDoS) detection in cloud environments. Utilizing the $\\texttt{BCCC-cPacket-Cloud-DDoS-2024}$ dataset, we implement PU learning with four machine learning algorithms: XGBoost, Random Forest, Support Vector Machine, and Na\u00efve Bayes. Our results demonstrate the superior performance of ensemble methods, with XGBoost and Random Forest achieving $F_{1}$ scores exceeding 98%. We quantify the efficacy of each approach using metrics including $F_{1}$ score, ROC AUC, Recall, and Precision. This study bridges the gap between PU learning and cloud-based anomaly detection, providing a foundation for addressing Context-Aware DDoS Detection in multi-cloud environments. Our findings highlight the potential of PU learning in scenarios with limited labeled data, offering valuable insights for developing more robust and adaptive cloud security mechanisms.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": "9 pages, 5 figures"
    },
    {
        "paper id": "2410.18425",
        "abstract url": "https://arxiv.org/abs/2410.18425",
        "title": "Doubly Non-Central Beta Matrix Factorization for Stable Dimensionality Reduction of Bounded Support Matrix Data",
        "rating": "-1.5",
        "keywords": [
            [
                "cancer",
                "DNA"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider the problem of developing interpretable and computationally efficient matrix decomposition methods for matrices whose entries have bounded support. Such matrices are found in large-scale DNA methylation studies and many other settings. Our approach decomposes the data matrix into a Tucker representation wherein the number of columns in the constituent factor matrices is not constrained. We derive a computationally efficient sampling algorithm to solve for the Tucker decomposition. We evaluate the performance of our method using three criteria: predictability, computability, and stability. Empirical results show that our method has similar performance as other state-of-the-art approaches in terms of held-out prediction and computational complexity, but has significantly better performance in terms of stability to changes in hyper-parameters. The improved stability results in higher confidence in the results in applications where the constituent factors are used to generate and test scientific hypotheses such as DNA methylation analysis of cancer samples.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "33 pages, 18 figures"
    },
    {
        "paper id": "2410.18710",
        "abstract url": "https://arxiv.org/abs/2410.18710",
        "title": "Uncovering the Genetic Basis of Glioblastoma Heterogeneity through Multimodal Analysis of Whole Slide Images and RNA Sequencing Data",
        "rating": "-1.5",
        "keywords": [
            [
                "Whole Slide",
                "cancer"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Glioblastoma is a highly aggressive form of brain cancer characterized by rapid progression and poor prognosis. Despite advances in treatment, the underlying genetic mechanisms driving this aggressiveness remain poorly understood. In this study, we employed multimodal deep learning approaches to investigate glioblastoma heterogeneity using joint image/RNA-seq analysis. Our results reveal novel genes associated with glioblastoma. By leveraging a combination of whole-slide images and RNA-seq, as well as introducing novel methods to encode RNA-seq data, we identified specific genetic profiles that may explain different patterns of glioblastoma progression. These findings provide new insights into the genetic mechanisms underlying glioblastoma heterogeneity and highlight potential targets for therapeutic intervention.",
        "subjects": [
            "q-bio.QM",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.19867",
        "abstract url": "https://arxiv.org/abs/2410.19867",
        "title": "Simultaneous Dimensionality Reduction for Extracting Useful Representations of Large Empirical Multimodal Datasets",
        "rating": "-1.5",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The quest for simplification in physics drives the exploration of concise mathematical representations for complex systems. This Dissertation focuses on the concept of dimensionality reduction as a means to obtain low-dimensional descriptions from high-dimensional data, facilitating comprehension and analysis. We address the challenges posed by real-world data that defy conventional assumptions, such as complex interactions within neural systems or high-dimensional dynamical systems. Leveraging insights from both theoretical physics and machine learning, this work unifies diverse reduction methods under a comprehensive framework, the Deep Variational Multivariate Information Bottleneck. This framework enables the design of tailored reduction algorithms based on specific research questions. We explore and assert the efficacy of simultaneous reduction approaches over their independent reduction counterparts, demonstrating their superiority in capturing covariation between multiple modalities, while requiring less data. We also introduced novel techniques, such as the Deep Variational Symmetric Information Bottleneck, for general nonlinear simultaneous reduction. We show that the same principle of simultaneous reduction is the key to efficient estimation of mutual information. We show that our new method is able to discover the coordinates of high-dimensional observations of dynamical systems. Through analytical investigations and empirical validations, we shed light on the intricacies of dimensionality reduction methods, paving the way for enhanced data analysis across various domains. We underscore the potential of these methodologies to extract meaningful insights from complex datasets, driving advancements in fundamental research and applied sciences. As these methods evolve, they promise to deepen our understanding of complex systems and inform more effective data analysis strategies.",
        "subjects": [
            "cs.LG",
            "physics.bio-ph",
            "physics.data-an"
        ],
        "comment": "PhD Dissertation, available at Emory EDT @ https://etd.library.emory.edu/concern/etds/hd76s156x?locale=en"
    },
    {
        "paper id": "2411.03333",
        "abstract url": "https://arxiv.org/abs/2411.03333",
        "title": "Analysis of Bipartite Networks in Anime Series: Textual Analysis, Topic Clustering, and Modeling",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "This article analyzes a specific bipartite network that shows the relationships between users and anime, examining how the descriptions of anime influence the formation of user communities. In particular, we introduce a new variable that quantifies the frequency with which words from a description appear in specific word clusters. These clusters are generated from a bigram analysis derived from all descriptions in the database. This approach fully characterizes the dynamics of these communities and shows how textual content affect the cohesion and structure of the social network among anime enthusiasts. Our findings suggest that there may be significant implications for the design of recommendation systems and the enhancement of user experience on anime platforms.",
        "subjects": [
            "cs.SI",
            "stat.AP",
            "stat.CO",
            "stat.ME"
        ],
        "comment": "23 pages, 3 figures, 5 tables"
    },
    {
        "paper id": "2411.04133",
        "abstract url": "https://arxiv.org/abs/2411.04133",
        "title": "Enhancement of Approximation Spaces by the Use of Primals and Neighborhood",
        "rating": "-1.5",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Rough set theory is one of the most widely used and significant approaches for handling incomplete information. It divides the universe in the beginning and uses equivalency relations to produce blocks. Numerous generalized rough set models have been put out and investigated in an effort to increase flexibility and extend the range of possible uses. We introduce four new generalized rough set models that draw inspiration from \"neighborhoods and primals\" in order to make a contribution to this topic. By minimizing the uncertainty regions, these models are intended to assist decision makers in more effectively analyzing and evaluating the provided data. We verify this goal by demonstrating that the existing models outperform certain current method approaches in terms of improving the approximation operators (upper and lower) and accuracy measurements. We claim that the current models can preserve nearly all significant aspects associated with the rough set model. Preserving the monotonic property, which enables us to assess data uncertainty and boost confidence in outcomes, is one of the intriguing characterizations derived from the existing models. With the aid of specific instances, we also compare the areas of the current approach. Finally, we demonstrate that the new strategy we define for our everyday health-related problem yields more accurate findings.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17564",
        "abstract url": "https://arxiv.org/abs/2410.17564",
        "title": "DisenGCD: A Meta Multigraph-assisted Disentangled Graph Learning Framework for Cognitive Diagnosis",
        "rating": "-2",
        "keywords": [
            [
                "Graph"
            ],
            [
                "Diagnosis"
            ],
            [
                "cs.LG",
                "cs.CY"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Existing graph learning-based cognitive diagnosis (CD) methods have made relatively good results, but their student, exercise, and concept representations are learned and exchanged in an implicit unified graph, which makes the interaction-agnostic exercise and concept representations be learned poorly, failing to provide high robustness against noise in students' interactions. Besides, lower-order exercise latent representations obtained in shallow layers are not well explored when learning the student representation. To tackle the issues, this paper suggests a meta multigraph-assisted disentangled graph learning framework for CD (DisenGCD), which learns three types of representations on three disentangled graphs: student-exercise-concept interaction, exercise-concept relation, and concept dependency graphs, respectively. Specifically, the latter two graphs are first disentangled from the interaction graph. Then, the student representation is learned from the interaction graph by a devised meta multigraph learning module; multiple learnable propagation paths in this module enable current student latent representation to access lower-order exercise latent representations, which can lead to more effective nad robust student representations learned; the exercise and concept representations are learned on the relation and dependency graphs by graph attention modules. Finally, a novel diagnostic function is devised to handle three disentangled representations for prediction. Experiments show better performance and robustness of DisenGCD than state-of-the-art CD methods and demonstrate the effectiveness of the disentangled learning framework and meta multigraph module. The source code is available at \\textcolor{red}{\\url{https://github.com/BIMK/Intelligent-Education/tree/main/DisenGCD}}.",
        "subjects": [
            "cs.LG",
            "cs.CY"
        ],
        "comment": "21 pages, Accepted by NeurIPS 2024 as a poster"
    },
    {
        "paper id": "2410.17625",
        "abstract url": "https://arxiv.org/abs/2410.17625",
        "title": "Time-varying Graph Signal Estimation via Dynamic Multi-hop Topologies",
        "rating": "-2",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "Graph"
            ]
        ],
        "abstract": "The assumption of using a static graph to represent multivariate time-varying signals oversimplifies the complexity of modeling their interactions over time. We propose a Dynamic Multi-hop model that captures dynamic interactions among time-varying node signals, while also accounting for time-varying edge signals, by extracting latent edges through topological diffusion and edge pruning. The resulting graphs are time-varying and sparse, capturing key dynamic node interactions and representing signal diffusion to both near and distant neighbors over time. The Dynamic Multi-hop Estimation algorithm is further proposed, accurately representing the interaction dynamics among node signals while enabling adaptive estimation of time-varying multivariate signals spatially and temporally. The Dynamic Multi-hop Estimation is evaluated under two real-world datasets of brain network and stock market for the online estimation of partially observed time-varying signals corrupted by noise.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17643",
        "abstract url": "https://arxiv.org/abs/2410.17643",
        "title": "Approximate Kalman filtering for large-scale systems with an application to hyperthermia cancer treatments",
        "rating": "-2",
        "keywords": [
            [
                "cancer"
            ]
        ],
        "abstract": "Accurate state estimates are required for increasingly complex systems, to enable, for example, feedback control. However, available state estimation schemes are not necessarily real-time feasible for certain large-scale systems. Therefore, we develop in this paper, a real-time feasible state-estimation scheme for a class of large-scale systems that approximates the steady state Kalman filter. In particular, we focus on systems where the state-vector is the result of discretizing the spatial domain, as typically seen in Partial Differential Equations. In such cases, the correlation between states in the state-vector often have an intuitive interpretation on the spatial domain, which can be exploited to obtain a significant reduction in computational complexity, while still providing accurate state estimates. We illustrate these strengths of our method through a hyperthermia cancer treatment case study. The results of the case study show significant improvements in the computation time, while simultaneously obtaining good state estimates, when compared to Ensemble Kalman filters and Kalman filters using reduced-order models.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "6 pages, 6 figures, CDC 2022 conference"
    },
    {
        "paper id": "2410.17644",
        "abstract url": "https://arxiv.org/abs/2410.17644",
        "title": "Comprehensive Evaluation of Matrix Factorization Models for Collaborative Filtering Recommender Systems",
        "rating": "-2",
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "Matrix factorization models are the core of current commercial collaborative filtering Recommender Systems. This paper tested six representative matrix factorization models, using four collaborative filtering datasets. Experiments have tested a variety of accuracy and beyond accuracy quality measures, including prediction, recommendation of ordered and unordered lists, novelty, and diversity. Results show each convenient matrix factorization model attending to their simplicity, the required prediction quality, the necessary recommendation quality, the desired recommendation novelty and diversity, the need to explain recommendations, the adequacy of assigning semantic interpretations to hidden factors, the advisability of recommending to groups of users, and the need to obtain reliability values. To ensure the reproducibility of the experiments, an open framework has been used, and the implementation code is provided.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "14 pages, 5 figures, 3 tables"
    },
    {
        "paper id": "2410.17646",
        "abstract url": "https://arxiv.org/abs/2410.17646",
        "title": "Accelerating soft-constrained MPC for linear systems through online constraint removal",
        "rating": "-2",
        "keywords": [
            [
                "thermal"
            ]
        ],
        "abstract": "Optimization-based controllers, such as Model Predictive Control (MPC), have attracted significant research interest due to their intuitive concept, constraint handling capabilities, and natural application to multi-input multi-output systems. However, the computational complexity of solving a receding horizon problem at each time step remains a challenge for the deployment of MPC. This is particularly the case for systems constrained by many inequalities. Recently, we introduced the concept of constraint-adaptive MPC (ca-MPC) to address this challenge for linear systems with hard constraints. In ca-MPC, at each time step, a subset of the constraints is removed from the optimization problem, thereby accelerating the optimization procedure, while resulting in identical closed-loop behavior. The present paper extends this framework to soft-constrained MPC by detecting and removing constraints based on sub-optimal predicted input sequences, which is rather easy for soft-constrained MPC due to the receding horizon principle and the inclusion of slack variables. We will translate these new ideas explicitly to an offset-free output tracking problem. The effectiveness of these ideas is demonstrated on a two-dimensional thermal transport model, showing a three order of magnitude improvement in online computational time of the MPC scheme.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "6 pages, 5 figures, CDC 2023 conference"
    },
    {
        "paper id": "2410.17731",
        "abstract url": "https://arxiv.org/abs/2410.17731",
        "title": "Time-to-Lie: Identifying Industrial Control System Honeypots Using the Internet Control Message Protocol",
        "rating": "-2",
        "keywords": [
            [
                "Industrial"
            ]
        ],
        "abstract": "The convergence of information and operational technology networks has created previously unforeseen security issues. To address these issues, both researchers and practitioners have integrated threat intelligence methods into the security operations of converged networks, with some of the most valuable tools being honeypots that imitate industrial control systems (ICS). However, the development and deployment of such honeypots is a process rich with pitfalls, which can lead to undiagnosed weaknesses in the threat intelligence being gathered. This paper presents a side-channel method of covertly identifying ICS honeypots using the time-to-live (TTL) values of target devices. We show that many ICS honeypots can be readily identified, via minimal interactions, using only basic networking tools. In a study of over 8,000 devices presenting as ICS systems, we detail how our method compares to an existing honeypot detection approach, and outline what our methodology reveals about the current population of live ICS honeypots. In demonstrating our method, this study aims to raise awareness of the viability of the TTL heuristic and the prevalence of its misconfiguration despite its presence in literature.",
        "subjects": [
            "cs.CR",
            "eess.SY"
        ],
        "comment": "11 pages, 2 listings, 5 tables, 6 figures"
    },
    {
        "paper id": "2410.17734",
        "abstract url": "https://arxiv.org/abs/2410.17734",
        "title": "YOLO-Vehicle-Pro: A Cloud-Edge Collaborative Framework for Object Detection in Autonomous Driving under Adverse Weather Conditions",
        "rating": "-2",
        "keywords": [
            [
                "Autonomous Driving",
                "Vehicle"
            ],
            [
                "dehazing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "With the rapid advancement of autonomous driving technology, efficient and accurate object detection capabilities have become crucial factors in ensuring the safety and reliability of autonomous driving systems. However, in low-visibility environments such as hazy conditions, the performance of traditional object detection algorithms often degrades significantly, failing to meet the demands of autonomous driving. To address this challenge, this paper proposes two innovative deep learning models: YOLO-Vehicle and YOLO-Vehicle-Pro. YOLO-Vehicle is an object detection model tailored specifically for autonomous driving scenarios, employing multimodal fusion techniques to combine image and textual information for object detection. YOLO-Vehicle-Pro builds upon this foundation by introducing an improved image dehazing algorithm, enhancing detection performance in low-visibility environments. In addition to model innovation, this paper also designs and implements a cloud-edge collaborative object detection system, deploying models on edge devices and offloading partial computational tasks to the cloud in complex situations. Experimental results demonstrate that on the KITTI dataset, the YOLO-Vehicle-v1s model achieved 92.1% accuracy while maintaining a detection speed of 226 FPS and an inference time of 12ms, meeting the real-time requirements of autonomous driving. When processing hazy images, the YOLO-Vehicle-Pro model achieved a high accuracy of 82.3% mAP@50 on the Foggy Cityscapes dataset while maintaining a detection speed of 43 FPS.",
        "subjects": [
            "cs.CV",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17738",
        "abstract url": "https://arxiv.org/abs/2410.17738",
        "title": "Towards Safer Planetary Exploration: A Hybrid Architecture for Terrain Traversability Analysis in Mars Rovers",
        "rating": "-2",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "navigation"
            ]
        ],
        "abstract": "The field of autonomous navigation for unmanned ground vehicles (UGVs) is in continuous growth and increasing levels of autonomy have been reached in the last few years. However, the task becomes more challenging when the focus is on the exploration of planet surfaces such as Mars. In those situations, UGVs are forced to navigate through unstable and rugged terrains which, inevitably, open the vehicle to more hazards, accidents, and, in extreme cases, complete mission failure. The paper addresses the challenges of autonomous navigation for unmanned ground vehicles in planetary exploration, particularly on Mars, introducing a hybrid architecture for terrain traversability analysis that combines two approaches: appearance-based and geometry-based. The appearance-based method uses semantic segmentation via deep neural networks to classify different terrain types. This is further refined by pixel-level terrain roughness classification obtained from the same RGB image, assigning different costs based on the physical properties of the soil. The geometry-based method complements the appearance-based approach by evaluating the terrain's geometrical features, identifying hazards that may not be detectable by the appearance-based side. The outputs of both methods are combined into a comprehensive hybrid cost map. The proposed architecture was trained on synthetic datasets and developed as a ROS2 application to integrate into broader autonomous navigation systems for harsh environments. Simulations have been performed in Unity, showing the ability of the method to assess online traversability analysis.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17742",
        "abstract url": "https://arxiv.org/abs/2410.17742",
        "title": "Multi-Layered Safety of Redundant Robot Manipulators via Task-Oriented Planning and Control",
        "rating": "-2",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "Robot"
            ]
        ],
        "abstract": "Ensuring safety is crucial to promote the application of robot manipulators in open workspace. Factors such as sensor errors or unpredictable collisions make the environment full of uncertainties. In this work, we investigate these potential safety challenges on redundant robot manipulators, and propose a task-oriented planning and control framework to achieve multi-layered safety while maintaining efficient task execution. Our approach consists of two main parts: a task-oriented trajectory planner based on multiple-shooting model predictive control method, and a torque controller that allows safe and efficient collision reaction using only proprioceptive data. Through extensive simulations and real-hardware experiments, we demonstrate that the proposed framework can effectively handle uncertain static or dynamic obstacles, and perform disturbance resistance in manipulation tasks when unforeseen contacts occur. All code will be open-sourced to benefit the community.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages, 8 figures. This work has been submitted to the IEEE for possible publication"
    },
    {
        "paper id": "2410.17754",
        "abstract url": "https://arxiv.org/abs/2410.17754",
        "title": "Puncturing Quantum Stabilizer Codes",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Classical coding theory contains several techniques to obtain new codes from other codes, including puncturing and shortening. For quantum codes, a form of puncturing is known, but its description is based on the code space rather than its generators. In this work, we take a different approach and describe puncturing of a stabilizer code by focusing on its stabilizer matrix containing the generators. In this way, we are able to explicitly describe the stabilizer matrix of the punctured code given the stabilizer matrix of the original stabilizer code. This gives an immediate way to construct new codes from old. We indicate how this may be used to search for good or even optimal codes. Furthermore, we show the equivalence to the original description of puncturing and the connection to a shortening of a classical code. Lastly, we generalize the proof of the Griesmer bound from the classical setting to stabilizer codes since the proof relies heavily on the puncturing technique.",
        "subjects": [
            "cs.IT",
            "math.RA"
        ],
        "comment": "Preprint"
    },
    {
        "paper id": "2410.17780",
        "abstract url": "https://arxiv.org/abs/2410.17780",
        "title": "Fiber Activation by Bipolar Stimulation in Deep Brain Stimulation: A Patient Case Study",
        "rating": "-2",
        "keywords": [
            [
                "disease"
            ]
        ],
        "abstract": "Deep Brain Stimulation (DBS) is a therapy widely used for treating the symptoms of neurological disorders. Electrical pulses are chronically delivered in DBS to a disease-specific brain target via a surgically implanted electrode. The stimulating contact configuration, stimulation polarity, as well as amplitude, frequency, and pulse width of the DBS pulse sequence are utilized to optimize the therapeutic effect. In this paper, the utility of therapy individualization by means of patient-specific mathematical modeling is investigated with respect to a specific case of a patient diagnosed with Essential Tremor (ET). Two computational models are compared in their ability to elucidate the impact of DBS stimulation on the dentato-rubrothalamic tract: (i) a conventional model of Volume of Tissue Activated (VTA) and (ii) a well-established neural fiber activation modeling framework known as OSS-DBS. The simulation results are compared with tremor measured in the patient under different DBS settings using a smartphone application. The findings of the study highlight that temporally static VTA models do not adequately describe the differences in the outcomes of bipolar stimulation settings with switched polarity, whereas neural fiber activation models hold potential in this regard. However, it is noted that neither of the investigated models fully accounts for the measured symptom pattern, particularly regarding a bilateral effect produced by unilateral stimulation.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "7 pages, 5 figures, 5 tables"
    },
    {
        "paper id": "2410.17785",
        "abstract url": "https://arxiv.org/abs/2410.17785",
        "title": "TranSPORTmer: A Holistic Approach to Trajectory Understanding in Multi-Agent Sports",
        "rating": "-2",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "forecasting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Understanding trajectories in multi-agent scenarios requires addressing various tasks, including predicting future movements, imputing missing observations, inferring the status of unseen agents, and classifying different global states. Traditional data-driven approaches often handle these tasks separately with specialized models. We introduce TranSPORTmer, a unified transformer-based framework capable of addressing all these tasks, showcasing its application to the intricate dynamics of multi-agent sports scenarios like soccer and basketball. Using Set Attention Blocks, TranSPORTmer effectively captures temporal dynamics and social interactions in an equivariant manner. The model's tasks are guided by an input mask that conceals missing or yet-to-be-predicted observations. Additionally, we introduce a CLS extra agent to classify states along soccer trajectories, including passes, possessions, uncontrolled states, and out-of-play intervals, contributing to an enhancement in modeling trajectories. Evaluations on soccer and basketball datasets show that TranSPORTmer outperforms state-of-the-art task-specific models in player forecasting, player forecasting-imputation, ball inference, and ball imputation. https://youtu.be/8VtSRm8oGoE",
        "subjects": [
            "cs.CV",
            "cs.MA"
        ],
        "comment": "Accepted to ACCV 2024"
    },
    {
        "paper id": "2410.17800",
        "abstract url": "https://arxiv.org/abs/2410.17800",
        "title": "e-Values for Real-Time Residential Electricity Demand Forecast Model Selection",
        "rating": "-2",
        "keywords": [
            [
                "Forecast"
            ]
        ],
        "abstract": "With the growing number of forecasting techniques and the increasing significance of forecast-based operation - particularly in the rapidly evolving energy sector - selecting the most effective forecasting model has become a critical task. Given the dynamic nature of energy forecasting, it is highly advantageous to assess the superiority of forecasting models not only retrospectively but continuously in real-time as new data and evidence becomes available, while simultaneously providing strong probabilistic guarantees for these decisions. In this work, we show that this can be achieved through the mathematical concept of e-values, which has recently gained massive attention in the field of statistics. It allows for unified construction principles for powerful tests and accurate statistical decisions, which can be evaluated at any chosen time points while maintaining an overall probabilistic error control. We extend the use of e-values by developing a simple persistence approach that dynamically combines input forecasts to generate new fused predictions. To demonstrate the performance of our method we apply it to electricity demand forecasts based on different artificial intelligence based models. Our results indicate that e-values are able to improve the accuracy and reliability of forecasts in a dynamic environment, offering a valuable tool for real-time decision-making in the energy sector.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "25 pages, 9 figures"
    },
    {
        "paper id": "2410.17812",
        "abstract url": "https://arxiv.org/abs/2410.17812",
        "title": "PGDiffSeg: Prior-Guided Denoising Diffusion Model with Parameter-Shared Attention for Breast Cancer Segmentation",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "medical",
                "diagnosis",
                "Cancer",
                "tumor"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Early detection through imaging and accurate diagnosis is crucial in mitigating the high mortality rate associated with breast cancer. However, locating tumors from low-resolution and high-noise medical images is extremely challenging. Therefore, this paper proposes a novel PGDiffSeg (Prior-Guided Diffusion Denoising Model with Parameter-Shared Attention) that applies diffusion denoising methods to breast cancer medical image segmentation, accurately recovering the affected areas from Gaussian noise. Firstly, we design a parallel pipeline for noise processing and semantic information processing and propose a parameter-shared attention module (PSA) in multi-layer that seamlessly integrates these two pipelines. This integration empowers PGDiffSeg to incorporate semantic details at multiple levels during the denoising process, producing highly accurate segmentation maps. Secondly, we introduce a guided strategy that leverages prior knowledge to simulate the decision-making process of medical professionals, thereby enhancing the model's ability to locate tumor positions precisely. Finally, we provide the first-ever discussion on the interpretability of the generative diffusion model in the context of breast cancer segmentation. Extensive experiments have demonstrated the superiority of our model over the current state-of-the-art approaches, confirming its effectiveness as a flexible diffusion denoising method suitable for medical image research. Our code will be publicly available later.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17813",
        "abstract url": "https://arxiv.org/abs/2410.17813",
        "title": "Optimal Fault-Tolerant Dispersion on Oriented Grids",
        "rating": "-2",
        "keywords": [
            [
                "robot"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Dispersion of mobile robots over the nodes of an anonymous graph is an important problem and turns out to be a crucial subroutine for designing efficient algorithms for many fundamental graph problems via mobile robots. In this problem, starting from an arbitrary initial distribution of $n$ robots across the $n$ nodes, the goal is to achieve a final configuration where each node holds at most one robot. This paper investigates the dispersion problem on an oriented grid, considering the possibility of robot failures (crashes) at any time during the algorithm's execution. We present a crash-tolerant dispersion algorithm that solves the dispersion problem on an anonymous oriented grid in $O(\\sqrt{n})$ time and using $O(\\log n)$ bits of memory per robot. The algorithm is optimal in terms of both time and memory per robot. We further extend this algorithm to deal with weak Byzantine robots. The weak Byzantine fault dispersion algorithm takes optimal $O(\\sqrt{n})$ rounds but requires $O(n\\log n)$ bits of memory per robot.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2405.02002"
    },
    {
        "paper id": "2410.17814",
        "abstract url": "https://arxiv.org/abs/2410.17814",
        "title": "Learning Lossless Compression for High Bit-Depth Volumetric Medical Image",
        "rating": "-2",
        "keywords": [
            [
                "Depth"
            ],
            [
                "Medical"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Recent advances in learning-based methods have markedly enhanced the capabilities of image compression. However, these methods struggle with high bit-depth volumetric medical images, facing issues such as degraded performance, increased memory demand, and reduced processing speed. To address these challenges, this paper presents the Bit-Division based Lossless Volumetric Image Compression (BD-LVIC) framework, which is tailored for high bit-depth medical volume compression. The BD-LVIC framework skillfully divides the high bit-depth volume into two lower bit-depth segments: the Most Significant Bit-Volume (MSBV) and the Least Significant Bit-Volume (LSBV). The MSBV concentrates on the most significant bits of the volumetric medical image, capturing vital structural details in a compact manner. This reduction in complexity greatly improves compression efficiency using traditional codecs. Conversely, the LSBV deals with the least significant bits, which encapsulate intricate texture details. To compress this detailed information effectively, we introduce an effective learning-based compression model equipped with a Transformer-Based Feature Alignment Module, which exploits both intra-slice and inter-slice redundancies to accurately align features. Subsequently, a Parallel Autoregressive Coding Module merges these features to precisely estimate the probability distribution of the least significant bit-planes. Our extensive testing demonstrates that the BD-LVIC framework not only sets new performance benchmarks across various datasets but also maintains a competitive coding speed, highlighting its significant potential and practical utility in the realm of volumetric medical image compression.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "13 pages"
    },
    {
        "paper id": "2410.17834",
        "abstract url": "https://arxiv.org/abs/2410.17834",
        "title": "Non-intrusive Speech Quality Assessment with Diffusion Models Trained on Clean Speech",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Quality Assessment"
            ],
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Diffusion models have found great success in generating high quality, natural samples of speech, but their potential for density estimation for speech has so far remained largely unexplored. In this work, we leverage an unconditional diffusion model trained only on clean speech for the assessment of speech quality. We show that the quality of a speech utterance can be assessed by estimating the likelihood of a corresponding sample in the terminating Gaussian distribution, obtained via a deterministic noising process. The resulting method is purely unsupervised, trained only on clean speech, and therefore does not rely on annotations. Our diffusion-based approach leverages clean speech priors to assess quality based on how the input relates to the learned distribution of clean data. Our proposed log-likelihoods show promising results, correlating well with intrusive speech quality metrics such as POLQA and SI-SDR.",
        "subjects": [
            "eess.AS",
            "cs.LG",
            "cs.SD"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17876",
        "abstract url": "https://arxiv.org/abs/2410.17876",
        "title": "Fast classical simulation of qubit-qudit hybrid systems",
        "rating": "-2",
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "Simulating quantum circuits is a computationally intensive task that relies heavily on tensor products and matrix multiplications, which can be inefficient. Recent advancements, eliminate the need for tensor products and matrix multiplications, offering significant improvements in efficiency and parallelization. Extending these optimizations, we adopt a block-simulation methodology applicable to qubit-qudit hybrid systems. This method interprets the statevector as a collection of blocks and applies gates without computing the entire circuit unitary. Our method, a spiritual successor of the simulator QuDiet \\cite{Chatterjee_2023}, utilizes this block-simulation method, thereby gaining major improvements over the simulation methods used by its predecessor. We exhibit that the proposed method is approximately 10$\\times$ to 1000$\\times$ faster than the state-of-the-art simulator for simulating multi-level quantum systems with various benchmark circuits.",
        "subjects": [
            "quant-ph",
            "cs.ET"
        ],
        "comment": "12 pages, 10 figures"
    },
    {
        "paper id": "2410.17887",
        "abstract url": "https://arxiv.org/abs/2410.17887",
        "title": "Average-case matrix discrepancy: satisfiability bounds",
        "rating": "-2",
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "Given a sequence of $d \\times d$ symmetric matrices $\\{\\mathbf{W}_i\\}_{i=1}^n$, and a margin $\u0394> 0$, we investigate whether it is possible to find signs $(\u03b5_1, \\dots, \u03b5_n) \\in \\{\\pm 1\\}^n$ such that the operator norm of the signed sum satisfies $\\|\\sum_{i=1}^n \u03b5_i \\mathbf{W}_i\\|_\\mathrm{op} \\leq \u0394$. Kunisky and Zhang (2023) recently introduced a random version of this problem, where the matrices $\\{\\mathbf{W}_i\\}_{i=1}^n$ are drawn from the Gaussian orthogonal ensemble. This model can be seen as a random variant of the celebrated Matrix Spencer conjecture and as a matrix-valued analog of the symmetric binary perceptron in statistical physics. In this work, we establish a satisfiability transition in this problem as $n, d \\to \\infty$ with $n / d^2 \\to \u03c4> 0$. Our contributions are twofold. First, we prove that the expected number of solutions with margin $\u0394=\u03ba\\sqrt{n}$ has a sharp threshold at a critical $\u03c4_1(\u03ba)$: for $\u03c4< \u03c4_1(\u03ba)$ the problem is typically unsatisfiable, while for $\u03c4> \u03c4_1(\u03ba)$ the average number of solutions becomes exponentially large. Second, combining a second-moment method with recent results from Altschuler (2023) on margin concentration in perceptron-type problems, we identify a second threshold $\u03c4_2(\u03ba)$, such that for $\u03c4>\u03c4_2(\u03ba)$ the problem admits solutions with high probability. In particular, we establish for the first time that a system of $n = \u0398(d^2)$ Gaussian random matrices can be balanced so that the spectrum of the resulting matrix macroscopically shrinks compared to the typical semicircle law. Our proofs rely on deriving concentration inequalities and large deviation properties for the law of correlated Gaussian matrices under spectral norm constraints, extending results in the literature and offering insights of independent interest.",
        "subjects": [
            "math.PR",
            "cond-mat.dis-nn",
            "cs.DM",
            "math.CO"
        ],
        "comment": "29 pages, 2 figures"
    },
    {
        "paper id": "2410.17901",
        "abstract url": "https://arxiv.org/abs/2410.17901",
        "title": "ELAICHI: Enhancing Low-resource TTS by Addressing Infrequent and Low-frequency Character Bigrams",
        "rating": "-2",
        "keywords": [
            [
                "speech enhancement"
            ],
            [
                "Text-to-Speech"
            ],
            [
                "cs.CL",
                "eess.AS"
            ]
        ],
        "abstract": "Recent advancements in Text-to-Speech (TTS) technology have led to natural-sounding speech for English, primarily due to the availability of large-scale, high-quality web data. However, many other languages lack access to such resources, relying instead on limited studio-quality data. This scarcity results in synthesized speech that often suffers from intelligibility issues, particularly with low-frequency character bigrams. In this paper, we propose three solutions to address this challenge. First, we leverage high-quality data from linguistically or geographically related languages to improve TTS for the target language. Second, we utilize low-quality Automatic Speech Recognition (ASR) data recorded in non-studio environments, which is refined using denoising and speech enhancement models. Third, we apply knowledge distillation from large-scale models using synthetic data to generate more robust outputs. Our experiments with Hindi demonstrate significant reductions in intelligibility issues, as validated by human evaluators. We propose this methodology as a viable alternative for languages with limited access to high-quality data, enabling them to collectively benefit from shared resources.",
        "subjects": [
            "cs.CL",
            "eess.AS"
        ],
        "comment": "11 pages, 1 figure, 3 tables"
    },
    {
        "paper id": "2410.17908",
        "abstract url": "https://arxiv.org/abs/2410.17908",
        "title": "Fully smooth one shot multipartite soft covering of quantum states without pairwise independence",
        "rating": "-2",
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "We provide a powerful machinery to prove fully smooth one shot multipartite covering, aka convex split, type results for quantum states. In the important case of smooth multipartite convex split for classical quantum states, aka smooth multipartite soft covering, our machinery works even when certain marginals of these states do not satisfy pairwise independence. The recent paper of Sen gave the first proof of fully smooth multipartite convex split by simplifying and extending a technique called telescoping, developed originally for convex split by (arXiv:2304.12056). However, that work as well as all earlier works on convex split assumed pairwise or even more independence amongst suitable marginals of the quantum states. We develop our machinery by leveraging known results from (arXiv:1806.07278) involving tilting and augmentation smoothing of quantum states, combined with a novel observation that a natural quantum operation `flattening' quantum states actually preserves the fidelity. This machinery is powerful enough to lead to non pairwise independent results as mentioned above. As an application of our soft covering lemma without pairwise independence, we prove the `natural' one shot inner bounds for sending private classical information over a quantum wiretap interference channel, even when the classical encoders at the input lose pairwise independence in their encoding strategies to a certain extent. This result was unknown earlier even in the classical setting.",
        "subjects": [
            "quant-ph",
            "cs.IT"
        ],
        "comment": "53 pages"
    },
    {
        "paper id": "2410.17910",
        "abstract url": "https://arxiv.org/abs/2410.17910",
        "title": "Slot: Provenance-Driven APT Detection through Graph Reinforcement Learning",
        "rating": "-2",
        "keywords": [
            [
                "Graph"
            ],
            [
                "attack"
            ]
        ],
        "abstract": "Advanced Persistent Threats (APTs) represent sophisticated cyberattacks characterized by their ability to remain undetected within the victim system for extended periods, aiming to exfiltrate sensitive data or disrupt operations. Existing detection approaches often struggle to effectively identify these complex threats, construct the attack chain for defense facilitation, or resist adversarial attacks. To overcome these challenges, we propose Slot, an advanced APT detection approach based on provenance graphs and graph reinforcement learning. Slot excels in uncovering multi-level hidden relationships, such as causal, contextual, and indirect connections, among system behaviors through provenance graph mining. By pioneering the integration of graph reinforcement learning, Slot dynamically adapts to new user activities and evolving attack strategies, enhancing its resilience against adversarial attacks. Additionally, Slot automatically constructs the attack chain according to detected attacks with clustering algorithms, providing precise identification of attack paths and facilitating the development of defense strategies. Evaluations with real-world datasets demonstrate Slot's outstanding accuracy, efficiency, adaptability, and robustness in APT detection, with most metrics surpassing state-of-the-art methods. Additionally, case studies conducted to assess Slot's effectiveness in supporting APT defense further establish it as a practical and reliable tool for cybersecurity protection.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17927",
        "abstract url": "https://arxiv.org/abs/2410.17927",
        "title": "Dynamic Modeling and Vibration Analysis of Large Deployable Mesh Reflectors",
        "rating": "-2",
        "keywords": [
            [
                "satellite"
            ]
        ],
        "abstract": "Large deployable mesh reflectors are essential for space applications, providing precise reflecting surfaces for high-gain antennas used in satellite communications, Earth observation, and deep-space missions. During on-orbit missions, active shape adjustment and attitude control are crucial for maintaining surface accuracy and proper orientation for these reflectors, ensuring optimal performance. Preventing resonance through thorough dynamic modeling and vibration analysis is vital to avoid structural damage and ensure stability and reliability. Existing dynamic modeling approaches, such as wave and finite element methods, often fail to accurately predict dynamic responses due to the limited capability of handling three-dimensional reflectors or the oversimplification of cable members of a reflector. This paper proposes the Cartesian spatial discretization method for dynamic modeling and vibration analysis of cable-network structures in large deployable mesh reflectors. This method defines cable member positions as a summation of internal and boundary-induced terms within a global Cartesian coordinate system. Numerical simulation on a two-dimensional cable-network structure and a center-feed mesh reflector demonstrates the superiority of the proposed method over traditional approaches, highlighting its accuracy and versatility, and establishing it as a robust tool for analyzing three-dimensional complex reflector configurations.",
        "subjects": [
            "cs.CE",
            "physics.app-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17959",
        "abstract url": "https://arxiv.org/abs/2410.17959",
        "title": "Medical Imaging Complexity and its Effects on GAN Performance",
        "rating": "-2",
        "keywords": [
            [
                "GAN"
            ],
            [
                "Medical",
                "clinical"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The proliferation of machine learning models in diverse clinical applications has led to a growing need for high-fidelity, medical image training data. Such data is often scarce due to cost constraints and privacy concerns. Alleviating this burden, medical image synthesis via generative adversarial networks (GANs) emerged as a powerful method for synthetically generating photo-realistic images based on existing sets of real medical images. However, the exact image set size required to efficiently train such a GAN is unclear. In this work, we experimentally establish benchmarks that measure the relationship between a sample dataset size and the fidelity of the generated images, given the dataset's distribution of image complexities. We analyze statistical metrics based on delentropy, an image complexity measure rooted in Shannon's entropy in information theory. For our pipeline, we conduct experiments with two state-of-the-art GANs, StyleGAN 3 and SPADE-GAN, trained on multiple medical imaging datasets with variable sample sizes. Across both GANs, general performance improved with increasing training set size but suffered with increasing complexity.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Accepted to ACCV, Workshop on Generative AI for Synthetic Medical Data"
    },
    {
        "paper id": "2410.17989",
        "abstract url": "https://arxiv.org/abs/2410.17989",
        "title": "Striking a New Chord: Neural Networks in Music Information Dynamics",
        "rating": "-2",
        "keywords": [
            [
                "Music"
            ]
        ],
        "abstract": "Initiating a quest to unravel the complexities of musical aesthetics through the lens of information dynamics, our study delves into the realm of musical sequence modeling, drawing a parallel between the sequential structured nature of music and natural language. Despite the prevalence of neural network models in MIR, the modeling of symbolic music events as applied to music cognition and music neuroscience has largely relied on statistical models. In this \"proof of concept\" paper we posit the superiority of neural network models over statistical models for predicting musical events. Specifically, we compare LSTM, Transformer, and GPT models against a widely-used markov model to predict a chord event following a sequence of chords. Utilizing chord sequences from the McGill Billboard dataset, we trained each model to predict the next chord from a given sequence of chords. We found that neural models significantly outperformed statistical ones in our study. Specifically, the LSTM with attention model led with an accuracy of 0.329, followed by Transformer models at 0.321, GPT at 0.301, and standard LSTM at 0.191. Variable Order Markov and Markov trailed behind with accuracies of 0.277 and 0.140, respectively. Encouraged by these results, we extended our investigation to multidimensional modeling, employing a many-to-one LSTM, LSTM with attention, Transformer, and GPT predictors. These models were trained on both chord and melody lines as two-dimensional data using the CoCoPops Billboard dataset, achieving an accuracy of 0.083, 0.312, 0.271, and 0.120, respectively, in predicting the next chord.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "Presented in Society for Music Perception and Cognition 2024 Conference at the Banff Centre for Arts and Creativity"
    },
    {
        "paper id": "2410.18002",
        "abstract url": "https://arxiv.org/abs/2410.18002",
        "title": "Digital Network Twins for Next-generation Wireless: Creation, Optimization, and Challenges",
        "rating": "-2",
        "keywords": [
            [
                "forecasting"
            ]
        ],
        "abstract": "Digital network twins (DNTs), by representing a physical network using a virtual model, offer significant benefits such as streamlined network development, enhanced productivity, and cost reduction for next-generation (nextG) communication infrastructure. Existing works mainly describe the deployment of DNT technologies in various service sections.The full life cycle of DNTs for telecommunication has not yet been comprehensively studied, particularly in the aspects of fine-grained creation, real-time adaptation, resource-efficient deployment, and security protection. This article presents an in-depth overview of DNTs, exploring their concrete integration into networks and communication, covering the fundamental designs, the emergent applications, and critical challenges in multiple dimensions. We also include two detailed case studies to illustrate how DNTs can be applied in real-world scenarios such as wireless traffic forecasting and edge caching. Additionally, a forward-looking vision of the research opportunities in tackling the challenges of DNTs is provided, aiming to fully maximize the benefits of DNTs in nextG networks.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18005",
        "abstract url": "https://arxiv.org/abs/2410.18005",
        "title": "Random space-time sampling and reconstruction of sparse bandlimited graph diffusion field",
        "rating": "-2",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "In this work, we investigate the sampling and reconstruction of spectrally $s$-sparse bandlimited graph signals governed by heat diffusion processes. We propose a random space-time sampling regime, referred to as {randomized} dynamical sampling, where a small subset of space-time nodes is randomly selected at each time step based on a probability distribution. To analyze the recovery problem, we establish a rigorous mathematical framework by introducing the parameter \\textit{the dynamic spectral graph weighted coherence}. This key parameter governs the number of space-time samples needed for stable recovery and extends the idea of variable density sampling to the context of dynamical systems. By optimizing the sampling probability distribution, we show that as few as $\\mathcal{O}(s \\log(k))$ space-time samples are sufficient for accurate reconstruction in optimal scenarios, where $k$ denotes the bandwidth of the signal. Our framework encompasses both static and dynamic cases, demonstrating a reduction in the number of spatial samples needed at each time step by exploiting temporal correlations. Furthermore, we provide a computationally efficient and robust algorithm for signal reconstruction. Numerical experiments validate our theoretical results and illustrate the practical efficacy of our proposed methods.",
        "subjects": [
            "math.NA",
            "cs.IT"
        ],
        "comment": "23 pages"
    },
    {
        "paper id": "2410.18161",
        "abstract url": "https://arxiv.org/abs/2410.18161",
        "title": "Bridging the Diagnostic Divide: Classical Computer Vision and Advanced AI methods for distinguishing ITB and CD through CTE Scans",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "biomarker",
                "diagnosis",
                "CT",
                "Disease",
                "clinical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Differentiating between Intestinal Tuberculosis (ITB) and Crohn's Disease (CD) poses a significant clinical challenge due to their similar symptoms, clinical presentations, and imaging features. This study leverages Computed Tomography Enterography (CTE) scans, deep learning, and traditional computer vision to address this diagnostic dilemma. A consensus among radiologists from renowned institutions has recognized the visceral-to-subcutaneous fat (VF/SF) ratio as a surrogate biomarker for differentiating between ITB and CD. Previously done manually, we propose a novel 2D image computer vision algorithm for auto-segmenting subcutaneous fat to automate this ratio calculation, enhancing diagnostic efficiency and objectivity. As a benchmark, we compare the results to those obtained using the TotalSegmentator tool, a popular deep learning-based software for automatic segmentation of anatomical structures, and manual calculations by radiologists. We also demonstrated the performance on 3D CT volumes using a slicing method and provided a benchmark comparison of the algorithm with the TotalSegmentator tool. Additionally, we propose a scoring approach to integrate scores from radiological features, such as the fat ratio and pulmonary TB probability, into a single score for diagnosis. We trained a ResNet10 model on a dataset of CTE scans with samples from ITB, CD, and normal patients, achieving an accuracy of 75%. To enhance interpretability and gain clinical trust, we integrated the explainable AI technique Grad-CAM with ResNet10 to explain the model's predictions. Due to the small dataset size (100 total cases), the feature-based scoring system is considered more reliable and trusted by radiologists compared to the deep learning model for disease diagnosis.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.GR"
        ],
        "comment": "9 pages, 3 figures, 3 algorithms"
    },
    {
        "paper id": "2410.18178",
        "abstract url": "https://arxiv.org/abs/2410.18178",
        "title": "Quantum linear system algorithm with optimal queries to initial state preparation",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum algorithms for linear systems produce the solution state $A^{-1}|b\\rangle$ by querying two oracles: $O_A$ that block encodes the coefficient matrix and $O_b$ that prepares the initial state. We present a quantum linear system algorithm making $\\mathbf\u0398\\left(1/\\sqrt{p}\\right)$ queries to $O_b$, which is optimal in the success probability, and $\\mathbf{O}\\left(\u03ba\\log\\left(1/p\\right)\\left(\\log\\log\\left(1/p\\right)+\\log\\left({1}/\u03b5\\right)\\right)\\right)$ queries to $O_A$, nearly optimal in all parameters including the condition number and accuracy. Notably, our complexity scaling of initial state preparation holds even when $p$ is not known $\\textit{a priori}$. This contrasts with recent results achieving $\\mathbf{O}\\left(\u03ba\\log\\left({1}/\u03b5\\right)\\right)$ complexity to both oracles, which, while optimal in $O_A$, is highly suboptimal in $O_b$ as $\u03ba$ can be arbitrarily larger than $1/\\sqrt{p}$. In various applications such as solving differential equations, preparing ground states of operators with real spectra, and estimating and transforming eigenvalues of non-normal matrices, we can further improve the dependence on $p$ using a block preconditioning scheme to nearly match or outperform best previous results based on other methods, which also furnishes an extremely simple quantum linear system algorithm with an optimal query complexity to $O_A$. Underlying our results is a new Variable Time Amplitude Amplification algorithm with Tunable thresholds (Tunable VTAA), which fully characterizes generic nested amplitude amplifications, improves the $\\ell_1$-norm input cost scaling of Ambainis to an $\\ell_{\\frac{2}{3}}$-quasinorm scaling, and admits a deterministic amplification schedule for the quantum linear system problem.",
        "subjects": [
            "quant-ph",
            "cs.DS",
            "math.NA"
        ],
        "comment": "86 pages, 3 figures"
    },
    {
        "paper id": "2410.18254",
        "abstract url": "https://arxiv.org/abs/2410.18254",
        "title": "Refining Ky Fan's majorization relation with linear programming",
        "rating": "-2",
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "A separable version of Ky Fan's majorization relation is proven for a sum of two operators that are each a tensor product of two positive semi-definite operators. In order to prove it, upper bounds are established for the relevant largest eigenvalue sums in terms of the optimal values of certain linear programs. The objective function of these linear programs is the dual of the direct sum of the spectra of the summands. The feasible sets are bounded polyhedra determined by positive numbers, called alignment terms, that quantify the overlaps between pairs of largest eigenvalue spaces of the summands. By appealing to geometric considerations, tight upper bounds are established on the alignment terms of tensor products of positive semi-definite operators. As an application, the spin alignment conjecture in quantum information theory is affirmatively resolved to the 2-letter level. Consequently, the coherent information of platypus channels is additive to the 2-letter level.",
        "subjects": [
            "quant-ph",
            "cs.IT",
            "math.RA"
        ],
        "comment": "34 pages, 2 figures"
    },
    {
        "paper id": "2410.18297",
        "abstract url": "https://arxiv.org/abs/2410.18297",
        "title": "A Pilot Study on Clinician-AI Collaboration in Diagnosing Depression from Speech",
        "rating": "-2",
        "keywords": [
            [
                "Diagnosing"
            ]
        ],
        "abstract": "This study investigates clinicians' perceptions and attitudes toward an assistive artificial intelligence (AI) system that employs a speech-based explainable ML algorithm for detecting depression. The AI system detects depression from vowel-based spectrotemporal variations of speech and generates explanations through explainable AI (XAI) methods. It further provides decisions and explanations at various temporal granularities, including utterance groups, individual utterances, and within each utterance. A small-scale user study was conducted to evaluate users' perceived usability of the system, trust in the system, and perceptions of design factors associated with several elements of the system. Quantitative and qualitative analysis of the collected data indicates both positive and negative aspects that influence clinicians' perception toward the AI. Results from quantitative analysis indicate that providing more AI explanations enhances user trust but also increases system complexity. Qualitative analysis indicates the potential of integrating such systems into the current diagnostic and screening workflow, but also highlights existing limitations including clinicians' reduced familiarity with AI/ML systems and the need for user-friendly and intuitive visualizations of speech information.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "accepted at the IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI 2024)"
    },
    {
        "paper id": "2410.18323",
        "abstract url": "https://arxiv.org/abs/2410.18323",
        "title": "Experimental Validation of a 3GPP Compliant 5G-Based Positioning System",
        "rating": "-2",
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "The advent of 5G positioning techniques by 3GPP has unlocked possibilities for applications in public safety, vehicular systems, and location-based services. However, these applications demand accurate and reliable positioning performance, which has led to the proposal of newer positioning techniques. To further advance the research on these techniques, in this paper, we develop a 3GPP-compliant 5G positioning testbed, incorporating gNodeBs (gNBs) and User Equipment (UE). The testbed uses New Radio (NR) Positioning Reference Signals (PRS) transmitted by the gNB to generate Time of Arrival (TOA) estimates at the UE. We mathematically model the inter-gNB and UE-gNB time offsets affecting the TOA estimates and examine their impact on positioning performance. Additionally, we propose a calibration method for estimating these time offsets. Furthermore, we investigate the environmental impact on the TOA estimates. Our findings are based on our mathematical model and supported by experimental results.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "8 pages, 9 figures, Accepted in ACM Wintech 2024"
    },
    {
        "paper id": "2410.18329",
        "abstract url": "https://arxiv.org/abs/2410.18329",
        "title": "When Group Spirit Meets Personal Journeys: Exploring Motivational Dynamics and Design Opportunities in Group Therapy",
        "rating": "-2",
        "keywords": [
            [
                "medical",
                "health",
                "healthcare"
            ]
        ],
        "abstract": "Psychotherapy, such as cognitive-behavioral therapy (CBT), is effective in treating various mental disorders. Technology-facilitated mental health therapy improves client engagement through methods like digitization or gamification. However, these innovations largely cater to individual therapy, ignoring the potential of group therapy-a treatment for multiple clients concurrently, which enables individual clients to receive various perspectives in the treatment process and also addresses the scarcity of healthcare practitioners to reduce costs. Notwithstanding its cost-effectiveness and unique social dynamics that foster peer learning and community support, group therapy, such as group CBT, faces the issue of attrition. While existing medical work has developed guidelines for therapists, such as establishing leadership and empathy to facilitate group therapy, understanding about the interactions between each stakeholder is still missing. To bridge this gap, this study examined a group CBT program called the Serigaya Methamphetamine Relapse Prevention Program (SMARPP) as a case study to understand stakeholder coordination and communication, along with factors promoting and hindering continuous engagement in group therapy. In-depth interviews with eight facilitators and six former clients from SMARPP revealed the motivators and demotivators for facilitator-facilitator, client-client, and facilitator-client communications. Our investigation uncovers the presence of discernible conflicts between clients' intrapersonal motivation as well as interpersonal motivation in the context of group therapy through the lens of self-determination theory. We discuss insights and research opportunities for the HCI community to mediate such tension and enhance stakeholder communication in future technology-assisted group therapy settings.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18367",
        "abstract url": "https://arxiv.org/abs/2410.18367",
        "title": "Synthesis of Binary-Input Multi-Valued Output Optical Cascades for Reversible and Quantum Technologies",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "This paper extends the decomposition from the group theory based methods of Sasao and Saraivanov to design binary input multivalued output quantum cascades realized with optical NOT, SWAP, and Fredkin Gates. We present this method for 3, 5, and 7 valued outputs, but in general it can be used for odd prime valued outputs. The method can be extended to realize hybrid functions with different valued outputs. A class of local transformations is presented that can simplify the final cascade circuits. Using these simplifying transformations, we present an upper bound on the maximum number of gates in an arbitrary $n$-variable input and $k$-valued output function.",
        "subjects": [
            "quant-ph",
            "cs.ET",
            "math.GR"
        ],
        "comment": "31 pages, 64 figures"
    },
    {
        "paper id": "2410.18370",
        "abstract url": "https://arxiv.org/abs/2410.18370",
        "title": "Structured Connectivity for 6G Reflex Arc: Task-Oriented Virtual User and New Uplink-Downlink Tradeoff",
        "rating": "-2",
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "To accommodate the evolving demands of unmanned operations, the future sixth-generation (6G) network will support not only communication links but also sensing-communication-computing-control ($\\mathbf{SC}^3$) loops. In each $\\mathbf{SC}^3$ cycle, the sensor uploads sensing data to the computing center, and the computing center calculates the control command and sends it to the actuator to take action. To maintain the task-level connections between the sensor-computing center link and the computing center-actuator link, we propose to treat the sensor and actuator as a virtual user. In this way, the two communication links of the $\\mathbf{SC}^3$ loop become the uplink and downlink (UL&DL) of the virtual user. Based on the virtual user, we propose a task-oriented UL&DL optimization scheme. This scheme jointly optimizes UL&DL transmit power, time, bandwidth, and CPU frequency to minimize the control linear quadratic regulator (LQR) cost. We decouple the complex problem into a convex UL&DL bandwidth allocation problem with the closed-form solution for the optimal time allocation. Simulation results demonstrate that the proposed scheme achieves a task-level balance between the UL&DL, surpassing conventional communication schemes that optimize each link separately.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18388",
        "abstract url": "https://arxiv.org/abs/2410.18388",
        "title": "Irregular Tensor Low-Rank Representation for Hyperspectral Image Representation",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Hyperspectral Image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Spectral variation is a common problem for hyperspectral image (HSI) representation. Low-rank tensor representation is an important approach to alleviate spectral variations. However, the spatial distribution of the HSI is always irregular, while the previous tensor low-rank representation methods can only be applied to the regular data cubes, which limits the performance. To remedy this issue, in this paper we propose a novel irregular tensor low-rank representation model. We first segment the HSI data into several irregular homogeneous regions. Then, we propose a novel irregular tensor low-rank representation method that can efficiently model the irregular 3D cubes. We further use a non-convex nuclear norm to pursue the low-rankness and introduce a negative global low-rank term that improves global consistency. This proposed model is finally formulated as a convex-concave optimization problem and solved by alternative augmented Lagrangian method. Through experiments on four public datasets, the proposed method outperforms the existing low-rank based HSI methods significantly. Code is available at: https://github.com/hb-studying/ITLRR.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18413",
        "abstract url": "https://arxiv.org/abs/2410.18413",
        "title": "AC-Network-Informed DC Optimal Power Flow for Electricity Markets",
        "rating": "-2",
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "This paper presents a parametric quadratic approximation of the AC optimal power flow (AC-OPF) problem for time-sensitive and market-based applications. The parametric approximation preserves the physics-based but simple representation provided by the DC-OPF model and leverages market and physics information encoded in the data-driven demand-dependent parameters. To enable the deployment of the proposed model for real-time applications, we propose a supervised learning approach to predict near-optimal parameters, given a certain metric concerning the dispatch quantities and locational marginal prices (LMPs). The training dataset is generated based on the solution of the accurate AC-OPF problem and a bilevel optimization problem, which calibrates parameters satisfying two market properties: cost recovery and revenue adequacy. We show the proposed approach's performance in various test systems in terms of cost and dispatch approximation errors, LMPs, market properties satisfaction, dispatch feasibility, and generalizability with respect to N-1 network topologies.",
        "subjects": [
            "math.OC",
            "eess.SY"
        ],
        "comment": "11 pages, 6 figures, 52nd Hawaii International Conference on System"
    },
    {
        "paper id": "2410.19861",
        "abstract url": "https://arxiv.org/abs/2410.19861",
        "title": "Impact of uncertainties on the Stability Lobe Diagram for vibration evaluation in milling",
        "rating": "-2",
        "keywords": [
            [
                "depth"
            ],
            [
                "trajectory"
            ]
        ],
        "abstract": "Despite being the subject of study for several years, excessive vibration persists in the machining of metal parts. In this context, the Stability Lobe Diagram (SLD) is presented as a viable tool to mitigate this problem as a function of axial depth of cut and spindle speed. However, its accurate construction is subject to the consideration of multiple parameters and models, whose application may be affected by certain inherent uncertainties. In turn, this impacts its accuracy, especially in the stability and instability regions. The present study aims to characterize these uncertainties, analyze their influence on the SLD, and propose strategies for their reduction. Ultimately, the goal is to facilitate the user's decision-making when choosing the trajectory generation parameters.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17655",
        "abstract url": "https://arxiv.org/abs/2410.17655",
        "title": "Mapping the Media Landscape: Predicting Factual Reporting and Political Bias Through Web Interactions",
        "rating": "-2.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "bird's-eye view"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Bias assessment of news sources is paramount for professionals, organizations, and researchers who rely on truthful evidence for information gathering and reporting. While certain bias indicators are discernible from content analysis, descriptors like political bias and fake news pose greater challenges. In this paper, we propose an extension to a recently presented news media reliability estimation method that focuses on modeling outlets and their longitudinal web interactions. Concretely, we assess the classification performance of four reinforcement learning strategies on a large news media hyperlink graph. Our experiments, targeting two challenging bias descriptors, factual reporting and political bias, showed a significant performance improvement at the source media level. Additionally, we validate our methods on the CLEF 2023 CheckThat! Lab challenge, outperforming the reported results in both, F1-score and the official MAE metric. Furthermore, we contribute by releasing the largest annotated dataset of news source media, categorized with factual reporting and political bias labels. Our findings suggest that profiling news media sources based on their hyperlink interactions over time is feasible, offering a bird's-eye view of evolving media landscapes.",
        "subjects": [
            "cs.AI",
            "cs.CY",
            "cs.LG"
        ],
        "comment": "Accepted to CLEF 2024"
    },
    {
        "paper id": "2410.17859",
        "abstract url": "https://arxiv.org/abs/2410.17859",
        "title": "DataTales: A Benchmark for Real-World Intelligent Data Narration",
        "rating": "-2.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "tabular"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "We introduce DataTales, a novel benchmark designed to assess the proficiency of language models in data narration, a task crucial for transforming complex tabular data into accessible narratives. Existing benchmarks often fall short in capturing the requisite analytical complexity for practical applications. DataTales addresses this gap by offering 4.9k financial reports paired with corresponding market data, showcasing the demand for models to create clear narratives and analyze large datasets while understanding specialized terminology in the field. Our findings highlights the significant challenge that language models face in achieving the necessary precision and analytical depth for proficient data narration, suggesting promising avenues for future model development and evaluation methodologies.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17922",
        "abstract url": "https://arxiv.org/abs/2410.17922",
        "title": "Guide for Defense (G4D): Dynamic Guidance for Robust and Balanced Defense in Large Language Models",
        "rating": "-2.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "chemistry"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "With the extensive deployment of Large Language Models (LLMs), ensuring their safety has become increasingly critical. However, existing defense methods often struggle with two key issues: (i) inadequate defense capabilities, particularly in domain-specific scenarios like chemistry, where a lack of specialized knowledge can lead to the generation of harmful responses to malicious queries. (ii) over-defensiveness, which compromises the general utility and responsiveness of LLMs. To mitigate these issues, we introduce a multi-agents-based defense framework, Guide for Defense (G4D), which leverages accurate external information to provide an unbiased summary of user intentions and analytically grounded safety response guidance. Extensive experiments on popular jailbreak attacks and benign datasets show that our G4D can enhance LLM's robustness against jailbreak attacks on general and domain-specific scenarios without compromising the model's general functionality.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17933",
        "abstract url": "https://arxiv.org/abs/2410.17933",
        "title": "Multi-Continental Healthcare Modelling Using Blockchain-Enabled Federated Learning",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Healthcare"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "One of the biggest challenges of building artificial intelligence (AI) model in healthcare area is the data sharing. Since healthcare data is private, sensitive, and heterogeneous, collecting sufficient data for modelling is exhausted, costly, and sometimes impossible. In this paper, we propose a framework for global healthcare modelling using datasets from multi-continents (Europe, North America and Asia) while without sharing the local datasets, and choose glucose management as a study model to verify its effectiveness. Technically, blockchain-enabled federated learning is implemented with adaption to make it meet with the privacy and safety requirements of healthcare data, meanwhile rewards honest participation and penalize malicious activities using its on-chain incentive mechanism. Experimental results show that the proposed framework is effective, efficient, and privacy preserved. Its prediction accuracy is much better than the models trained from limited personal data and is similar to, and even slightly better than, the results from a centralized dataset. This work paves the way for international collaborations on healthcare projects, where additional data is crucial for reducing bias and providing benefits to humanity.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CR"
        ],
        "comment": "Accepted by IEEE Global Blockchain Conference"
    },
    {
        "paper id": "2410.17934",
        "abstract url": "https://arxiv.org/abs/2410.17934",
        "title": "Retrieving snow depth distribution by downscaling ERA5 Reanalysis with ICESat-2 laser altimetry",
        "rating": "-2.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "satellite"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Estimating the variability of seasonal snow cover, in particular snow depth in remote areas, poses significant challenges due to limited spatial and temporal data availability. This study uses snow depth measurements from the ICESat-2 satellite laser altimeter, which are sparse in both space and time, and incorporates them with climate reanalysis data into a downscaling-calibration scheme to produce monthly gridded snow depth maps at microscale (10 m). Snow surface elevation measurements from ICESat-2 along profiles are compared to a digital elevation model to determine snow depth at each point. To efficiently turn sparse measurements into snow depth maps, a regression model is fitted to establish a relationship between the retrieved snow depth and the corresponding ERA5 Land snow depth. This relationship, referred to as subgrid variability, is then applied to downscale the monthly ERA5 Land snow depth data. The method can provide timeseries of monthly snow depth maps for the entire ERA5 time range (since 1950). The validation of downscaled snow depth data was performed at an intermediate scale (100 m x 500 m) using datasets from airborne laser scanning (ALS) in the Hardangervidda region of southern Norway. Results show that snow depth prediction achieved R2 values ranging from 0.74 to 0.88 (post-calibration). The method relies on globally available data and is applicable to other snow regions above the treeline. Though requiring area-specific calibration, our approach has the potential to provide snow depth maps in areas where no such data exist and can be used to extrapolate existing snow surveys in time and over larger areas. With this, it can offer valuable input data for hydrological, ecological or permafrost modeling tasks.",
        "subjects": [
            "physics.geo-ph",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17970",
        "abstract url": "https://arxiv.org/abs/2410.17970",
        "title": "Optical Generative Models",
        "rating": "-2.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "music"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Generative models cover various application areas, including image, video and music synthesis, natural language processing, and molecular design, among many others. As digital generative models become larger, scalable inference in a fast and energy-efficient manner becomes a challenge. Here, we present optical generative models inspired by diffusion models, where a shallow and fast digital encoder first maps random noise into phase patterns that serve as optical generative seeds for a desired data distribution; a jointly-trained free-space-based reconfigurable decoder all-optically processes these generative seeds to create novel images (never seen before) following the target data distribution. Except for the illumination power and the random seed generation through a shallow encoder, these optical generative models do not consume computing power during the synthesis of novel images. We report the optical generation of monochrome and multi-color novel images of handwritten digits, fashion products, butterflies, and human faces, following the data distributions of MNIST, Fashion MNIST, Butterflies-100, and Celeb-A datasets, respectively, achieving an overall performance comparable to digital neural network-based generative models. To experimentally demonstrate optical generative models, we used visible light to generate, in a snapshot, novel images of handwritten digits and fashion products. These optical generative models might pave the way for energy-efficient, scalable and rapid inference tasks, further exploiting the potentials of optics and photonics for artificial intelligence-generated content.",
        "subjects": [
            "cs.NE",
            "cs.LG",
            "physics.app-ph",
            "physics.optics"
        ],
        "comment": "24 Pages, 9 Figures"
    },
    {
        "paper id": "2410.18060",
        "abstract url": "https://arxiv.org/abs/2410.18060",
        "title": "Explaining Bayesian Networks in Natural Language using Factor Arguments. Evaluation in the medical domain",
        "rating": "-2.5",
        "keywords": [
            [
                "graphs"
            ],
            [
                "medical"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In this paper, we propose a model for building natural language explanations for Bayesian Network Reasoning in terms of factor arguments, which are argumentation graphs of flowing evidence, relating the observed evidence to a target variable we want to learn about. We introduce the notion of factor argument independence to address the outstanding question of defining when arguments should be presented jointly or separately and present an algorithm that, starting from the evidence nodes and a target node, produces a list of all independent factor arguments ordered by their strength. Finally, we implemented a scheme to build natural language explanations of Bayesian Reasoning using this approach. Our proposal has been validated in the medical domain through a human-driven evaluation study where we compare the Bayesian Network Reasoning explanations obtained using factor arguments with an alternative explanation method. Evaluation results indicate that our proposed explanation approach is deemed by users as significantly more useful for understanding Bayesian Network Reasoning than another existing explanation method it is compared to.",
        "subjects": [
            "cs.AI",
            "cs.LO"
        ],
        "comment": "First Workshop on Explainable Artificial Intelligence for the medical domain - EXPLIMED. THE 27TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE"
    },
    {
        "paper id": "2410.18267",
        "abstract url": "https://arxiv.org/abs/2410.18267",
        "title": "Backdoor in Seconds: Unlocking Vulnerabilities in Large Pre-trained Models via Model Editing",
        "rating": "-2.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "Model Editing"
            ],
            [
                "attack"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large pre-trained models have achieved notable success across a range of downstream tasks. However, recent research shows that a type of adversarial attack ($\\textit{i.e.,}$ backdoor attack) can manipulate the behavior of machine learning models through contaminating their training dataset, posing significant threat in the real-world application of large pre-trained model, especially for those customized models. Therefore, addressing the unique challenges for exploring vulnerability of pre-trained models is of paramount importance. Through empirical studies on the capability for performing backdoor attack in large pre-trained models ($\\textit{e.g.,}$ ViT), we find the following unique challenges of attacking large pre-trained models: 1) the inability to manipulate or even access large training datasets, and 2) the substantial computational resources required for training or fine-tuning these models. To address these challenges, we establish new standards for an effective and feasible backdoor attack in the context of large pre-trained models. In line with these standards, we introduce our EDT model, an \\textbf{E}fficient, \\textbf{D}ata-free, \\textbf{T}raining-free backdoor attack method. Inspired by model editing techniques, EDT injects an editing-based lightweight codebook into the backdoor of large pre-trained models, which replaces the embedding of the poisoned image with the target image without poisoning the training dataset or training the victim model. Our experiments, conducted across various pre-trained models such as ViT, CLIP, BLIP, and stable diffusion, and on downstream tasks including image classification, image captioning, and image generation, demonstrate the effectiveness of our method. Our code is available in the supplementary material.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18424",
        "abstract url": "https://arxiv.org/abs/2410.18424",
        "title": "A Causal Graph-Enhanced Gaussian Process Regression for Modeling Engine-out NOx",
        "rating": "-2.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The stringent regulatory requirements on nitrogen oxides (NOx) emissions from diesel compression ignition engines require accurate and reliable models for real-time monitoring and diagnostics. Although traditional methods such as physical sensors and virtual engine control module (ECM) sensors provide essential data, they are only used for estimation. Ubiquitous literature primarily focuses on deterministic models with little emphasis on capturing the uncertainties due to sensors. The lack of probabilistic frameworks restricts the applicability of these models for robust diagnostics. The objective of this paper is to develop and validate a probabilistic model to predict engine-out NOx emissions using Gaussian process regression. Our approach is as follows. We employ three variants of Gaussian process models: the first with a standard radial basis function kernel with input window, the second incorporating a deep kernel using convolutional neural networks to capture temporal dependencies, and the third enriching the deep kernel with a causal graph derived via graph convolutional networks. The causal graph embeds physics knowledge into the learning process. All models are compared against a virtual ECM sensor using both quantitative and qualitative metrics. We conclude that our model provides an improvement in predictive performance when using an input window and a deep kernel structure. Even more compelling is the further enhancement achieved by the incorporation of a causal graph into the deep kernel. These findings are corroborated across different validation datasets.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.19868",
        "abstract url": "https://arxiv.org/abs/2410.19868",
        "title": "Hypergraph Neural Networks Reveal Spatial Domains from Single-cell Transcriptomics Data",
        "rating": "-2.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "biological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The task of spatial clustering of transcriptomics data is of paramount importance. It enables the classification of tissue samples into diverse subpopulations of cells, which, in turn, facilitates the analysis of the biological functions of clusters, tissue reconstruction, and cell-cell interactions. Many approaches leverage gene expressions, spatial locations, and histological images to detect spatial domains; however, Graph Neural Networks (GNNs) as state of the art models suffer from a limitation in the assumption of pairwise connections between nodes. In the case of domain detection in spatial transcriptomics, some cells are found to be not directly related. Still, they are grouped as the same domain, which shows the incapability of GNNs for capturing implicit connections among the cells. While graph edges connect only two nodes, hyperedges connect an arbitrary number of nodes along their edges, which lets Hypergraph Neural Networks (HGNNs) capture and utilize richer and more complex structural information than traditional GNNs. We use autoencoders to address the limitation of not having the actual labels, which are well-suited for unsupervised learning. Our model has demonstrated exceptional performance, achieving the highest iLISI score of 1.843 compared to other methods. This score indicates the greatest diversity of cell types identified by our method. Furthermore, our model outperforms other methods in downstream clustering, achieving the highest ARI values of 0.51 and Leiden score of 0.60.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17572",
        "abstract url": "https://arxiv.org/abs/2410.17572",
        "title": "Ultra-reliable urban air mobility networks",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "flight"
            ],
            [
                "navigation"
            ]
        ],
        "abstract": "Recently, urban air mobility (UAM) has attracted attention as an emerging technology that will bring innovation to urban transportation and aviation systems. Since the UAM systems pursue fully autonomous flight without a pilot, wireless communication is a key function not only for flight control signals, but also for navigation and safety information. The essential information is called a command and control (C2) message, and the UAM networks must be configured so that the UAM can receive the C2 message by securing a continuous link stability without any interruptions. Nevertheless, a lot of prior works have focused only on improving the average performance without solving the low-reliability in the cell edges and coverage holes of urban areas. In this dissertation, we identify the factors that hinder the communication link reliability in considering three-dimensional (3D) urban environments, and propose a antenna configuration, resource utilization, and transmission strategy to enable UAM receiving C2 messages regardless of time and space. First, through stochastic geometry modeling, we analyze the signal blockage effects caused by the urban buildings. The blockage probability is calculated according to the shape, height, and density of the buildings, and the coverage probability of the received signal is derived by reflecting the blockage events. Furthermore, the low-reliability area is identified by analyzing the coverage performance according to the positions of the UAMs. To overcome the low-reliability region, we propose three methods for UAM network operation: i) optimization of antennas elevation tilting, ii) frequency reuse with multi-layered narrow beam, and iii) assistive transmissions by the master UAM.",
        "subjects": [
            "cs.NI",
            "eess.SP"
        ],
        "comment": "PhD thesis, 64 pages, 24 figures, 3 tables"
    },
    {
        "paper id": "2410.17651",
        "abstract url": "https://arxiv.org/abs/2410.17651",
        "title": "Testing Deep Learning Recommender Systems Models on Synthetic GAN-Generated Datasets",
        "rating": "-3",
        "keywords": [
            [
                "GAN"
            ],
            [
                "recommendation"
            ]
        ],
        "abstract": "The published method Generative Adversarial Networks for Recommender Systems (GANRS) allows generating data sets for collaborative filtering recommendation systems. The GANRS source code is available along with a representative set of generated datasets. We have tested the GANRS method by creating multiple synthetic datasets from three different real datasets taken as a source. Experiments include variations in the number of users in the synthetic datasets, as well as a different number of samples. We have also selected six state-of-the-art collaborative filtering deep learning models to test both their comparative performance and the GANRS method. The results show a consistent behavior of the generated datasets compared to the source ones; particularly, in the obtained values and trends of the precision and recall quality measures. The tested deep learning models have also performed as expected on all synthetic datasets, making it possible to compare the results with those obtained from the real source data. Future work is proposed, including different cold start scenarios, unbalanced data, and demographic fairness.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "14 pages, 7 figures, In press"
    },
    {
        "paper id": "2410.17664",
        "abstract url": "https://arxiv.org/abs/2410.17664",
        "title": "Deep Generative Models for 3D Medical Image Synthesis",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "Medical",
                "diagnosis",
                "disease"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Deep generative modeling has emerged as a powerful tool for synthesizing realistic medical images, driving advances in medical image analysis, disease diagnosis, and treatment planning. This chapter explores various deep generative models for 3D medical image synthesis, with a focus on Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Denoising Diffusion Models (DDMs). We discuss the fundamental principles, recent advances, as well as strengths and weaknesses of these models and examine their applications in clinically relevant problems, including unconditional and conditional generation tasks like image-to-image translation and image reconstruction. We additionally review commonly used evaluation metrics for assessing image fidelity, diversity, utility, and privacy and provide an overview of current challenges in the field.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17782",
        "abstract url": "https://arxiv.org/abs/2410.17782",
        "title": "Pointer: An Energy-Efficient ReRAM-based Point Cloud Recognition Accelerator with Inter-layer and Intra-layer Optimizations",
        "rating": "-3",
        "keywords": [
            [
                "Point Cloud"
            ],
            [
                "autonomous driving"
            ],
            [
                "robotics"
            ]
        ],
        "abstract": "Point cloud is an important data structure for a wide range of applications, including robotics, AR/VR, and autonomous driving. To process the point cloud, many deep-learning-based point cloud recognition algorithms have been proposed. However, to meet the requirement of applications like autonomous driving, the algorithm must be fast enough, rendering accelerators necessary at the inference stage. But existing point cloud accelerators are still inefficient due to two challenges. First, the multi-layer perceptron (MLP) during feature computation is the performance bottleneck. Second, the feature vector fetching operation incurs heavy DRAM access. In this paper, we propose Pointer, an efficient Resistive Random Access Memory (ReRAM)-based point cloud recognition accelerator with inter- and intra-layer optimizations. It proposes three techniques for point cloud acceleration. First, Pointer adopts ReRAM-based architecture to significantly accelerate the MLP in feature computation. Second, to reduce DRAM access, Pointer proposes inter-layer coordination. It schedules the next layer to fetch the results of the previous layer as soon as they are available, which allows on-chip fetching thus reduces DRAM access. Third, Pointer proposes topology-aware intra-layer reordering, which improves the execution order for better data locality. Pointer proves to achieve 40x to 393x speedup and 22x to 163x energy efficiency over prior accelerators without any accuracy loss.",
        "subjects": [
            "cs.AR"
        ],
        "comment": "Published in ASPDAC'25"
    },
    {
        "paper id": "2410.18207",
        "abstract url": "https://arxiv.org/abs/2410.18207",
        "title": "Trajectory Optimization for Spatial Microstructure Control in Electron Beam Metal Additive Manufacturing",
        "rating": "-3",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "thermal",
                "alloy"
            ]
        ],
        "abstract": "Metal additive manufacturing (AM) opens the possibility for spatial control of as-fabricated microstructure and properties. However, since the solid state diffusional transformations that drive microstructure outcomes are governed by nonlinear ODEs in terms of temperature, which is itself governed by PDEs over the entire part domain, solving for the system inputs needed to achieve desired microstructure distributions has proven difficult. In this work, we present a trajectory optimization approach for spatial control of microstructure in metal AM, which we demonstrate by controlling the hardness of a low-alloy steel in electron beam powder bed fusion (EB-PBF). To this end, we present models for thermal and microstructural dynamics. Next, we use experimental data to identify the parameters of the microstructure transformation dynamics. We then pose spatial microstructure control as a finite-horizon optimal control problem. The optimal power field trajectory is computed using an augmented Lagrangian differential dynamic programming (AL-DDP) method with GPU acceleration. The resulting time-varying power fields are then realized on an EB-PBF machine through an approximation scheme. Measurements of the resultant hardness shows that the optimized power field trajectory is able to closely produce the desired hardness distribution.",
        "subjects": [
            "eess.SY",
            "math.OC"
        ],
        "comment": "6 pages, 6 figures"
    },
    {
        "paper id": "2410.18208",
        "abstract url": "https://arxiv.org/abs/2410.18208",
        "title": "Automated Defect Detection and Grading of Piarom Dates Using Deep Learning",
        "rating": "-3",
        "keywords": [
            [
                "industrial"
            ],
            [
                "agricultural"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Grading and quality control of Piarom dates, a premium and high-value variety cultivated predominantly in Iran, present significant challenges due to the complexity and variability of defects, as well as the absence of specialized automated systems tailored to this fruit. Traditional manual inspection methods are labor intensive, time consuming, and prone to human error, while existing AI-based sorting solutions are insufficient for addressing the nuanced characteristics of Piarom dates. In this study, we propose an innovative deep learning framework designed specifically for the real-time detection, classification, and grading of Piarom dates. Leveraging a custom dataset comprising over 9,900 high-resolution images annotated across 11 distinct defect categories, our framework integrates state-of-the-art object detection algorithms and Convolutional Neural Networks (CNNs) to achieve high precision in defect identification. Furthermore, we employ advanced segmentation techniques to estimate the area and weight of each date, thereby optimizing the grading process according to industry standards. Experimental results demonstrate that our system significantly outperforms existing methods in terms of accuracy and computational efficiency, making it highly suitable for industrial applications requiring real-time processing. This work not only provides a robust and scalable solution for automating quality control in the Piarom date industry but also contributes to the broader field of AI-driven food inspection technologies, with potential applications across various agricultural products.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18233",
        "abstract url": "https://arxiv.org/abs/2410.18233",
        "title": "DMTG: A Human-Like Mouse Trajectory Generation Bot Based on Entropy-Controlled Diffusion Networks",
        "rating": "-3",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Trajectory"
            ],
            [
                "attacks"
            ]
        ],
        "abstract": "CAPTCHAs protect against resource misuse and data theft by distinguishing human activity from automated bots. Advances in machine learning have made traditional image and text-based CAPTCHAs vulnerable to attacks, leading modern CAPTCHAs, such as GeeTest and Akamai, to incorporate behavioral analysis like mouse trajectory detection. Existing bypass techniques struggle to fully mimic human behavior, making it difficult to evaluate the effectiveness of anti-bot measures. To address this, we propose a diffusion model-based mouse trajectory generation framework (DMTG), which controls trajectory complexity and produces realistic human-like mouse movements. DMTG also provides white-box and black-box testing methods to assess its ability to bypass CAPTCHA systems. In experiments, DMTG reduces bot detection accuracy by 4.75%-9.73% compared to other models. Additionally, it mimics physical human behaviors, such as slow initiation and directional force differences, demonstrating improved performance in both simulation and real-world CAPTCHA scenarios.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18291",
        "abstract url": "https://arxiv.org/abs/2410.18291",
        "title": "Enhancing Enterprise Security with Zero Trust Architecture",
        "rating": "-3",
        "keywords": [
            [
                "attack"
            ],
            [
                "healthcare"
            ]
        ],
        "abstract": "Zero Trust Architecture (ZTA) represents a transformative approach to modern cybersecurity, directly addressing the shortcomings of traditional perimeter-based security models. With the rise of cloud computing, remote work, and increasingly sophisticated cyber threats, perimeter defenses have proven ineffective at mitigating risks, particularly those involving insider threats and lateral movement within networks. ZTA shifts the security paradigm by assuming that no user, device, or system can be trusted by default, requiring continuous verification and the enforcement of least privilege access for all entities. This paper explores the key components of ZTA, such as identity and access management (IAM), micro-segmentation, continuous monitoring, and behavioral analytics, and evaluates their effectiveness in reducing vulnerabilities across diverse sectors, including finance, healthcare, and technology. Through case studies and industry reports, the advantages of ZTA in mitigating insider threats and minimizing attack surfaces are discussed. Additionally, the paper addresses the challenges faced during ZTA implementation, such as scalability, integration complexity, and costs, while providing best practices for overcoming these obstacles. Lastly, future research directions focusing on emerging technologies like AI, machine learning, blockchain, and their integration into ZTA are examined to enhance its capabilities further.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "21 pages, 4 figures, focusing on Zero Trust Architecture (ZTA) implementation and its impact on mitigating insider threats and reducing vulnerabilities. Case studies from sectors like finance and healthcare are included"
    },
    {
        "paper id": "2410.18340",
        "abstract url": "https://arxiv.org/abs/2410.18340",
        "title": "Thermal Chameleon: Task-Adaptive Tone-mapping for Radiometric Thermal-Infrared images",
        "rating": "-3",
        "keywords": [
            [
                "depth"
            ],
            [
                "Infrared"
            ],
            [
                "Thermal"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Thermal Infrared (TIR) imaging provides robust perception for navigating in challenging outdoor environments but faces issues with poor texture and low image contrast due to its 14/16-bit format. Conventional methods utilize various tone-mapping methods to enhance contrast and photometric consistency of TIR images, however, the choice of tone-mapping is largely dependent on knowing the task and temperature dependent priors to work well. In this paper, we present Thermal Chameleon Network (TCNet), a task-adaptive tone-mapping approach for RAW 14-bit TIR images. Given the same image, TCNet tone-maps different representations of TIR images tailored for each specific task, eliminating the heuristic image rescaling preprocessing and reliance on the extensive prior knowledge of the scene temperature or task-specific characteristics. TCNet exhibits improved generalization performance across object detection and monocular depth estimation, with minimal computational overhead and modular integration to existing architectures for various tasks. Project Page: https://github.com/donkeymouse/ThermalChameleon",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": "Published in IEEE Robotics and Automation Letters (2024)"
    },
    {
        "paper id": "2410.18366",
        "abstract url": "https://arxiv.org/abs/2410.18366",
        "title": "Cochlear Implantation of Slim Pre-curved Arrays using Automatic Pre-operative Insertion Plans",
        "rating": "-3",
        "keywords": [
            [
                "depth"
            ],
            [
                "trajectory"
            ],
            [
                "clinical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Hypothesis: Pre-operative cochlear implant (CI) electrode array (EL) insertion plans created by automated image analysis methods can improve positioning of slim pre-curved EL. Background: This study represents the first evaluation of a system for patient-customized EL insertion planning for a slim pre-curved EL. Methods: Twenty-one temporal bone specimens were divided into experimental and control groups and underwent cochlear implantation. For the control group, the surgeon performed a traditional insertion without an insertion plan. For the experimental group, customized insertion plans guided entry site, trajectory, curl direction, and base insertion depth. An additional 35 clinical insertions from the same surgeon were analyzed, 7 of which were conducted using the insertion plans. EL positioning was analyzed using post-operative imaging auto-segmentation techniques, allowing measurement of angular insertion depth (AID), mean modiolar distance (MMD), and scalar position. Results: In the cadaveric temporal bones, 3 scalar translocations, including 2 foldovers, occurred in 14 control group insertions. In the clinical insertions, translocations occurred in 2 of 28 control cases. No translocations or folds occurred in the 7 experimental temporal bone and the 7 experimental clinical insertions. Among the non-translocated cases, overall AID and MMD were 401(41) degrees and 0.34(0.13) mm for the control insertions. AID and MMD for the experimental insertions were 424(43) degrees and 0.34(0.09) mm overall and were 432(19) and 0.30(0.07) mm for cases where the planned insertion depth was achieved. Conclusions: Trends toward improved EL positioning within scala tympani were observed when EL insertion plans are used. Variability in MMD was significantly reduced (0.07mm vs 0.13 mm, p=0.039) when the planned depth was achieved.",
        "subjects": [
            "eess.IV"
        ],
        "comment": "First two listed authors are co-first authors"
    },
    {
        "paper id": "2410.17878",
        "abstract url": "https://arxiv.org/abs/2410.17878",
        "title": "Relaxed Equivariance via Multitask Learning",
        "rating": "-3.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "graphs"
            ],
            [
                "chemistry"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Incorporating equivariance as an inductive bias into deep learning architectures to take advantage of the data symmetry has been successful in multiple applications, such as chemistry and dynamical systems. In particular, roto-translations are crucial for effectively modeling geometric graphs and molecules, where understanding the 3D structures enhances generalization. However, equivariant models often pose challenges due to their high computational complexity. In this paper, we introduce REMUL, a training procedure for approximating equivariance with multitask learning. We show that unconstrained models (which do not build equivariance into the architecture) can learn approximate symmetries by minimizing an additional simple equivariance loss. By formulating equivariance as a new learning objective, we can control the level of approximate equivariance in the model. Our method achieves competitive performance compared to equivariant baselines while being $10 \\times$ faster at inference and $2.5 \\times$ at training.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17917",
        "abstract url": "https://arxiv.org/abs/2410.17917",
        "title": "regAL: Python Package for Active Learning of Regression Problems",
        "rating": "-3.5",
        "keywords": [
            [
                "bio)chemistry"
            ],
            [
                "chemistry"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Increasingly more research areas rely on machine learning methods to accelerate discovery while saving resources. Machine learning models, however, usually require large datasets of experimental or computational results, which in certain fields, such as (bio)chemistry, materials science, or medicine, are rarely given and often prohibitively expensive to obtain. To bypass that obstacle, active learning methods are employed to develop machine learning models with a desired performance while requiring the least possible number of computational or experimental results from the domain of application. For this purpose, the model's knowledge about certain regions of the application domain is estimated to guide the choice of the model's training set. Although active learning is widely studied for classification problems (discrete outcomes), comparatively few works handle this method for regression problems (continuous outcomes). In this work, we present our Python package regAL, which allows users to evaluate different active learning strategies for regression problems. With a minimal input of just the dataset in question, but many additional customization and insight options, this package is intended for anyone who aims to perform and understand active learning in their problem-specific scope.",
        "subjects": [
            "cs.LG",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18262",
        "abstract url": "https://arxiv.org/abs/2410.18262",
        "title": "Hamiltonian Matching for Symplectic Neural Integrators",
        "rating": "-3.5",
        "keywords": [
            [
                "astronomy"
            ],
            [
                "quantum",
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Hamilton's equations of motion form a fundamental framework in various branches of physics, including astronomy, quantum mechanics, particle physics, and climate science. Classical numerical solvers are typically employed to compute the time evolution of these systems. However, when the system spans multiple spatial and temporal scales numerical errors can accumulate, leading to reduced accuracy. To address the challenges of evolving such systems over long timescales, we propose SympFlow, a novel neural network-based symplectic integrator, which is the composition of a sequence of exact flow maps of parametrised time-dependent Hamiltonian functions. This architecture allows for a backward error analysis: we can identify an underlying Hamiltonian function of the architecture and use it to define a Hamiltonian matching objective function, which we use for training. In numerical experiments, we show that SympFlow exhibits promising results, with qualitative energy conservation behaviour similar to that of time-stepping symplectic integrators.",
        "subjects": [
            "cs.LG",
            "math.NA",
            "physics.comp-ph"
        ],
        "comment": "NeurReps 2024"
    },
    {
        "paper id": "2410.17639",
        "abstract url": "https://arxiv.org/abs/2410.17639",
        "title": "Constraint Removal for MPC with Performance Preservation and a Hyperthermia Cancer Treatment Case Study",
        "rating": "-4",
        "keywords": [
            [
                "Cancer"
            ],
            [
                "industrial"
            ]
        ],
        "abstract": "Model predictive control (MPC) is an optimization-based control strategy with broad industrial adoption. Unfortunately, the required computation time to solve the receding-horizon MPC optimization problem can become prohibitively large for many applications with a large number of state constraints. This large number of state constraints can, for instance, originate from spatially discretizing a partial differential equation of which the solution has to satisfy constraints over the full spatial domain. This is particularly the case in MPC for RF-based hyperthermia cancer treatments, which forms a strong motivation for this study. To address this problem, we propose a novel constraint-adaptive MPC framework for linear discrete-time systems. In this framework, we select at each time-step a subset of the state constraints that are included in the optimization problem, thereby reducing the online computational burden. Critically, our framework guarantees the same closed-loop performance, recursive feasibility, and constraint satisfaction properties as the original (non-reduced) MPC scheme. We achieve this result by efficiently exploiting reachable set computations and the MPC cost function. We will demonstrate our novel method using a hyperthermia cancer treatment case study showing a two-orders of magnitude improvement in computation time, with identical closed-loop performance as the original (non-reduced) MPC scheme.",
        "subjects": [
            "eess.SY",
            "math.OC"
        ],
        "comment": "6 pages, 3 figures, CDC 2021 conference"
    },
    {
        "paper id": "2410.17685",
        "abstract url": "https://arxiv.org/abs/2410.17685",
        "title": "Human-Robot Collaboration System Setup for Weed Harvesting Scenarios in Aquatic Lakes",
        "rating": "-4",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "Robot",
                "NAvigation"
            ],
            [
                "biological"
            ]
        ],
        "abstract": "Artificial Water Bodies (AWBs) are human-made and require continuous monitoring due to their artificial biological processes. These systems necessitate regular maintenance to manage their ecosystems effectively. Unmanned Surface Vehicle (USV) offers a collaborative approach for monitoring these environments, working alongside human operators such as boat skippers to identify specific locations. This paper discusses a weed harvesting scenario, demonstrating how human-robot collaboration can be achieved, supported by preliminary results. The USV mainly utilises multibeam SOund NAvigation and Ranging (SONAR) for underwater weed monitoring, showing promising outcomes in these scenarios.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "3 pages, 5 figures. This paper was accepted for poster presentation at IROS 2024 Workshop on Maritime Heteregenous Unmanned Robotic Systems (MHURS)"
    },
    {
        "paper id": "2410.17691",
        "abstract url": "https://arxiv.org/abs/2410.17691",
        "title": "Longitudinal Causal Image Synthesis",
        "rating": "-4",
        "keywords": [
            [
                "graph"
            ],
            [
                "medical",
                "diagnosis",
                "MRI",
                "disease",
                "Clinical"
            ],
            [
                "tabular"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Clinical decision-making relies heavily on causal reasoning and longitudinal analysis. For example, for a patient with Alzheimer's disease (AD), how will the brain grey matter atrophy in a year if intervened on the A-beta level in cerebrospinal fluid? The answer is fundamental to diagnosis and follow-up treatment. However, this kind of inquiry involves counterfactual medical images which can not be acquired by instrumental or correlation-based image synthesis models. Yet, such queries require counterfactual medical images, not obtainable through standard image synthesis models. Hence, a causal longitudinal image synthesis (CLIS) method, enabling the synthesis of such images, is highly valuable. However, building a CLIS model confronts three primary yet unmet challenges: mismatched dimensionality between high-dimensional images and low-dimensional tabular variables, inconsistent collection intervals of follow-up data, and inadequate causal modeling capability of existing causal graph methods for image data. In this paper, we established a tabular-visual causal graph (TVCG) for CLIS overcoming these challenges through a novel integration of generative imaging, continuous-time modeling, and structural causal models combined with a neural network. We train our CLIS based on the ADNI dataset and evaluate it on two other AD datasets, which illustrate the outstanding yet controllable quality of the synthesized images and the contributions of synthesized MRI to the characterization of AD progression, substantiating the reliability and utility in clinics.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "q-bio.NC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17751",
        "abstract url": "https://arxiv.org/abs/2410.17751",
        "title": "VISAGE: Video Synthesis using Action Graphs for Surgery",
        "rating": "-4",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "robot"
            ],
            [
                "Graphs"
            ],
            [
                "Surgical",
                "Surgery"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Surgical data science (SDS) is a field that analyzes patient data before, during, and after surgery to improve surgical outcomes and skills. However, surgical data is scarce, heterogeneous, and complex, which limits the applicability of existing machine learning methods. In this work, we introduce the novel task of future video generation in laparoscopic surgery. This task can augment and enrich the existing surgical data and enable various applications, such as simulation, analysis, and robot-aided surgery. Ultimately, it involves not only understanding the current state of the operation but also accurately predicting the dynamic and often unpredictable nature of surgical procedures. Our proposed method, VISAGE (VIdeo Synthesis using Action Graphs for Surgery), leverages the power of action scene graphs to capture the sequential nature of laparoscopic procedures and utilizes diffusion models to synthesize temporally coherent video sequences. VISAGE predicts the future frames given only a single initial frame, and the action graph triplets. By incorporating domain-specific knowledge through the action graph, VISAGE ensures the generated videos adhere to the expected visual and motion patterns observed in real laparoscopic procedures. The results of our experiments demonstrate high-fidelity video generation for laparoscopy procedures, which enables various applications in SDS.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted at MICCAI 2024 Embodied AI and Robotics for HealTHcare (EARTH) Workshop"
    },
    {
        "paper id": "2410.17822",
        "abstract url": "https://arxiv.org/abs/2410.17822",
        "title": "DREB-Net: Dual-stream Restoration Embedding Blur-feature Fusion Network for High-mobility UAV Object Detection",
        "rating": "-4",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "UAV"
            ],
            [
                "image Restoration"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Object detection algorithms are pivotal components of unmanned aerial vehicle (UAV) imaging systems, extensively employed in complex fields. However, images captured by high-mobility UAVs often suffer from motion blur cases, which significantly impedes the performance of advanced object detection algorithms. To address these challenges, we propose an innovative object detection algorithm specifically designed for blurry images, named DREB-Net (Dual-stream Restoration Embedding Blur-feature Fusion Network). First, DREB-Net addresses the particularities of blurry image object detection problem by incorporating a Blurry image Restoration Auxiliary Branch (BRAB) during the training phase. Second, it fuses the extracted shallow features via Multi-level Attention-Guided Feature Fusion (MAGFF) module, to extract richer features. Here, the MAGFF module comprises local attention modules and global attention modules, which assign different weights to the branches. Then, during the inference phase, the deep feature extraction of the BRAB can be removed to reduce computational complexity and improve detection speed. In loss function, a combined loss of MSE and SSIM is added to the BRAB to restore blurry images. Finally, DREB-Net introduces Fast Fourier Transform in the early stages of feature extraction, via a Learnable Frequency domain Amplitude Modulation Module (LFAMM), to adjust feature amplitude and enhance feature processing capability. Experimental results indicate that DREB-Net can still effectively perform object detection tasks under motion blur in captured images, showcasing excellent performance and broad application prospects. Our source code will be available at https://github.com/EEIC-Lab/DREB-Net.git.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17831",
        "abstract url": "https://arxiv.org/abs/2410.17831",
        "title": "Gaussian Process Distance Fields Obstacle and Ground Constraints for Safe Navigation",
        "rating": "-4",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "trajectory"
            ],
            [
                "robot",
                "Navigation"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "Navigating cluttered environments is a challenging task for any mobile system. Existing approaches for ground-based mobile systems primarily focus on small wheeled robots, which face minimal constraints with overhanging obstacles and cannot manage steps or stairs, making the problem effectively 2D. However, navigation for legged robots (or even humans) has to consider an extra dimension. This paper proposes a tailored scene representation coupled with an advanced trajectory optimisation algorithm to enable safe navigation. Our 3D navigation approach is suitable for any ground-based mobile robot, whether wheeled or legged, as well as for human assistance. Given a 3D point cloud of the scene and the segmentation of the ground and non-ground points, we formulate two Gaussian Process distance fields to ensure a collision-free path and maintain distance to the ground constraints. Our method adeptly handles uneven terrain, steps, and overhanging objects through an innovative use of a quadtree structure, constructing a multi-resolution map of the free space and its connectivity graph based on a 2D projection of the relevant scene. Evaluations with both synthetic and real-world datasets demonstrate that this approach provides safe and smooth paths, accommodating a wide range of ground-based mobile systems.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17841",
        "abstract url": "https://arxiv.org/abs/2410.17841",
        "title": "Truly Sub-Nyquist Method Based Matrix Pencil and CRT with Super Resolution",
        "rating": "-4",
        "keywords": [
            [
                "Super Resolution"
            ],
            [
                "radar"
            ],
            [
                "medical"
            ]
        ],
        "abstract": "The emergence of ultra-wideband (UWB) and high-throughput signals has necessitated advancements in data sampling technologies1. Sub-Nyquist sampling methods, such as the modulated wideband converter (MWC) and compressed auto-correlation spectrum sensing (CCS), address the limitations of traditional analog-to-digital converters (ADCs) by capturing signals below the Nyquist rate. However, these methods face challenges like spectral leakage and complex hardware requirements. This paper proposes a novel super-resolution generalized eigenvalue method that integrates the matrix pencil method with the Chinese Remainder Theorem (CRT) to enhance signal processing capabilities within a true sub-Nyquist framework3. This approach aims to improve frequency resolution and accuracy in high-frequency signal extraction, with potential applications in telecommunications, radar, and medical imaging.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "Sub-nyquist sampling, Matrix pencil method, CRT, Super resolution"
    },
    {
        "paper id": "2410.17758",
        "abstract url": "https://arxiv.org/abs/2410.17758",
        "title": "Escaping the Forest: Sparse Interpretable Neural Networks for Tabular Data",
        "rating": "-4.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "biology"
            ],
            [
                "Tabular"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Tabular datasets are widely used in scientific disciplines such as biology. While these disciplines have already adopted AI methods to enhance their findings and analysis, they mainly use tree-based methods due to their interpretability. At the same time, artificial neural networks have been shown to offer superior flexibility and depth for rich and complex non-tabular problems, but they are falling behind tree-based models for tabular data in terms of performance and interpretability. Although sparsity has been shown to improve the interpretability and performance of ANN models for complex non-tabular datasets, enforcing sparsity structurally and formatively for tabular data before training the model, remains an open question. To address this question, we establish a method that infuses sparsity in neural networks by utilising attention mechanisms to capture the features' importance in tabular datasets. We show that our models, Sparse TABular NET or sTAB-Net with attention mechanisms, are more effective than tree-based models, reaching the state-of-the-art on biological datasets. They further permit the extraction of insights from these datasets and achieve better performance than post-hoc methods like SHAP.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17585",
        "abstract url": "https://arxiv.org/abs/2410.17585",
        "title": "Energy-Optimal Planning of Waypoint-Based UAV Missions -- Does Minimum Distance Mean Minimum Energy?",
        "rating": "-5",
        "keywords": [
            [
                "3D"
            ],
            [
                "vehicle",
                "flight"
            ],
            [
                "robot"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "Multirotor unmanned aerial vehicle is a prevailing type of aerial robots with wide real-world applications. The energy efficiency of the robot is a critical aspect of its performance, determining the range and duration of the missions that can be performed. This paper studies the energy-optimal planning of the multirotor, which aims at finding the optimal ordering of waypoints with the minimum energy consumption for missions in 3D space. The study is performed based on a previously developed model capturing first-principle energy dynamics of the multirotor. We found that in majority of the cases (up to 95%) the solutions of the energy-optimal planning are different from those of the traditional traveling salesman problem which minimizes the total distance. The difference can be as high as 14.9%, with the average at 1.6%-3.3% and 90th percentile at 3.7%-6.5% depending on the range and number of waypoints in the mission. We then identified and explained the key features of the minimum-energy order by correlating to the underlying flight energy dynamics. It is shown that instead of minimizing the distance, coordination of vertical and horizontal motion to promote aerodynamic efficiency is the key to optimizing energy consumption.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "This paper has been accepted for presentation at the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2024"
    },
    {
        "paper id": "2410.18301",
        "abstract url": "https://arxiv.org/abs/2410.18301",
        "title": "LEO-based Positioning: Foundations, Signal Design, and Receiver Enhancements for 6G NTN",
        "rating": "-5",
        "keywords": [
            [
                "navigation"
            ],
            [
                "5G",
                "6G"
            ],
            [
                "Satellite"
            ]
        ],
        "abstract": "The integration of non-terrestrial networks (NTN) into 5G new radio (NR) has opened up the possibility of developing a new positioning infrastructure using NR signals from Low-Earth Orbit (LEO) satellites. LEO-based cellular positioning offers several advantages, such as a superior link budget, higher operating bandwidth, and large forthcoming constellations. Due to these factors, LEO-based positioning, navigation, and timing (PNT) is a potential enhancement for NTN in 6G cellular networks. However, extending the existing terrestrial cellular positioning methods to LEO-based NTN positioning requires considering key fundamental enhancements. These include creating broad positioning beams orthogonal to conventional communication beams, time-domain processing at the user equipment (UE) to resolve large delay and Doppler uncertainties, and efficiently accommodating positioning reference signals (PRS) from multiple satellites within the communication resource grid. In this paper, we present the first set of design insights by incorporating these enhancements and thoroughly evaluating LEO-based positioning, considering the constraints and capabilities of the NR-NTN physical layer. To evaluate the performance of LEO-based NTN positioning, we develop a comprehensive NR-compliant simulation framework, including LEO orbit simulation, transmission (Tx) and receiver (Rx) architectures, and a positioning engine incorporating the necessary enhancements. Our findings suggest that LEO-based NTN positioning could serve as a complementary infrastructure to existing Global Navigation Satellite Systems (GNSS) and, with appropriate enhancements, may also offer a viable alternative.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "7 pages, 6 figures, submitted to IEEE Communications Magazine"
    },
    {
        "paper id": "2410.18382",
        "abstract url": "https://arxiv.org/abs/2410.18382",
        "title": "Sensing-Communication-Computing-Control Closed-Loop Optimization for 6G Unmanned Robotic Systems",
        "rating": "-5",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "6G"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "Rapid advancements in field robots have brought a new kind of cyber physical system (CPS)--unmanned robotic system--under the spotlight. In the upcoming sixth-generation (6G) era, these systems hold great potential to replace humans in hazardous tasks. This paper investigates an unmanned robotic system comprising a multi-functional unmanned aerial vehicle (UAV), sensors, and actuators. The UAV carries communication and computing modules, acting as an edge information hub (EIH) that transfers and processes information. During the task execution, the EIH gathers sensing data, calculates control commands, and transmits commands to actuators--leading to reflex-arc-like sensing-communication-computing-control ($\\mathbf{SC}^3$) loops. Unlike existing studies that design $\\mathbf{SC}^3$ loop components separately, we take each $\\mathbf{SC}^3$ loop as an integrated structure and propose a goal-oriented closed-loop optimization scheme. This scheme jointly optimizes uplink and downlink (UL&DL) communication and computing within and across the $\\mathbf{SC}^3$ loops to minimize the total linear quadratic regulator (LQR) cost. We derive optimal closed-form solutions for intra-loop allocation and propose an efficient iterative algorithm for inter-loop optimization. Under the condition of adequate CPU frequency availability, we derive an approximate closed-form solution for inter-loop bandwidth allocation. Simulation results demonstrate that the proposed scheme achieves a two-tier task-level balance within and across $\\mathbf{SC}^3$ loops.",
        "subjects": [
            "eess.SY",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18403",
        "abstract url": "https://arxiv.org/abs/2410.18403",
        "title": "Structure Language Models for Protein Conformation Generation",
        "rating": "-5.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "biological"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Proteins adopt multiple structural conformations to perform their diverse biological functions, and understanding these conformations is crucial for advancing drug discovery. Traditional physics-based simulation methods often struggle with sampling equilibrium conformations and are computationally expensive. Recently, deep generative models have shown promise in generating protein conformations as a more efficient alternative. However, these methods predominantly rely on the diffusion process within a 3D geometric space, which typically centers around the vicinity of metastable states and is often inefficient in terms of runtime. In this paper, we introduce Structure Language Modeling (SLM) as a novel framework for efficient protein conformation generation. Specifically, the protein structures are first encoded into a compact latent space using a discrete variational auto-encoder, followed by conditional language modeling that effectively captures sequence-specific conformation distributions. This enables a more efficient and interpretable exploration of diverse ensemble modes compared to existing methods. Based on this general framework, we instantiate SLM with various popular LM architectures as well as proposing the ESMDiff, a novel BERT-like structure language model fine-tuned from ESM3 with masked diffusion. We verify our approach in various scenarios, including the equilibrium dynamics of BPTI, conformational change pairs, and intrinsically disordered proteins. SLM provides a highly efficient solution, offering a 20-100x speedup than existing methods in generating diverse conformations, shedding light on promising avenues for future research.",
        "subjects": [
            "q-bio.BM",
            "cs.LG"
        ],
        "comment": "Preprint. Under Review"
    },
    {
        "paper id": "2410.17577",
        "abstract url": "https://arxiv.org/abs/2410.17577",
        "title": "Arcus: SLO Management for Accelerators in the Cloud with Traffic Shaping",
        "rating": "-10",
        "keywords": [],
        "abstract": "Cloud servers use accelerators for common tasks (e.g., encryption, compression, hashing) to improve CPU/GPU efficiency and overall performance. However, users' Service-level Objectives (SLOs) can be violated due to accelerator-related contention. The root cause is that existing solutions for accelerators only focus on isolation or fair allocation of compute and memory resources; they overlook the contention for communication-related resources. Specifically, three communication-induced challenges drive us to re-think the problem: (1) Accelerator traffic patterns are diverse, hard to predict, and mixed across users, (2) communication-related components lack effective low-level isolation mechanism to configure, and (3) computational heterogeneity of accelerators lead to unique relationships between the traffic mixture and the corresponding accelerator performance. The focus of this work is meeting SLOs in accelerator-rich systems. We present \\design{}, treating accelerator SLO management as traffic management with proactive traffic shaping. We develop an SLO-aware protocol coupled with an offloaded interface on an architecture that supports precise and scalable traffic shaping. We guarantee accelerator SLO for various circumstances, with up to 45% tail latency reduction and less than 1% throughput variance.",
        "subjects": [
            "cs.AR",
            "cs.OS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17581",
        "abstract url": "https://arxiv.org/abs/2410.17581",
        "title": "Risk-sensitive Affine Control Synthesis for Stationary LTI Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "To address deviations from expected performance in stochastic systems, we propose a risk-sensitive control synthesis method to minimize certain risk measures over the limiting stationary distribution. Specifically, we extend Worst-case Conditional Value-at-Risk (W-CVaR) optimization for Linear Time-invariant (LTI) systems to handle nonzero-mean noise and affine controllers, using only the first and second moments of noise, which enhances robustness against model uncertainty. Highlighting the strong coupling between the linear and bias terms of the controller, we reformulate the synthesis problem as a Bilinear Matrix Inequality (BMI), and propose an alternating optimization algorithm with guaranteed convergence. Finally, we demonstrate the numerical performance of our approach in two representative settings, which shows that the proposed algorithm successfully synthesizes risk-sensitive controllers that outperform the na\u00efve LQR baseline.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "8 pages, 4 figures, 2 illustrations"
    },
    {
        "paper id": "2410.17586",
        "abstract url": "https://arxiv.org/abs/2410.17586",
        "title": "Efficient and Aesthetic UI Design with a Deep Learning-Based Interface Generation Tree Algorithm",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents a novel method for user interface (UI) generation based on the Transformer architecture, addressing the increasing demand for efficient and aesthetically pleasing UI designs in software development. Traditional UI design relies heavily on designers' expertise, which can be time-consuming and costly. Leveraging the capabilities of Transformers, particularly their ability to capture complex design patterns and long-range dependencies, we propose a Transformer-based interface generation tree algorithm. This method constructs a hierarchical representation of UI components as nodes in a tree structure, utilizing pre-trained Transformer models for encoding and decoding. We define a markup language to describe UI components and their properties and use a rich dataset of real-world web and mobile application interfaces for training. The experimental results demonstrate that our approach not only significantly enhances design quality and efficiency but also outperforms traditional models in user satisfaction and aesthetic appeal. We also provide a comparative analysis with existing models, illustrating the advantages of our method in terms of accuracy, user ratings, and design similarity. Overall, our study underscores the potential of the Transformer based approach to revolutionize the UI design process, making it accessible for non-professionals while maintaining high standards of quality.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17593",
        "abstract url": "https://arxiv.org/abs/2410.17593",
        "title": "Pillow Box Design",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper focuses on packaging design using origami techniques, specifically designs incorporating curves, known as pillow boxes. While conventional paper packaging boxes are typically cuboid, pillow box designs include curved surfaces, offering both aesthetic and practical advantages. This study analyzes the specific curved folds of pillow boxes, clarifying the fundamental geometric condition these curves must meet. Additionally, it proposes new design variations for pillow boxes based on the condition. The relationship between the shape of the folds and the volume of the final three-dimensional shape is also explored. This research extends the boundaries of functionality and aesthetics in origami design and explores new possibilities in packaging solutions.",
        "subjects": [
            "cs.CG"
        ],
        "comment": "Submitted to the Proceedings of 8OSME"
    },
    {
        "paper id": "2410.17603",
        "abstract url": "https://arxiv.org/abs/2410.17603",
        "title": "Scaling Analysis in a Multi-Energy System",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents a scaling study on the planning phase of a multi-energy system (MES), which is becoming increasingly prominent in the energy sector. The research aims to investigate the interactions and challenges associated with integrating heat and electrical systems and scaling their components. In this context, interaction between these two domains are investigated and the size of the distributed energy resources in the MES is scaled to examine the impact of sizing on the integrating networks and their controlling system. To achieve this, the paper uses sensitivity analysis and a meta-modeling technique, both incorporated in a toolbox for scaling analysis. These methodologies are validated through simulations, and the results obtained from the simulations can contribute to the advancement of MESs and their implementation in laboratory and field testing.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "6 pages, 9 figures, conference proceedings Asia Meeting on Environment and Electrical Engineering (EEE-AM) 2023"
    },
    {
        "paper id": "2410.17607",
        "abstract url": "https://arxiv.org/abs/2410.17607",
        "title": "Exploiting Data Centres and Local Energy Communities Synergies for Market Participation",
        "rating": "-10",
        "keywords": [],
        "abstract": "The evolving energy landscape has propelled energy communities to the forefront of modern energy management. However, existing research has yet to explore the potential synergies between data centres and energy communities, necessitating an assessment on their collective capabilities for cost efficiency, waste heat optimisation, and market participation. This paper presents a mixed integer linear programming model to assess the collaborative performance of energy communities, data centres and energy markets. The evaluation focuses on the efficient use of waste heat and the flexibility of job scheduling while minimising system energy costs and maintaining quality of service requirements for data centres. Our results, based on realistic profiles of an energy community and a data centre, showcase significant benefits of these synergies, with a 38% reduction in operating costs and an 87% decrease in heat demand.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Accepted at IEEE PES ISGT Europe 2024"
    },
    {
        "paper id": "2410.17609",
        "abstract url": "https://arxiv.org/abs/2410.17609",
        "title": "Exploring the Impact of RIS on Cooperative NOMA URLLC Systems: A Theoretical Perspective",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we conduct a theoretical analysis of how to integrate reconfigurable intelligent surfaces (RIS) with cooperative non-orthogonal multiple access (NOMA), considering URLLC. We consider a downlink two-user cooperative NOMA system employing short-packet communications, where the two users are denoted by the central user (CU) and the cell-edge user (CEU), respectively, and an RIS is deployed to enhance signal quality. Specifically, compared to CEU, CU lies nearer from BS and enjoys the higher channel gains. Closed-form expressions for the CU's average block error rate (BLER) are derived. Furthermore, we evaluate the CEU's BLER performance utilizing selective combining (SC) and derive a tight lower bound under maximum ratio combining (MRC). Simulation results are provided to our analyses and demonstrate that the RIS-assisted system significantly outperforms its counterpart without RIS in terms of BLER. Notably, MRC achieves a squared multiple of the diversity gain of the SC, leading to more reliable performance, especially for the CEU. Furthermore, by dividing the RIS into two zones, each dedicated to a specific user, the average BLER can be further reduced, particularly for the CEU.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17615",
        "abstract url": "https://arxiv.org/abs/2410.17615",
        "title": "Higher inductive types in $(\\infty,1)$-categories",
        "rating": "-10",
        "keywords": [],
        "abstract": "We propose a definition of higher inductive types in $(\\infty,1)$-categories with finite limits. We show that the $(\\infty,1)$-category of $(\\infty,1)$-categories with higher inductive types is finitarily presentable. In particular, the initial $(\\infty,1)$-category with higher inductive types exists. We prove a form of canonicity: the global section functor for the initial $(\\infty,1)$-category with higher inductive types preserves higher inductive types.",
        "subjects": [
            "math.CT",
            "cs.LO",
            "math.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17623",
        "abstract url": "https://arxiv.org/abs/2410.17623",
        "title": "Signature-based IaaS Performance Change Detection",
        "rating": "-10",
        "keywords": [],
        "abstract": "We propose a novel change detection framework to identify changes in the long-term performance behavior of an IaaS service. An IaaS service's long-term performance behavior is represented by an IaaS performance signature. The proposed framework leverages time series similarity measures and a sliding window technique to detect changes in IaaS performance signatures. We introduce a new IaaS performance noise model that enables the proposed framework to distinguish between performance noise and actual changes in performance. The proposed framework utilizes a novel Signal-to-Noise Ratio (SNR) based approach to detect changes when prior knowledge about performance noise is available. A set of experiments is conducted using real-world datasets to demonstrate the effectiveness of the proposed change detection framework.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "accepted in ACM transaction on Internet Technology in October 2024. arXiv admin note: text overlap with arXiv:2007.11705"
    },
    {
        "paper id": "2410.17689",
        "abstract url": "https://arxiv.org/abs/2410.17689",
        "title": "Flexible Process Variant Binding in Information Systems with Software Product Line Engineering",
        "rating": "-10",
        "keywords": [],
        "abstract": "Different organisations often run similar digitised business processes to achieve their business goals. However, organisations often need to slightly adapt the business processes implemented in an information system in order to adopt them. Various approaches have been proposed to manage variants in process models. While these approaches mainly deal with control flow variability, in previous work we introduced an approach to manage implementation variants of digitised business processes. In this context Software Product Line (SPL) Engineering was applied to manage a set of common core artefacts including a process model from which Process-Aware Information Systems (PAIS) can be derived, which differ in the implementation of their process activities. % substitute the implementation of activities of a business process. When deriving a PAIS, implementations are selected for each process activity and then included in the PAIS at compilation time. One challenge that has not yet been solved is giving users of digitised business processes the option of selecting several features at runtime, i.e. selecting multiple activity implementations at runtime. This paper extends our previous work by not only allowing for the selection of activity implementations at compile time, but also at start time and runtime. Consequently, it becomes possible to defer the decision as to which features should be selected to start time and runtime. Furthermore, multiple implementations of a particular activity may be selected and executed concurrently. As another challenge different organisation may want to collect and base their decision on different information in a digitised business process. Consequently, the presented approach also allows customising the input and output data of activities when deriving a PAIS for a specific organisation.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17709",
        "abstract url": "https://arxiv.org/abs/2410.17709",
        "title": "Deoxys: A Causal Inference Engine for Unhealthy Node Mitigation in Large-scale Cloud Infrastructure",
        "rating": "-10",
        "keywords": [],
        "abstract": "The presence of unhealthy nodes in cloud infrastructure signals the potential failure of machines, which can significantly impact the availability and reliability of cloud services, resulting in negative customer experiences. Effectively addressing unhealthy node mitigation is therefore vital for sustaining cloud system performance. This paper introduces Deoxys, a causal inference engine tailored to recommending mitigation actions for unhealthy node in cloud systems to minimize virtual machine downtime and interruptions during unhealthy events. It employs double machine learning combined with causal forest to produce precise and reliable mitigation recommendations based solely on limited observational data collected from the historical unhealthy events. To enhance the causal inference model, Deoxys further incorporates a policy fallback mechanism based on model uncertainty and action overriding mechanisms to (i) improve the reliability of the system, and (ii) strike a good tradeoff between downtime reduction and resource utilization, thereby enhancing the overall system performance. After deploying Deoxys in a large-scale cloud infrastructure at Microsoft, our observations demonstrate that Deoxys significantly reduces average VM downtime by 53% compared to a legacy policy, while leading to 49.5% lower VM interruption rate. This substantial improvement enhances the reliability and stability of cloud platforms, resulting in a seamless customer experience.",
        "subjects": [
            "eess.SY",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17743",
        "abstract url": "https://arxiv.org/abs/2410.17743",
        "title": "Internship report: Coherent differentiation in models of Linear Logic",
        "rating": "-10",
        "keywords": [],
        "abstract": "Coherent differentiation was introduced by Ehrhard in order to generalize differential categories to a setting in which the sum is only partially defined, in order to account for the deterministic nature of most models of computation. This internship report proves that the deriving transformation of a differential category with a categorical product always induces a coherent differentiation. This ensures that coherent differentiation is indeed a generalization of differential categories.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17789",
        "abstract url": "https://arxiv.org/abs/2410.17789",
        "title": "FirePower: Towards a Foundation with Generalizable Knowledge for Architecture-Level Power Modeling",
        "rating": "-10",
        "keywords": [],
        "abstract": "Power efficiency is a critical design objective in modern processor design. A high-fidelity architecture-level power modeling method is greatly needed by CPU architects for guiding early optimizations. However, traditional architecture-level power models can not meet the accuracy requirement, largely due to the discrepancy between the power model and actual design implementation. While some machine learning (ML)-based architecture-level power modeling methods have been proposed in recent years, the data-hungry ML model training process requires sufficient similar known designs, which are unrealistic in many development scenarios. This work proposes a new power modeling solution FirePower that targets few-shot learning scenario for new target architectures. FirePower proposes multiple new policies to utilize cross-architecture knowledge. First, it develops power models at component level, and components are defined in a power-friendly manner. Second, it supports different generalization strategies for models of different components. Third, it formulates generalizable and architecture-specific design knowledge into two separate models. FirePower also supports the evaluation of the generalization quality. In our experiments, FirePower can achieve a low error percentage of 5.8% and a high correlation R of 0.98 on average only using two configurations of target architecture. This is 8.8% lower in error percentage and 0.03 higher in R compared with directly training McPAT-Calib baseline on configurations of target architecture.",
        "subjects": [
            "cs.AR"
        ],
        "comment": "Published in ASPDAC'25"
    },
    {
        "paper id": "2410.17805",
        "abstract url": "https://arxiv.org/abs/2410.17805",
        "title": "Experimental Demonstration of End-to-End Optimization for Directly Modulated Laser-based IM/DD Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "We experimentally demonstrate the joint optimization of transmitter and receiver parameters in directly modulated laser systems, showing superior performance compared to nonlinear receiver-only equalization while using fewer memory taps, less bandwidth, and lower radiofrequency power.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "submitted to the Optical Fiber Conference (OFC)"
    },
    {
        "paper id": "2410.17821",
        "abstract url": "https://arxiv.org/abs/2410.17821",
        "title": "On the formalization of the notion of a concurrent algorithm",
        "rating": "-10",
        "keywords": [],
        "abstract": "Previous papers give accounts of quests for satisfactory formalizations of the classical informal notion of an algorithm and the contemporary informal notion of an interactive algoritm. In this paper, an attempt is made to generalize the results of the former quest to the contemporary informal notion of a concurrent algorithm. The notion of a concurrent proto-algorithm is introduced. The thought is that concurrent algorithms are equivalence classes of concurrent proto-algorithms under an appropriate equivalence relation. Three equivalence relations are defined. Two of them are deemed to be bounds for an appropriate equivalence relation and the third is likely an appropriate one. The connection between concurrency and non-determinism in the presented setting is also addressed.",
        "subjects": [
            "cs.CC",
            "cs.DS",
            "cs.LO"
        ],
        "comment": "19 pages, in Sections 2, 3, and 4 there is some text overlap with arXiv:2401.08366 and substantial overlap with arXiv:2405.19037"
    },
    {
        "paper id": "2410.17830",
        "abstract url": "https://arxiv.org/abs/2410.17830",
        "title": "An iteration-free approach to excitation harmonization",
        "rating": "-10",
        "keywords": [],
        "abstract": "Sinusoidal excitation is particularly popular for testing structures in the nonlinear regime. Due to the nonlinear behavior and the inevitable feedback of the structure on the exciter, higher harmonics in the applied excitation are generated. This is undesired, because the acquired response may deviate substantially from that of the structure under purely sinusoidal excitation, in particular if one of the higher harmonics engages into resonance. We present a new approach to suppress those higher excitation harmonics and thus the unwanted exciter-structure interaction: Higher harmonics are added to the voltage input to the shaker whose Fourier coefficients are adjusted via feedback control until the excitation is purely sinusoidal. The stability of this method is analyzed for a simplified model; the resulting closed-form expressions are useful, among others, to select an appropriate exciter configuration, including the drive point. A practical procedure for the control design is suggested. The proposed method is validated in virtual and real experiments of internally resonant structures, in the two common configurations of force excitation via a stinger and base excitation. Excellent performance is achieved already when using the same control gains for all harmonics, throughout the tested range of amplitudes and frequencies, even in the strongly nonlinear regime. Compared to the iterative state of the art, it is found that the proposed method is simpler to implement, enables faster testing and it is easy to achieve a lower harmonic distortion.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17847",
        "abstract url": "https://arxiv.org/abs/2410.17847",
        "title": "A formal characterization of discrete condensed objects",
        "rating": "-10",
        "keywords": [],
        "abstract": "Condensed mathematics, developed by Clausen and Scholze over the last few years, proposes a generalization of topology with better categorical properties. It replaces the concept of a topological space by that of a condensed set, which can be defined as a sheaf on a certain site of compact Hausdorff spaces. Since condensed sets are supposed to be a generalization of topological spaces, one would like to be able to study the notion of discreteness. There are various ways to define what it means for a condensed set to be discrete. In this paper we describe them, and prove that they are equivalent. The results have been fully formalized in the Lean proof assistant.",
        "subjects": [
            "math.CT",
            "cs.FL",
            "cs.LO"
        ],
        "comment": "Updated the introduction and corrected a few typos in version 3"
    },
    {
        "paper id": "2410.17849",
        "abstract url": "https://arxiv.org/abs/2410.17849",
        "title": "Systematic teaching of UML and behavioral diagrams",
        "rating": "-10",
        "keywords": [],
        "abstract": "When studying software engineering, learning to create UML diagrams is crucial. Similar to how an architect would never build a house without a building plan, designing software architectures is important for developing high-quality software. UML diagrams are a standardized notation for the visualization of software architectures and software behavior. The research question that inspired this work was how to effectively evaluate hand-drawn diagrams without relying on model parsers. The findings of this investigation are presented in this paper. This article discusses the systematic acquisition of skills required for creating UML diagrams. Especially well-formed activity diagrams are one highlight. Additionally, the paper provides a variety of exercises. The exercises use recommended question types. The more unusual question types are related to images, such as questions about image annotation, finding hotspots on an image and positioning a target on an image. All the demonstrated exercises are suitable for both digital and handwritten training or exams.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Accepted for publication at the 36th Conference on Software Engineering Education and Training (CSEE&T)"
    },
    {
        "paper id": "2410.17872",
        "abstract url": "https://arxiv.org/abs/2410.17872",
        "title": "A Method to Reduce the Complexity of Computing the Complete Weight Distribution of Polar Codes",
        "rating": "-10",
        "keywords": [],
        "abstract": "The code spectrum of polar codes is crucial to the performance of polar codes. Based on the lower-triangular affine group (LTA) of decreasing monomial codes and the one-variable descendance (ovd) relation, we define a new subgroup of LTA which can find more cosets with the same weight distribution. Using this algebraic structure, we further reduce the complexity by proofing the group action on a coset set is transitive. Our method is an enhanced version of previous research, and the complexity of most cases can be reduced exceeding several times.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "9 pages, 3 tables"
    },
    {
        "paper id": "2410.17896",
        "abstract url": "https://arxiv.org/abs/2410.17896",
        "title": "Gradient-Based Meta Learning for Uplink RSMA with Beyond Diagonal RIS",
        "rating": "-10",
        "keywords": [],
        "abstract": "Beyond diagonal reconfigurable intelligent surface (BD-RIS) has emerged as an innovative and generalized RIS framework that provides greater flexibility in wave manipulation and enhanced coverage. In comparison to conventional RIS, optimization of BD-RIS is more challenging due to the large number of optimization variables associated with it. Typically, optimization of large-scale optimization problems utilizing traditional optimization methods results in high complexity. To tackle this issue, we propose a gradient-based meta learning algorithm which works without pre-training and is able to solve large-scale optimization problems. With the objective to maximize the sum rate of the system, to the best of our knowledge, this is the first work considering joint optimization of receiving beamforming vectors at the base station (BS), scattering matrix of BD-RIS and transmission power of users equipment (UEs) in uplink rate-splitting multiple access (RSMA) communication. Numerical results demonstrate that our proposed scheme can outperform the conventional RIS RSMA framework by 22.5$\\%$.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17907",
        "abstract url": "https://arxiv.org/abs/2410.17907",
        "title": "Adaptive Test Generation with Qgrams",
        "rating": "-10",
        "keywords": [],
        "abstract": "Adaptive Random Testing (ART) has faced criticism, particularly for its computational inefficiency, as highlighted by Arcuri and Briand. Their analysis clarified how ART requires a quadratic number of distance computations as the number of test executions increases, which limits its scalability in scenarios requiring extensive testing to uncover faults. Simulation results support this, showing that the computational overhead of these distance calculations often outweighs ART's benefits. While various ART variants have attempted to reduce these costs, they frequently do so at the expense of fault detection, lack complexity guarantees, or are restricted to specific input types, such as numerical or discrete data. In this paper, we introduce a novel framework for adaptive random testing that replaces pairwise distance computations with a compact aggregation of past executions, such as counting the Qgrams observed in previous runs. Test case selection then leverages this aggregated data to measure diversity (e.g., entropy of Qgrams), allowing us to reduce the computational complexity from quadratic to linear. Experiments with a benchmark of six web applications, show that ART with Qgrams covers, on average, 4x more unique targets than random testing, and 3.5x more than ART using traditional distance-based methods.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17909",
        "abstract url": "https://arxiv.org/abs/2410.17909",
        "title": "AI as a Bridge Across Ages: Exploring The Opportunities of Artificial Intelligence in Supporting Inter-Generational Communication in Virtual Reality",
        "rating": "-10",
        "keywords": [],
        "abstract": "Inter-generational communication is essential for bridging generational gaps and fostering mutual understanding. However, maintaining it is complex due to cultural, communicative, and geographical differences. Recent research indicated that while Virtual Reality (VR) creates a relaxed atmosphere and promotes companionship, it inadequately addresses the complexities of inter-generational dialogue, including variations in values and relational dynamics. To address this gap, we explored the opportunities of Artificial Intelligence (AI) in supporting inter-generational communication in VR. We developed three technology probes (e.g., Content Generator, Communication Facilitator, and Info Assistant) in VR and employed them in a probe-based participatory design study with twelve inter-generational pairs. Our results show that AI-powered VR facilitates inter-generational communication by enhancing mutual understanding, fostering conversation fluency, and promoting active participation. We also introduce several challenges when using AI-powered VR in supporting inter-generational communication and derive design implications for future VR platforms, aiming to improve inter-generational communication.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17925",
        "abstract url": "https://arxiv.org/abs/2410.17925",
        "title": "Securing Stack Smashing Protection in WebAssembly Applications",
        "rating": "-10",
        "keywords": [],
        "abstract": "WebAssembly is an instruction set architecture and binary format standard, designed for secure execution by an interpreter. Previous work has shown that WebAssembly is vulnerable to buffer overflow due to the lack of effective protection mechanisms. In this paper, we evaluate the implementation of Stack Smashing Protection (SSP) in WebAssembly standalone runtimes, and uncover two weaknesses in their current implementation. The first one is the possibility to overwrite the SSP reference value because of the contiguous memory zones inside a WebAssembly process. The second comes from the reliance of WebAssembly on the runtime to provide randomness in order to initialize the SSP reference value, which impacts the robustness of the solution. We address these two flaws by hardening the SSP implementation in terms of storage and random generator failure, in a way that is generalizable to all of WebAssembly. We evaluate our new, more robust, solution to prove that the implemented improvements do not reduce the efficiency of SSP.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Presented at PLAS 24 (ACM CCS workshop) https://plas24.github.io/"
    },
    {
        "paper id": "2410.17931",
        "abstract url": "https://arxiv.org/abs/2410.17931",
        "title": "ARAS: An Adaptive Low-Cost ReRAM-Based Accelerator for DNNs",
        "rating": "-10",
        "keywords": [],
        "abstract": "Processing Using Memory (PUM) accelerators have the potential to perform Deep Neural Network (DNN) inference by using arrays of memory cells as computation engines. Among various memory technologies, ReRAM crossbars show promising performance in computing dot-product operations in the analog domain. Nevertheless, the expensive writing procedure of ReRAM cells has led researchers to design accelerators whose crossbars have enough capacity to store the full DNN. Given the tremendous and continuous increase in DNN model sizes, this approach is unfeasible for some networks, or inefficient due to the huge hardware requirements. Those accelerators lack the flexibility to adapt to any given DNN model, facing an challenge. To address this issue we introduce ARAS, a cost-effective ReRAM-based accelerator that employs a smart scheduler to adapt different DNNs to the resource-limited hardware. ARAS also overlaps the computation of a layer with the weight writing of several layers to mitigate the high writing latency of ReRAM. Furthermore, ARAS introduces three optimizations aimed at reducing the energy overheads of writing in ReRAM. Our key optimization capitalizes on the observation that DNN weights can be re-encoded to augment their similarity between layers, increasing the amount of bitwise values that are equal or similar when overwriting ReRAM cells and, hence, reducing the amount of energy required to update the cells. Overall, ARAS greatly reduces the ReRAM writing activity. We evaluate ARAS on a popular set of DNNs. ARAS provides up to 2.2x speedup and 45% energy savings over a baseline PUM accelerator without any optimization. Compared to a TPU-like accelerator, ARAS provides up to 1.5x speedup and 61% energy savings.",
        "subjects": [
            "cs.AR"
        ],
        "comment": "13 pages, 17 figures"
    },
    {
        "paper id": "2410.17937",
        "abstract url": "https://arxiv.org/abs/2410.17937",
        "title": "Toward path-invariant embeddings for local distance source characterization",
        "rating": "-10",
        "keywords": [],
        "abstract": "This work builds on recent advances in foundation models in the language and image domains to explore similar approaches for seismic source characterization. We rely on an architecture called Barlow Twins, borrowed from an understanding of the human visual cortical system and originally envisioned for the image domain and adapt it for learning path invariance in seismic event time series. Our model improves the performance on event characterization tasks such as source discrimination across catalogs by 10-12% and provides more reliable predictive uncertainty estimates. We suggest that dataset scale and diversity more than architecture may determine aspects of the current ceiling on performance. We leverage decision trees, linear models, and visualization to understanding the dependencies in learned representations.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.17947",
        "abstract url": "https://arxiv.org/abs/2410.17947",
        "title": "Role of hydrogen in decarbonizing China's electricity and hard-to-abate sectors",
        "rating": "-10",
        "keywords": [],
        "abstract": "Green hydrogen has the potential to address two pressing problems in a zero-carbon energy system: balancing seasonal variability of solar and wind in the electricity sector, and replacing fossil fuels in hard-to-abate sectors. However, the previous research only separately modeled the electricity and hard-to-abate sectors, which is unable to capture how the interaction between the two sectors influences the energy system cost. In this study, focusing on China, we deploy an electricity system planning model to examine the cost implications of green hydrogen to fully decarbonize the electricity system and hard-to-abate sectors. Our results reveal that green hydrogen enables a 17% reduction in the levelized cost of a zero-carbon electricity system relative to that without hydrogen. However, cost savings hinge on the availability of underground hydrogen storage capacities and electric transmission expansion. More importantly, coupling hydrogen infrastructure in the electricity and hard-to-abate sectors not only reduces energy costs compared to a decoupled energy system but also makes green hydrogen cost-competitive compared to fossil fuel-based gray and blue hydrogen in China.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "4 figures, 33 pages"
    },
    {
        "paper id": "2410.17958",
        "abstract url": "https://arxiv.org/abs/2410.17958",
        "title": "Lower Bounds for Convexity Testing",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider the problem of testing whether an unknown and arbitrary set $S \\subseteq \\mathbb{R}^n$ (given as a black-box membership oracle) is convex, versus $\\varepsilon$-far from every convex set, under the standard Gaussian distribution. The current state-of-the-art testing algorithms for this problem make $2^{\\tilde{O}(\\sqrt{n})\\cdot \\mathrm{poly}(1/\\varepsilon)}$ non-adaptive queries, both for the standard testing problem and for tolerant testing. We give the first lower bounds for convexity testing in the black-box query model: - We show that any one-sided tester (which may be adaptive) must use at least $n^{\u03a9(1)}$ queries in order to test to some constant accuracy $\\varepsilon>0$. - We show that any non-adaptive tolerant tester (which may make two-sided errors) must use at least $2^{\u03a9(n^{1/4})}$ queries to distinguish sets that are $\\varepsilon_1$-close to convex versus $\\varepsilon_2$-far from convex, for some absolute constants $0<\\varepsilon_1<\\varepsilon_2$. Finally, we also show that for any constant $c>0$, any non-adaptive tester (which may make two-sided errors) must use at least $n^{1/4 - c}$ queries in order to test to some constant accuracy $\\varepsilon>0$.",
        "subjects": [
            "cs.CC",
            "cs.DS"
        ],
        "comment": "52 pages, to appear in SODA 2025"
    },
    {
        "paper id": "2410.18026",
        "abstract url": "https://arxiv.org/abs/2410.18026",
        "title": "EON: A practical energy-preserving rough diffuse BRDF",
        "rating": "-10",
        "keywords": [],
        "abstract": "We introduce the \"Energy-preserving Oren--Nayar\" (EON) model for reflection from rough surfaces. Unlike the popular qualitative Oren--Nayar model (QON) and its variants, our model is energy-preserving via analytical energy compensation. We include self-contained GLSL source code for efficient evaluation of the new model and importance sampling based on a novel technique we term \"Clipped Linearly Transformed Cosine\" (CLTC) sampling.",
        "subjects": [
            "cs.GR"
        ],
        "comment": "Corrected uniform_lobe_sample function. Provided link to source code"
    },
    {
        "paper id": "2410.18042",
        "abstract url": "https://arxiv.org/abs/2410.18042",
        "title": "Charon: An Analysis Framework for Rust",
        "rating": "-10",
        "keywords": [],
        "abstract": "With the explosion in popularity of the Rust programming language, a wealth of tools have recently been developed to analyze, verify, and test Rust programs. Alas, the Rust ecosystem remains relatively young, meaning that every one of these tools has had to re-implement difficult, time-consuming machinery to interface with the Rust compiler and its cargo build system, to hook into the Rust compiler's internal representation, and to expose an abstract syntax tree (AST) that is suitable for analysis rather than optimized for efficiency. We address this missing building block of the Rust ecosystem, and propose Charon, an analysis framework for Rust. Charon acts as a swiss-army knife for analyzing Rust programs, and deals with all of the tedium above, providing clients with a clean, stable AST that can serve as the foundation of many analyses. We demonstrate the usefulness of Charon through a series of case studies, ranging from a Rust verification framework (Aeneas), a compiler from Rust to C (Eurydice), and a novel taint-checker for cryptographic code. To drive the point home, we also re-implement a popular existing analysis (Rudra), and show that it can be replicated by leveraging the Charon framework.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18048",
        "abstract url": "https://arxiv.org/abs/2410.18048",
        "title": "A Comparative Assessment of Technology Acceptance and Learning Outcomes in Computer-based versus VR-based Pedagogical Agents",
        "rating": "-10",
        "keywords": [],
        "abstract": "As educational technology evolves, the potential of Pedagogical Agents (PAs) in supporting education is extensively explored. Typically, research on PAs has primarily focused on computer-based learning environments, but their use in VR-based environments and integration into education is still in its infancy. To address this gap, this paper presents a mixed method comparative study that has been conducted to evaluate and examine how these computer-based PAs and VR-based PAs compare, towards their learning efficacy and technology acceptance. 92 Computing and Engineering undergraduate students were recruited and participated in an educational experience focusing on computing machinery education. The findings of this study revealed that both approaches can effectively facilitate learning acquisition, and both technologies have been positively perceived by participants toward acceptance, without any significant differences. The findings of this study shed light on the potential of utilizing intelligent PAs to support education, contributing towards the advancement of our understanding of how to integrate such technologies to develop learning interventions, and establishing the foundation for future investigations that aim to successfully integrate and use PAs in education.",
        "subjects": [
            "cs.HC",
            "cs.ET"
        ],
        "comment": "Accepted as 4-pages poster paper in 23rd IEEE International Symposium on Mixed and Augmented Reality (ISMAR), 21-25/10/2024"
    },
    {
        "paper id": "2410.18053",
        "abstract url": "https://arxiv.org/abs/2410.18053",
        "title": "B-Side: Binary-Level Static System Call Identification",
        "rating": "-10",
        "keywords": [],
        "abstract": "System call filtering is widely used to secure programs in multi-tenant environments, and to sandbox applications in modern desktop software deployment and package management systems. Filtering rules are hard to write and maintain manually, hence generating them automatically is essential. To that aim, analysis tools able to identify every system call that can legitimately be invoked by a program are needed. Existing static analysis works lack precision because of a high number of false positives, and/or assume the availability of program/libraries source code -- something unrealistic in many scenarios such as cloud production environments. We present B-Side, a static binary analysis tool able to identify a superset of the system calls that an x86-64 static/dynamic executable may invoke at runtime. B-Side assumes no access to program/libraries sources, and shows a good degree of precision by leveraging symbolic execution, combined with a heuristic to detect system call wrappers, which represent an important source of precision loss in existing works. B-Side also allows to statically detect phases of execution in a program in which different filtering rules can be applied. We validate B-Side and demonstrate its higher precision compared to state-of-the-art works: over a set of popular applications, B-Side's average $F_1$ score is 0.81, vs. 0.31 and 0.53 for competitors. Over 557 static and dynamically-compiled binaries taken from the Debian repositories, B-Side identifies an average of 43 system calls, vs. 271 and 95 for two state-of-the art competitors. We further evaluate the strictness of the phase-based filtering policies that can be obtained with B-Side.",
        "subjects": [
            "cs.CR",
            "cs.OS"
        ],
        "comment": "Accepted to appear in the 25th ACM/IFIP International Middleware Conference (Middleware'24)"
    },
    {
        "paper id": "2410.18157",
        "abstract url": "https://arxiv.org/abs/2410.18157",
        "title": "A Type System to Ensure Non-Interference in ReScript",
        "rating": "-10",
        "keywords": [],
        "abstract": "Protecting confidential data from leaking is a critical challenge in computer systems, particularly given the growing number of observers on the internet. Therefore, limiting information flow using robust security policies becomes increasingly vital. We focus on the non-interference policy, where the goal is to ensure that confidential data can not impact public data. This paper presents a type system, for a subset of the ReScript syntax, designed to enforce non-interference. We conclude with a proof of soundness for the type system, demonstrating that if an expression is type-able, it is inherently non-interferent. In addition, we provide a brief overview of a type checker that implements the previously mentioned type system.",
        "subjects": [
            "cs.CR",
            "cs.PL"
        ],
        "comment": "Benjamin Bennetzen: 0009-0007-1751-6862 Daniel Vang Kleist: 0009-0005-1785-2124 Emilie Sonne Steinmann: 0009-0000-4733-5842 Loke Walsted: 0009-0002-1758-4594 Nikolaj Rossander Kristensen: 0009-0005-2339-8247 Peter Buus Steffensen: 0009-0005-6410-5869"
    },
    {
        "paper id": "2410.18158",
        "abstract url": "https://arxiv.org/abs/2410.18158",
        "title": "Cost Analysis for Import and Export Using an Abstract Machine",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents the syntax and reduction rules for an abstract machine based on the JavaScript XML language. We incorporate the notion of cost into our reduction rules, and create a type system that over-approximate this cost. This over-approximation results in an equation that may contain unknowns originating from while loops. We conclude with a formal proof of soundness of the type system for our abstract machine, demonstrating that it over-approximates the cost of any terminating program. An implementation of the type system, constraint gathering, and the abstract machine is also presented",
        "subjects": [
            "cs.PL"
        ],
        "comment": "Benjamin Bennetzen: 0009-0007-1751-6862 Daniel Vang Kleist: 0009-0005-1785-2124 Emilie Sonne Steinmann: 0009-0000-4733-5842 Loke Walsted: 0009-0002-1758-4594 Nikolaj Rossander Kristensen: 0009-0005-2339-8247 Peter Buus Steffensen: 0009-0005-6410-5869"
    },
    {
        "paper id": "2410.18212",
        "abstract url": "https://arxiv.org/abs/2410.18212",
        "title": "CUTECat: Concolic Execution for Computational Law",
        "rating": "-10",
        "keywords": [],
        "abstract": "Many legal computations, including the amount of tax owed by a citizen, whether they are eligible to social benefits, or the wages due to civil state servants, are specified by computational laws. Their application, however, is performed by expert computer programs intended to faithfully transcribe the law into computer code. Bugs in these programs can lead to dramatic societal impact, e.g., paying employees incorrect amounts, or not awarding benefits to families in need. To address this issue, we consider concolic unit testing, a combination of concrete execution with SMT-based symbolic execution, and propose CUTECat, a concolic execution tool targeting implementations of computational laws. Such laws typically follow a pattern where a base case is later refined by many exceptions in following law articles, a pattern that can be formally modeled using default logic. We show how to handle default logic inside a concolic execution tool, and implement our approach in the context of Catala, a recent domain-specific language tailored to implement computational laws. We evaluate CUTECat on several programs, including the Catala implementation of the French housing benefits and Section 132 of the US tax code. We show that CUTECat can successfully generate hundreds of thousands of testcases covering all branches of these bodies of law. Through several heuristics, we improve CUTECat's scalability and usability, making the testcases understandable by lawyers and programmers alike. We believe CUTECat thus paves the way for the use of formal methods during legislative processes.",
        "subjects": [
            "cs.PL",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18276",
        "abstract url": "https://arxiv.org/abs/2410.18276",
        "title": "Navigating the Landscape of Reproducible Research: A Predictive Modeling Approach",
        "rating": "-10",
        "keywords": [],
        "abstract": "The reproducibility of scientific articles is central to the advancement of science. Despite this importance, evaluating reproducibility remains challenging due to the scarcity of ground truth data. Predictive models can address this limitation by streamlining the tedious evaluation process. Typically, a paper's reproducibility is inferred based on the availability of artifacts such as code, data, or supplemental information, often without extensive empirical investigation. To address these issues, we utilized artifacts of papers as fundamental units to develop a novel, dual-spectrum framework that focuses on author-centric and external-agent perspectives. We used the author-centric spectrum, followed by the external-agent spectrum, to guide a structured, model-based approach to quantify and assess reproducibility. We explored the interdependencies between different factors influencing reproducibility and found that linguistic features such as readability and lexical diversity are strongly correlated with papers achieving the highest statuses on both spectrums. Our work provides a model-driven pathway for evaluating the reproducibility of scientific research. The code, methods, and artifacts for our study are publicly available at: https://github.com/reproducibilityproject/NLRR/",
        "subjects": [
            "cs.DL"
        ],
        "comment": "10 pages, CIKM 2024"
    },
    {
        "paper id": "2410.18280",
        "abstract url": "https://arxiv.org/abs/2410.18280",
        "title": "Crux, a Precise Verifier for Rust and Other Languages",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present Crux, a cross-language verification tool for Rust and C/LLVM. Crux targets bounded, intricate pieces of code that are difficult for humans to get right: for example, cryptographic modules and serializer / deserializer pairs. Crux builds on the same framework as the mature SAW-Cryptol toolchain, but Crux provides an interface where proofs are phrased as symbolic unit tests. Crux is designed for use in production environments, and has already seen use in industry. In this paper, we focus on Crux-MIR, our verification tool for Rust. Crux-MIR provides a bit-precise model of safe and unsafe Rust which can be used to check both inline properties about Rust code, and extensional equality to executable specifications written in Cryptol or in the hacspec dialect of Rust. Notably, Crux-MIR supports compositional reasoning, which is necessary to scale to even moderately complex proofs. We demonstrate Crux-MIR by verifying the Ring library implementations of SHA1 and SHA2 against pre-existing functional specifications. Crux is available at https://crux.galois.com.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18299",
        "abstract url": "https://arxiv.org/abs/2410.18299",
        "title": "CAMeleon: Interactively Exploring Craft Workflows in CAD",
        "rating": "-10",
        "keywords": [],
        "abstract": "Designers of physical objects make assumptions on the material and fabrication workflow early in the design process. Recovering from bad assumptions is hard, because the design and resulting CAD model are locked-in to those assumptions. We present CAMeleon, a software tool to interactively explore fabrication workflows at any stage of the CAD process. CAMeleon's modular architecture allows users to execute their design with different workflows, and preview results. Users can freely explore alternative workflows. CAMeleon's architecture, can be extended with new workflows, increasing the richness of workflows available. We implemented five fabrication workflows in CAMeleon. We demonstrate CAMeleon's extensibility through collaboration with six craftsmen whose workflows we replicated. We also implemented workflows of three papers and reflect on the process of these nine extensions. A usability study (n=12) showed that CAMeleon allowed participants to explore and select workflows for their design which they did not know before.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18300",
        "abstract url": "https://arxiv.org/abs/2410.18300",
        "title": "A Bayesian Approach to Low-Thrust Maneuvering Spacecraft Tracking",
        "rating": "-10",
        "keywords": [],
        "abstract": "Bayesian estimation with an explicit transitional prior is required for a tracking algorithm to be embedded in most multi-target tracking frameworks. This paper describes a novel approach capable of tracking maneuvering spacecraft with an explicit transitional prior and in a Bayesian framework, with fewer than two observations passes per day. The algorithm samples thrust profiles according to a multivariate Laplace distribution. It is shown that multivariate Laplace distributions are particularly suited to track maneuvering spacecraft, leading to a log probability function that is almost linear with the thrust. Principles from rare event simulation theory are used to propagate the tails of the distribution. Fast propagation is enabled by multi-fidelity methods. Because of the diffuse transitional prior, a novel k-nearest neighbor-based ensemble Gaussian mixture filter is developed and used.The method allows Bayesian tracking of maneuvering spacecraft for several scenarios with fewer than two measurement passes per day, and with a mismatch between the true and expected thrust magnitude of up to a factor of 200. The validity domain and statistical significance of the method are shown by simulation through several Monte Carlo trials in different scenarios and with different filter settings.",
        "subjects": [
            "eess.SY",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18308",
        "abstract url": "https://arxiv.org/abs/2410.18308",
        "title": "Exact schedulability test for sporadic mixed-criticality real-time systems using antichains and oracles",
        "rating": "-10",
        "keywords": [],
        "abstract": "This work addresses the problem of exact schedulability assessment in uniprocessor mixed-criticality real-time systems with sporadic task sets. We model the problem by means of a finite automaton that has to be explored in order to check for schedulability. To mitigate the state explosion problem, we provide a generic algorithm which is parameterised by several techniques called oracles and simulation relations. These techniques leverage results from the scheduling literature as \"plug-ins\" that make the algorithm more efficient in practice. Our approach achieves up to a 99.998% reduction in the search space required for exact schedulability testing, making it practical for a range of task sets, up to 8 tasks or maximum periods of 350. This method enables to challenge the pessimism of an existing schedulability test and to derive a new dynamic-priority scheduler, demonstrating its good performance. This is the full version of an RTNS 2024 paper.",
        "subjects": [
            "cs.OS"
        ],
        "comment": "In the proceedings of 32nd International Conference on Real-Time Networks and Systems, RTNS24, ACM, 2024"
    },
    {
        "paper id": "2410.18332",
        "abstract url": "https://arxiv.org/abs/2410.18332",
        "title": "Advancing Network Security: A Comprehensive Testbed and Dataset for Machine Learning-Based Intrusion Detection",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper introduces a Testbed designed for generating network traffic, leveraging the capabilities of containers, Kubernetes, and eBPF/XDP technologies. Our Testbed serves as an advanced platform for producing network traffic for machine learning based network experiments. By utilizing this Testbed, we offer small malicious network traffic dataset publically that satisfy ground truth property completely.",
        "subjects": [
            "cs.CR",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.18334",
        "abstract url": "https://arxiv.org/abs/2410.18334",
        "title": "Dear Diary: A randomized controlled trial of Generative AI coding tools in the workplace",
        "rating": "-10",
        "keywords": [],
        "abstract": "Generative AI coding tools are relatively new, and their impact on developers extends beyond traditional coding metrics, influencing beliefs about work and developers' roles in the workplace. This study aims to illuminate developers' preexisting beliefs about generative AI tools, their self perceptions, and how regular use of these tools may alter these beliefs. Using a mixed methods approach, including surveys, a randomized controlled trial, and a three week diary study, we explored the real world application of generative AI tools within a large multinational software company. Our findings reveal that the introduction and sustained use of generative AI coding tools significantly increases developers' perceptions of these tools as both useful and enjoyable. However, developers' views on the trustworthiness of AI generated code remained unchanged. We also discovered unexpected uses of these tools, such as replacing web searches and fostering creative ideation. Additionally, 84 percent of participants reported positive changes in their daily work practices, and 66 percent noted shifts in their feelings about their work, ranging from increased enthusiasm to heightened awareness of the need to stay current with technological advances. This research provides both qualitative and quantitative insights into the evolving role of generative AI in software development and offers practical recommendations for maximizing the benefits of this emerging technology, particularly in balancing the productivity gains from AI-generated code with the need for increased scrutiny and critical evaluation of its outputs.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "11 pages, 2 tables, 1 figure"
    },
    {
        "paper id": "2410.18411",
        "abstract url": "https://arxiv.org/abs/2410.18411",
        "title": "Federated Single Sign-On and Zero Trust Co-design for AI and HPC Digital Research Infrastructures",
        "rating": "-10",
        "keywords": [],
        "abstract": "Scientific workflows have become highly heterogenous, leveraging distributed facilities such as High Performance Computing (HPC), Artificial Intelligence (AI), Machine Learning (ML), scientific instruments (data-driven pipelines) and edge computing. As a result, Identity and Access Management (IAM) and Cybersecurity challenges across the diverse hardware and software stacks are growing. Nevertheless, scientific productivity relies on lowering access barriers via seamless, single sign-on (SSO) and federated login while ensuring access controls and compliance. We present an implementation of a federated IAM solution, which is coupled with multiple layers of security controls, multi-factor authentication, cloud-native protocols, and time-limited role-based access controls (RBAC) that has been co-designed and deployed for the Isambard-AI and HPC supercomputing Digital Research Infrastructures (DRIs) in the UK. Isambard DRIs as a national research resource are expected to comply with regulatory frameworks. Implementation details for monitoring, alerting and controls are outlined in the paper alongside selected user stories for demonstrating IAM workflows for different roles.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "9 pages, 2 figures"
    },
    {
        "paper id": "2410.18412",
        "abstract url": "https://arxiv.org/abs/2410.18412",
        "title": "HardRace: A Dynamic Data Race Monitor for Production Use",
        "rating": "-10",
        "keywords": [],
        "abstract": "Data races are critical issues in multithreaded program, leading to unpredictable, catastrophic and difficult-to-diagnose problems. Despite the extensive in-house testing, data races often escape to deployed software and manifest in production runs. Existing approaches suffer from either prohibitively high runtime overhead or incomplete detection capability. In this paper, we introduce HardRace, a data race monitor to detect races on-the-fly while with sufficiently low runtime overhead and high detection capability. HardRace firstly employs sound static analysis to determine a minimal set of essential memory accesses relevant to data races. It then leverages hardware trace instruction, i.e., Intel PTWRITE, to selectively record only these memory accesses and thread synchronization events during execution with negligible runtime overhead. Given the tracing data, HardRace performs standard data race detection algorithms to timely report potential races occurred in production runs. The experimental evaluations show that HardRace outperforms state-of-the-art tools like ProRace and Kard in terms of both runtime overhead and detection capability -- HardRace can detect all kinds of data races in read-world applications while maintaining a negligible overhead, less than 2% on average.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.19866",
        "abstract url": "https://arxiv.org/abs/2410.19866",
        "title": "A totally empirical basis of science",
        "rating": "-10",
        "keywords": [],
        "abstract": "Statistical hypothesis testing is the central method to demarcate scientific theories in both exploratory and inferential analyses. However, whether this method befits such purpose remains a matter of debate. Established approaches to hypothesis testing make several assumptions on the data generation process beyond the scientific theory. Most of these assumptions not only remain unmet in realistic datasets, but often introduce unwarranted bias in the analysis. Here, we depart from such restrictive assumptions to propose an alternative framework of total empiricism. We derive the Information-test ($I$-test) which allows for testing versatile hypotheses including non-null effects. To exemplify the adaptability of the $I$-test to application and study design, we revisit the hypothesis of interspecific metabolic scaling in mammals, ultimately rejecting both competing theories of pure allometry.",
        "subjects": [
            "physics.data-an",
            "cs.IT"
        ],
        "comment": "Main Article and Supplementary Material, 1 Table, 2 Figures"
    },
    {
        "paper id": "2411.02425",
        "abstract url": "https://arxiv.org/abs/2411.02425",
        "title": "A Study on Characterization of Near-Field Sub-Regions For Phased-Array Antennas",
        "rating": "-10",
        "keywords": [],
        "abstract": "We characterize three near-field sub-regions for phased array antennas by elaborating on the boundaries {\\it Fraunhofer}, {\\it radial-focal}, and {\\it non-radiating} distances. The {\\it Fraunhofer distance} which is the boundary between near and far field has been well studied in the literature on the principal axis (PA) of single-element center-fed antennas, where PA denotes the axis perpendicular to the antenna surface passing from the antenna center. The results are also valid for phased arrays if the PA coincides with the boresight, which is not commonly the case in practice. In this work, we completely characterize the Fraunhofer distance by considering various angles between the PA and the boresight. For the {\\it radial-focal distance}, below which beamfocusing is feasible in the radial domain, a formal characterization of the corresponding region based on the general model of near-field channels (GNC) is missing in the literature. We investigate this and elaborate that the maximum-ratio-transmission (MRT) beamforming based on the simple uniform spherical wave (USW) channel model results in a radial gap between the achieved and the desired focal points. While the gap vanishes when the array size $N$ becomes sufficiently large, we propose a practical algorithm to remove this gap in the non-asymptotic case when $N$ is not very large. Finally, the {\\it non-radiating} distance, below which the reactive power dominates active power, has been studied in the literature for single-element antennas. We analytically explore this for phased arrays and show how different excitation phases of the antenna array impact it. We also clarify some misconceptions about the non-radiating and Fresnel distances prevailing in the literature.",
        "subjects": [
            "eess.SP",
            "cs.IT"
        ],
        "comment": null
    }
]