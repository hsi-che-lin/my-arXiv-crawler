[
    {
        "paper id": "2410.20494",
        "abstract url": "https://arxiv.org/abs/2410.20494",
        "title": "MatViX: Multimodal Information Extraction from Visually Rich Articles",
        "rating": "2",
        "keywords": [
            [
                "vision-language",
                "VLMs"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Multimodal information extraction (MIE) is crucial for scientific literature, where valuable data is often spread across text, figures, and tables. In materials science, extracting structured information from research articles can accelerate the discovery of new materials. However, the multimodal nature and complex interconnections of scientific content present challenges for traditional text-based methods. We introduce \\textsc{MatViX}, a benchmark consisting of $324$ full-length research articles and $1,688$ complex structured JSON files, carefully curated by domain experts. These JSON files are extracted from text, tables, and figures in full-length documents, providing a comprehensive challenge for MIE. We introduce an evaluation method to assess the accuracy of curve similarity and the alignment of hierarchical structures. Additionally, we benchmark vision-language models (VLMs) in a zero-shot manner, capable of processing long contexts and multimodal inputs, and show that using a specialized model (DePlot) can improve performance in extracting curves. Our results demonstrate significant room for improvement in current models. Our dataset and evaluation code are available\\footnote{\\url{https://matvix-bench.github.io/}}.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20625",
        "abstract url": "https://arxiv.org/abs/2410.20625",
        "title": "LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization",
        "rating": "2",
        "keywords": [
            [
                "parameter-efficient",
                "efficient finetuning"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning method for LLM that reduces memory requirements. However, current LoRA optimizers lack transformation invariance, meaning the actual updates to the weights depends on how the two LoRA factors are scaled or rotated. This deficiency leads to inefficient learning and sub-optimal solutions in practice. This paper introduces LoRA-RITE, a novel adaptive matrix preconditioning method for LoRA optimization, which can achieve transformation invariance and remain computationally efficient. We provide theoretical analysis to demonstrate the benefit of our method and conduct experiments on various LLM tasks with different models including Gemma 2B, 7B, and mT5-XXL. The results demonstrate consistent improvements against existing optimizers. For example, replacing Adam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yielded 4.6\\% accuracy gain on Super-Natural Instructions and 3.5\\% accuracy gain across other four LLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA).",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20346",
        "abstract url": "https://arxiv.org/abs/2410.20346",
        "title": "Historical Test-time Prompt Tuning for Vision Foundation Models",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Test-time prompt tuning, which learns prompts online with unlabelled test samples during the inference stage, has demonstrated great potential by learning effective prompts on-the-fly without requiring any task-specific annotations. However, its performance often degrades clearly along the tuning process when the prompts are continuously updated with the test data flow, and the degradation becomes more severe when the domain of test samples changes continuously. We propose HisTPT, a Historical Test-time Prompt Tuning technique that memorizes the useful knowledge of the learnt test samples and enables robust test-time prompt tuning with the memorized knowledge. HisTPT introduces three types of knowledge banks, namely, local knowledge bank, hard-sample knowledge bank, and global knowledge bank, each of which works with different mechanisms for effective knowledge memorization and test-time prompt optimization. In addition, HisTPT features an adaptive knowledge retrieval mechanism that regularizes the prediction of each test sample by adaptively retrieving the memorized knowledge. Extensive experiments show that HisTPT achieves superior prompt tuning performance consistently while handling different visual recognition tasks (e.g., image classification, semantic segmentation, and object detection) and test samples from continuously changing domains.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "NeurIPS 2024 Camera Ready"
    },
    {
        "paper id": "2410.20371",
        "abstract url": "https://arxiv.org/abs/2410.20371",
        "title": "Open-Vocabulary Object Detection via Language Hierarchy",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Recent studies on generalizable object detection have attracted increasing attention with additional weak supervision from large-scale datasets with image-level labels. However, weakly-supervised detection learning often suffers from image-to-box label mismatch, i.e., image-level labels do not convey precise object information. We design Language Hierarchical Self-training (LHST) that introduces language hierarchy into weakly-supervised detector training for learning more generalizable detectors. LHST expands the image-level labels with language hierarchy and enables co-regularization between the expanded labels and self-training. Specifically, the expanded labels regularize self-training by providing richer supervision and mitigating the image-to-box label mismatch, while self-training allows assessing and selecting the expanded labels according to the predicted reliability. In addition, we design language hierarchical prompt generation that introduces language hierarchy into prompt generation which helps bridge the vocabulary gaps between training and testing. Extensive experiments show that the proposed techniques achieve superior generalization performance consistently across 14 widely studied object detection datasets.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "NeurIPS 2024 Camera Ready"
    },
    {
        "paper id": "2410.20444",
        "abstract url": "https://arxiv.org/abs/2410.20444",
        "title": "Vector Quantization Prompting for Continual Learning",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Continual learning requires to overcome catastrophic forgetting when training a single model on a sequence of tasks. Recent top-performing approaches are prompt-based methods that utilize a set of learnable parameters (i.e., prompts) to encode task knowledge, from which appropriate ones are selected to guide the fixed pre-trained model in generating features tailored to a certain task. However, existing methods rely on predicting prompt identities for prompt selection, where the identity prediction process cannot be optimized with task loss. This limitation leads to sub-optimal prompt selection and inadequate adaptation of pre-trained features for a specific task. Previous efforts have tried to address this by directly generating prompts from input queries instead of selecting from a set of candidates. However, these prompts are continuous, which lack sufficient abstraction for task knowledge representation, making them less effective for continual learning. To address these challenges, we propose VQ-Prompt, a prompt-based continual learning method that incorporates Vector Quantization (VQ) into end-to-end training of a set of discrete prompts. In this way, VQ-Prompt can optimize the prompt selection process with task loss and meanwhile achieve effective abstraction of task knowledge for continual learning. Extensive experiments show that VQ-Prompt outperforms state-of-the-art continual learning methods across a variety of benchmarks under the challenging class-incremental setting. The code is available at \\href{https://github.com/jiaolifengmi/VQ-Prompt}{this https URL}.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": "To appear in NeurIPS 2024"
    },
    {
        "paper id": "2410.20482",
        "abstract url": "https://arxiv.org/abs/2410.20482",
        "title": "What Factors Affect Multi-Modal In-Context Learning? An In-Depth Exploration",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Recently, rapid advancements in Multi-Modal In-Context Learning (MM-ICL) have achieved notable success, which is capable of achieving superior performance across various tasks without requiring additional parameter tuning. However, the underlying rules for the effectiveness of MM-ICL remain under-explored. To fill this gap, this work aims to investigate the research question: \"What factors affect the performance of MM-ICL?'' To this end, we investigate extensive experiments on the three core steps of MM-ICL including demonstration retrieval, demonstration ordering, and prompt construction using 6 vision large language models and 20 strategies. Our findings highlight (1) the necessity of a multi-modal retriever for demonstration retrieval, (2) the importance of intra-demonstration ordering over inter-demonstration ordering, and (3) the enhancement of task comprehension through introductory instructions in prompts. We hope this study can serve as a foundational guide for optimizing MM-ICL strategies in future research.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "Accepted at NeurIPS 2024"
    },
    {
        "paper id": "2410.20490",
        "abstract url": "https://arxiv.org/abs/2410.20490",
        "title": "$\\textit{Who Speaks Matters}$: Analysing the Influence of the Speaker's Ethnicity on Hate Classification",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Large Language Models (LLMs) offer a lucrative promise for scalable content moderation, including hate speech detection. However, they are also known to be brittle and biased against marginalised communities and dialects. This requires their applications to high-stakes tasks like hate speech detection to be critically scrutinized. In this work, we investigate the robustness of hate speech classification using LLMs, particularly when explicit and implicit markers of the speaker's ethnicity are injected into the input. For the explicit markers, we inject a phrase that mentions the speaker's identity. For the implicit markers, we inject dialectal features. By analysing how frequently model outputs flip in the presence of these markers, we reveal varying degrees of brittleness across 4 popular LLMs and 5 ethnicities. We find that the presence of implicit dialect markers in inputs causes model outputs to flip more than the presence of explicit markers. Further, the percentage of flips varies across ethnicities. Finally, we find that larger models are more robust. Our findings indicate the need for exercising caution in deploying LLMs for high-stakes tasks like hate speech detection.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "9 pages, 3 figures, 3 tables. To appear in NeurIPS SafeGenAI 2024 Workshop"
    },
    {
        "paper id": "2410.20535",
        "abstract url": "https://arxiv.org/abs/2410.20535",
        "title": "Asynchronous Perception Machine For Efficient Test-Time-Training",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In this work, we propose Asynchronous Perception Machine (APM), a computationally-efficient architecture for test-time-training (TTT). APM can process patches of an image one at a time in any order asymmetrically and still encode semantic-awareness in the net. We demonstrate APM's ability to recognize out-of-distribution images without dataset-specific pre-training, augmentation or any-pretext task. APM offers competitive performance over existing TTT approaches. To perform TTT, APM just distills test sample's representation once. APM possesses a unique property: it can learn using just this single representation and starts predicting semantically-aware features. APM demostrates potential applications beyond test-time-training: APM can scale up to a dataset of 2D images and yield semantic-clusterings in a single forward pass. APM also provides first empirical evidence towards validating GLOM's insight, i.e. input percept is a field. Therefore, APM helps us converge towards an implementation which can do both interpolation and perception on a shared-connectionist hardware. Our code is publicly available at this link: https://rajatmodi62.github.io/apm_project_page/.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Accepted to NeurIPS 2024 Main Track. APM is a step to getting Geoffrey Hinton's GLOM working"
    },
    {
        "paper id": "2410.20650",
        "abstract url": "https://arxiv.org/abs/2410.20650",
        "title": "NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks",
        "rating": "1.5",
        "keywords": [
            [
                "Memory-Efficient"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The performance of neural networks improves when more parameters are used. However, the model sizes are constrained by the available on-device memory during training and inference. Although applying techniques like quantization can alleviate the constraint, they suffer from performance degradation. In this work, we introduce NeuZip, a new weight compression scheme based on the entropy of floating-point numbers in neural networks. With NeuZip, we are able to achieve memory-efficient training and inference without sacrificing performance. Notably, we significantly reduce the memory footprint of training a Llama-3 8B model from 31GB to less than 16GB, while keeping the training dynamics fully unchanged. In inference, our method can reduce memory usage by more than half while maintaining near-lossless performance. Our code is publicly available.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20651",
        "abstract url": "https://arxiv.org/abs/2410.20651",
        "title": "SubjECTive-QA: Measuring Subjectivity in Earnings Call Transcripts' QA Through Six-Dimensional Feature Analysis",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Fact-checking is extensively studied in the context of misinformation and disinformation, addressing objective inaccuracies. However, a softer form of misinformation involves responses that are factually correct but lack certain features such as clarity and relevance. This challenge is prevalent in formal Question-Answer (QA) settings such as press conferences in finance, politics, sports, and other domains, where subjective answers can obscure transparency. Despite this, there is a lack of manually annotated datasets for subjective features across multiple dimensions. To address this gap, we introduce SubjECTive-QA, a human annotated dataset on Earnings Call Transcripts' (ECTs) QA sessions as the answers given by company representatives are often open to subjective interpretations and scrutiny. The dataset includes 49,446 annotations for long-form QA pairs across six features: Assertive, Cautious, Optimistic, Specific, Clear, and Relevant. These features are carefully selected to encompass the key attributes that reflect the tone of the answers provided during QA sessions across different domain. Our findings are that the best-performing Pre-trained Language Model (PLM), RoBERTa-base, has similar weighted F1 scores to Llama-3-70b-Chat on features with lower subjectivity, such as Relevant and Clear, with a mean difference of 2.17% in their weighted F1 scores. The models perform significantly better on features with higher subjectivity, such as Specific and Assertive, with a mean difference of 10.01% in their weighted F1 scores. Furthermore, testing SubjECTive-QA's generalizability using QAs from White House Press Briefings and Gaggles yields an average weighted F1 score of 65.97% using our best models for each feature, demonstrating broader applicability beyond the financial domain. SubjECTive-QA is publicly available under the CC BY 4.0 license",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Accepted at NeurIPS 2024"
    },
    {
        "paper id": "2410.20362",
        "abstract url": "https://arxiv.org/abs/2410.20362",
        "title": "Rethinking Data Synthesis: A Teacher Model Training Recipe with Interpretation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Recent advances in large language model (LLM) training have highlighted the need for diverse, high-quality instruction data. Recently, many works are exploring synthetic data generation using LLMs. However, they primarily focus on prompt engineering with standard supervised instruction-finetuned models, which contains a fundamental limitation: these models are optimized for general question-answering/problem-solving rather than data generation. We propose a paradigm shift named \\textbf{NOMAD} by investigating how to specifically train models for data generation, demonstrating that this task differs significantly from training a classical LM. We identify two key factors: no-prompt-masked training and proper training set size selection. Our method, NOMAD, shows substantial improvements over baselines, achieving >4\\% gains in TriviaQA and >2\\% in GSM8K with limited training data. Finally, we offer new insights by interpreting synthetic data through the lenses of \"relevance\" and \"novelty\".",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20421",
        "abstract url": "https://arxiv.org/abs/2410.20421",
        "title": "NT-VOT211: A Large-Scale Benchmark for Night-time Visual Object Tracking",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Many current visual object tracking benchmarks such as OTB100, NfS, UAV123, LaSOT, and GOT-10K, predominantly contain day-time scenarios while the challenges posed by the night-time has been less investigated. It is primarily because of the lack of a large-scale, well-annotated night-time benchmark for rigorously evaluating tracking algorithms. To this end, this paper presents NT-VOT211, a new benchmark tailored for evaluating visual object tracking algorithms in the challenging night-time conditions. NT-VOT211 consists of 211 diverse videos, offering 211,000 well-annotated frames with 8 attributes including camera motion, deformation, fast motion, motion blur, tiny target, distractors, occlusion and out-of-view. To the best of our knowledge, it is the largest night-time tracking benchmark to-date that is specifically designed to address unique challenges such as adverse visibility, image blur, and distractors inherent to night-time tracking scenarios. Through a comprehensive analysis of results obtained from 42 diverse tracking algorithms on NT-VOT211, we uncover the strengths and limitations of these algorithms, highlighting opportunities for enhancements in visual object tracking, particularly in environments with suboptimal lighting. Besides, a leaderboard for revealing performance rankings, annotation tools, comprehensive meta-information and all the necessary code for reproducibility of results is made publicly available. We believe that our NT-VOT211 benchmark will not only be instrumental in facilitating field deployment of VOT algorithms, but will also help VOT enhancements and it will unlock new real-world tracking applications. Our dataset and other assets can be found at: {https://github.com/LiuYuML/NV-VOT211.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Oral Acceptance at the Asian Conference on Computer Vision (ACCV) 2024, Hanoi, Vietnam"
    },
    {
        "paper id": "2410.20463",
        "abstract url": "https://arxiv.org/abs/2410.20463",
        "title": "A Derivational ChainBank for Modern Standard Arabic",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "This study presents the ``Arabic Derivational ChainBank,'' a novel framework for modeling Arabic derivational morphology. It establishes connections between forms and meanings by constructing a chain of derived words that reflect their derivational significance. To expedite the process, a rule-based methodology was employed, avoiding time-consuming manual annotation. The derivational network was then aligned with the CamelMorph morphological analyzer database. This two-step process resulted in a chain of derived word lemmas linked to their roots, encompassing 23,333 evaluated derivational relations, thereby demonstrating the efficiency of the ChainBank.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20483",
        "abstract url": "https://arxiv.org/abs/2410.20483",
        "title": "Improving Decision Sparsity",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Sparsity is a central aspect of interpretability in machine learning. Typically, sparsity is measured in terms of the size of a model globally, such as the number of variables it uses. However, this notion of sparsity is not particularly relevant for decision-making; someone subjected to a decision does not care about variables that do not contribute to the decision. In this work, we dramatically expand a notion of decision sparsity called the Sparse Explanation Value(SEV) so that its explanations are more meaningful. SEV considers movement along a hypercube towards a reference point. By allowing flexibility in that reference and by considering how distances along the hypercube translate to distances in feature space, we can derive sparser and more meaningful explanations for various types of function classes. We present cluster-based SEV and its variant tree-based SEV, introduce a method that improves credibility of explanations, and propose algorithms that optimize decision sparsity in machine learning models.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Accepted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024)"
    },
    {
        "paper id": "2410.20488",
        "abstract url": "https://arxiv.org/abs/2410.20488",
        "title": "FIRP: Faster LLM inference via future intermediate representation prediction",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in Large Language Models (LLMs) have shown remarkable performance across a wide range of tasks. Despite this, the auto-regressive nature of LLM decoding, which generates only a single token per forward propagation, fails to fully exploit the parallel computational power of GPUs, leading to considerable latency. To address this, we introduce a novel speculative decoding method named FIRP which generates multiple tokens instead of one at each decoding step. We achieve this by predicting the intermediate hidden states of future tokens (tokens have not been decoded yet) and then using these pseudo hidden states to decode future tokens, specifically, these pseudo hidden states are predicted with simple linear transformation in intermediate layers of LLMs. Once predicted, they participate in the computation of all the following layers, thereby assimilating richer semantic information. As the layers go deeper, the semantic gap between pseudo and real hidden states is narrowed and it becomes feasible to decode future tokens with high accuracy. To validate the effectiveness of FIRP, we conduct extensive experiments, showing a speedup ratio of 1.9x-3x in several models and datasets, analytical experiments also prove our motivations.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20513",
        "abstract url": "https://arxiv.org/abs/2410.20513",
        "title": "Is Moral Self-correction An Innate Capability of Large Language Models? A Mechanistic Analysis to Self-correction",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Though intensive attentions to the self-correction capability of Large Language Models (LLMs), the underlying mechanism of this capability is still under-explored. In this paper, we aim to answer two fundamental questions for moral self-correction: (1) how different components in self-correction, such as Chain-of-Thought (CoT) reasoning, external feedback, and instructional prompts, interact to enable moral self-correction; and (2) is the self-correction one of LLMs' innate capabilities? To answer the first question, we examine how different self-correction components interact to intervene the embedded morality within hidden states, therefore contributing to different performance. For the second question, we (i) evaluate the robustness of moral self-correction by introducing natural language interventions of weak evidence into prompts; (ii) propose a validation framework, self-distinguish, that requires effective self-correction to enable LLMs to distinguish between desirable and undesirable outputs. Our experimental results indicate that there is no universally optimal self-correction method for the tasks considered, although external feedback and CoT can contribute to additional performance gains. However, our mechanistic analysis reveals negative interactions among instructional prompts, CoT, and external feedback, suggesting a conflict between internal knowledge and external feedback. The self-distinguish experiments demonstrate that while LLMs can self-correct their responses, they are unable to reliably distinguish between desired and undesired outputs. With our empirical evidence, we can conclude that moral self-correction is not an innate capability of LLMs acquired during pretraining.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20519",
        "abstract url": "https://arxiv.org/abs/2410.20519",
        "title": "Fractal and Turbulent Feature Extraction and NFT Label Generation for Pollock Style Migration Paintings Based on VGG19",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper puts forth an innovative approach that fuses deep learning, fractal analysis, and turbulence feature extraction techniques to create abstract artworks in the style of Pollock. The content and style characteristics of the image are extracted by the MindSpore deep learning framework and a pre-trained VGG19 model. An optimisation process is then employed to The method generates high-quality Pollock-style images by combining content loss, style loss and full variance loss to achieve accurate style migration. Furthermore, this paper implements a fractal dimension calculation method based on the difference box-counting method, which effectively estimates the fractal dimension of an image through edge extraction and fractal analysis. The method is based on a two-dimensional discrete wavelet transform using a Haar wavelet to decompose the image in order to extract different frequency information. This is followed by the combination of multiple features to generate unique non-homogeneous token (NFT) labels for the authentication and protection of digital artwork. The experimental results demonstrate that the generated artworks exhibit The method demonstrates significant diversity and complexity in terms of fractal dimensions and turbulence features, while the generated NFT tags ensure the uniqueness and tamperability of each digital collection. The present method organically combines computer vision, digital signal processing and blockchain technology to provide a new solution for the creation and authentication of digital artworks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "9 pages, 4 figures"
    },
    {
        "paper id": "2410.20526",
        "abstract url": "https://arxiv.org/abs/2410.20526",
        "title": "Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Sparse Autoencoders (SAEs) have emerged as a powerful unsupervised method for extracting sparse representations from language models, yet scalable training remains a significant challenge. We introduce a suite of 256 SAEs, trained on each layer and sublayer of the Llama-3.1-8B-Base model, with 32K and 128K features. Modifications to a state-of-the-art SAE variant, Top-K SAEs, are evaluated across multiple dimensions. In particular, we assess the generalizability of SAEs trained on base models to longer contexts and fine-tuned models. Additionally, we analyze the geometry of learned SAE latents, confirming that \\emph{feature splitting} enables the discovery of new features. The Llama Scope SAE checkpoints are publicly available at~\\url{https://huggingface.co/fnlp/Llama-Scope}, alongside our scalable training, interpretation, and visualization tools at \\url{https://github.com/OpenMOSS/Language-Model-SAEs}. These contributions aim to advance the open-source Sparse Autoencoder ecosystem and support mechanistic interpretability research by reducing the need for redundant SAE training.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": "22pages, 12 figures"
    },
    {
        "paper id": "2410.20533",
        "abstract url": "https://arxiv.org/abs/2410.20533",
        "title": "Guiding Through Complexity: What Makes Good Supervision for Hard Reasoning Tasks?",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "How can \"weak teacher models\" such as average human annotators or existing AI systems, effectively supervise LLMs to improve performance on hard reasoning tasks, especially those that challenge and requires expertise or daily practice from the teacher models? In this paper, we seek for empirical answers to this question by investigating various data-driven strategies that offer supervision data at different quality levels upon tasks of varying complexity. Two intuitive strategies emerge for teacher models to provide supervision during alignment training: 1) using lower-quality supervision from complete tasks that match the difficulty of the target reasoning tasks, and 2) leveraging higher-quality supervision from easier subtasks that are less challenging. Interestingly, we find that even when the outcome error rate for hard task supervision is high (e.g., 90\\%), training on such data can outperform perfectly correct supervision on easier subtasks on multiple hard math benchmarks. We further identify a more critical factor influencing training performance: step-wise error rates, which indicate the severity of errors in solutions. Specifically, training on hard task supervision with the same outcome error rates but disparate step-wise error rates can lead to a 30\\% accuracy gap on MATH benchmark. Our results also reveal that supplementing hard task supervision with the corresponding subtask supervision can yield notable performance improvements than simply combining rephrased hard full task supervision, suggesting new avenues for data augmentation. Data and code are released at \\url{https://github.com/hexuan21/Weak-to-Strong}.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20540",
        "abstract url": "https://arxiv.org/abs/2410.20540",
        "title": "Automatic Estimation of Singing Voice Musical Dynamics",
        "rating": "1",
        "keywords": [
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Musical dynamics form a core part of expressive singing voice performances. However, automatic analysis of musical dynamics for singing voice has received limited attention partly due to the scarcity of suitable datasets and a lack of clear evaluation frameworks. To address this challenge, we propose a methodology for dataset curation. Employing the proposed methodology, we compile a dataset comprising 509 musical dynamics annotated singing voice performances, aligned with 163 score files, leveraging state-of-the-art source separation and alignment techniques. The scores are sourced from the OpenScore Lieder corpus of romantic-era compositions, widely known for its wealth of expressive annotations. Utilizing the curated dataset, we train a multi-head attention based CNN model with varying window sizes to evaluate the effectiveness of estimating musical dynamics. We explored two distinct perceptually motivated input representations for the model training: log-Mel spectrum and bark-scale based features. For testing, we manually curate another dataset of 25 musical dynamics annotated performances in collaboration with a professional vocalist. We conclude through our experiments that bark-scale based features outperform log-Mel-features for the task of singing voice dynamics prediction. The dataset along with the code is shared publicly for further research on the topic.",
        "subjects": [
            "cs.SD",
            "cs.IR",
            "eess.AS"
        ],
        "comment": "To be published in ISMIR 2024, 6 pages"
    },
    {
        "paper id": "2410.20564",
        "abstract url": "https://arxiv.org/abs/2410.20564",
        "title": "Using Confidence Scores to Improve Eyes-free Detection of Speech Recognition Errors",
        "rating": "1",
        "keywords": [
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Conversational systems rely heavily on speech recognition to interpret and respond to user commands and queries. Nevertheless, recognition errors may occur, which can significantly affect the performance of such systems. While visual feedback can help detect errors, it may not always be practical, especially for people who are blind or low-vision. In this study, we investigate ways to improve error detection by manipulating the audio output of the transcribed text based on the recognizer's confidence level in its result. Our findings show that selectively slowing down the audio when the recognizer exhibited uncertainty led to a relative increase of 12% in participants' error detection ability compared to uniformly slowing down the audio.",
        "subjects": [
            "cs.HC",
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20573",
        "abstract url": "https://arxiv.org/abs/2410.20573",
        "title": "Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using Space-Filling Vector Quantization",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Generative adversarial networks (GANs) learn a latent space whose samples can be mapped to real-world images. Such latent spaces are difficult to interpret. Some earlier supervised methods aim to create an interpretable latent space or discover interpretable directions that require exploiting data labels or annotated synthesized samples for training. However, we propose using a modification of vector quantization called space-filling vector quantization (SFVQ), which quantizes the data on a piece-wise linear curve. SFVQ can capture the underlying morphological structure of the latent space and thus make it interpretable. We apply this technique to model the latent space of pretrained StyleGAN2 and BigGAN networks on various datasets. Our experiments show that the SFVQ curve yields a general interpretable model of the latent space that determines which part of the latent space corresponds to what specific generative factors. Furthermore, we demonstrate that each line of SFVQ's curve can potentially refer to an interpretable direction for applying intelligible image transformations. We also showed that the points located on an SFVQ line can be used for controllable data augmentation.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20595",
        "abstract url": "https://arxiv.org/abs/2410.20595",
        "title": "A Framework for Real-Time Volcano-Seismic Event Recognition Based on Multi-Station Seismograms and Semantic Segmentation Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "In volcano monitoring, effective recognition of seismic events is essential for understanding volcanic activity and raising timely warning alerts. Traditional methods rely on manual analysis, which can be subjective and labor-intensive. Furthermore, current automatic approaches often tackle detection and classification separately, mostly rely on single station information and generally require tailored preprocessing and representations to perform predictions. These limitations often hinder their application to real-time monitoring and utilization across different volcano conditions. This study introduces a novel approach that utilizes Semantic Segmentation models to automate seismic event recognition by applying a straight forward transformation of multi-channel 1D signals into 2D representations, enabling their use as images. Our framework employs a data-driven, end-to-end design that integrates multi-station seismic data with minimal preprocessing, performing both detection and classification simultaneously for five seismic event classes. We evaluated four state-of-the-art segmentation models (UNet, UNet++, DeepLabV3+ and SwinUNet) on approximately 25.000 seismic events recorded at four different Chilean volcanoes: Nevados del Chill\u00e1n Volcanic Complex, Laguna del Maule, Villarrica and Puyehue-Cord\u00f3n Caulle. Among these models, the UNet architecture was identified as the most effective model, achieving mean F1 and Intersection over Union (IoU) scores of up to 0.91 and 0.88, respectively, and demonstrating superior noise robustness and model flexibility to unseen volcano datasets.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "eess.SP"
        ],
        "comment": "10 pages, 9 figures. This is a pre-print, it is currently under review for publication"
    },
    {
        "paper id": "2410.20596",
        "abstract url": "https://arxiv.org/abs/2410.20596",
        "title": "Practical Bayesian Algorithm Execution via Posterior Sampling",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We consider Bayesian algorithm execution (BAX), a framework for efficiently selecting evaluation points of an expensive function to infer a property of interest encoded as the output of a base algorithm. Since the base algorithm typically requires more evaluations than are feasible, it cannot be directly applied. Instead, BAX methods sequentially select evaluation points using a probabilistic numerical approach. Current BAX methods use expected information gain to guide this selection. However, this approach is computationally intensive. Observing that, in many tasks, the property of interest corresponds to a target set of points defined by the function, we introduce PS-BAX, a simple, effective, and scalable BAX method based on posterior sampling. PS-BAX is applicable to a wide range of problems, including many optimization variants and level set estimation. Experiments across diverse tasks demonstrate that PS-BAX performs competitively with existing baselines while being significantly faster, simpler to implement, and easily parallelizable, setting a strong baseline for future research. Additionally, we establish conditions under which PS-BAX is asymptotically convergent, offering new insights into posterior sampling as an algorithm design paradigm.",
        "subjects": [
            "cs.LG",
            "math.OC",
            "stat.ML"
        ],
        "comment": "Published as a conference paper at the 38th Conference on Neural Information Processing Systems (NeurIPS 2024)"
    },
    {
        "paper id": "2410.20621",
        "abstract url": "https://arxiv.org/abs/2410.20621",
        "title": "Exocentric To Egocentric Transfer For Action Recognition: A Short Survey",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Egocentric vision captures the scene from the point of view of the camera wearer while exocentric vision captures the overall scene context. Jointly modeling ego and exo views is crucial to developing next-generation AI agents. The community has regained interest in the field of egocentric vision. While the third-person view and first-person have been thoroughly investigated, very few works aim to study both synchronously. Exocentric videos contain many relevant signals that are transferrable to egocentric videos. In this paper, we provide a broad overview of works combining egocentric and exocentric visions.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20631",
        "abstract url": "https://arxiv.org/abs/2410.20631",
        "title": "PViT: Prior-augmented Vision Transformer for Out-of-distribution Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Vision Transformers (ViTs) have achieved remarkable success over various vision tasks, yet their robustness against data distribution shifts and inherent inductive biases remain underexplored. To enhance the robustness of ViT models for image Out-of-Distribution (OOD) detection, we introduce a novel and generic framework named Prior-augmented Vision Transformer (PViT). PViT identifies OOD samples by quantifying the divergence between the predicted class logits and the prior logits obtained from pre-trained models. Unlike existing state-of-the-art OOD detection methods, PViT shapes the decision boundary between ID and OOD by utilizing the proposed prior guide confidence, without requiring additional data modeling, generation methods, or structural modifications. Extensive experiments on the large-scale ImageNet benchmark demonstrate that PViT significantly outperforms existing state-of-the-art OOD detection methods. Additionally, through comprehensive analyses, ablation studies, and discussions, we show how PViT can strategically address specific challenges in managing large vision models, paving the way for new advancements in OOD detection.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20638",
        "abstract url": "https://arxiv.org/abs/2410.20638",
        "title": "Ant Detective: An Automated Approach for Counting Ants in Densely Populated Images and Gaining Insight into Ant Foraging Behavior",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Ant foraging behavior is essential to understanding ecological dynamics and developing effective pest management strategies, but quantifying this behavior is challenging due to the labor-intensive nature of manual counting, especially in densely populated images. This study presents an automated approach using computer vision to count ants and analyze their foraging behavior. Leveraging the YOLOv8 model, the system was calibrated and evaluated on datasets encompassing various imaging scenarios and densities. The study results demonstrate that the system achieves average precision and recall of up to 87.96% and 87,78%, respectively, with only 64 calibration images provided when the both calibration and evaluation images share similar imaging backgrounds. When the background is more complex than the calibration images, the system requires a larger calibration set to generalize effectively, with 1,024 images yielding the precision and recall of up to 83.60% and 78.88, respectively. In more challenging scenarios where more than one thousand ants are present in a single image, the system significantly improves detection accuracy by slicing images into smaller patches, reaching a precision and recall of 77.97% and 71.36%, respectively. The system's ability to generate heatmaps visualizes the spatial distribution of ant activity over time, providing valuable insights into their foraging patterns. This spatial-temporal analysis enables a more comprehensive understanding of ant behavior, which is crucial for ecological studies and improving pest control methods. By automating the counting process and offering detailed behavioral analysis, this study provides an efficient tool for researchers and pest control professionals to develop more effective strategies.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20652",
        "abstract url": "https://arxiv.org/abs/2410.20652",
        "title": "Visualizing attention zones in machine reading comprehension models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The attention mechanism plays an important role in the machine reading comprehension (MRC) model. Here, we describe a pipeline for building an MRC model with a pretrained language model and visualizing the effect of each attention zone in different layers, which can indicate the explainability of the model. With the presented protocol and accompanying code, researchers can easily visualize the relevance of each attention zone in the MRC model. This approach can be generalized to other pretrained language models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "17 pages, published in STAR Protocols"
    },
    {
        "paper id": "2410.20666",
        "abstract url": "https://arxiv.org/abs/2410.20666",
        "title": "Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People with Visual Impairments",
        "rating": "1",
        "keywords": [
            [
                "vision-language",
                "VLMs"
            ],
            [
                "Navigation"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Navigation presents a significant challenge for persons with visual impairments (PVI). While traditional aids such as white canes and guide dogs are invaluable, they fall short in delivering detailed spatial information and precise guidance to desired locations. Recent developments in large language models (LLMs) and vision-language models (VLMs) offer new avenues for enhancing assistive navigation. In this paper, we introduce Guide-LLM, an embodied LLM-based agent designed to assist PVI in navigating large indoor environments. Our approach features a novel text-based topological map that enables the LLM to plan global paths using a simplified environmental representation, focusing on straight paths and right-angle turns to facilitate navigation. Additionally, we utilize the LLM's commonsense reasoning for hazard detection and personalized path planning based on user preferences. Simulated experiments demonstrate the system's efficacy in guiding PVI, underscoring its potential as a significant advancement in assistive technology. The results highlight Guide-LLM's ability to offer efficient, adaptive, and personalized navigation assistance, pointing to promising advancements in this field.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20682",
        "abstract url": "https://arxiv.org/abs/2410.20682",
        "title": "SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset Constructed from Movie Script",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Shared memories between two individuals strengthen their bond and are crucial for facilitating their ongoing conversations. This study aims to make long-term dialogue more engaging by leveraging these shared memories. To this end, we introduce a new long-term dialogue dataset named SHARE, constructed from movie scripts, which are a rich source of shared memories among various relationships. Our dialogue dataset contains the summaries of persona information and events of two individuals, as explicitly revealed in their conversation, along with implicitly extractable shared memories. We also introduce EPISODE, a long-term dialogue framework based on SHARE that utilizes shared experiences between individuals. Through experiments using SHARE, we demonstrate that shared memories between two individuals make long-term dialogues more engaging and sustainable, and that EPISODE effectively manages shared memories during dialogue. Our new dataset is publicly available at https://anonymous.4open.science/r/SHARE-AA1E/SHARE.json.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20699",
        "abstract url": "https://arxiv.org/abs/2410.20699",
        "title": "CIB-SE-YOLOv8: Optimized YOLOv8 for Real-Time Safety Equipment Detection on Construction Sites",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Ensuring safety on construction sites is critical, with helmets playing a key role in reducing injuries. Traditional safety checks are labor-intensive and often insufficient. This study presents a computer vision-based solution using YOLO for real-time helmet detection, leveraging the SHEL5K dataset. Our proposed CIB-SE-YOLOv8 model incorporates SE attention mechanisms and modified C2f blocks, enhancing detection accuracy and efficiency. This model offers a more effective solution for promoting safety compliance on construction sites.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "5 pages, 5 figures"
    },
    {
        "paper id": "2410.20710",
        "abstract url": "https://arxiv.org/abs/2410.20710",
        "title": "Relation-based Counterfactual Data Augmentation and Contrastive Learning for Robustifying Natural Language Inference Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Although pre-trained language models show good performance on various natural language processing tasks, they often rely on non-causal features and patterns to determine the outcome. For natural language inference tasks, previous results have shown that even a model trained on a large number of data fails to perform well on counterfactually revised data, indicating that the model is not robustly learning the semantics of the classes. In this paper, we propose a method in which we use token-based and sentence-based augmentation methods to generate counterfactual sentence pairs that belong to each class, and apply contrastive learning to help the model learn the difference between sentence pairs of different classes with similar contexts. Evaluation results with counterfactually-revised dataset and general NLI datasets show that the proposed method can improve the performance and robustness of the NLI model.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "accepted at INTERSPEECH 2023"
    },
    {
        "paper id": "2410.20722",
        "abstract url": "https://arxiv.org/abs/2410.20722",
        "title": "Interpretable Image Classification with Adaptive Prototype-based Vision Transformers",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present ProtoViT, a method for interpretable image classification combining deep learning and case-based reasoning. This method classifies an image by comparing it to a set of learned prototypes, providing explanations of the form ``this looks like that.'' In our model, a prototype consists of \\textit{parts}, which can deform over irregular geometries to create a better comparison between images. Unlike existing models that rely on Convolutional Neural Network (CNN) backbones and spatially rigid prototypes, our model integrates Vision Transformer (ViT) backbones into prototype based models, while offering spatially deformed prototypes that not only accommodate geometric variations of objects but also provide coherent and clear prototypical feature representations with an adaptive number of prototypical parts. Our experiments show that our model can generally achieve higher performance than the existing prototype based models. Our comprehensive analyses ensure that the prototypes are consistent and the interpretations are faithful.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.21332",
        "abstract url": "https://arxiv.org/abs/2410.21332",
        "title": "Building, Reusing, and Generalizing Abstract Representations from Concrete Sequences",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Humans excel at learning abstract patterns across different sequences, filtering out irrelevant details, and transferring these generalized concepts to new sequences. In contrast, many sequence learning models lack the ability to abstract, which leads to memory inefficiency and poor transfer. We introduce a non-parametric hierarchical variable learning model (HVM) that learns chunks from sequences and abstracts contextually similar chunks as variables. HVM efficiently organizes memory while uncovering abstractions, leading to compact sequence representations. When learning on language datasets such as babyLM, HVM learns a more efficient dictionary than standard compression algorithms such as Lempel-Ziv. In a sequence recall task requiring the acquisition and transfer of variables embedded in sequences, we demonstrate HVM's sequence likelihood correlates with human recall times. In contrast, large language models (LLMs) struggle to transfer abstract variables as effectively as humans. From HVM's adjustable layer of abstraction, we demonstrate that the model realizes a precise trade-off between compression and generalization. Our work offers a cognitive model that captures the learning and transfer of abstract representations in human cognition and differentiates itself from the behavior of large language models.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.21333",
        "abstract url": "https://arxiv.org/abs/2410.21333",
        "title": "Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "Chain-of-thought (CoT) prompting has become a widely used strategy for working with large language and multimodal models. While CoT has been shown to improve performance across many tasks, determining the settings in which it is effective remains an ongoing effort. In particular, it is still an open question in what settings CoT systematically reduces model performance. In this paper, we seek to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, looking at cases where (i) verbal thinking or deliberation hurts performance in humans, and (ii) the constraints governing human performance generalize to language models. Three such cases are implicit statistical learning, visual recognition, and classifying with patterns containing exceptions. In extensive experiments across all three settings, we find that a diverse collection of state-of-the-art models exhibit significant drop-offs in performance (e.g., up to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using inference-time reasoning compared to zero-shot counterparts. We also identify three tasks that satisfy condition (i) but not (ii), and find that while verbal thinking reduces human performance in these tasks, CoT retains or increases model performance. Overall, our results show that while there is not an exact parallel between the cognitive processes of models and those of humans, considering cases where thinking has negative consequences for human performance can help us identify settings where it negatively impacts models. By connecting the literature on human deliberation with evaluations of CoT, we offer a new tool that can be used in understanding the impact of prompt choices and inference-time reasoning.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.21338",
        "abstract url": "https://arxiv.org/abs/2410.21338",
        "title": "FinTeamExperts: Role Specialized MOEs For Financial Analysis",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs), such as ChatGPT, Phi3 and Llama-3, are leading a significant leap in AI, as they can generalize knowledge from their training to new tasks without fine-tuning. However, their application in the financial domain remains relatively limited. The financial field is inherently complex, requiring a deep understanding across various perspectives, from macro, micro economic trend to quantitative analysis. Motivated by this complexity, a mixture of expert LLMs tailored to specific financial domains could offer a more comprehensive understanding for intricate financial tasks. In this paper, we present the FinTeamExperts, a role-specialized LLM framework structured as a Mixture of Experts (MOEs) for financial analysis. The framework simulates a collaborative team setting by training each model to specialize in distinct roles: Macro Analysts, Micro analysts, and Quantitative Analysts. This role-specific specialization enhances the model's ability to integrate their domain-specific expertise. We achieve this by training three 8-billion parameter models on different corpus, each dedicated to excelling in specific finance-related roles. We then instruct-tune FinTeamExperts on downstream tasks to align with practical financial tasks. The experimental results show that FinTeamExperts outperform all models of the same size and larger on three out of four datasets. On the fourth dataset, which presents a more complex task, FinTeamExperts still surpass all models of the same size. This highlights the success of our role-based specialization approach and the continued training approach for FinTeamExperts.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.22364",
        "abstract url": "https://arxiv.org/abs/2410.22364",
        "title": "Accelerating Augmentation Invariance Pretraining",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Our work tackles the computational challenges of contrastive learning methods, particularly for the pretraining of Vision Transformers (ViTs). Despite the effectiveness of contrastive learning, the substantial computational resources required for training often hinder their practical application. To mitigate this issue, we propose an acceleration framework, leveraging ViT's unique ability to generalize across inputs of varying sequence lengths. Our method employs a mix of sequence compression strategies, including randomized token dropout and flexible patch scaling, to reduce the cost of gradient estimation and accelerate convergence. We further provide an in-depth analysis of the gradient estimation error of various acceleration strategies as well as their impact on downstream tasks, offering valuable insights into the trade-offs between acceleration and performance. We also propose a novel procedure to identify an optimal acceleration schedule to adjust the sequence compression ratios to the training progress, ensuring efficient training without sacrificing downstream performance. Our approach significantly reduces computational overhead across various self-supervised learning algorithms on large-scale datasets. In ImageNet, our method achieves speedups of 4$\\times$ in MoCo, 3.3$\\times$ in SimCLR, and 2.5$\\times$ in DINO, demonstrating substantial efficiency gains.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20345",
        "abstract url": "https://arxiv.org/abs/2410.20345",
        "title": "Logarithmically Quantized Distributed Optimization over Dynamic Multi-Agent Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Distributed optimization finds many applications in machine learning, signal processing, and control systems. In these real-world applications, the constraints of communication networks, particularly limited bandwidth, necessitate implementing quantization techniques. In this paper, we propose distributed optimization dynamics over multi-agent networks subject to logarithmically quantized data transmission. Under this condition, data exchange benefits from representing smaller values with more bits and larger values with fewer bits. As compared to uniform quantization, this allows for higher precision in representing near-optimal values and more accuracy of the distributed optimization algorithm. The proposed optimization dynamics comprise a primary state variable converging to the optimizer and an auxiliary variable tracking the objective function's gradient. Our setting accommodates dynamic network topologies, resulting in a hybrid system requiring convergence analysis using matrix perturbation theory and eigenspectrum analysis.",
        "subjects": [
            "eess.SY",
            "cs.LG",
            "cs.MA",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20349",
        "abstract url": "https://arxiv.org/abs/2410.20349",
        "title": "Idempotent Unsupervised Representation Learning for Skeleton-Based Action Recognition",
        "rating": "0.5",
        "keywords": [
            [
                "Skeleton"
            ],
            [
                "cs.AI",
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Generative models, as a powerful technique for generation, also gradually become a critical tool for recognition tasks. However, in skeleton-based action recognition, the features obtained from existing pre-trained generative methods contain redundant information unrelated to recognition, which contradicts the nature of the skeleton's spatially sparse and temporally consistent properties, leading to undesirable performance. To address this challenge, we make efforts to bridge the gap in theory and methodology and propose a novel skeleton-based idempotent generative model (IGM) for unsupervised representation learning. More specifically, we first theoretically demonstrate the equivalence between generative models and maximum entropy coding, which demonstrates a potential route that makes the features of generative models more compact by introducing contrastive learning. To this end, we introduce the idempotency constraint to form a stronger consistency regularization in the feature space, to push the features only to maintain the critical information of motion semantics for the recognition task. Our extensive experiments on benchmark datasets, NTU RGB+D and PKUMMD, demonstrate the effectiveness of our proposed method. On the NTU 60 xsub dataset, we observe a performance improvement from 84.6$\\%$ to 86.2$\\%$. Furthermore, in zero-shot adaptation scenarios, our model demonstrates significant efficacy by achieving promising results in cases that were previously unrecognizable. Our project is available at \\url{https://github.com/LanglandsLin/IGM}.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "ECCV 2024"
    },
    {
        "paper id": "2410.20359",
        "abstract url": "https://arxiv.org/abs/2410.20359",
        "title": "Conditional GAN for Enhancing Diffusion Models in Efficient and Authentic Global Gesture Generation from Audios",
        "rating": "0.5",
        "keywords": [
            [
                "Diffusion",
                "GAN"
            ],
            [
                "cs.AI",
                "cs.CV",
                "cs.SD",
                "eess.AS"
            ],
            [
                "WACV"
            ]
        ],
        "abstract": "Audio-driven simultaneous gesture generation is vital for human-computer communication, AI games, and film production. While previous research has shown promise, there are still limitations. Methods based on VAEs are accompanied by issues of local jitter and global instability, whereas methods based on diffusion models are hampered by low generation efficiency. This is because the denoising process of DDPM in the latter relies on the assumption that the noise added at each step is sampled from a unimodal distribution, and the noise values are small. DDIM borrows the idea from the Euler method for solving differential equations, disrupts the Markov chain process, and increases the noise step size to reduce the number of denoising steps, thereby accelerating generation. However, simply increasing the step size during the step-by-step denoising process causes the results to gradually deviate from the original data distribution, leading to a significant drop in the quality of the generated actions and the emergence of unnatural artifacts. In this paper, we break the assumptions of DDPM and achieves breakthrough progress in denoising speed and fidelity. Specifically, we introduce a conditional GAN to capture audio control signals and implicitly match the multimodal denoising distribution between the diffusion and denoising steps within the same sampling step, aiming to sample larger noise values and apply fewer denoising steps for high-speed generation.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "cs.CV",
            "cs.GR",
            "eess.AS"
        ],
        "comment": "Accepted by WACV 2025 (Round 1)"
    },
    {
        "paper id": "2410.20361",
        "abstract url": "https://arxiv.org/abs/2410.20361",
        "title": "Effective Data Stewardship in Higher Education: Skills, Competences, and the Emerging Role of Open Data Stewards",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "The significance of open data in higher education stems from the changing tendencies towards open science, and open research in higher education encourages new ways of making scientific inquiry more transparent, collaborative and accessible. This study focuses on the critical role of open data stewards in this transition, essential for managing and disseminating research data effectively in universities, while it also highlights the increasing demand for structured training and professional policies for data stewards in academic settings. Building upon this context, the paper investigates the essential skills and competences required for effective data stewardship in higher education institutions by elaborating on a critical literature review, coupled with practical engagement in open data stewardship at universities, provided insights into the roles and responsibilities of data stewards. In response to these identified needs, the paper proposes a structured training framework and comprehensive curriculum for data stewardship, a direct response to the gaps identified in the literature. It addresses five key competence categories for open data stewards, aligning them with current trends and essential skills and knowledge in the field. By advocating for a structured approach to data stewardship education, this work sets the foundation for improved data management in universities and serves as a critical step towards professionalizing the role of data stewards in higher education. The emphasis on the role of open data stewards is expected to advance data accessibility and sharing practices, fostering increased transparency, collaboration, and innovation in academic research. This approach contributes to the evolution of universities into open ecosystems, where there is free flow of data for global education and research advancement.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20383",
        "abstract url": "https://arxiv.org/abs/2410.20383",
        "title": "Multiple kernel concept factorization algorithm based on global fusion",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Non-negative Matrix Factorization(NMF) algorithm can only be used to find low rank approximation of original non-negative data while Concept Factorization(CF) algorithm extends matrix factorization to single non-linear kernel space, improving learning ability and adaptability of matrix factorization. In unsupervised environment, to design or select proper kernel function for specific dataset, a new algorithm called Globalized Multiple Kernel CF(GMKCF)was proposed. Multiple candidate kernel functions were input in the same time and learned in the CF framework based on global linear fusion, obtaining a clustering result with high quality and stability and solving the problem of kernel function selection that the CF faced. The convergence of the proposed algorithm was verified by solving the model with alternate iteration. The experimental results on several real databases show that the proposed algorithm outperforms comparison algorithms in data clustering, such as Kernel K-Means(KKM), Spectral Clustering(SC), Kernel CF(KCF), Co-regularized multi-view spectral clustering(Coreg), and Robust Multiple KKM(RMKKM).",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "in Chinese language"
    },
    {
        "paper id": "2410.20388",
        "abstract url": "https://arxiv.org/abs/2410.20388",
        "title": "Unsupervised Feature Selection Algorithm Based on Dual Manifold Re-ranking",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "High-dimensional data is commonly encountered in numerous data analysis tasks. Feature selection techniques aim to identify the most representative features from the original high-dimensional data. Due to the absence of class label information, it is significantly more challenging to select appropriate features in unsupervised learning scenarios compared to supervised ones. Traditional unsupervised feature selection methods typically score the features of samples based on certain criteria, treating samples indiscriminately. However, these approaches fail to fully capture the internal structure of the data. The importance of different samples should vary, and there is a dual relationship between the weight of samples and features that will influence each other. Therefore, an unsupervised feature selection algorithm based on dual manifold re-ranking (DMRR) is proposed in this paper. Different similarity matrices are constructed to depict the manifold structures among samples, between samples and features, and among features themselves. Then, manifold re-ranking is performed by combining the initial scores of samples and features. By comparing DMRR with three original unsupervised feature selection algorithms and two unsupervised feature selection post-processing algorithms, experimental results confirm that the importance information of different samples and the dual relationship between sample and feature are beneficial for achieving better feature selection.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "in Chinese language"
    },
    {
        "paper id": "2410.20399",
        "abstract url": "https://arxiv.org/abs/2410.20399",
        "title": "ThunderKittens: Simple, Fast, and Adorable AI Kernels",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The challenge of mapping AI architectures to GPU hardware is creating a critical bottleneck in AI progress. Despite substantial efforts, hand-written custom kernels fail to meet their theoretical performance thresholds, even on well-established operations like linear attention. The diverse hardware capabilities of GPUs might suggest that we need a wide variety of techniques to achieve high performance. However, our work explores whether a small number of key abstractions can drastically simplify the process. We present ThunderKittens (TK), a framework for writing performant AI kernels while remaining easy to use and maintain. Our abstractions map to the three levels of the GPU hierarchy: (1) at the warp-level, we provide 16x16 matrix tiles as basic data structures and PyTorch-like parallel compute operations over tiles, (2) at the thread-block level, we provide a template for overlapping asynchronous operations across parallel warps, and (3) at the grid-level, we provide support to help hide the block launch and tear-down, and memory costs. We show the value of TK by providing kernels that match or outperform prior kernels for a range of AI operations. We match CuBLAS and FlashAttention-3 on GEMM and attention inference performance and outperform the strongest baselines by $10-40\\%$ on attention backwards, $8\\times$ on state space models, and $14\\times$ on linear attention.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20401",
        "abstract url": "https://arxiv.org/abs/2410.20401",
        "title": "Prototypical Extreme Multi-label Classification with a Dynamic Margin Loss",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Extreme Multi-label Classification (XMC) methods predict relevant labels for a given query in an extremely large label space. Recent works in XMC address this problem using deep encoders that project text descriptions to an embedding space suitable for recovering the closest labels. However, learning deep models can be computationally expensive in large output spaces, resulting in a trade-off between high performing brute-force approaches and efficient solutions. In this paper, we propose PRIME, a XMC method that employs a novel prototypical contrastive learning technique to reconcile efficiency and performance surpassing brute-force approaches. We frame XMC as a data-to-prototype prediction task where label prototypes aggregate information from related queries. More precisely, we use a shallow transformer encoder that we coin as Label Prototype Network, which enriches label representations by aggregating text-based embeddings, label centroids and learnable free vectors. We jointly train a deep encoder and the Label Prototype Network using an adaptive triplet loss objective that better adapts to the high granularity and ambiguity of extreme label spaces. PRIME achieves state-of-the-art results in several public benchmarks of different sizes and domains, while keeping the model efficient.",
        "subjects": [
            "cs.LG",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20406",
        "abstract url": "https://arxiv.org/abs/2410.20406",
        "title": "Point-PRC: A Prompt Learning Based Regulation Framework for Generalizable Point Cloud Analysis",
        "rating": "0.5",
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "3D",
                "Point Cloud"
            ],
            [
                "trajectory"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "This paper investigates the 3D domain generalization (3DDG) ability of large 3D models based on prevalent prompt learning. Recent works demonstrate the performances of 3D point cloud recognition can be boosted remarkably by parameter-efficient prompt tuning. However, we observe that the improvement on downstream tasks comes at the expense of a severe drop in 3D domain generalization. To resolve this challenge, we present a comprehensive regulation framework that allows the learnable prompts to actively interact with the well-learned general knowledge in large 3D models to maintain good generalization. Specifically, the proposed framework imposes multiple explicit constraints on the prompt learning trajectory by maximizing the mutual agreement between task-specific predictions and task-agnostic knowledge. We design the regulation framework as a plug-and-play module to embed into existing representative large 3D models. Surprisingly, our method not only realizes consistently increasing generalization ability but also enhances task-specific 3D recognition performances across various 3DDG benchmarks by a clear margin. Considering the lack of study and evaluation on 3DDG, we also create three new benchmarks, namely base-to-new, cross-dataset and few-shot generalization benchmarks, to enrich the field and inspire future research. Code and benchmarks are available at \\url{https://github.com/auniquesun/Point-PRC}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "accepted by NeurIPS 2024"
    },
    {
        "paper id": "2410.20451",
        "abstract url": "https://arxiv.org/abs/2410.20451",
        "title": "BlinkVision: A Benchmark for Optical Flow, Scene Flow and Point Tracking Estimation using RGB Frames and Events",
        "rating": "0.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Recent advances in event-based vision suggest that these systems complement traditional cameras by providing continuous observation without frame rate limitations and a high dynamic range, making them well-suited for correspondence tasks such as optical flow and point tracking. However, there is still a lack of comprehensive benchmarks for correspondence tasks that include both event data and images. To address this gap, we propose BlinkVision, a large-scale and diverse benchmark with multiple modalities and dense correspondence annotations. BlinkVision offers several valuable features: 1) Rich modalities: It includes both event data and RGB images. 2) Extensive annotations: It provides dense per-pixel annotations covering optical flow, scene flow, and point tracking. 3) Large vocabulary: It contains 410 everyday categories, sharing common classes with popular 2D and 3D datasets like LVIS and ShapeNet. 4) Naturalistic: It delivers photorealistic data and covers various naturalistic factors, such as camera shake and deformation. BlinkVision enables extensive benchmarks on three types of correspondence tasks (optical flow, point tracking, and scene flow estimation) for both image-based and event-based methods, offering new observations, practices, and insights for future research. The benchmark website is https://www.blinkvision.net/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to ECCV 2024. Project Page: https://www.blinkvision.net/"
    },
    {
        "paper id": "2410.20459",
        "abstract url": "https://arxiv.org/abs/2410.20459",
        "title": "Unlocking Comics: The AI4VA Dataset for Visual Understanding",
        "rating": "0.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "In the evolving landscape of deep learning, there is a pressing need for more comprehensive datasets capable of training models across multiple modalities. Concurrently, in digital humanities, there is a growing demand to leverage technology for diverse media adaptation and creation, yet limited by sparse datasets due to copyright and stylistic constraints. Addressing this gap, our paper presents a novel dataset comprising Franco-Belgian comics from the 1950s annotated for tasks including depth estimation, semantic segmentation, saliency detection, and character identification. It consists of two distinct and consistent styles and incorporates object concepts and labels taken from natural images. By including such diverse information across styles, this dataset not only holds promise for computational creativity but also offers avenues for the digitization of art and storytelling innovation. This dataset is a crucial component of the AI4VA Workshop Challenges~\\url{https://sites.google.com/view/ai4vaeccv2024}, where we specifically explore depth and saliency. Dataset details at \\url{https://github.com/IVRL/AI4VA}.",
        "subjects": [
            "cs.CV",
            "cs.ET"
        ],
        "comment": "ECCV 2024 Workshop Proceedings"
    },
    {
        "paper id": "2410.20474",
        "abstract url": "https://arxiv.org/abs/2410.20474",
        "title": "GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation",
        "rating": "0.5",
        "keywords": [
            [
                "Diffusion",
                "text-to-image"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We introduce GrounDiT, a novel training-free spatial grounding technique for text-to-image generation using Diffusion Transformers (DiT). Spatial grounding with bounding boxes has gained attention for its simplicity and versatility, allowing for enhanced user control in image generation. However, prior training-free approaches often rely on updating the noisy image during the reverse diffusion process via backpropagation from custom loss functions, which frequently struggle to provide precise control over individual bounding boxes. In this work, we leverage the flexibility of the Transformer architecture, demonstrating that DiT can generate noisy patches corresponding to each bounding box, fully encoding the target object and allowing for fine-grained control over each region. Our approach builds on an intriguing property of DiT, which we refer to as semantic sharing. Due to semantic sharing, when a smaller patch is jointly denoised alongside a generatable-size image, the two become semantic clones. Each patch is denoised in its own branch of the generation process and then transplanted into the corresponding region of the original noisy image at each timestep, resulting in robust spatial grounding for each bounding box. In our experiments on the HRS and DrawBench benchmarks, we achieve state-of-the-art performance compared to previous training-free approaches.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to NeurIPS 2024. Project Page: https://groundit-diffusion.github.io/"
    },
    {
        "paper id": "2410.20495",
        "abstract url": "https://arxiv.org/abs/2410.20495",
        "title": "When Less is More: Achieving Faster Convergence in Distributed Edge Machine Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Distributed Machine Learning (DML) on resource-constrained edge devices holds immense potential for real-world applications. However, achieving fast convergence in DML in these heterogeneous environments remains a significant challenge. Traditional frameworks like Bulk Synchronous Parallel and Asynchronous Stochastic Parallel rely on frequent, small updates that incur substantial communication overhead and hinder convergence speed. Furthermore, these frameworks often employ static dataset sizes, neglecting the heterogeneity of edge devices and potentially leading to straggler nodes that slow down the entire training process. The straggler nodes, i.e., edge devices that take significantly longer to process their assigned data chunk, hinder the overall training speed. To address these limitations, this paper proposes Hermes, a novel probabilistic framework for efficient DML on edge devices. This framework leverages a dynamic threshold based on recent test loss behavior to identify statistically significant improvements in the model's generalization capability, hence transmitting updates only when major improvements are detected, thereby significantly reducing communication overhead. Additionally, Hermes employs dynamic dataset allocation to optimize resource utilization and prevents performance degradation caused by straggler nodes. Our evaluations on a real-world heterogeneous resource-constrained environment demonstrate that Hermes achieves faster convergence compared to state-of-the-art methods, resulting in a remarkable $13.22$x reduction in training time and a $62.1\\%$ decrease in communication overhead.",
        "subjects": [
            "cs.DC",
            "cs.LG",
            "cs.PF"
        ],
        "comment": "11 pages, 19 figures, 3 tables; code: https://github.com/DaSH-Lab-CSIS/Hermes"
    },
    {
        "paper id": "2410.20508",
        "abstract url": "https://arxiv.org/abs/2410.20508",
        "title": "Referring Human Pose and Mask Estimation in the Wild",
        "rating": "0.5",
        "keywords": [
            [
                "robotics"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We introduce Referring Human Pose and Mask Estimation (R-HPM) in the wild, where either a text or positional prompt specifies the person of interest in an image. This new task holds significant potential for human-centric applications such as assistive robotics and sports analysis. In contrast to previous works, R-HPM (i) ensures high-quality, identity-aware results corresponding to the referred person, and (ii) simultaneously predicts human pose and mask for a comprehensive representation. To achieve this, we introduce a large-scale dataset named RefHuman, which substantially extends the MS COCO dataset with additional text and positional prompt annotations. RefHuman includes over 50,000 annotated instances in the wild, each equipped with keypoint, mask, and prompt annotations. To enable prompt-conditioned estimation, we propose the first end-to-end promptable approach named UniPHD for R-HPM. UniPHD extracts multimodal representations and employs a proposed pose-centric hierarchical decoder to process (text or positional) instance queries and keypoint queries, producing results specific to the referred person. Extensive experiments demonstrate that UniPHD produces quality results based on user-friendly prompts and achieves top-tier performance on RefHuman val and MS COCO val2017. Data and Code: https://github.com/bo-miao/RefHuman",
        "subjects": [
            "cs.CV",
            "cs.HC"
        ],
        "comment": "Accepted by NeurIPS 2024. https://github.com/bo-miao/RefHuman"
    },
    {
        "paper id": "2410.20522",
        "abstract url": "https://arxiv.org/abs/2410.20522",
        "title": "Props for Machine-Learning Security",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "We propose protected pipelines or props for short, a new approach for authenticated, privacy-preserving access to deep-web data for machine learning (ML). By permitting secure use of vast sources of deep-web data, props address the systemic bottleneck of limited high-quality training data in ML development. Props also enable privacy-preserving and trustworthy forms of inference, allowing for safe use of sensitive data in ML applications. Props are practically realizable today by leveraging privacy-preserving oracle systems initially developed for blockchain applications.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20527",
        "abstract url": "https://arxiv.org/abs/2410.20527",
        "title": "CodeRosetta: Pushing the Boundaries of Unsupervised Code Translation for Parallel Programming",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Recent advancements in Large Language Models (LLMs) have renewed interest in automatic programming language translation. Encoder-decoder transformer models, in particular, have shown promise in translating between different programming languages. However, translating between a language and its high-performance computing (HPC) extensions remains underexplored due to challenges such as complex parallel semantics. In this paper, we introduce CodeRosetta, an encoder-decoder transformer model designed specifically for translating between programming languages and their HPC extensions. CodeRosetta is evaluated on C++ to CUDA and Fortran to C++ translation tasks. It uses a customized learning framework with tailored pretraining and training objectives to effectively capture both code semantics and parallel structural nuances, enabling bidirectional translation. Our results show that CodeRosetta outperforms state-of-the-art baselines in C++ to CUDA translation by 2.9 BLEU and 1.72 CodeBLEU points while improving compilation accuracy by 6.05%. Compared to general closed-source LLMs, our method improves C++ to CUDA translation by 22.08 BLEU and 14.39 CodeBLEU, with 2.75% higher compilation accuracy. Finally, CodeRosetta exhibits proficiency in Fortran to parallel C++ translation, marking it, to our knowledge, as the first encoder-decoder model for this complex task, improving CodeBLEU by at least 4.63 points compared to closed-source and open-code LLMs.",
        "subjects": [
            "cs.DC",
            "cs.AI",
            "cs.LG",
            "cs.PF",
            "cs.PL",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20539",
        "abstract url": "https://arxiv.org/abs/2410.20539",
        "title": "Info-CELS: Informative Saliency Map Guided Counterfactual Explanation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "As the demand for interpretable machine learning approaches continues to grow, there is an increasing necessity for human involvement in providing informative explanations for model decisions. This is necessary for building trust and transparency in AI-based systems, leading to the emergence of the Explainable Artificial Intelligence (XAI) field. Recently, a novel counterfactual explanation model, CELS, has been introduced. CELS learns a saliency map for the interest of an instance and generates a counterfactual explanation guided by the learned saliency map. While CELS represents the first attempt to exploit learned saliency maps not only to provide intuitive explanations for the reason behind the decision made by the time series classifier but also to explore post hoc counterfactual explanations, it exhibits limitations in terms of high validity for the sake of ensuring high proximity and sparsity. In this paper, we present an enhanced approach that builds upon CELS. While the original model achieved promising results in terms of sparsity and proximity, it faced limitations in validity. Our proposed method addresses this limitation by removing mask normalization to provide more informative and valid counterfactual explanations. Through extensive experimentation on datasets from various domains, we demonstrate that our approach outperforms the CELS model, achieving higher validity and producing more informative explanations.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20550",
        "abstract url": "https://arxiv.org/abs/2410.20550",
        "title": "Deep Reinforcement Learning Agents for Strategic Production Policies in Microeconomic Market Simulations",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Traditional economic models often rely on fixed assumptions about market dynamics, limiting their ability to capture the complexities and stochastic nature of real-world scenarios. However, reality is more complex and includes noise, making traditional models assumptions not met in the market. In this paper, we explore the application of deep reinforcement learning (DRL) to obtain optimal production strategies in microeconomic market environments to overcome the limitations of traditional models. Concretely, we propose a DRL-based approach to obtain an effective policy in competitive markets with multiple producers, each optimizing their production decisions in response to fluctuating demand, supply, prices, subsidies, fixed costs, total production curve, elasticities and other effects contaminated by noise. Our framework enables agents to learn adaptive production policies to several simulations that consistently outperform static and random strategies. As the deep neural networks used by the agents are universal approximators of functions, DRL algorithms can represent in the network complex patterns of data learnt by trial and error that explain the market. Through extensive simulations, we demonstrate how DRL can capture the intricate interplay between production costs, market prices, and competitor behavior, providing insights into optimal decision-making in dynamic economic settings. The results show that agents trained with DRL can strategically adjust production levels to maximize long-term profitability, even in the face of volatile market conditions. We believe that the study bridges the gap between theoretical economic modeling and practical market simulation, illustrating the potential of DRL to revolutionize decision-making in market strategies.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20553",
        "abstract url": "https://arxiv.org/abs/2410.20553",
        "title": "SPICEPilot: Navigating SPICE Code Generation and Simulation with AI Guidance",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have shown great potential in automating code generation; however, their ability to generate accurate circuit-level SPICE code remains limited due to a lack of hardware-specific knowledge. In this paper, we analyze and identify the typical limitations of existing LLMs in SPICE code generation. To address these limitations, we present SPICEPilot a novel Python-based dataset generated using PySpice, along with its accompanying framework. This marks a significant step forward in automating SPICE code generation across various circuit configurations. Our framework automates the creation of SPICE simulation scripts, introduces standardized benchmarking metrics to evaluate LLM's ability for circuit generation, and outlines a roadmap for integrating LLMs into the hardware design process. SPICEPilot is open-sourced under the permissive MIT license at https://github.com/ACADLab/SPICEPilot.git.",
        "subjects": [
            "cs.AR",
            "cs.AI"
        ],
        "comment": "6 pages, 2 figures, 5 tables"
    },
    {
        "paper id": "2410.20593",
        "abstract url": "https://arxiv.org/abs/2410.20593",
        "title": "Normal-GS: 3D Gaussian Splatting with Normal-Involved Rendering",
        "rating": "0.5",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Rendering and reconstruction are long-standing topics in computer vision and graphics. Achieving both high rendering quality and accurate geometry is a challenge. Recent advancements in 3D Gaussian Splatting (3DGS) have enabled high-fidelity novel view synthesis at real-time speeds. However, the noisy and discrete nature of 3D Gaussian primitives hinders accurate surface estimation. Previous attempts to regularize 3D Gaussian normals often degrade rendering quality due to the fundamental disconnect between normal vectors and the rendering pipeline in 3DGS-based methods. Therefore, we introduce Normal-GS, a novel approach that integrates normal vectors into the 3DGS rendering pipeline. The core idea is to model the interaction between normals and incident lighting using the physically-based rendering equation. Our approach re-parameterizes surface colors as the product of normals and a designed Integrated Directional Illumination Vector (IDIV). To optimize memory usage and simplify optimization, we employ an anchor-based 3DGS to implicitly encode locally-shared IDIVs. Additionally, Normal-GS leverages optimized normals and Integrated Directional Encoding (IDE) to accurately model specular effects, enhancing both rendering quality and surface normal precision. Extensive experiments demonstrate that Normal-GS achieves near state-of-the-art visual quality while obtaining accurate surface normals and preserving real-time rendering performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "9 pages, 5 figures, accepted at NeurIPS 2024"
    },
    {
        "paper id": "2410.20605",
        "abstract url": "https://arxiv.org/abs/2410.20605",
        "title": "Advancing Towards Green Blockchain: A Practical Energy-Efficient Blockchain Based Application for CV Verification",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Blockchain has been widely criticized due to the use of inefficient consensus protocols and energy-intensive mechanisms that derived into a global enormous power consumption. Fortunately, since the first blockchain was conceived in 2008 (the one that supports Bitcoin), hardware and consensus protocols have evolved, decreasing energy consumption significantly. This article describes a green blockchain solution and quantifies energy savings when deploying the system on traditional computers and embedded Single-Board Computers (SBCs). To illustrate such savings, it is proposed a solution for tackling the problem of academic certificate forgery, which has a significant cost to society, since it harms the trustworthiness of certificates and academic institutions. The proposed solution is aimed at recording and verifying academic records (ARs) through a decentralized application (DApp) that is supported by a smart contract deployed in the Ethereum blockchain. The application stores the raw data (i.e., the data that are not managed by the blockchain) on a decentralized storage system based on Inter-Planetary File System (IPFS). To demonstrate the efficiency of the developed solution, it is evaluated in terms of performance (transaction latency and throughput) and efficiency (CPU usage and energy consumption), comparing the results obtained with a traditional Proof-of-Work (PoW) consensus protocol and the new Proof-of-Authority (PoA) protocol. The results shown in this paper indicate that the latter is clearly greener and demands less CPU load. Moreover, this article compares the performance of a traditional computer and two SBCs (a Raspberry Pi 4 and an Orange Pi One), showing that is possible to make use of the latter low-power devices to implement blockchain nodes for proposed DApp, but at the cost of higher response latency that varies greatly depending on the used SBCs [...]",
        "subjects": [
            "cs.DC",
            "cs.CR",
            "cs.CY",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20619",
        "abstract url": "https://arxiv.org/abs/2410.20619",
        "title": "Evolving interdisciplinary contributions to global societal challenges: A 50-year overview",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI",
                "cs.CY"
            ]
        ],
        "abstract": "Addressing global societal challenges necessitates insights and expertise that transcend the boundaries of individual disciplines. In recent decades, interdisciplinary collaboration has been recognised as a vital driver of innovation and effective problem-solving, with the potential to profoundly influence policy and practice worldwide. However, quantitative evidence remains limited regarding how cross-disciplinary efforts contribute to societal challenges, as well as the evolving roles and relevance of specific disciplines in addressing these issues. To fill this gap, this study examines the long-term evolution of interdisciplinary contributions to the United Nations' Sustainable Development Goals (SDGs), drawing on extensive bibliometric data from OpenAlex. By analysing publication and citation trends across 19 research fields from 1970 to 2022, we reveal how the relative presence of different disciplines in addressing particular SDGs has shifted over time. Our results also provide unique evidence of the increasing interconnection between fields since the 2000s, coinciding with the United Nations' initiative to tackle global societal challenges through interdisciplinary efforts. These insights will benefit policymakers and practitioners as they reflect on past progress and plan for future action, particularly with the SDG target deadline approaching in the next five years.",
        "subjects": [
            "cs.DL",
            "cs.CY",
            "cs.SI",
            "physics.data-an",
            "physics.soc-ph"
        ],
        "comment": "Main Text: 16 pages (4 figures); Supplementary Materials: 1 page (2 tables)"
    },
    {
        "paper id": "2410.20622",
        "abstract url": "https://arxiv.org/abs/2410.20622",
        "title": "Kernel Approximation of Fisher-Rao Gradient Flows",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The purpose of this paper is to answer a few open questions in the interface of kernel methods and PDE gradient flows. Motivated by recent advances in machine learning, particularly in generative modeling and sampling, we present a rigorous investigation of Fisher-Rao and Wasserstein type gradient flows concerning their gradient structures, flow equations, and their kernel approximations. Specifically, we focus on the Fisher-Rao (also known as Hellinger) geometry and its various kernel-based approximations, developing a principled theoretical framework using tools from PDE gradient flows and optimal transport theory. We also provide a complete characterization of gradient flows in the maximum-mean discrepancy (MMD) space, with connections to existing learning and inference algorithms. Our analysis reveals precise theoretical insights linking Fisher-Rao flows, Stein flows, kernel discrepancies, and nonparametric regression. We then rigorously prove evolutionary $\u0393$-convergence for kernel-approximated Fisher-Rao flows, providing theoretical guarantees beyond pointwise convergence. Finally, we analyze energy dissipation using the Helmholtz-Rayleigh principle, establishing important connections between classical theory in mechanics and modern machine learning practice. Our results provide a unified theoretical foundation for understanding and analyzing approximations of gradient flows in machine learning applications through a rigorous gradient flow and variational method perspective.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.AP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20634",
        "abstract url": "https://arxiv.org/abs/2410.20634",
        "title": "Plastic Learning with Deep Fourier Features",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep neural networks can struggle to learn continually in the face of non-stationarity. This phenomenon is known as loss of plasticity. In this paper, we identify underlying principles that lead to plastic algorithms. In particular, we provide theoretical results showing that linear function approximation, as well as a special case of deep linear networks, do not suffer from loss of plasticity. We then propose deep Fourier features, which are the concatenation of a sine and cosine in every layer, and we show that this combination provides a dynamic balance between the trainability obtained through linearity and the effectiveness obtained through the nonlinearity of neural networks. Deep networks composed entirely of deep Fourier features are highly trainable and sustain their trainability over the course of learning. Our empirical results show that continual learning performance can be drastically improved by replacing ReLU activations with deep Fourier features. These results hold for different continual learning scenarios (e.g., label noise, class incremental learning, pixel permutations) on all major supervised learning datasets used for continual learning research, such as CIFAR10, CIFAR100, and tiny-ImageNet.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20640",
        "abstract url": "https://arxiv.org/abs/2410.20640",
        "title": "Near Optimal Pure Exploration in Logistic Bandits",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Bandit algorithms have garnered significant attention due to their practical applications in real-world scenarios. However, beyond simple settings such as multi-arm or linear bandits, optimal algorithms remain scarce. Notably, no optimal solution exists for pure exploration problems in the context of generalized linear model (GLM) bandits. In this paper, we narrow this gap and develop the first track-and-stop algorithm for general pure exploration problems under the logistic bandit called logistic track-and-stop (Log-TS). Log-TS is an efficient algorithm that asymptotically matches an approximation for the instance-specific lower bound of the expected sample complexity up to a logarithmic factor.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "25 pages, 2 figures"
    },
    {
        "paper id": "2410.20646",
        "abstract url": "https://arxiv.org/abs/2410.20646",
        "title": "Injectivity capacity of ReLU gates",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider the injectivity property of the ReLU networks layers. Determining the ReLU injectivity capacity (ratio of the number of layer's inputs and outputs) is established as isomorphic to determining the capacity of the so-called $\\ell_0$ spherical perceptron. Employing \\emph{fully lifted random duality theory} (fl RDT) a powerful program is developed and utilized to handle the $\\ell_0$ spherical perceptron and implicitly the ReLU layers injectivity. To put the entire fl RDT machinery in practical use, a sizeable set of numerical evaluations is conducted as well. The lifting mechanism is observed to converge remarkably fast with relative corrections in the estimated quantities not exceeding $\\sim 0.1\\%$ already on the third level of lifting. Closed form explicit analytical relations among key lifting parameters are uncovered as well. In addition to being of incredible importance in handling all the required numerical work, these relations also shed a new light on beautiful parametric interconnections within the lifting structure. Finally, the obtained results are also shown to fairly closely match the replica predictions from [40].",
        "subjects": [
            "stat.ML",
            "cond-mat.dis-nn",
            "cs.IT",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20647",
        "abstract url": "https://arxiv.org/abs/2410.20647",
        "title": "General Causal Imputation via Synthetic Interventions",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Given two sets of elements (such as cell types and drug compounds), researchers typically only have access to a limited subset of their interactions. The task of causal imputation involves using this subset to predict unobserved interactions. Squires et al. (2022) have proposed two estimators for this task based on the synthetic interventions (SI) estimator: SI-A (for actions) and SI-C (for contexts). We extend their work and introduce a novel causal imputation estimator, generalized synthetic interventions (GSI). We prove the identifiability of this estimator for data generated from a more complex latent factor model. On synthetic and real data we show empirically that it recovers or outperforms their estimators.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20649",
        "abstract url": "https://arxiv.org/abs/2410.20649",
        "title": "Learning Variational Inequalities from Data: Fast Generalization Rates under Strong Monotonicity",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Variational inequalities (VIs) are a broad class of optimization problems encompassing machine learning problems ranging from standard convex minimization to more complex scenarios like min-max optimization and computing the equilibria of multi-player games. In convex optimization, strong convexity allows for fast statistical learning rates requiring only $\u0398(1/\u03b5)$ stochastic first-order oracle calls to find an $\u03b5$-optimal solution, rather than the standard $\u0398(1/\u03b5^2)$ calls. In this paper, we explain how one can similarly obtain fast $\u0398(1/\u03b5)$ rates for learning VIs that satisfy strong monotonicity, a generalization of strong convexity. Specifically, we demonstrate that standard stability-based generalization arguments for convex minimization extend directly to VIs when the domain admits a small covering, or when the operator is integrable and suboptimality is measured by potential functions; such as when finding equilibria in multi-player games.",
        "subjects": [
            "cs.LG",
            "math.OC",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20709",
        "abstract url": "https://arxiv.org/abs/2410.20709",
        "title": "Let a million entrepreneurs grow!",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "India produces about nine hundred thousand (900K) engineers annually, and many seek computer science and related technology jobs. Given that the IT workforce in India is still young, new graduates get jobs only when the industry grows. A liberal estimate based on the data from MeitY (Ministry of Electronics and Information Technology) and NASSCOM puts the annual job growth to three hundred thousand (300K), less than one-third of the graduation rate. In other words, about half a million graduates don't get a job every year (even when we consider that some students don't opt for jobs or go for higher studies). This position paper demonstrates that given the current growth rate of the Indian economy, such a significant shortfall will continue to exist. It then proposes a way to address this shortfall. The paper proposes to develop micro-entrepreneurs at scale, enabling many graduates to start micro-enterprises focused on AI, Software, and Technology (MAST). These MAST enterprises offer technology products and services to meet the hyperlocal needs of the businesses and individuals in the local community (a retailer in the neighborhood, a high net-worth person, or a factory). Such an endeavor will require curricular, policy, and societal interventions. The paper presents an approach to enable MAST education across campuses, outlining the key curricular changes required and important policies that must be created and implemented. This supply-demand gap is an existential problem for engineering education in India, and this position paper aims to trigger debates and collaborations to devise solutions that will work at India scale.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "Submitted to COMPUTE 2024 (https://compute2024.netlify.app/). It was not accepted as it was considered out of scope for the conference"
    },
    {
        "paper id": "2410.20727",
        "abstract url": "https://arxiv.org/abs/2410.20727",
        "title": "Faster WIND: Accelerating Iterative Best-of-$N$ Distillation for LLM Alignment",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent advances in aligning large language models with human preferences have corroborated the growing importance of best-of-N distillation (BOND). However, the iterative BOND algorithm is prohibitively expensive in practice due to the sample and computation inefficiency. This paper addresses the problem by revealing a unified game-theoretic connection between iterative BOND and self-play alignment, which unifies seemingly disparate algorithmic paradigms. Based on the connection, we establish a novel framework, WIN rate Dominance (WIND), with a series of efficient algorithms for regularized win rate dominance optimization that approximates iterative BOND in the parameter space. We provides provable sample efficiency guarantee for one of the WIND variant with the square loss objective. The experimental results confirm that our algorithm not only accelerates the computation, but also achieves superior sample efficiency compared to existing methods.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.21331",
        "abstract url": "https://arxiv.org/abs/2410.21331",
        "title": "Beyond Interpretability: The Gains of Feature Monosemanticity on Model Robustness",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Deep learning models often suffer from a lack of interpretability due to polysemanticity, where individual neurons are activated by multiple unrelated semantics, resulting in unclear attributions of model behavior. Recent advances in monosemanticity, where neurons correspond to consistent and distinct semantics, have significantly improved interpretability but are commonly believed to compromise accuracy. In this work, we challenge the prevailing belief of the accuracy-interpretability tradeoff, showing that monosemantic features not only enhance interpretability but also bring concrete gains in model performance. Across multiple robust learning scenarios-including input and label noise, few-shot learning, and out-of-domain generalization-our results show that models leveraging monosemantic features significantly outperform those relying on polysemantic features. Furthermore, we provide empirical and theoretical understandings on the robustness gains of feature monosemanticity. Our preliminary analysis suggests that monosemanticity, by promoting better separation of feature representations, leads to more robust decision boundaries. This diverse evidence highlights the generality of monosemanticity in improving model robustness. As a first step in this new direction, we embark on exploring the learning benefits of monosemanticity beyond interpretability, supporting the long-standing hypothesis of linking interpretability and robustness. Code is available at \\url{https://github.com/PKU-ML/Beyond_Interpretability}.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.21340",
        "abstract url": "https://arxiv.org/abs/2410.21340",
        "title": "Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The deployment of large-scale models, such as large language models (LLMs) and sophisticated image generation systems, incurs substantial costs due to their computational demands. To mitigate these costs and address challenges related to scalability and data security, there is a growing shift towards decentralized systems for deploying such models. In these decentralized environments, efficient inference acceleration becomes crucial to manage computational resources effectively and enhance system responsiveness. In this work, we address the challenge of selecting optimal acceleration methods in decentralized systems by introducing a meta-learning-based framework. This framework automates the selection process by learning from historical performance data of various acceleration techniques across different tasks. Unlike traditional methods that rely on random selection or expert intuition, our approach systematically identifies the best acceleration strategies based on the specific characteristics of each task. We demonstrate that our meta-learning framework not only streamlines the decision-making process but also consistently outperforms conventional methods in terms of efficiency and performance. Our results highlight the potential of meta-learning to revolutionize inference acceleration in decentralized AI systems, offering a path towards more democratic and economically feasible artificial intelligence solutions.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20395",
        "abstract url": "https://arxiv.org/abs/2410.20395",
        "title": "Depth Attention for Robust RGB Tracking",
        "rating": "0",
        "keywords": [
            [
                "RGB-D",
                "Depth"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "RGB video object tracking is a fundamental task in computer vision. Its effectiveness can be improved using depth information, particularly for handling motion-blurred target. However, depth information is often missing in commonly used tracking benchmarks. In this work, we propose a new framework that leverages monocular depth estimation to counter the challenges of tracking targets that are out of view or affected by motion blur in RGB video sequences. Specifically, our work introduces following contributions. To the best of our knowledge, we are the first to propose a depth attention mechanism and to formulate a simple framework that allows seamlessly integration of depth information with state of the art tracking algorithms, without RGB-D cameras, elevating accuracy and robustness. We provide extensive experiments on six challenging tracking benchmarks. Our results demonstrate that our approach provides consistent gains over several strong baselines and achieves new SOTA performance. We believe that our method will open up new possibilities for more sophisticated VOT solutions in real-world scenarios. Our code and models are publicly released: https://github.com/LiuYuML/Depth-Attention.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": "Oral Acceptance at the Asian Conference on Computer Vision (ACCV) 2024, Hanoi, Vietnam"
    },
    {
        "paper id": "2410.20427",
        "abstract url": "https://arxiv.org/abs/2410.20427",
        "title": "YourSkatingCoach: A Figure Skating Video Benchmark for Fine-Grained Element Analysis",
        "rating": "0",
        "keywords": [
            [
                "skeletons"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Combining sports and machine learning involves leveraging ML algorithms and techniques to extract insight from sports-related data such as player statistics, game footage, and other relevant information. However, datasets related to figure skating in the literature focus primarily on element classification and are currently unavailable or exhibit only limited access, which greatly raise the entry barrier to developing visual sports technology for it. Moreover, when using such data to help athletes improve their skills, we find they are very coarse-grained: they work for learning what an element is, but they are poorly suited to learning whether the element is good or bad. Here we propose air time detection, a novel motion analysis task, the goal of which is to accurately detect the duration of the air time of a jump. We present YourSkatingCoach, a large, novel figure skating dataset which contains 454 videos of jump elements, the detected skater skeletons in each video, along with the gold labels of the start and ending frames of each jump, together as a video benchmark for figure skating. In addition, although this type of task is often viewed as classification, we cast it as a sequential labeling problem and propose a Transformer-based model to calculate the duration. Experimental results show that the proposed model yields a favorable results for a strong baseline. To further verify the generalizability of the fine-grained labels, we apply the same process to other sports as cross-sports tasks but for coarse-grained task action classification. Here we fine-tune the classification to demonstrate that figure skating, as it contains the essential body movements, constitutes a strong foundation for adaptation to other sports.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20445",
        "abstract url": "https://arxiv.org/abs/2410.20445",
        "title": "TrajAgent: An Agent Framework for Unified Trajectory Modelling",
        "rating": "0",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Trajectory modeling, which includes research on trajectory data pattern mining and future prediction, has widespread applications in areas such as life services, urban transportation, and public administration. Numerous methods have been proposed to address specific problems within trajectory modelling. However, due to the heterogeneity of data and the diversity of trajectory tasks, achieving unified trajectory modelling remains an important yet challenging task. In this paper, we propose TrajAgent, a large language model-based agentic framework, to unify various trajectory modelling tasks. In TrajAgent, we first develop UniEnv, an execution environment with a unified data and model interface, to support the execution and training of various models. Building on UniEnv, we introduce TAgent, an agentic workflow designed for automatic trajectory modelling across various trajectory tasks. Specifically, we design AutOpt, a systematic optimization module within TAgent, to further improve the performance of the integrated model. With diverse trajectory tasks input in natural language, TrajAgent automatically generates competitive results via training and executing appropriate models. Extensive experiments on four tasks using four real-world datasets demonstrate the effectiveness of TrajAgent in unified trajectory modelling, achieving an average performance improvement of 15.43% over baseline methods.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "12 pages; the code will be openly accessible at: https://github.com/tsinghua-fib-lab/TrajAgent"
    },
    {
        "paper id": "2410.20469",
        "abstract url": "https://arxiv.org/abs/2410.20469",
        "title": "Graph Neural Networks on Discriminative Graphs of Words",
        "rating": "0",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "In light of the recent success of Graph Neural Networks (GNNs) and their ability to perform inference on complex data structures, many studies apply GNNs to the task of text classification. In most previous methods, a heterogeneous graph, containing both word and document nodes, is constructed using the entire corpus and a GNN is used to classify document nodes. In this work, we explore a new Discriminative Graph of Words Graph Neural Network (DGoW-GNN) approach encapsulating both a novel discriminative graph construction and model to classify text. In our graph construction, containing only word nodes and no document nodes, we split the training corpus into disconnected subgraphs according to their labels and weight edges by the pointwise mutual information of the represented words. Our graph construction, for which we provide theoretical motivation, allows us to reformulate the task of text classification as the task of walk classification. We also propose a new model for the graph-based classification of text, which combines a GNN and a sequence model. We evaluate our approach on seven benchmark datasets and find that it is outperformed by several state-of-the-art baseline models. We analyse reasons for this performance difference and hypothesise under which conditions it is likely to change.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20502",
        "abstract url": "https://arxiv.org/abs/2410.20502",
        "title": "ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "Text-to-video"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Text-to-video models have recently undergone rapid and substantial advancements. Nevertheless, due to limitations in data and computational resources, achieving efficient generation of long videos with rich motion dynamics remains a significant challenge. To generate high-quality, dynamic, and temporally consistent long videos, this paper presents ARLON, a novel framework that boosts diffusion Transformers with autoregressive models for long video generation, by integrating the coarse spatial and long-range temporal information provided by the AR model to guide the DiT model. Specifically, ARLON incorporates several key innovations: 1) A latent Vector Quantized Variational Autoencoder (VQ-VAE) compresses the input latent space of the DiT model into compact visual tokens, bridging the AR and DiT models and balancing the learning complexity and information density; 2) An adaptive norm-based semantic injection module integrates the coarse discrete visual units from the AR model into the DiT model, ensuring effective guidance during video generation; 3) To enhance the tolerance capability of noise introduced from the AR inference, the DiT model is trained with coarser visual latent tokens incorporated with an uncertainty sampling module. Experimental results demonstrate that ARLON significantly outperforms the baseline OpenSora-V1.2 on eight out of eleven metrics selected from VBench, with notable improvements in dynamic degree and aesthetic quality, while delivering competitive results on the remaining three and simultaneously accelerating the generation process. In addition, ARLON achieves state-of-the-art performance in long video generation. Detailed analyses of the improvements in inference efficiency are presented, alongside a practical application that demonstrates the generation of long videos using progressive text prompts. See demos of ARLON at \\url{http://aka.ms/arlon}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20546",
        "abstract url": "https://arxiv.org/abs/2410.20546",
        "title": "Sebica: Lightweight Spatial and Efficient Bidirectional Channel Attention Super Resolution Network",
        "rating": "0",
        "keywords": [
            [
                "Super Resolution"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Single Image Super-Resolution (SISR) is a vital technique for improving the visual quality of low-resolution images. While recent deep learning models have made significant advancements in SISR, they often encounter computational challenges that hinder their deployment in resource-limited or time-sensitive environments. To overcome these issues, we present Sebica, a lightweight network that incorporates spatial and efficient bidirectional channel attention mechanisms. Sebica significantly reduces computational costs while maintaining high reconstruction quality, achieving PSNR/SSIM scores of 28.29/0.7976 and 30.18/0.8330 on the Div2K and Flickr2K datasets, respectively. These results surpass most baseline lightweight models and are comparable to the highest-performing model, but with only 17% and 15% of the parameters and GFLOPs. Additionally, our small version of Sebica has only 7.9K parameters and 0.41 GFLOPS, representing just 3% of the parameters and GFLOPs of the highest-performing model, while still achieving PSNR and SSIM metrics of 28.12/0.7931 and 0.3009/0.8317, on the Flickr2K dataset respectively. In addition, Sebica demonstrates significant improvements in real-world applications, specifically in object detection tasks, where it enhances detection accuracy in traffic video scenarios.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "7 pages, 5 figures, 26 conferences"
    },
    {
        "paper id": "2410.20624",
        "abstract url": "https://arxiv.org/abs/2410.20624",
        "title": "Towards an LLM-Based Speech Interface for Robot-Assisted Feeding",
        "rating": "0",
        "keywords": [
            [
                "Robot"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Physically assistive robots present an opportunity to significantly increase the well-being and independence of individuals with motor impairments or other forms of disability who are unable to complete activities of daily living (ADLs). Speech interfaces, especially ones that utilize Large Language Models (LLMs), can enable individuals to effectively and naturally communicate high-level commands and nuanced preferences to robots. In this work, we demonstrate an LLM-based speech interface for a commercially available assistive feeding robot. Our system is based on an iteratively designed framework, from the paper \"VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots,\" that incorporates human-centric elements for integrating LLMs as interfaces for robots. It has been evaluated through a user study with 11 older adults at an independent living facility. Videos are located on our project website: https://sites.google.com/andrew.cmu.edu/voicepilot/.",
        "subjects": [
            "cs.RO",
            "cs.CL",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20657",
        "abstract url": "https://arxiv.org/abs/2410.20657",
        "title": "Video to Video Generative Adversarial Network for Few-shot Learning Based on Policy Gradient",
        "rating": "0",
        "keywords": [
            [
                "GAN"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "The development of sophisticated models for video-to-video synthesis has been facilitated by recent advances in deep reinforcement learning and generative adversarial networks (GANs). In this paper, we propose RL-V2V-GAN, a new deep neural network approach based on reinforcement learning for unsupervised conditional video-to-video synthesis. While preserving the unique style of the source video domain, our approach aims to learn a mapping from a source video domain to a target video domain. We train the model using policy gradient and employ ConvLSTM layers to capture the spatial and temporal information by designing a fine-grained GAN architecture and incorporating spatio-temporal adversarial goals. The adversarial losses aid in content translation while preserving style. Unlike traditional video-to-video synthesis methods requiring paired inputs, our proposed approach is more general because it does not require paired inputs. Thus, when dealing with limited videos in the target domain, i.e., few-shot learning, it is particularly effective. Our experiments show that RL-V2V-GAN can produce temporally coherent video results. These results highlight the potential of our approach for further advances in video-to-video synthesis.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": "18 pages, 11 figures, submitting to IEEE TNNLS"
    },
    {
        "paper id": "2410.20672",
        "abstract url": "https://arxiv.org/abs/2410.20672",
        "title": "Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA",
        "rating": "0",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit \"layer tying\" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller \"Recursive Transformers\" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "48 pages, 17 figures, 17 tables"
    },
    {
        "paper id": "2410.20686",
        "abstract url": "https://arxiv.org/abs/2410.20686",
        "title": "ODGS: 3D Scene Reconstruction from Omnidirectional Images with 3D Gaussian Splattings",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian splatting",
                "NeRF",
                "radiance fields"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Omnidirectional (or 360-degree) images are increasingly being used for 3D applications since they allow the rendering of an entire scene with a single image. Existing works based on neural radiance fields demonstrate successful 3D reconstruction quality on egocentric videos, yet they suffer from long training and rendering times. Recently, 3D Gaussian splatting has gained attention for its fast optimization and real-time rendering. However, directly using a perspective rasterizer to omnidirectional images results in severe distortion due to the different optical properties between two image domains. In this work, we present ODGS, a novel rasterization pipeline for omnidirectional images, with geometric interpretation. For each Gaussian, we define a tangent plane that touches the unit sphere and is perpendicular to the ray headed toward the Gaussian center. We then leverage a perspective camera rasterizer to project the Gaussian onto the corresponding tangent plane. The projected Gaussians are transformed and combined into the omnidirectional image, finalizing the omnidirectional rasterization process. This interpretation reveals the implicit assumptions within the proposed pipeline, which we verify through mathematical proofs. The entire rasterization process is parallelized using CUDA, achieving optimization and rendering speeds 100 times faster than NeRF-based methods. Our comprehensive experiments highlight the superiority of ODGS by delivering the best reconstruction and perceptual quality across various datasets. Additionally, results on roaming datasets demonstrate that ODGS restores fine details effectively, even when reconstructing large 3D scenes. The source code is available on our project page (https://github.com/esw0116/ODGS).",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20717",
        "abstract url": "https://arxiv.org/abs/2410.20717",
        "title": "Face-MLLM: A Large Face Perception Model",
        "rating": "0",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Although multimodal large language models (MLLMs) have achieved promising results on a wide range of vision-language tasks, their ability to perceive and understand human faces is rarely explored. In this work, we comprehensively evaluate existing MLLMs on face perception tasks. The quantitative results reveal that existing MLLMs struggle to handle these tasks. The primary reason is the lack of image-text datasets that contain fine-grained descriptions of human faces. To tackle this problem, we design a practical pipeline for constructing datasets, upon which we further build a novel multimodal large face perception model, namely Face-MLLM. Specifically, we re-annotate LAION-Face dataset with more detailed face captions and facial attribute labels. Besides, we re-formulate traditional face datasets using the question-answer style, which is fit for MLLMs. Together with these enriched datasets, we develop a novel three-stage MLLM training method. In the first two stages, our model learns visual-text alignment and basic visual question answering capability, respectively. In the third stage, our model learns to handle multiple specialized face perception tasks. Experimental results show that our model surpasses previous MLLMs on five famous face perception tasks. Besides, on our newly introduced zero-shot facial attribute analysis task, our Face-MLLM also presents superior performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20723",
        "abstract url": "https://arxiv.org/abs/2410.20723",
        "title": "CompGS: Unleashing 2D Compositionality for Compositional Text-to-3D via Dynamically Optimizing 3D Gaussians",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent breakthroughs in text-guided image generation have significantly advanced the field of 3D generation. While generating a single high-quality 3D object is now feasible, generating multiple objects with reasonable interactions within a 3D space, a.k.a. compositional 3D generation, presents substantial challenges. This paper introduces CompGS, a novel generative framework that employs 3D Gaussian Splatting (GS) for efficient, compositional text-to-3D content generation. To achieve this goal, two core designs are proposed: (1) 3D Gaussians Initialization with 2D compositionality: We transfer the well-established 2D compositionality to initialize the Gaussian parameters on an entity-by-entity basis, ensuring both consistent 3D priors for each entity and reasonable interactions among multiple entities; (2) Dynamic Optimization: We propose a dynamic strategy to optimize 3D Gaussians using Score Distillation Sampling (SDS) loss. CompGS first automatically decomposes 3D Gaussians into distinct entity parts, enabling optimization at both the entity and composition levels. Additionally, CompGS optimizes across objects of varying scales by dynamically adjusting the spatial parameters of each entity, enhancing the generation of fine-grained details, particularly in smaller entities. Qualitative comparisons and quantitative evaluations on T3Bench demonstrate the effectiveness of CompGS in generating compositional 3D objects with superior image quality and semantic alignment over existing methods. CompGS can also be easily extended to controllable 3D editing, facilitating scene generation. We hope CompGS will provide new insights to the compositional 3D generation. Project page: https://chongjiange.github.io/compgs.html.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20724",
        "abstract url": "https://arxiv.org/abs/2410.20724",
        "title": "Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation",
        "rating": "0",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) demonstrate strong reasoning abilities but face limitations such as hallucinations and outdated knowledge. Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by grounding LLM outputs in structured external knowledge from KGs. However, current KG-based RAG frameworks still struggle to optimize the trade-off between retrieval effectiveness and efficiency in identifying a suitable amount of relevant graph information for the LLM to digest. We introduce SubgraphRAG, extending the KG-based RAG framework that retrieves subgraphs and leverages LLMs for reasoning and answer prediction. Our approach innovatively integrates a lightweight multilayer perceptron with a parallel triple-scoring mechanism for efficient and flexible subgraph retrieval while encoding directional structural distances to enhance retrieval effectiveness. The size of retrieved subgraphs can be flexibly adjusted to match the query's need and the downstream LLM's capabilities. This design strikes a balance between model complexity and reasoning power, enabling scalable and generalizable retrieval processes. Notably, based on our retrieved subgraphs, smaller LLMs like Llama3.1-8B-Instruct deliver competitive results with explainable reasoning, while larger models like GPT-4o achieve state-of-the-art accuracy compared with previous baselines -- all without fine-tuning. Extensive evaluations on the WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency, accuracy, and reliability by reducing hallucinations and improving response grounding.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20731",
        "abstract url": "https://arxiv.org/abs/2410.20731",
        "title": "BLAPose: Enhancing 3D Human Pose Estimation with Bone Length Adjustment",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Current approaches in 3D human pose estimation primarily focus on regressing 3D joint locations, often neglecting critical physical constraints such as bone length consistency and body symmetry. This work introduces a recurrent neural network architecture designed to capture holistic information across entire video sequences, enabling accurate prediction of bone lengths. To enhance training effectiveness, we propose a novel augmentation strategy using synthetic bone lengths that adhere to physical constraints. Moreover, we present a bone length adjustment method that preserves bone orientations while substituting bone lengths with predicted values. Our results demonstrate that existing 3D human pose estimation models can be significantly enhanced through this adjustment process. Furthermore, we fine-tune human pose estimation models using inferred bone lengths, observing notable improvements. Our bone length prediction model surpasses the previous best results, and our adjustment and fine-tuning method enhance performance across several metrics on the Human3.6M dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "16 pages, 8 Postscript figures, uses accv.sty and accvabbrv.sty"
    },
    {
        "paper id": "2410.20733",
        "abstract url": "https://arxiv.org/abs/2410.20733",
        "title": "SEG:Seeds-Enhanced Iterative Refinement Graph Neural Network for Entity Alignment",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Entity alignment is crucial for merging knowledge across knowledge graphs, as it matches entities with identical semantics. The standard method matches these entities based on their embedding similarities using semi-supervised learning. However, diverse data sources lead to non-isomorphic neighborhood structures for aligned entities, complicating alignment, especially for less common and sparsely connected entities. This paper presents a soft label propagation framework that integrates multi-source data and iterative seed enhancement, addressing scalability challenges in handling extensive datasets where scale computing excels. The framework uses seeds for anchoring and selects optimal relationship pairs to create soft labels rich in neighborhood features and semantic relationship data. A bidirectional weighted joint loss function is implemented, which reduces the distance between positive samples and differentially processes negative samples, taking into account the non-isomorphic neighborhood structures. Our method outperforms existing semi-supervised approaches, as evidenced by superior results on multiple datasets, significantly improving the quality of entity alignment.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "7, 2 figures"
    },
    {
        "paper id": "2410.21337",
        "abstract url": "https://arxiv.org/abs/2410.21337",
        "title": "Fine-tuned Large Language Models (LLMs): Improved Prompt Injection Attacks Detection",
        "rating": "0",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are becoming a popular tool as they have significantly advanced in their capability to tackle a wide range of language-based tasks. However, LLMs applications are highly vulnerable to prompt injection attacks, which poses a critical problem. These attacks target LLMs applications through using carefully designed input prompts to divert the model from adhering to original instruction, thereby it could execute unintended actions. These manipulations pose serious security threats which potentially results in data leaks, biased outputs, or harmful responses. This project explores the security vulnerabilities in relation to prompt injection attacks. To detect whether a prompt is vulnerable or not, we follows two approaches: 1) a pre-trained LLM, and 2) a fine-tuned LLM. Then, we conduct a thorough analysis and comparison of the classification performance. Firstly, we use pre-trained XLM-RoBERTa model to detect prompt injections using test dataset without any fine-tuning and evaluate it by zero-shot classification. Then, this proposed work will apply supervised fine-tuning to this pre-trained LLM using a task-specific labeled dataset from deepset in huggingface, and this fine-tuned model achieves impressive results with 99.13\\% accuracy, 100\\% precision, 98.33\\% recall and 99.15\\% F1-score thorough rigorous experimentation and evaluation. We observe that our approach is highly efficient in detecting prompt injection attacks.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20356",
        "abstract url": "https://arxiv.org/abs/2410.20356",
        "title": "Uncovering Capabilities of Model Pruning in Graph Contrastive Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Graph contrastive learning has achieved great success in pre-training graph neural networks without ground-truth labels. Leading graph contrastive learning follows the classical scheme of contrastive learning, forcing model to identify the essential information from augmented views. However, general augmented views are produced via random corruption or learning, which inevitably leads to semantics alteration. Although domain knowledge guided augmentations alleviate this issue, the generated views are domain specific and undermine the generalization. In this work, motivated by the firm representation ability of sparse model from pruning, we reformulate the problem of graph contrastive learning via contrasting different model versions rather than augmented views. We first theoretically reveal the superiority of model pruning in contrast to data augmentations. In practice, we take original graph as input and dynamically generate a perturbed graph encoder to contrast with the original encoder by pruning its transformation weights. Furthermore, considering the integrity of node embedding in our method, we are capable of developing a local contrastive loss to tackle the hard negative samples that disturb the model training. We extensively validate our method on various benchmarks regarding graph classification via unsupervised and transfer learning. Compared to the state-of-the-art (SOTA) works, better performance can always be obtained by the proposed method.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "MM' 24"
    },
    {
        "paper id": "2410.20357",
        "abstract url": "https://arxiv.org/abs/2410.20357",
        "title": "Dynamics as Prompts: In-Context Learning for Sim-to-Real System Identifications",
        "rating": "-0.5",
        "keywords": [
            [
                "robotics"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Sim-to-real transfer remains a significant challenge in robotics due to the discrepancies between simulated and real-world dynamics. Traditional methods like Domain Randomization often fail to capture fine-grained dynamics, limiting their effectiveness for precise control tasks. In this work, we propose a novel approach that dynamically adjusts simulation environment parameters online using in-context learning. By leveraging past interaction histories as context, our method adapts the simulation environment dynamics to real-world dynamics without requiring gradient updates, resulting in faster and more accurate alignment between simulated and real-world performance. We validate our approach across two tasks: object scooping and table air hockey. In the sim-to-sim evaluations, our method significantly outperforms the baselines on environment parameter estimation by 80% and 42% in the object scooping and table air hockey setups, respectively. Furthermore, our method achieves at least 70% success rate in sim-to-real transfer on object scooping across three different objects. By incorporating historical interaction data, our approach delivers efficient and smooth system identification, advancing the deployment of robots in dynamic real-world scenarios. Demos are available on our project page: https://sim2real-capture.github.io/",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "website: https://sim2real-capture.github.io/"
    },
    {
        "paper id": "2410.20380",
        "abstract url": "https://arxiv.org/abs/2410.20380",
        "title": "FuseFL: One-Shot Federated Learning through the Lens of Causality with Progressive Model Fusion",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "One-shot Federated Learning (OFL) significantly reduces communication costs in FL by aggregating trained models only once. However, the performance of advanced OFL methods is far behind the normal FL. In this work, we provide a causal view to find that this performance drop of OFL methods comes from the isolation problem, which means that local isolatedly trained models in OFL may easily fit to spurious correlations due to the data heterogeneity. From the causal perspective, we observe that the spurious fitting can be alleviated by augmenting intermediate features from other clients. Built upon our observation, we propose a novel learning approach to endow OFL with superb performance and low communication and storage costs, termed as FuseFL. Specifically, FuseFL decomposes neural networks into several blocks, and progressively trains and fuses each block following a bottom-up manner for feature augmentation, introducing no additional communication costs. Comprehensive experiments demonstrate that FuseFL outperforms existing OFL and ensemble FL by a significant margin. We conduct comprehensive experiments to show that FuseFL supports high scalability of clients, heterogeneous model training, and low memory costs. Our work is the first attempt using causality to analyze and alleviate data heterogeneity of OFL.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20391",
        "abstract url": "https://arxiv.org/abs/2410.20391",
        "title": "Hierarchical Multiple Kernel K-Means Algorithm Based on Sparse Connectivity",
        "rating": "-0.5",
        "keywords": [
            [
                "kernel learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Multiple kernel learning (MKL) aims to find an optimal, consistent kernel function. In the hierarchical multiple kernel clustering (HMKC) algorithm, sample features are extracted layer by layer from a high-dimensional space to maximize the retention of effective information. However, information interaction between layers is often ignored. In this model, only corresponding nodes in adjacent layers exchange information; other nodes remain isolated, and if full connectivity is adopted, the diversity of the final consistency matrix is reduced. Therefore, this paper proposes a hierarchical multiple kernel K-Means (SCHMKKM) algorithm based on sparse connectivity, which controls the assignment matrix to achieve sparse connections through a sparsity rate, thereby locally fusing the features obtained by distilling information between layers. Finally, we conduct cluster analysis on multiple datasets and compare it with the fully connected hierarchical multiple kernel K-Means (FCHMKKM) algorithm in experiments. It is shown that more discriminative information fusion is beneficial for learning a better consistent partition matrix, and the fusion strategy based on sparse connection outperforms the full connection strategy.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "in Chinese language"
    },
    {
        "paper id": "2410.20432",
        "abstract url": "https://arxiv.org/abs/2410.20432",
        "title": "Integrating uncertainty quantification into randomized smoothing based robustness guarantees",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Deep neural networks have proven to be extremely powerful, however, they are also vulnerable to adversarial attacks which can cause hazardous incorrect predictions in safety-critical applications. Certified robustness via randomized smoothing gives a probabilistic guarantee that the smoothed classifier's predictions will not change within an $\\ell_2$-ball around a given input. On the other hand (uncertainty) score-based rejection is a technique often applied in practice to defend models against adversarial attacks. In this work, we fuse these two approaches by integrating a classifier that abstains from predicting when uncertainty is high into the certified robustness framework. This allows us to derive two novel robustness guarantees for uncertainty aware classifiers, namely (i) the radius of an $\\ell_2$-ball around the input in which the same label is predicted and uncertainty remains low and (ii) the $\\ell_2$-radius of a ball in which the predictions will either not change or be uncertain. While the former provides robustness guarantees with respect to attacks aiming at increased uncertainty, the latter informs about the amount of input perturbation necessary to lead the uncertainty aware model into a wrong prediction. Notably, this is on CIFAR10 up to 20.93% larger than for models not allowing for uncertainty based rejection. We demonstrate, that the novel framework allows for a systematic robustness evaluation of different network architectures and uncertainty measures and to identify desired properties of uncertainty quantification techniques. Moreover, we show that leveraging uncertainty in a smoothed classifier helps out-of-distribution detection.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20470",
        "abstract url": "https://arxiv.org/abs/2410.20470",
        "title": "Hamiltonian Score Matching and Generative Flows",
        "rating": "-0.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Classical Hamiltonian mechanics has been widely used in machine learning in the form of Hamiltonian Monte Carlo for applications with predetermined force fields. In this work, we explore the potential of deliberately designing force fields for Hamiltonian ODEs, introducing Hamiltonian velocity predictors (HVPs) as a tool for score matching and generative models. We present two innovations constructed with HVPs: Hamiltonian Score Matching (HSM), which estimates score functions by augmenting data via Hamiltonian trajectories, and Hamiltonian Generative Flows (HGFs), a novel generative model that encompasses diffusion models and flow matching as HGFs with zero force fields. We showcase the extended design space of force fields by introducing Oscillation HGFs, a generative model inspired by harmonic oscillators. Our experiments validate our theoretical insights about HSM as a novel score matching metric and demonstrate that HGFs rival leading generative modeling techniques.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20478",
        "abstract url": "https://arxiv.org/abs/2410.20478",
        "title": "MusicFlow: Cascaded Flow Matching for Text Guided Music Generation",
        "rating": "-0.5",
        "keywords": [
            [
                "Music",
                "text-to-music"
            ],
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "We introduce MusicFlow, a cascaded text-to-music generation model based on flow matching. Based on self-supervised representations to bridge between text descriptions and music audios, we construct two flow matching networks to model the conditional distribution of semantic and acoustic features. Additionally, we leverage masked prediction as the training objective, enabling the model to generalize to other tasks such as music infilling and continuation in a zero-shot manner. Experiments on MusicCaps reveal that the music generated by MusicFlow exhibits superior quality and text coherence despite being over $2\\sim5$ times smaller and requiring $5$ times fewer iterative steps. Simultaneously, the model can perform other music generation tasks and achieves competitive performance in music infilling and continuation. Our code and model will be publicly available.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "eess.AS"
        ],
        "comment": "ICML 2024"
    },
    {
        "paper id": "2410.20487",
        "abstract url": "https://arxiv.org/abs/2410.20487",
        "title": "Efficient Diversity-based Experience Replay for Deep Reinforcement Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Robotic Manipulation"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Deep Reinforcement Learning (DRL) has achieved remarkable success in solving complex decision-making problems by combining the representation capabilities of deep learning with the decision-making power of reinforcement learning. However, learning in sparse reward environments remains challenging due to insufficient feedback to guide the optimization of agents, especially in real-life environments with high-dimensional states. To tackle this issue, experience replay is commonly introduced to enhance learning efficiency through past experiences. Nonetheless, current methods of experience replay, whether based on uniform or prioritized sampling, frequently struggle with suboptimal learning efficiency and insufficient utilization of samples. This paper proposes a novel approach, diversity-based experience replay (DBER), which leverages the deterministic point process to prioritize diverse samples in state realizations. We conducted extensive experiments on Robotic Manipulation tasks in MuJoCo, Atari games, and realistic in-door environments in Habitat. The results show that our method not only significantly improves learning efficiency but also demonstrates superior performance in sparse reward environments with high-dimensional states, providing a simple yet effective solution for this field.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20537",
        "abstract url": "https://arxiv.org/abs/2410.20537",
        "title": "SIGMA: Single Interpolated Generative Model for Anomalies",
        "rating": "-0.5",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "A key step in any resonant anomaly detection search is accurate modeling of the background distribution in each signal region. Data-driven methods like CATHODE accomplish this by training separate generative models on the complement of each signal region, and interpolating them into their corresponding signal regions. Having to re-train the generative model on essentially the entire dataset for each signal region is a major computational cost in a typical sliding window search with many signal regions. Here, we present SIGMA, a new, fully data-driven, computationally-efficient method for estimating background distributions. The idea is to train a single generative model on all of the data and interpolate its parameters in sideband regions in order to obtain a model for the background in the signal region. The SIGMA method significantly reduces the computational cost compared to previous approaches, while retaining a similar high quality of background modeling and sensitivity to anomalous signals.",
        "subjects": [
            "hep-ph",
            "cs.LG",
            "hep-ex",
            "physics.data-an"
        ],
        "comment": "12 pages, 6 figures"
    },
    {
        "paper id": "2410.20542",
        "abstract url": "https://arxiv.org/abs/2410.20542",
        "title": "PaPaGei: Open Foundation Models for Optical Physiological Signals",
        "rating": "-0.5",
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "biosignals",
                "health",
                "clinical",
                "Physiological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Photoplethysmography (PPG) is the most widely used non-invasive technique for monitoring biosignals and cardiovascular health, with applications in both clinical settings and consumer health through wearable devices. Current machine learning models trained on PPG signals are mostly task-specific and lack generalizability. Previous works often used single-device datasets, did not explore out-of-domain generalization, or did not release their models, hindering reproducibility and further research. We introduce PaPaGei, the first open foundation model for PPG signals. PaPaGei is pre-trained on more than 57,000 hours of 20 million unlabeled segments of PPG signals using publicly available datasets exclusively. We evaluate against popular time-series foundation models and other benchmarks on 20 tasks of 10 diverse datasets spanning cardiovascular health, sleep disorders, pregnancy monitoring, and wellbeing assessment. Our architecture incorporates novel representation learning approaches that leverage differences in PPG signal morphology across individuals, enabling it to capture richer representations than traditional contrastive learning methods. Across 20 tasks, PaPaGei improves classification and regression performance by an average of 6.3% and 2.9%, respectively, compared to other competitive time-series foundation models in at least 14 tasks. PaPaGei is more data- and parameter-efficient than other foundation models or methods, as it outperforms 70x larger models. Beyond accuracy, we also investigate robustness against different skin tones, establishing a benchmark for bias evaluations of future models. Notably, PaPaGei can be used out of the box as both a feature extractor and an encoder for other multimodal models, opening up new opportunities for multimodal health monitoring",
        "subjects": [
            "cs.LG",
            "eess.SP"
        ],
        "comment": "Code and models: https://github.com/nokia-bell-labs/papagei-foundation-model"
    },
    {
        "paper id": "2410.20587",
        "abstract url": "https://arxiv.org/abs/2410.20587",
        "title": "Generator Matching: Generative modeling with arbitrary Markov processes",
        "rating": "-0.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We introduce generator matching, a modality-agnostic framework for generative modeling using arbitrary Markov processes. Generators characterize the infinitesimal evolution of a Markov process, which we leverage for generative modeling in a similar vein to flow matching: we construct conditional generators which generate single data points, then learn to approximate the marginal generator which generates the full data distribution. We show that generator matching unifies various generative modeling methods, including diffusion models, flow matching and discrete diffusion models. Furthermore, it provides the foundation to expand the design space to new and unexplored Markov processes such as jump processes. Finally, generator matching enables the construction of superpositions of Markov generative processes and enables the construction of multimodal models in a rigorous manner. We empirically validate our method on protein and image structure generation, showing that superposition with a jump process improves image generation.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20659",
        "abstract url": "https://arxiv.org/abs/2410.20659",
        "title": "A Statistical Analysis of Deep Federated Learning for Intrinsically Low-dimensional Data",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL) has emerged as a groundbreaking paradigm in collaborative machine learning, emphasizing decentralized model training to address data privacy concerns. While significant progress has been made in optimizing federated learning, the exploration of generalization error, particularly in heterogeneous settings, has been limited, focusing mainly on parametric cases. This paper investigates the generalization properties of deep federated regression within a two-stage sampling model. Our findings highlight that the intrinsic dimension, defined by the entropic dimension, is crucial for determining convergence rates when appropriate network sizes are used. Specifically, if the true relationship between response and explanatory variables is charecterized by a $\u03b2$-H\u00f6lder function and there are $n$ independent and identically distributed (i.i.d.) samples from $m$ participating clients, the error rate for participating clients scales at most as $\\tilde{O}\\left((mn)^{-2\u03b2/(2\u03b2+ \\bar{d}_{2\u03b2}(\u03bb))}\\right)$, and for non-participating clients, it scales as $\\tilde{O}\\left(\u0394\\cdot m^{-2\u03b2/(2\u03b2+ \\bar{d}_{2\u03b2}(\u03bb))} + (mn)^{-2\u03b2/(2\u03b2+ \\bar{d}_{2\u03b2}(\u03bb))}\\right)$. Here, $\\bar{d}_{2\u03b2}(\u03bb)$ represents the $2\u03b2$-entropic dimension of $\u03bb$, the marginal distribution of the explanatory variables, and $\u0394$ characterizes the dependence between the sampling stages. Our results explicitly account for the \"closeness\" of clients, demonstrating that the convergence rates of deep federated learners depend on intrinsic rather than nominal high-dimensionality.",
        "subjects": [
            "stat.ML",
            "cs.AI",
            "cs.LG",
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20664",
        "abstract url": "https://arxiv.org/abs/2410.20664",
        "title": "Embedding with Large Language Models for Classification of HIPAA Safeguard Compliance Rules",
        "rating": "-0.5",
        "keywords": [
            [
                "Support Vector Machine"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Although software developers of mHealth apps are responsible for protecting patient data and adhering to strict privacy and security requirements, many of them lack awareness of HIPAA regulations and struggle to distinguish between HIPAA rules categories. Therefore, providing guidance of HIPAA rules patterns classification is essential for developing secured applications for Google Play Store. In this work, we identified the limitations of traditional Word2Vec embeddings in processing code patterns. To address this, we adopt multilingual BERT (Bidirectional Encoder Representations from Transformers) which offers contextualized embeddings to the attributes of dataset to overcome the issues. Therefore, we applied this BERT to our dataset for embedding code patterns and then uses these embedded code to various machine learning approaches. Our results demonstrate that the models significantly enhances classification performance, with Logistic Regression achieving a remarkable accuracy of 99.95\\%. Additionally, we obtained high accuracy from Support Vector Machine (99.79\\%), Random Forest (99.73\\%), and Naive Bayes (95.93\\%), outperforming existing approaches. This work underscores the effectiveness and showcases its potential for secure application development.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20716",
        "abstract url": "https://arxiv.org/abs/2410.20716",
        "title": "Physics-Free Spectrally Multiplexed Photometric Stereo under Unknown Spectral Composition",
        "rating": "-0.5",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "In this paper, we present a groundbreaking spectrally multiplexed photometric stereo approach for recovering surface normals of dynamic surfaces without the need for calibrated lighting or sensors, a notable advancement in the field traditionally hindered by stringent prerequisites and spectral ambiguity. By embracing spectral ambiguity as an advantage, our technique enables the generation of training data without specialized multispectral rendering frameworks. We introduce a unique, physics-free network architecture, SpectraM-PS, that effectively processes multiplexed images to determine surface normals across a wide range of conditions and material types, without relying on specific physically-based knowledge. Additionally, we establish the first benchmark dataset, SpectraM14, for spectrally multiplexed photometric stereo, facilitating comprehensive evaluations against existing calibrated methods. Our contributions significantly enhance the capabilities for dynamic surface recovery, particularly in uncalibrated setups, marking a pivotal step forward in the application of photometric stereo across various domains.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "ECCV2024 (Oral)"
    },
    {
        "paper id": "2410.20344",
        "abstract url": "https://arxiv.org/abs/2410.20344",
        "title": "Deep Learning-Assisted Jamming Mitigation with Movable Antenna Array",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "This paper reveals the potential of movable antennas in enhancing anti-jamming communication. We consider a legitimate communication link in the presence of multiple jammers and propose deploying a movable antenna array at the receiver to combat jamming attacks. We formulate the problem as a signal-to-interference-plus-noise ratio maximization, by jointly optimizing the receive beamforming and antenna element positioning. Due to the non-convexity and multi-fold difficulties from an optimization perspective, we develop a deep learning-based framework where beamforming is tackled as a Rayleigh quotient problem, while antenna positioning is addressed through multi-layer perceptron training. The neural network parameters are optimized using stochastic gradient descent to achieve effective jamming mitigation strategy, featuring offline training with marginal complexity for online inference. Numerical results demonstrate that the proposed approach achieves near-optimal anti-jamming performance thereby significantly improving the efficiency in strategy determination.",
        "subjects": [
            "eess.SP",
            "cs.IT"
        ],
        "comment": "5 pages, under consideration"
    },
    {
        "paper id": "2410.20352",
        "abstract url": "https://arxiv.org/abs/2410.20352",
        "title": "An approach to hummed-tune and song sequences matching",
        "rating": "-1",
        "keywords": [
            [
                "song"
            ],
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Melody stuck in your head, also known as \"earworm\", is tough to get rid of, unless you listen to it again or sing it out loud. But what if you can not find the name of that song? It must be an intolerable feeling. Recognizing a song name base on humming sound is not an easy task for a human being and should be done by machines. However, there is no research paper published about hum tune recognition. Adapting from Hum2Song Zalo AI Challenge 2021 - a competition about querying the name of a song by user's giving humming tune, which is similar to Google's Hum to Search. This paper covers details about the pre-processed data from the original type (mp3) to usable form for training and inference. In training an embedding model for the feature extraction phase, we ran experiments with some states of the art, such as ResNet, VGG, AlexNet, MobileNetV2. And for the inference phase, we use the Faiss module to effectively search for a song that matched the sequence of humming sound. The result comes at nearly 94\\% in MRR@10 metric on the public test set, along with the top 1 result on the public leaderboard.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "cs.IR",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20353",
        "abstract url": "https://arxiv.org/abs/2410.20353",
        "title": "Distributed Complexity of $P_k$-freeness: Decision and Certification",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "The class of graphs that do not contain a path on $k$ nodes as an induced subgraph ($P_k$-free graphs) has rich applications in the theory of graph algorithms. This paper explores the problem of deciding $P_k$-freeness from the viewpoint of distributed computing. For specific small values of $k$, we present the \\textit{first} $\\mathsf{CONGEST}$ algorithms specified for $P_k$-freeness, utilizing structural properties of $P_k$-free graphs in a novel way. Specifically, we show that $P_k$-freeness can be decided in $\\tilde{O}(1)$ rounds for $k=4$ in the $\\mathsf{broadcast\\;CONGEST}$ model, and in $\\tilde{O}(n)$ rounds for $k=5$ in the $\\mathsf{CONGEST}$ model, where $n$ is the number of nodes in the network and $\\tilde{O}(\\cdot)$ hides a $\\mathrm{polylog}(n)$ factor. These results significantly improve the previous $O(n^{2-2/(3k+2)})$ upper bounds by Eden et al. (Dist.~Comp.~2022). We also construct a local certification of $P_5$-freeness with certificates of size $\\tilde{O}(n)$. This is nearly optimal, given our $\u03a9(n^{1-o(1)})$ lower bound on certificate size, and marks a significant advancement as no nontrivial bounds for proof-labeling schemes of $P_5$-freeness were previously known. For general $k$, we establish the first $\\mathsf{CONGEST}$ lower bound, which is of the form $n^{2-1/\u0398(k)}$. The $n^{1/\u0398(k)}$ factor is unavoidable, in view of the $O(n^{2-2/(3k+2)})$ upper bound mentioned above. Additionally, our approach yields the \\textit{first} superlinear lower bound on certificate size for local certification. This partially answers the conjecture on the optimal certificate size of $P_k$-freeness, asked by Bousquet et al. (arXiv:2402.12148). Finally, we propose a novel variant of the problem called ordered $P_k$ detection, and show a linear lower bound and its nontrivial connection to distributed subgraph detection.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "Abstract shorten to meet arXiv limits"
    },
    {
        "paper id": "2410.20366",
        "abstract url": "https://arxiv.org/abs/2410.20366",
        "title": "Rethinking Reconstruction-based Graph-Level Anomaly Detection: Limitations and a Simple Remedy",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "cs.LG",
                "cs.SI"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Graph autoencoders (Graph-AEs) learn representations of given graphs by aiming to accurately reconstruct them. A notable application of Graph-AEs is graph-level anomaly detection (GLAD), whose objective is to identify graphs with anomalous topological structures and/or node features compared to the majority of the graph population. Graph-AEs for GLAD regard a graph with a high mean reconstruction error (i.e. mean of errors from all node pairs and/or nodes) as anomalies. Namely, the methods rest on the assumption that they would better reconstruct graphs with similar characteristics to the majority. We, however, report non-trivial counter-examples, a phenomenon we call reconstruction flip, and highlight the limitations of the existing Graph-AE-based GLAD methods. Specifically, we empirically and theoretically investigate when this assumption holds and when it fails. Through our analyses, we further argue that, while the reconstruction errors for a given graph are effective features for GLAD, leveraging the multifaceted summaries of the reconstruction errors, beyond just mean, can further strengthen the features. Thus, we propose a novel and simple GLAD method, named MUSE. The key innovation of MUSE involves taking multifaceted summaries of reconstruction errors as graph features for GLAD. This surprisingly simple method obtains SOTA performance in GLAD, performing best overall among 14 methods across 10 datasets.",
        "subjects": [
            "cs.LG",
            "cs.SI"
        ],
        "comment": "Published as a conference paper at NeurIPS 2024"
    },
    {
        "paper id": "2410.20381",
        "abstract url": "https://arxiv.org/abs/2410.20381",
        "title": "Efficient and Effective Retrieval of Dense-Sparse Hybrid Vectors using Graph-based Approximate Nearest Neighbor Search",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "ANNS for embedded vector representations of texts is commonly used in information retrieval, with two important information representations being sparse and dense vectors. While it has been shown that combining these representations improves accuracy, the current method of conducting sparse and dense vector searches separately suffers from low scalability and high system complexity. Alternatively, building a unified index faces challenges with accuracy and efficiency. To address these issues, we propose a graph-based ANNS algorithm for dense-sparse hybrid vectors. Firstly, we propose a distribution alignment method to improve accuracy, which pre-samples dense and sparse vectors to analyze their distance distribution statistic, resulting in a 1%$\\sim$9% increase in accuracy. Secondly, to improve efficiency, we design an adaptive two-stage computation strategy that initially computes dense distances only and later computes hybrid distances. Further, we prune the sparse vectors to speed up the calculation. Compared to naive implementation, we achieve $\\sim2.1\\times$ acceleration. Thorough experiments show that our algorithm achieves 8.9x$\\sim$11.7x throughput at equal accuracy compared to existing hybrid vector search algorithms.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2410.20384",
        "abstract url": "https://arxiv.org/abs/2410.20384",
        "title": "Addressing the Pitfalls of Image-Based Structural Health Monitoring: A Focus on False Positives, False Negatives, and Base Rate Bias",
        "rating": "-1",
        "keywords": [
            [
                "Health"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "This study explores the limitations of image-based structural health monitoring (SHM) techniques in detecting structural damage. Leveraging machine learning and computer vision, image-based SHM offers a scalable and efficient alternative to manual inspections. However, its reliability is impacted by challenges such as false positives, false negatives, and environmental variability, particularly in low base rate damage scenarios. The Base Rate Bias plays a significant role, as low probabilities of actual damage often lead to misinterpretation of positive results. This study uses both Bayesian analysis and a frequentist approach to evaluate the precision of damage detection systems, revealing that even highly accurate models can yield misleading results when the occurrence of damage is rare. Strategies for mitigating these limitations are discussed, including hybrid systems that combine multiple data sources, human-in-the-loop approaches for critical assessments, and improving the quality of training data. These findings provide essential insights into the practical applicability of image-based SHM techniques, highlighting both their potential and their limitations for real-world infrastructure monitoring.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20402",
        "abstract url": "https://arxiv.org/abs/2410.20402",
        "title": "Deep Learning-Driven Microstructure Characterization and Vickers Hardness Prediction of Mg-Gd Alloys",
        "rating": "-1",
        "keywords": [
            [
                "Alloys"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "In the field of materials science, exploring the relationship between composition, microstructure, and properties has long been a critical research focus. The mechanical performance of solid-solution Mg-Gd alloys is significantly influenced by Gd content, dendritic structures, and the presence of secondary phases. To better analyze and predict the impact of these factors, this study proposes a multimodal fusion learning framework based on image processing and deep learning techniques. This framework integrates both elemental composition and microstructural features to accurately predict the Vickers hardness of solid-solution Mg-Gd alloys. Initially, deep learning methods were employed to extract microstructural information from a variety of solid-solution Mg-Gd alloy images obtained from literature and experiments. This provided precise grain size and secondary phase microstructural features for performance prediction tasks. Subsequently, these quantitative analysis results were combined with Gd content information to construct a performance prediction dataset. Finally, a regression model based on the Transformer architecture was used to predict the Vickers hardness of Mg-Gd alloys. The experimental results indicate that the Transformer model performs best in terms of prediction accuracy, achieving an R^2 value of 0.9. Additionally, SHAP analysis identified critical values for four key features affecting the Vickers hardness of Mg-Gd alloys, providing valuable guidance for alloy design. These findings not only enhance the understanding of alloy performance but also offer theoretical support for future material design and optimization.",
        "subjects": [
            "cs.LG",
            "cond-mat.mtrl-sci",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20424",
        "abstract url": "https://arxiv.org/abs/2410.20424",
        "title": "AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions",
        "rating": "-1",
        "keywords": [
            [
                "tabular"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Data science tasks involving tabular data present complex challenges that require sophisticated problem-solving approaches. We propose AutoKaggle, a powerful and user-centric framework that assists data scientists in completing daily data pipelines through a collaborative multi-agent system. AutoKaggle implements an iterative development process that combines code execution, debugging, and comprehensive unit testing to ensure code correctness and logic consistency. The framework offers highly customizable workflows, allowing users to intervene at each phase, thus integrating automated intelligence with human expertise. Our universal data science toolkit, comprising validated functions for data cleaning, feature engineering, and modeling, forms the foundation of this solution, enhancing productivity by streamlining common tasks. We selected 8 Kaggle competitions to simulate data processing workflows in real-world application scenarios. Evaluation results demonstrate that AutoKaggle achieves a validation submission rate of 0.85 and a comprehensive score of 0.82 in typical data science pipelines, fully proving its effectiveness and practicality in handling complex data science tasks.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": "44 pages, 10 figures"
    },
    {
        "paper id": "2410.20428",
        "abstract url": "https://arxiv.org/abs/2410.20428",
        "title": "MedGo: A Chinese Medical Large Language Model",
        "rating": "-1",
        "keywords": [
            [
                "Medical",
                "clinical"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large models are a hot research topic in the field of artificial intelligence. Leveraging their generative capabilities has the potential to enhance the level and quality of medical services. In response to the limitations of current large language models, which often struggle with accuracy and have narrow capabilities in medical applications, this paper presents a Chinese medical large language model, MedGo. MedGo was trained using a combination of high quality unsupervised medical data, supervised data, and preference alignment data, aimed at enhancing both its versatility and precision in medical tasks. The model was evaluated through the public CBLUE benchmark and a manually constructed dataset ClinicalQA. The results demonstrate that MedGo achieved promising performance across various Chinese medical information processing tasks, achieved the first place in the CBLUE evaluation. Additionally, on our constructed dataset ClinicalQA, MedGo outperformed its base model Qwen2, highlighting its potential to improve both automated medical question answering and clinical decision support. These experimental results demonstrate that MedGo possesses strong information processing capabilities in the medical field. At present, we have successfully deployed MedGo at Shanghai East Hospital.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "12 pages, 1 figure"
    },
    {
        "paper id": "2410.20436",
        "abstract url": "https://arxiv.org/abs/2410.20436",
        "title": "CoralSCOP-LAT: Labeling and Analyzing Tool for Coral Reef Images with Dense Mask",
        "rating": "-1",
        "keywords": [
            [
                "biologists"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Images of coral reefs provide invaluable information, which is essentially critical for surveying and monitoring the coral reef ecosystems. Robust and precise identification of coral reef regions within surveying imagery is paramount for assessing coral coverage, spatial distribution, and other statistical analyses. However, existing coral reef analytical approaches mainly focus on sparse points sampled from the whole imagery, which are highly subject to the sampling density and cannot accurately express the coral ambulance. Meanwhile, the analysis is both time-consuming and labor-intensive, and it is also limited to coral biologists. In this work, we propose CoralSCOP-LAT, an automatic and semi-automatic coral reef labeling and analysis tool, specially designed to segment coral reef regions (dense pixel masks) in coral reef images, significantly promoting analysis proficiency and accuracy. CoralSCOP-LAT leverages the advanced coral reef foundation model to accurately delineate coral regions, supporting dense coral reef analysis and reducing the dependency on manual annotation. The proposed CoralSCOP-LAT surpasses the existing tools by a large margin from analysis efficiency, accuracy, and flexibility. We perform comprehensive evaluations from various perspectives and the comparison demonstrates that CoralSCOP-LAT not only accelerates the coral reef analysis but also improves accuracy in coral segmentation and analysis. Our CoralSCOP-LAT, as the first dense coral reef analysis tool in the market, facilitates repeated large-scale coral reef monitoring analysis, contributing to more informed conservation efforts and sustainable management of coral reef ecosystems. Our tool will be available at https://coralscop.hkustvgd.com/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The coral reef labeling and analysis tool is available at https://coralscop.hkustvgd.com/"
    },
    {
        "paper id": "2410.20471",
        "abstract url": "https://arxiv.org/abs/2410.20471",
        "title": "Improved Online Reachability Preservers",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "A reachability preserver is a basic kind of graph sparsifier, which preserves the reachability relation of an $n$-node directed input graph $G$ among a set of given demand pairs $P$ of size $|P|=p$. We give constructions of sparse reachability preservers in the online setting, where $G$ is given on input, the demand pairs $(s, t) \\in P$ arrive one at a time, and we must irrevocably add edges to a preserver $H$ to ensure reachability for the pair $(s, t)$ before we can see the next demand pair. Our main results are: -- There is a construction that guarantees a maximum preserver size of $$|E(H)| \\le O\\left( n^{0.72}p^{0.56} + n^{0.6}p^{0.7} + n\\right).$$ This improves polynomially on the previous online upper bound of $O( \\min\\{np^{0.5}, n^{0.5}p\\}) + n$, implicit in the work of Coppersmith and Elkin [SODA '05]. -- Given a promise that the demand pairs will satisfy $P \\subseteq S \\times V$ for some vertex set $S$ of size $|S|=:\u03c3$, there is a construction that guarantees a maximum preserver size of $$|E(H)| \\le O\\left( (np\u03c3)^{1/2} + n\\right).$$ A slightly different construction gives the same result for the setting $P \\subseteq V \\times S$. This improves polynomially on the previous online upper bound of $O( \u03c3n)$ (folklore). All of these constructions are polynomial time, deterministic, and they do not require knowledge of the values of $p, \u03c3$, or $S$. Our techniques also give a small polynomial improvement in the current upper bounds for offline reachability preservers, and they extend to a stronger model in which we must commit to a path for all possible reachable pairs in $G$ before any demand pairs have been received. As an application, we improve the competitive ratio for Online Unweighted Directed Steiner Forest to $O(n^{3/5 + \\varepsilon})$.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "SODA 2025"
    },
    {
        "paper id": "2410.20496",
        "abstract url": "https://arxiv.org/abs/2410.20496",
        "title": "Trust-Aware Assistance Seeking in Human-Supervised Autonomy",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Our goal is to model and experimentally assess trust evolution to predict future beliefs and behaviors of human-robot teams in dynamic environments. Research suggests that maintaining trust among team members in a human-robot team is vital for successful team performance. Research suggests that trust is a multi-dimensional and latent entity that relates to past experiences and future actions in a complex manner. Employing a human-robot collaborative task, we design an optimal assistance-seeking strategy for the robot using a POMDP framework. In the task, the human supervises an autonomous mobile manipulator collecting objects in an environment. The supervisor's task is to ensure that the robot safely executes its task. The robot can either choose to attempt to collect the object or seek human assistance. The human supervisor actively monitors the robot's activities, offering assistance upon request, and intervening if they perceive the robot may fail. In this setting, human trust is the hidden state, and the primary objective is to optimize team performance. We execute two sets of human-robot interaction experiments. The data from the first experiment are used to estimate POMDP parameters, which are used to compute an optimal assistance-seeking policy evaluated in the second experiment. The estimated POMDP parameters reveal that, for most participants, human intervention is more probable when trust is low, particularly in high-complexity tasks. Our estimates suggest that the robot's action of asking for assistance in high-complexity tasks can positively impact human trust. Our experimental results show that the proposed trust-aware policy is better than an optimal trust-agnostic policy. By comparing model estimates of human trust, obtained using only behavioral data, with the collected self-reported trust values, we show that model estimates are isomorphic to self-reported responses.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20499",
        "abstract url": "https://arxiv.org/abs/2410.20499",
        "title": "Solving Sequential Greedy Problems Distributedly with Sub-Logarithmic Energy Cost",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "We study the awake complexity of graph problems that belong to the class O-LOCAL, which includes a large subset of problems solvable by sequential greedy algorithms, such as $(\u0394+1)$-coloring, maximal independent set, maximal matching, etc. It is known from previous work that, in $n$-node graphs of maximum degree $\u0394$, any problem in the class O-LOCAL can be solved by a deterministic distributed algorithm with awake complexity $O(\\log\u0394+\\log^\\star n)$. In this paper, we show that any problem belonging to the class O-LOCAL can be solved by a deterministic distributed algorithm with awake complexity $O(\\sqrt{\\log n}\\cdot\\log^\\star n)$. This leads to a polynomial improvement over the state of the art when $\u0394\\gg 2^{\\sqrt{\\log n}}$, e.g., $\u0394=n^\u03b5$ for some arbitrarily small $\u03b5>0$. The key ingredient for achieving our results is the computation of a network decomposition, that uses a small-enough number of colors, in sub-logarithmic time in the Sleeping model, which can be of independent interest.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20514",
        "abstract url": "https://arxiv.org/abs/2410.20514",
        "title": "Uncertainty-Aware Decision-Making and Planning for Autonomous Forced Merging",
        "rating": "-1",
        "keywords": [
            [
                "trajectory",
                "vehicle"
            ]
        ],
        "abstract": "In this paper, we develop an uncertainty-aware decision-making and motion-planning method for an autonomous ego vehicle in forced merging scenarios, considering the motion uncertainty of surrounding vehicles. The method dynamically captures the uncertainty of surrounding vehicles by online estimation of their acceleration bounds, enabling a reactive but rapid understanding of the uncertainty characteristics of the surrounding vehicles. By leveraging these estimated bounds, a non-conservative forward occupancy of surrounding vehicles is predicted over a horizon, which is incorporated in both the decision-making process and the motion-planning strategy, to enhance the resilience and safety of the planned reference trajectory. The method successfully fulfills the tasks in challenging forced merging scenarios, and the properties are illustrated by comparison with several alternative approaches.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": "Accepted by the 63rd IEEE Conference on Decision and Control, 2024"
    },
    {
        "paper id": "2410.20515",
        "abstract url": "https://arxiv.org/abs/2410.20515",
        "title": "Symbotunes: unified hub for symbolic music generative models",
        "rating": "-1",
        "keywords": [
            [
                "music"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Implementations of popular symbolic music generative models often differ significantly in terms of the libraries utilized and overall project structure. Therefore, directly comparing the methods or becoming acquainted with them may present challenges. To mitigate this issue we introduce Symbotunes, an open-source unified hub for symbolic music generative models. Symbotunes contains modern Python implementations of well-known methods for symbolic music generation, as well as a unified pipeline for generating and training.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "cs.LG",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20516",
        "abstract url": "https://arxiv.org/abs/2410.20516",
        "title": "A Cosmic-Scale Benchmark for Symmetry-Preserving Data Processing",
        "rating": "-1",
        "keywords": [
            [
                "point cloud"
            ],
            [
                "graph"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Efficiently processing structured point cloud data while preserving multiscale information is a key challenge across domains, from graphics to atomistic modeling. Using a curated dataset of simulated galaxy positions and properties, represented as point clouds, we benchmark the ability of graph neural networks to simultaneously capture local clustering environments and long-range correlations. Given the homogeneous and isotropic nature of the Universe, the data exhibits a high degree of symmetry. We therefore focus on evaluating the performance of Euclidean symmetry-preserving ($E(3)$-equivariant) graph neural networks, showing that they can outperform non-equivariant counterparts and domain-specific information extraction techniques in downstream performance as well as simulation-efficiency. However, we find that current architectures fail to capture information from long-range correlations as effectively as domain-specific baselines, motivating future work on architectures better suited for extracting long-range information.",
        "subjects": [
            "cs.LG",
            "astro-ph.IM"
        ],
        "comment": "19 pages, 3 figures; To appear at the NeurReps Workshop @ NeurIPS 2024"
    },
    {
        "paper id": "2410.20518",
        "abstract url": "https://arxiv.org/abs/2410.20518",
        "title": "MidiTok Visualizer: a tool for visualization and analysis of tokenized MIDI symbolic music",
        "rating": "-1",
        "keywords": [
            [
                "music"
            ],
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Symbolic music research plays a crucial role in music-related machine learning, but MIDI data can be complex for those without musical expertise. To address this issue, we present MidiTok Visualizer, a web application designed to facilitate the exploration and visualization of various MIDI tokenization methods from the MidiTok Python package. MidiTok Visualizer offers numerous customizable parameters, enabling users to upload MIDI files to visualize tokenized data alongside an interactive piano roll.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "cs.MM",
            "eess.AS"
        ],
        "comment": "in Extended Abstracts for the Late-Breaking Demo Sessionof the 25th Int. Society for Music Information Retrieval Conf., San Francisco, United States, 2024"
    },
    {
        "paper id": "2410.20525",
        "abstract url": "https://arxiv.org/abs/2410.20525",
        "title": "On Sparsest Cut and Conductance in Directed Polymatroidal Networks",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "We consider algorithms and spectral bounds for sparsest cut and conductance in directed polymatrodal networks. This is motivated by recent work on submodular hypergraphs \\cite{Yoshida19,LiM18,ChenOT23,Veldt23} and previous work on multicommodity flows and cuts in polymatrodial networks \\cite{ChekuriKRV15}. We obtain three results. First, we obtain an $O(\\sqrt{\\log n})$-approximation for sparsest cut and point out how this generalizes the result in \\cite{ChenOT23}. Second, we consider the symmetric version of conductance and obtain an $O(\\sqrt{OPT \\log r})$ approximation where $r$ is the maximum degree and we point out how this generalizes previous work on vertex expansion in graphs. Third, we prove a non-constructive Cheeger like inequality that generalizes previous work on hypergraphs. We provide a unified treatment via line-embeddings which were shown to be effective for submodular cuts in \\cite{ChekuriKRV15}.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20532",
        "abstract url": "https://arxiv.org/abs/2410.20532",
        "title": "Search Wide, Focus Deep: Automated Fetal Brain Extraction with Sparse Training Data",
        "rating": "-1",
        "keywords": [
            [
                "MRI",
                "clinical"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Automated fetal brain extraction from full-uterus MRI is a challenging task due to variable head sizes, orientations, complex anatomy, and prevalent artifacts. While deep-learning (DL) models trained on synthetic images have been successful in adult brain extraction, adapting these networks for fetal MRI is difficult due to the sparsity of labeled data, leading to increased false-positive predictions. To address this challenge, we propose a test-time strategy that reduces false positives in networks trained on sparse, synthetic labels. The approach uses a breadth-fine search (BFS) to identify a subvolume likely to contain the fetal brain, followed by a deep-focused sliding window (DFS) search to refine the extraction, pooling predictions to minimize false positives. We train models at different window sizes using synthetic images derived from a small number of fetal brain label maps, augmented with random geometric shapes. Each model is trained on diverse head positions and scales, including cases with partial or no brain tissue. Our framework matches state-of-the-art brain extraction methods on clinical HASTE scans of third-trimester fetuses and exceeds them by up to 5\\% in terms of Dice in the second trimester as well as EPI scans across both trimesters. Our results demonstrate the utility of a sliding-window approach and combining predictions from several models trained on synthetic images, for improving brain-extraction accuracy by progressively refining regions of interest and minimizing the risk of missing brain mask slices or misidentifying other tissues as brain.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20545",
        "abstract url": "https://arxiv.org/abs/2410.20545",
        "title": "ChartA11y: Designing Accessible Touch Experiences of Visualizations with Blind Smartphone Users",
        "rating": "-1",
        "keywords": [
            [
                "navigation"
            ]
        ],
        "abstract": "We introduce ChartA11y, an app developed to enable accessible 2-D visualizations on smartphones for blind users through a participatory and iterative design process involving 13 sessions with two blind partners. We also present a design journey for making accessible touch experiences that go beyond simple auditory feedback, incorporating multimodal interactions and multisensory data representations. Together, ChartA11y aimed at providing direct chart accessing and comprehensive chart understanding by applying a two-mode setting: a semantic navigation framework mode and a direct touch mapping mode. By re-designing traditional touch-to-audio interactions, ChartA11y also extends to accessible scatter plots, addressing the under-explored challenges posed by their non-linear data distribution. Our main contributions encompass the detailed participatory design process and the resulting system, ChartA11y, offering a novel approach for blind users to access visualizations on their smartphones.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20549",
        "abstract url": "https://arxiv.org/abs/2410.20549",
        "title": "Comparing the Consistency of User Studies Conducted in Simulations and Laboratory Settings",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Human-robot collaboration enables highly adaptive co-working. The variety of resulting workflows makes it difficult to measure metrics as, e.g. makespans or idle times for multiple systems and tasks in a comparable manner. This issue can be addressed with virtual commissioning, where arbitrary numbers of non-deterministic human-robot workflows in assembly tasks can be simulated. To this end, data-driven models of human decisions are needed. Gathering the required large corpus of data with on-site user studies is quite time-consuming. In comparison, simulation-based studies (e.g., by crowdsourcing) would allow us to access a large pool of study participants with less effort. To rely on respective study results, human action sequences observed in a browser-based simulation environment must be shown to match those gathered in a laboratory setting. To this end, this work aims to understand to what extent cooperative assembly work in a simulated environment differs from that in an on-site laboratory setting. We show how a simulation environment can be aligned with a laboratory setting in which a robot and a human perform pick-and-place tasks together. A user study (N=29) indicates that participants' assembly decisions and perception of the situation are consistent across these different environments.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted for presentation at 2024 IEEE International Conference on Robotic Computing (IRC)"
    },
    {
        "paper id": "2410.20578",
        "abstract url": "https://arxiv.org/abs/2410.20578",
        "title": "Meta-Learning Approaches for Improving Detection of Unseen Speech Deepfakes",
        "rating": "-1",
        "keywords": [
            [
                "deepfake"
            ],
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Current speech deepfake detection approaches perform satisfactorily against known adversaries; however, generalization to unseen attacks remains an open challenge. The proliferation of speech deepfakes on social media underscores the need for systems that can generalize to unseen attacks not observed during training. We address this problem from the perspective of meta-learning, aiming to learn attack-invariant features to adapt to unseen attacks with very few samples available. This approach is promising since generating of a high-scale training dataset is often expensive or infeasible. Our experiments demonstrated an improvement in the Equal Error Rate (EER) from 21.67% to 10.42% on the InTheWild dataset, using just 96 samples from the unseen dataset. Continuous few-shot adaptation ensures that the system remains up-to-date.",
        "subjects": [
            "eess.AS",
            "cs.AI",
            "cs.SD"
        ],
        "comment": "6 pages, accepted to the IEEE Spoken Language Technology Workshop (SLT) 2024"
    },
    {
        "paper id": "2410.20579",
        "abstract url": "https://arxiv.org/abs/2410.20579",
        "title": "Toward Conditional Distribution Calibration in Survival Prediction",
        "rating": "-1",
        "keywords": [
            [
                "Survival"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Survival prediction often involves estimating the time-to-event distribution from censored datasets. Previous approaches have focused on enhancing discrimination and marginal calibration. In this paper, we highlight the significance of conditional calibration for real-world applications -- especially its role in individual decision-making. We propose a method based on conformal prediction that uses the model's predicted individual survival probability at that instance's observed time. This method effectively improves the model's marginal and conditional calibration, without compromising discrimination. We provide asymptotic theoretical guarantees for both marginal and conditional calibration and test it extensively across 15 diverse real-world datasets, demonstrating the method's practical effectiveness and versatility in various settings.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": "Accepted to NeurIPS 2024. 41 pages, 23 figures"
    },
    {
        "paper id": "2410.20629",
        "abstract url": "https://arxiv.org/abs/2410.20629",
        "title": "Parameterized Saga of First-Fit and Last-Fit Coloring",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "The classic greedy coloring (first-fit) algorithm considers the vertices of an input graph $G$ in a given order and assigns the first available color to each vertex $v$ in $G$. In the {\\sc Grundy Coloring} problem, the task is to find an ordering of the vertices that will force the greedy algorithm to use as many colors as possible. In the {\\sc Partial Grundy Coloring}, the task is also to color the graph using as many colors as possible. This time, however, we may select both the ordering in which the vertices are considered and which color to assign the vertex. The only constraint is that the color assigned to a vertex $v$ is a color previously used for another vertex if such a color is available. Whether {\\sc Grundy Coloring} and {\\sc Partial Grundy Coloring} admit fixed-parameter tractable (FPT) algorithms, algorithms with running time $f(k)n^{\\OO(1)}$, where $k$ is the number of colors, was posed as an open problem by Zaker and by Effantin et al., respectively. Recently, Aboulker et al. (STACS 2020 and Algorithmica 2022) resolved the question for \\Grundycol\\ in the negative by showing that the problem is W[1]-hard. For {\\sc Partial Grundy Coloring}, they obtain an FPT algorithm on graphs that do not contain $K_{i,j}$ as a subgraph (a.k.a. $K_{i,j}$-free graphs). Aboulker et al.~re-iterate the question of whether there exists an FPT algorithm for {\\sc Partial Grundy Coloring} on general graphs and also asks whether {\\sc Grundy Coloring} admits an FPT algorithm on $K_{i,j}$-free graphs. We give FPT algorithms for {\\sc Partial Grundy Coloring} on general graphs and for {\\sc Grundy Coloring} on $K_{i,j}$-free graphs, resolving both the questions in the affirmative. We believe that our new structural theorems for partial Grundy coloring and ``representative-family'' like sets for $K_{i,j}$-free graphs that we use in obtaining our results may have wider algorithmic applications.",
        "subjects": [
            "cs.DM",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20635",
        "abstract url": "https://arxiv.org/abs/2410.20635",
        "title": "Generating and Optimizing Topologically Distinct Guesses for Mobile Manipulator Path Planning",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Optimal path planning often suffers from getting stuck in a local optimum. This is often the case for mobile manipulators due to nonconvexities induced by obstacles and robot kinematics. This paper attempts to circumvent this issue by proposing a pipeline to obtain multiple distinct local optima. By evaluating and selecting the optimum among multiple distinct local optima, it is likely to obtain a closer approximation of the global optimum. We demonstrate this capability in optimal path planning of nonholonomic mobile manipulators in the presence of obstacles and subject to end effector path constraints. The nonholomicity, obstacles, and end effector path constraints often cause direct optimal path planning approaches to get stuck in local optima. We demonstrate that our pipeline is able to circumvent this issue and produce a final local optimum that is close to the global optimum.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20637",
        "abstract url": "https://arxiv.org/abs/2410.20637",
        "title": "Observability of Linear Time-Invariant Systems with Relative Measurements: A Geometric Approach",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "This paper explores the observability and estimation capability of dynamical systems using predominantly relative measurements of the system's state-space variables, with minimal to no reliance on absolute measurements of these variables. We concentrate on linear time-invariant systems, in which the observation matrix serves as the algebraic representation of a graph object. This graph object encapsulates the availability of relative measurements. Utilizing algebraic graph theory and abstract linear algebra (geometric) tools, we establish a link between the structure of the graph of relative measurements and the system-theoretic observability subspace of linear systems. Special emphasis is given to multi-agent networked systems whose dynamics are governed by the linear consensus protocol. We demonstrate the importance of absolute information and its placement to the system's dynamics in achieving full-state estimation. Finally, the analysis shifts to the synthesis of a distributed observer with relative measurements for single integrator dynamics, exemplifying the relevance of the preceding analytical findings. We support our theoretical analysis with numerical simulations.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20639",
        "abstract url": "https://arxiv.org/abs/2410.20639",
        "title": "A Comparative Study of Multiple Deep Learning Algorithms for Efficient Localization of Bone Joints in the Upper Limbs of Human Body",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "diagnosis",
                "CT",
                "X-Ray"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper addresses the medical imaging problem of joint detection in the upper limbs, viz. elbow, shoulder, wrist and finger joints. Localization of joints from X-Ray and Computerized Tomography (CT) scans is an essential step for the assessment of various bone-related medical conditions like Osteoarthritis, Rheumatoid Arthritis, and can even be used for automated bone fracture detection. Automated joint localization also detects the corresponding bones and can serve as input to deep learning-based models used for the computerized diagnosis of the aforementioned medical disorders. This in-creases the accuracy of prediction and aids the radiologists with analyzing the scans, which is quite a complex and exhausting task. This paper provides a detailed comparative study between diverse Deep Learning (DL) models - YOLOv3, YOLOv7, EfficientDet and CenterNet in multiple bone joint detections in the upper limbs of the human body. The research analyses the performance of different DL models, mathematically, graphically and visually. These models are trained and tested on a portion of the openly available MURA (musculoskeletal radiographs) dataset. The study found that the best Mean Average Precision (mAP at 0.5:0.95) values of YOLOv3, YOLOv7, EfficientDet and CenterNet are 35.3, 48.3, 46.5 and 45.9 respectively. Besides, it has been found YOLOv7 performed the best for accurately predicting the bounding boxes while YOLOv3 performed the worst in the Visual Analysis test. Code available at https://github.com/Sohambasu07/BoneJointsLocalization",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20642",
        "abstract url": "https://arxiv.org/abs/2410.20642",
        "title": "Collaborative Knowledge Fusion: A Novel Approach for Multi-task Recommender Systems via LLMs",
        "rating": "-1",
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "recommendation"
            ]
        ],
        "abstract": "Owing to the impressive general intelligence of large language models (LLMs), there has been a growing trend to integrate them into recommender systems to gain a more profound insight into human interests and intentions. Existing LLMs-based recommender systems primarily leverage item attributes and user interaction histories in textual format, improving the single task like rating prediction or explainable recommendation. Nevertheless, these approaches overlook the crucial contribution of traditional collaborative signals in discerning users' profound intentions and disregard the interrelatedness among tasks. To address these limitations, we introduce a novel framework known as CKF, specifically developed to boost multi-task recommendations via personalized collaborative knowledge fusion into LLMs. Specifically, our method synergizes traditional collaborative filtering models to produce collaborative embeddings, subsequently employing the meta-network to construct personalized mapping bridges tailored for each user. Upon mapped, the embeddings are incorporated into meticulously designed prompt templates and then fed into an advanced LLM to represent user interests. To investigate the intrinsic relationship among diverse recommendation tasks, we develop Multi-Lora, a new parameter-efficient approach for multi-task optimization, adept at distinctly segregating task-shared and task-specific information. This method forges a connection between LLMs and recommendation scenarios, while simultaneously enriching the supervisory signal through mutual knowledge transfer among various tasks. Extensive experiments and in-depth robustness analyses across four common recommendation tasks on four large public data sets substantiate the effectiveness and superiority of our framework.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20670",
        "abstract url": "https://arxiv.org/abs/2410.20670",
        "title": "Segmenting Watermarked Texts From Language Models",
        "rating": "-1",
        "keywords": [
            [
                "Watermarking"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Watermarking is a technique that involves embedding nearly unnoticeable statistical signals within generated content to help trace its source. This work focuses on a scenario where an untrusted third-party user sends prompts to a trusted language model (LLM) provider, who then generates a text from their LLM with a watermark. This setup makes it possible for a detector to later identify the source of the text if the user publishes it. The user can modify the generated text by substitutions, insertions, or deletions. Our objective is to develop a statistical method to detect if a published text is LLM-generated from the perspective of a detector. We further propose a methodology to segment the published text into watermarked and non-watermarked sub-strings. The proposed approach is built upon randomization tests and change point detection techniques. We demonstrate that our method ensures Type I and Type II error control and can accurately identify watermarked sub-strings by finding the corresponding change point locations. To validate our technique, we apply it to texts generated by several language models with prompts extracted from Google's C4 dataset and obtain encouraging numerical results. We release all code publicly at https://github.com/doccstat/llm-watermark-cpd.",
        "subjects": [
            "cs.LG",
            "cs.MM",
            "cs.NE",
            "stat.ML"
        ],
        "comment": "25 pages, 12 figures, 2 tables, NeurIPS 2024"
    },
    {
        "paper id": "2410.20675",
        "abstract url": "https://arxiv.org/abs/2410.20675",
        "title": "Impact of Translation and Viewpoint Transition Methods in VR on Spatial Learning and Cybersickness",
        "rating": "-1",
        "keywords": [
            [
                "navigation"
            ]
        ],
        "abstract": "Virtual locomotion technique (VLT) is a fundamental component of virtual reality (VR) systems that translates physical and controller inputs into virtual translational movements and viewpoint transitions. Poorly designed VLTs can result in discomfort, nausea, and reductions in task performance. Understanding the effectiveness of VLTs across various levels of interaction fidelity is crucial to enhance user experience and spatial awareness. The current study addressed a significant gap in VR design research and practice, as few previous efforts have been made to comprehensively evaluate the effectiveness of controller-based VLTs in virtual indoor environments. We conducted a user study in which participants navigated through two complex virtual environments, one focusing on exploratory tasks and the other on goal-oriented navigation. The findings offer insights into the trade-offs among spatial knowledge acquisition, wayfinding performance, cybersickness, and sense of presence, and have design implications for future VR interfaces.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20712",
        "abstract url": "https://arxiv.org/abs/2410.20712",
        "title": "COBRA: Interaction-Aware Bytecode-Level Vulnerability Detector for Smart Contracts",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "The detection of vulnerabilities in smart contracts remains a significant challenge. While numerous tools are available for analyzing smart contracts in source code, only about 1.79% of smart contracts on Ethereum are open-source. For existing tools that target bytecodes, most of them only consider the semantic logic context and disregard function interface information in the bytecodes. In this paper, we propose COBRA, a novel framework that integrates semantic context and function interfaces to detect vulnerabilities in bytecodes of the smart contract. To our best knowledge, COBRA is the first framework that combines these two features. Moreover, to infer the function signatures that are not present in signature databases, we present SRIF (Signatures Reverse Inference from Functions), automatically learn the rules of function signatures from the smart contract bytecodes. The bytecodes associated with the function signatures are collected by constructing a control flow graph (CFG) for the SRIF training. We optimize the semantic context using the operation code in the static single assignment (SSA) format. Finally, we integrate the context and function interface representations in the latent space as the contract feature embedding. The contract features in the hidden space are decoded for vulnerability classifications with a decoder and attention module. Experimental results demonstrate that SRIF can achieve 94.76% F1-score for function signature inference. Furthermore, when the ground truth ABI exists, COBRA achieves 93.45% F1-score for vulnerability classification. In the absence of ABI, the inferred function feature fills the encoder, and the system accomplishes an 89.46% recall rate.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "This work is accepted by ASE'24"
    },
    {
        "paper id": "2410.20713",
        "abstract url": "https://arxiv.org/abs/2410.20713",
        "title": "Detecting Malicious Accounts in Web3 through Transaction Graph",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "The web3 applications have recently been growing, especially on the Ethereum platform, starting to become the target of scammers. The web3 scams, imitating the services provided by legitimate platforms, mimic regular activity to deceive users. The current phishing account detection tools utilize graph learning or sampling algorithms to obtain graph features. However, large-scale transaction networks with temporal attributes conform to a power-law distribution, posing challenges in detecting web3 scams. In this paper, we present ScamSweeper, a novel framework to identify web3 scams on Ethereum. Furthermore, we collect a large-scale transaction dataset consisting of web3 scams, phishing, and normal accounts. Our experiments indicate that ScamSweeper exceeds the state-of-the-art in detecting web3 scams.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "This work is accepted by ASE'24"
    },
    {
        "paper id": "2410.21341",
        "abstract url": "https://arxiv.org/abs/2410.21341",
        "title": "Retrieval-Retro: Retrieval-based Inorganic Retrosynthesis with Expert Knowledge",
        "rating": "-1",
        "keywords": [
            [
                "chemical"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "While inorganic retrosynthesis planning is essential in the field of chemical science, the application of machine learning in this area has been notably less explored compared to organic retrosynthesis planning. In this paper, we propose Retrieval-Retro for inorganic retrosynthesis planning, which implicitly extracts the precursor information of reference materials that are retrieved from the knowledge base regarding domain expertise in the field. Specifically, instead of directly employing the precursor information of reference materials, we propose implicitly extracting it with various attention layers, which enables the model to learn novel synthesis recipes more effectively. Moreover, during retrieval, we consider the thermodynamic relationship between target material and precursors, which is essential domain expertise in identifying the most probable precursor set among various options. Extensive experiments demonstrate the superiority of Retrieval-Retro in retrosynthesis planning, especially in discovering novel synthesis recipes, which is crucial for materials discovery. The source code for Retrieval-Retro is available at https://github.com/HeewoongNoh/Retrieval-Retro.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "NeurIPS 2024"
    },
    {
        "paper id": "2410.20350",
        "abstract url": "https://arxiv.org/abs/2410.20350",
        "title": "Beyond Trivial Edges: A Fractional Approach to Cohesive Subgraph Detection in Hypergraphs",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Hypergraphs serve as a powerful tool for modeling complex relationships across domains like social networks, transactions, and recommendation systems. The (k,g)-core model effectively identifies cohesive subgraphs by assessing internal connections and co-occurrence patterns, but it is susceptible to inflated cohesiveness due to trivial hyperedges. To address this, we propose the $(k,g,p)$-core model, which incorporates the relative importance of hyperedges for more accurate subgraph detection. We develop both Na\u00efve and Advanced pruning algorithms, demonstrating through extensive experiments that our approach reduces the execution frequency of costly operations by 51.9% on real-world datasets.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20358",
        "abstract url": "https://arxiv.org/abs/2410.20358",
        "title": "RopeTP: Global Human Motion Recovery via Integrating Robust Pose Estimation with Diffusion Trajectory Prior",
        "rating": "-1.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "Trajectory",
                "SLAM"
            ],
            [
                "cs.AI",
                "cs.CV"
            ],
            [
                "WACV"
            ]
        ],
        "abstract": "We present RopeTP, a novel framework that combines Robust pose estimation with a diffusion Trajectory Prior to reconstruct global human motion from videos. At the heart of RopeTP is a hierarchical attention mechanism that significantly improves context awareness, which is essential for accurately inferring the posture of occluded body parts. This is achieved by exploiting the relationships with visible anatomical structures, enhancing the accuracy of local pose estimations. The improved robustness of these local estimations allows for the reconstruction of precise and stable global trajectories. Additionally, RopeTP incorporates a diffusion trajectory model that predicts realistic human motion from local pose sequences. This model ensures that the generated trajectories are not only consistent with observed local actions but also unfold naturally over time, thereby improving the realism and stability of 3D human motion reconstruction. Extensive experimental validation shows that RopeTP surpasses current methods on two benchmark datasets, particularly excelling in scenarios with occlusions. It also outperforms methods that rely on SLAM for initial camera estimates and extensive optimization, delivering more accurate and realistic trajectories.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Accepted by WACV 2025 (Round 1)"
    },
    {
        "paper id": "2410.20405",
        "abstract url": "https://arxiv.org/abs/2410.20405",
        "title": "Causal Modeling in Multi-Context Systems: Distinguishing Multiple Context-Specific Causal Graphs which Account for Observational Support",
        "rating": "-1.5",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "anomaly detection"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Causal structure learning with data from multiple contexts carries both opportunities and challenges. Opportunities arise from considering shared and context-specific causal graphs enabling to generalize and transfer causal knowledge across contexts. However, a challenge that is currently understudied in the literature is the impact of differing observational support between contexts on the identifiability of causal graphs. Here we study in detail recently introduced [6] causal graph objects that capture both causal mechanisms and data support, allowing for the analysis of a larger class of context-specific changes, characterizing distribution shifts more precisely. We thereby extend results on the identifiability of context-specific causal structures and propose a framework to model context-specific independence (CSI) within structural causal models (SCMs) in a refined way that allows to explore scenarios where these graph objects differ. We demonstrate how this framework can help explaining phenomena like anomalies or extreme events, where causal mechanisms change or appear to change under different conditions. Our results contribute to the theoretical foundations for understanding causal relations in multi-context systems, with implications for generalization, transfer learning, and anomaly detection. Future work may extend this approach to more complex data types, such as time-series.",
        "subjects": [
            "cs.LG",
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20418",
        "abstract url": "https://arxiv.org/abs/2410.20418",
        "title": "Inevitable Trade-off between Watermark Strength and Speculative Sampling Efficiency for Language Models",
        "rating": "-1.5",
        "keywords": [
            [
                "Watermark"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large language models are probabilistic models, and the process of generating content is essentially sampling from the output distribution of the language model. Existing watermarking techniques inject watermarks into the generated content without altering the output quality. On the other hand, existing acceleration techniques, specifically speculative sampling, leverage a draft model to speed up the sampling process while preserving the output distribution. However, there is no known method to simultaneously accelerate the sampling process and inject watermarks into the generated content. In this paper, we investigate this direction and find that the integration of watermarking and acceleration is non-trivial. We prove a no-go theorem, which states that it is impossible to simultaneously maintain the highest watermark strength and the highest sampling efficiency. Furthermore, we propose two methods that maintain either the sampling efficiency or the watermark strength, but not both. Our work provides a rigorous theoretical foundation for understanding the inherent trade-off between watermark strength and sampling efficiency in accelerating the generation of watermarked tokens for large language models. We also conduct numerical experiments to validate our theoretical findings and demonstrate the effectiveness of the proposed methods.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20431",
        "abstract url": "https://arxiv.org/abs/2410.20431",
        "title": "Smart Transport Infrastructure Maintenance: A Smart-Contract Blockchain Approach",
        "rating": "-1.5",
        "keywords": [
            [
                "IoT"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "Infrastructure maintenance is inherently complex, especially for widely dispersed transport systems like roads and railroads. Maintaining this infrastructure involves multiple partners working together to ensure safe, efficient upkeep that meets technical and safety standards, with timely materials and budget adherence. Traditionally, these requirements are managed on paper, with each contract step checked manually. Smart contracts, based on blockchain distributed ledger technology, offer a new approach. Distributed ledgers facilitate secure, transparent transactions, enabling decentralized agreements where contract terms automatically execute when conditions are met. Beyond financial transactions, blockchains can track complex agreements, recording each stage of contract fulfillment between multiple parties. A smart contract is a set of coded rules stored on the blockchain that automatically executes each term upon meeting specified conditions. In infrastructure maintenance, this enables end-to-end automation-from contractor assignment to maintenance completion. Using an immutable, decentralized record, contract terms and statuses are transparent to all parties, enhancing trust and efficiency. Creating smart contracts for infrastructure requires a comprehensive understanding of procedural workflows to foresee all requirements and liabilities. This workflow includes continuous infrastructure monitoring through a dynamic, data-driven maintenance model that triggers necessary actions. Modern process mining can develop a resilient Maintenance Process Model, helping Operations Management to define contract terms, including asset allocation, logistics, materials, and skill requirements. Automation and reliable data quality across the procedural chain are essential, supported by IoT sensors, big data analytics, predictive maintenance, intelligent logistics, and asset management.",
        "subjects": [
            "cs.CY",
            "eess.SY"
        ],
        "comment": "17 pages, 6 figures"
    },
    {
        "paper id": "2410.20439",
        "abstract url": "https://arxiv.org/abs/2410.20439",
        "title": "TEAFormers: TEnsor-Augmented Transformers for Multi-Dimensional Time Series Forecasting",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Multi-dimensional time series data, such as matrix and tensor-variate time series, are increasingly prevalent in fields such as economics, finance, and climate science. Traditional Transformer models, though adept with sequential data, do not effectively preserve these multi-dimensional structures, as their internal operations in effect flatten multi-dimensional observations into vectors, thereby losing critical multi-dimensional relationships and patterns. To address this, we introduce the Tensor-Augmented Transformer (TEAFormer), a novel method that incorporates tensor expansion and compression within the Transformer framework to maintain and leverage the inherent multi-dimensional structures, thus reducing computational costs and improving prediction accuracy. The core feature of the TEAFormer, the Tensor-Augmentation (TEA) module, utilizes tensor expansion to enhance multi-view feature learning and tensor compression for efficient information aggregation and reduced computational load. The TEA module is not just a specific model architecture but a versatile component that is highly compatible with the attention mechanism and the encoder-decoder structure of Transformers, making it adaptable to existing Transformer architectures. Our comprehensive experiments, which integrate the TEA module into three popular time series Transformer models across three real-world benchmarks, show significant performance enhancements, highlighting the potential of TEAFormers for cutting-edge time series forecasting.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20536",
        "abstract url": "https://arxiv.org/abs/2410.20536",
        "title": "Malinowski in the Age of AI: Can large language models create a text game based on an anthropological classic?",
        "rating": "-1.5",
        "keywords": [
            [
                "biographical"
            ],
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "Recent advancements in Large Language Models (LLMs) like ChatGPT and GPT-4 have shown remarkable abilities in a wide range of tasks such as summarizing texts and assisting in coding. Scientific research has demonstrated that these models can also play text-adventure games. This study aims to explore whether LLMs can autonomously create text-based games based on anthropological classics, evaluating also their effectiveness in communicating knowledge. To achieve this, the study engaged anthropologists in discussions to gather their expectations and design inputs for an anthropologically themed game. Through iterative processes following the established HCI principle of 'design thinking', the prompts and the conceptual framework for crafting these games were refined. Leveraging GPT3.5, the study created three prototypes of games centered around the seminal anthropological work of the social anthropologist's Bronislaw Malinowski's \"Argonauts of the Western Pacific\" (1922). Subsequently, evaluations were conducted by inviting senior anthropologists to playtest these games, and based on their inputs, the game designs were refined. The tests revealed promising outcomes but also highlighted key challenges: the models encountered difficulties in providing in-depth thematic understandings, showed suspectibility to misinformation, tended towards monotonic responses after an extended period of play, and struggled to offer detailed biographical information. Despite these limitations, the study's findings open up new research avenues at the crossroads of artificial intelligence, machine learning, LLMs, ethnography, anthropology and human-computer interaction.",
        "subjects": [
            "cs.HC",
            "cs.AI",
            "cs.CY"
        ],
        "comment": "Accepted at KUI 2024"
    },
    {
        "paper id": "2410.20543",
        "abstract url": "https://arxiv.org/abs/2410.20543",
        "title": "Investigation into the Spread of Misinformation about UK Prime Ministers on Twitter",
        "rating": "-1.5",
        "keywords": [
            [
                "bio",
                "health",
                "healthcare"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Misinformation presents threats to societal mental well-being, public health initiatives, as well as satisfaction in democracy. Those who spread misinformation can leverage cognitive biases to make others more likely to believe and share their misinformation unquestioningly. For example, by sharing misinformation whilst claiming to be someone from a highly respectable profession, a propagandist may seek to increase the effectiveness of their campaign using authority bias. Using retweet data from the spread of misinformation about two former UK Prime Ministers (Boris Johnson and Theresa May), we find that 3.1% of those who retweeted such misinformation claimed to be teachers or lecturers (20.7% of those who claimed to have a profession in their Twitter bio field in our sample), despite such professions representing under 1.15% of the UK population. Whilst polling data shows teachers and healthcare workers are amongst the most trusted professions in society, these were amongst the most popular professions that those in our sample claimed to have.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20558",
        "abstract url": "https://arxiv.org/abs/2410.20558",
        "title": "Neural rendering enables dynamic tomography",
        "rating": "-1.5",
        "keywords": [
            [
                "3d",
                "radiance fields"
            ],
            [
                "CT",
                "X-ray"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Interrupted X-ray computed tomography (X-CT) has been the common way to observe the deformation of materials during an experiment. While this approach is effective for quasi-static experiments, it has never been possible to reconstruct a full 3d tomography during a dynamic experiment which cannot be interrupted. In this work, we propose that neural rendering tools can be used to drive the paradigm shift to enable 3d reconstruction during dynamic events. First, we derive theoretical results to support the selection of projections angles. Via a combination of synthetic and experimental data, we demonstrate that neural radiance fields can reconstruct data modalities of interest more efficiently than conventional reconstruction methods. Finally, we develop a spatio-temporal model with spline-based deformation field and demonstrate that such model can reconstruct the spatio-temporal deformation of lattice samples in real-world experiments.",
        "subjects": [
            "physics.ins-det",
            "cond-mat.mtrl-sci",
            "cs.CV",
            "cs.LG",
            "eess.IV"
        ],
        "comment": "24 pages, 14 figures. Submitted to NeurIPS 2024 ML4PS. For associated visualizations, see https://neural-xray.github.io/nerfxray"
    },
    {
        "paper id": "2410.20594",
        "abstract url": "https://arxiv.org/abs/2410.20594",
        "title": "Towards a Blockchain and Opportunistic Edge Driven Metaverse of Everything",
        "rating": "-1.5",
        "keywords": [
            [
                "IoT"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "Decentralized Metaverses, built on Web 3.0 and Web 4.0 technologies, have attracted significant attention across various fields. This innovation leverages blockchain, Decentralized Autonomous Organizations (DAOs), Extended Reality (XR) and advanced technologies to create immersive and interconnected digital environments that mirror the real world. This article delves into the Metaverse of Everything (MoE), a platform that fuses the Metaverse concept with the Internet of Everything (IoE), an advanced version of the Internet of Things (IoT) that connects not only physical devices but also people, data and processes within a networked environment. Thus, the MoE integrates generated data and virtual entities, creating an extensive network of interconnected components. This article seeks to advance current MoE, examining decentralization and the application of Opportunistic Edge Computing (OEC) for interactions with surrounding IoT devices and IoE entities. Moreover, it outlines the main challenges to guide researchers and businesses towards building a future cyber-resilient opportunistic MoE.",
        "subjects": [
            "cs.CY",
            "cs.CR",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20600",
        "abstract url": "https://arxiv.org/abs/2410.20600",
        "title": "Implementation and Application of an Intelligibility Protocol for Interaction with an LLM",
        "rating": "-1.5",
        "keywords": [
            [
                "radiology"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Our interest is in constructing interactive systems involving a human-expert interacting with a machine learning engine on data analysis tasks. This is of relevance when addressing complex problems arising in areas of science, the environment, medicine and so on, which are not immediately amenable to the usual methods of statistical or mathematical modelling. In such situations, it is possible that harnessing human expertise and creativity to modern machine-learning capabilities of identifying patterns by constructing new internal representations of the data may provide some insight to possible solutions. In this paper, we examine the implementation of an abstract protocol developed for interaction between agents, each capable of constructing predictions and explanations. The \\PXP protocol, described in [12] is motivated by the notion of ''two-way intelligibility'' and is specified using a pair of communicating finite-state machines. While the formalisation allows the authors to prove several properties about the protocol, no implementation was presented. Here, we address this shortcoming for the case in which one of the agents acts as a ''generator'' using a large language model (LLM) and the other is an agent that acts as a ''tester'' using either a human-expert, or a proxy for a human-expert (for example, a database compiled using human-expertise). We believe these use-cases will be a widely applicable form of interaction for problems of the kind mentioned above. We present an algorithmic description of general-purpose implementation, and conduct preliminary experiments on its use in two different areas (radiology and drug-discovery). The experimental results provide early evidence in support of the protocol's capability of capturing one- and two-way intelligibility in human-LLM in the manner proposed in [12].",
        "subjects": [
            "cs.AI",
            "cs.HC",
            "cs.LG",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20636",
        "abstract url": "https://arxiv.org/abs/2410.20636",
        "title": "Language Models And A Second Opinion Use Case: The Pocket Professional",
        "rating": "-1.5",
        "keywords": [
            [
                "medical",
                "clinical"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This research tests the role of Large Language Models (LLMs) as formal second opinion tools in professional decision-making, particularly focusing on complex medical cases where even experienced physicians seek peer consultation. The work analyzed 183 challenging medical cases from Medscape over a 20-month period, testing multiple LLMs' performance against crowd-sourced physician responses. A key finding was the high overall score possible in the latest foundational models (>80% accuracy compared to consensus opinion), which exceeds most human metrics reported on the same clinical cases (450 pages of patient profiles, test results). The study rates the LLMs' performance disparity between straightforward cases (>81% accuracy) and complex scenarios (43% accuracy), particularly in these cases generating substantial debate among human physicians. The research demonstrates that LLMs may be valuable as generators of comprehensive differential diagnoses rather than as primary diagnostic tools, potentially helping to counter cognitive biases in clinical decision-making, reduce cognitive loads, and thus remove some sources of medical error. The inclusion of a second comparative legal dataset (Supreme Court cases, N=21) provides added empirical context to the AI use to foster second opinions, though these legal challenges proved considerably easier for LLMs to analyze. In addition to the original contributions of empirical evidence for LLM accuracy, the research aggregated a novel benchmark for others to score highly contested question and answer reliability between both LLMs and disagreeing human practitioners. These results suggest that the optimal deployment of LLMs in professional settings may differ substantially from current approaches that emphasize automation of routine tasks.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20711",
        "abstract url": "https://arxiv.org/abs/2410.20711",
        "title": "Contextual Representation Anchor Network to Alleviate Selection Bias in Few-Shot Drug Discovery",
        "rating": "-1.5",
        "keywords": [
            [
                "chemical"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In the drug discovery process, the low success rate of drug candidate screening often leads to insufficient labeled data, causing the few-shot learning problem in molecular property prediction. Existing methods for few-shot molecular property prediction overlook the sample selection bias, which arises from non-random sample selection in chemical experiments. This bias in data representativeness leads to suboptimal performance. To overcome this challenge, we present a novel method named contextual representation anchor Network (CRA), where an anchor refers to a cluster center of the representations of molecules and serves as a bridge to transfer enriched contextual knowledge into molecular representations and enhance their expressiveness. CRA introduces a dual-augmentation mechanism that includes context augmentation, which dynamically retrieves analogous unlabeled molecules and captures their task-specific contextual knowledge to enhance the anchors, and anchor augmentation, which leverages the anchors to augment the molecular representations. We evaluate our approach on the MoleculeNet and FS-Mol benchmarks, as well as in domain transfer experiments. The results demonstrate that CRA outperforms the state-of-the-art by 2.60% and 3.28% in AUC and $\u0394$AUC-PR metrics, respectively, and exhibits superior generalization capabilities.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "q-bio.BM"
        ],
        "comment": "13 pages, 7 figures"
    },
    {
        "paper id": "2410.20730",
        "abstract url": "https://arxiv.org/abs/2410.20730",
        "title": "GPRec: Bi-level User Modeling for Deep Recommenders",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "GPRec explicitly categorizes users into groups in a learnable manner and aligns them with corresponding group embeddings. We design the dual group embedding space to offer a diverse perspective on group preferences by contrasting positive and negative patterns. On the individual level, GPRec identifies personal preferences from ID-like features and refines the obtained individual representations to be independent of group ones, thereby providing a robust complement to the group-level modeling. We also present various strategies for the flexible integration of GPRec into various DRS models. Rigorous testing of GPRec on three public datasets has demonstrated significant improvements in recommendation quality.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.21328",
        "abstract url": "https://arxiv.org/abs/2410.21328",
        "title": "Deconfounding Time Series Forecasting",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Time series forecasting is a critical task in various domains, where accurate predictions can drive informed decision-making. Traditional forecasting methods often rely on current observations of variables to predict future outcomes, typically overlooking the influence of latent confounders, unobserved variables that simultaneously affect both the predictors and the target outcomes. This oversight can introduce bias and degrade the performance of predictive models. In this study, we address this challenge by proposing an enhanced forecasting approach that incorporates representations of latent confounders derived from historical data. By integrating these confounders into the predictive process, our method aims to improve the accuracy and robustness of time series forecasts. The proposed approach is demonstrated through its application to climate science data, showing significant improvements over traditional methods that do not account for confounders.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.21329",
        "abstract url": "https://arxiv.org/abs/2410.21329",
        "title": "CloudCast -- Total Cloud Cover Nowcasting with Machine Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting",
                "satellite"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Cloud cover plays a critical role in weather prediction and impacts several sectors, including agriculture, solar power generation, and aviation. Despite advancements in numerical weather prediction (NWP) models, forecasting total cloud cover remains challenging due to the small-scale nature of cloud formation processes. In this study, we introduce CloudCast, a convolutional neural network (CNN) based on the U-Net architecture, designed to predict total cloud cover (TCC) up to five hours ahead. Trained on five years of satellite data, CloudCast significantly outperforms traditional NWP models and optical flow methods. Compared to a reference NWP model, CloudCast achieves a 24% lower mean absolute error and reduces multi-category prediction errors by 46%. The model demonstrates strong performance, particularly in capturing the large-scale structure of cloud cover in the first few forecast hours, though later predictions are subject to blurring and underestimation of cloud formation. An ablation study identified the optimal input features and loss functions, with MAE-based models performing the best. CloudCast has been integrated into the Finnish Meteorological Institute's operational nowcasting system, where it improves cloud cover forecasts used by public and private sector clients. While CloudCast is limited by a relatively short skillful lead time of about three hours, future work aims to extend this through more complex network architectures and higher-resolution data. CloudCast code is available at https://github.com/fmidev/cloudcast.",
        "subjects": [
            "physics.ao-ph",
            "cs.LG"
        ],
        "comment": "27 pages, 12 figures"
    },
    {
        "paper id": "2410.21342",
        "abstract url": "https://arxiv.org/abs/2410.21342",
        "title": "Heterogeneous Interaction Modeling With Reduced Accumulated Error for Multi-Agent Trajectory Prediction",
        "rating": "-1.5",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "graphs"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Dynamical complex systems composed of interactive heterogeneous agents are prevalent in the world, including urban traffic systems and social networks. Modeling the interactions among agents is the key to understanding and predicting the dynamics of the complex system, e.g., predicting the trajectories of traffic participants in the city. Compared with interaction modeling in homogeneous systems such as pedestrians in a crowded scene, heterogeneous interaction modeling is less explored. Worse still, the error accumulation problem becomes more severe since the interactions are more complex. To tackle the two problems, this paper proposes heterogeneous interaction modeling with reduced accumulated error for multi-agent trajectory prediction. Based on the historical trajectories, our method infers the dynamic interaction graphs among agents, featured by directed interacting relations and interacting effects. A heterogeneous attention mechanism is defined on the interaction graphs for aggregating the influence from heterogeneous neighbors to the target agent. To alleviate the error accumulation problem, this paper analyzes the error sources from the spatial and temporal perspectives, and proposes to introduce the graph entropy and the mixup training strategy for reducing the two types of errors respectively. Our method is examined on three real-world datasets containing heterogeneous agents, and the experimental results validate the superiority of our method.",
        "subjects": [
            "cs.MA",
            "cs.AI"
        ],
        "comment": "20 pages, accepted by IEEE TNNLS"
    },
    {
        "paper id": "2410.20374",
        "abstract url": "https://arxiv.org/abs/2410.20374",
        "title": "A CT-guided Control Framework of a Robotic Flexible Endoscope for the Diagnosis of the Maxillary Sinusitis",
        "rating": "-2",
        "keywords": [
            [
                "Diagnosis",
                "CT"
            ]
        ],
        "abstract": "Flexible endoscopes are commonly adopted in narrow and confined anatomical cavities due to their higher reachability and dexterity. However, prolonged and unintuitive manipulation of these endoscopes leads to an increased workload on surgeons and risks of collision. To address these challenges, this paper proposes a CT-guided control framework for the diagnosis of maxillary sinusitis by using a robotic flexible endoscope. In the CT-guided control framework, a feasible path to the target position in the maxillary sinus cavity for the robotic flexible endoscope is designed. Besides, an optimal control scheme is proposed to autonomously control the robotic flexible endoscope to follow the feasible path. This greatly improves the efficiency and reduces the workload for surgeons. Several experiments were conducted based on a widely utilized sinus phantom, and the results showed that the robotic flexible endoscope can accurately and autonomously follow the feasible path and reach the target position in the maxillary sinus cavity. The results also verified the feasibility of the CT-guided control framework, which contributes an effective approach to early diagnosis of sinusitis in the future.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20389",
        "abstract url": "https://arxiv.org/abs/2410.20389",
        "title": "Lodge++: High-quality and Long Dance Generation with Vivid Choreography Patterns",
        "rating": "-2",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "music"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "We propose Lodge++, a choreography framework to generate high-quality, ultra-long, and vivid dances given the music and desired genre. To handle the challenges in computational efficiency, the learning of complex and vivid global choreography patterns, and the physical quality of local dance movements, Lodge++ adopts a two-stage strategy to produce dances from coarse to fine. In the first stage, a global choreography network is designed to generate coarse-grained dance primitives that capture complex global choreography patterns. In the second stage, guided by these dance primitives, a primitive-based dance diffusion model is proposed to further generate high-quality, long-sequence dances in parallel, faithfully adhering to the complex choreography patterns. Additionally, to improve the physical plausibility, Lodge++ employs a penetration guidance module to resolve character self-penetration, a foot refinement module to optimize foot-ground contact, and a multi-genre discriminator to maintain genre consistency throughout the dance. Lodge++ is validated by extensive experiments, which show that our method can rapidly generate ultra-long dances suitable for various dance genres, ensuring well-organized global choreography patterns and high-quality local motion.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.GR"
        ],
        "comment": "Project page: https://li-ronghui.github.io/lodgepp"
    },
    {
        "paper id": "2410.20430",
        "abstract url": "https://arxiv.org/abs/2410.20430",
        "title": "Architectural Solutions for High-Speed Data Processing Demands of CERN LHC Detectors with FPGA and High-Level Synthesis",
        "rating": "-2",
        "keywords": [
            [
                "FPGA"
            ]
        ],
        "abstract": "The planned high-luminosity upgrade of the Large Hadron Collider (LHC) at CERN will bring much higher data rates that are far above the capabilities of currently installed software-based data processing systems. Therefore, new methods must be used to facilitate on-the-fly extraction of scientifically significant information from the immense flow of data produced by LHC particle detectors. This paper focuses on implementation of a tau lepton triggering algorithm in FPGA. Due to the algorithm's complexity and strict technical requirements, its implementation in FPGA fabric becomes a particularly challenging task. The paper presents a study of algorithm development with the help of High-Level Synthesis (HLS) technique that can generate hardware description from C++ code. Various architectural solutions and optimizations that were tried out during the design architecture exploration process are also discussed in the paper.",
        "subjects": [
            "cs.AR",
            "physics.ins-det"
        ],
        "comment": "7 pages, 6 figures, 2024 IEEE Nordic Circuits and Systems Conference (NORCAS)"
    },
    {
        "paper id": "2410.20528",
        "abstract url": "https://arxiv.org/abs/2410.20528",
        "title": "Characterization of Noise using variants of Unitarity Randomized Benchmarking",
        "rating": "-2",
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "Benchmarking of noise that is induced during the implementation of quantum gates is the main concern for practical quantum computers. Several protocols have been proposed that empirically calculate various metrics that quantify the error rates of the quantum gates chosen from a preferred gate set. Unitarity randomized benchmarking (URB) protocol is a method to estimate the coherence of noise induced by the quantum gates which is measured by the metric \\textit{unitarity}. In this paper, we for the first time, implement the URB protocol in a quantum simulator with all the parameters and noise model are used from a real quantum device. The direct implementation of the URB protocol in a quantum device is not possible using current technologies, as it requires the preparation of mixed states. To overcome this challenge, we propose a modification of the URB protocol, namely the m-URB protocol, that enables us to practically implement it on any quantum device. We validate our m-URB protocol using two single-qubit noise channels -- (a) depolarising channel and (b) bit-flip channel. We further alter the m-URB protocol, namely, native gate URB or Ng-URB protocol, to study the noise in the native gates into which the quantum circuits are compiled in a quantum computer. Using our Ng-URB protocol, we can also detect the presence of cross-talk errors which are correlated errors caused due to non-local and entangling gates such as CNOT gate. For illustration, we simulate the noise of the native gates taking the noise parameter from two real IBM-Q processors.",
        "subjects": [
            "quant-ph",
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20547",
        "abstract url": "https://arxiv.org/abs/2410.20547",
        "title": "A universal bound on the space complexity of Directed Acyclic Graph computations",
        "rating": "-2",
        "keywords": [
            [
                "depth"
            ],
            [
                "Graph"
            ]
        ],
        "abstract": "It is shown that $S(G) = O\\left(m/\\log_2 m + d\\right)$ pebbles are sufficient to pebble any DAG $G=(V,E)$, with $m$ edges and maximum in-degree $d$. It was previously known that $S(G) = O\\left(d n/\\log n\\right)$. The result builds on two novel ideas. The first is the notion of $B-budget\\ decomposition$ of a DAG $G$, an efficiently computable partition of $G$ into at most $2^{\\lfloor \\frac{m}{B} \\rfloor}$ sub-DAGs, whose cumulative space requirement is at most $B$. The second is the challenging vertices technique, which constructs a pebbling schedule for $G$ from a pebbling schedule for a simplified DAG $G'$, obtained by removing from $G$ a selected set of vertices $W$ and their incident edges. This technique also yields improved pebbling upper bounds for DAGs with bounded genus and for DAGs with bounded topological depth.",
        "subjects": [
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20552",
        "abstract url": "https://arxiv.org/abs/2410.20552",
        "title": "SympCam: Remote Optical Measurement of Sympathetic Arousal",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "facial"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Recent work has shown that a person's sympathetic arousal can be estimated from facial videos alone using basic signal processing. This opens up new possibilities in the field of telehealth and stress management, providing a non-invasive method to measure stress only using a regular RGB camera. In this paper, we present SympCam, a new 3D convolutional architecture tailored to the task of remote sympathetic arousal prediction. Our model incorporates a temporal attention module (TAM) to enhance the temporal coherence of our sequential data processing capabilities. The predictions from our method improve accuracy metrics of sympathetic arousal in prior work by 48% to a mean correlation of 0.77. We additionally compare our method with common remote photoplethysmography (rPPG) networks and show that they alone cannot accurately predict sympathetic arousal \"out-of-the-box\". Furthermore, we show that the sympathetic arousal predicted by our method allows detecting physical stress with a balanced accuracy of 90% - an improvement of 61% compared to the rPPG method commonly used in related work, demonstrating the limitations of using rPPG alone. Finally, we contribute a dataset designed explicitly for the task of remote sympathetic arousal prediction. Our dataset contains synchronized face and hand videos of 20 participants from two cameras synchronized with electrodermal activity (EDA) and photoplethysmography (PPG) measurements. We will make this dataset available to the community and use it to evaluate the methods in this paper. To the best of our knowledge, this is the first dataset available to other researchers designed for remote sympathetic arousal prediction.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Accepted for publication at the IEEE-EMBS International Conference on Biomedical and Health Informatics"
    },
    {
        "paper id": "2410.20555",
        "abstract url": "https://arxiv.org/abs/2410.20555",
        "title": "Privacy-Enhanced Adaptive Authentication: User Profiling with Privacy Guarantees",
        "rating": "-2",
        "keywords": [
            [
                "re-identification"
            ],
            [
                "attacks"
            ]
        ],
        "abstract": "User profiling is a critical component of adaptive risk-based authentication, yet it raises significant privacy concerns, particularly when handling sensitive data. Profiling involves collecting and aggregating various user features, potentially creating quasi-identifiers that can reveal identities and compromise privacy. Even anonymized profiling methods remain vulnerable to re-identification attacks through these quasi-identifiers. This paper introduces a novel privacy-enhanced adaptive authentication protocol that leverages Oblivious Pseudorandom Functions (OPRF), anonymous tokens, and Differential Privacy (DP) to provide robust privacy guarantees. Our proposed approach dynamically adjusts authentication requirements based on real-time risk assessments, enhancing security while safeguarding user privacy. By integrating privacy considerations into the core of adaptive risk-based adaptive authentication, this approach addresses a gap often overlooked in traditional models. Advanced cryptographic techniques ensure confidentiality, integrity, and unlinkability of user data, while differential privacy mechanisms minimize the impact of individual data points on overall analysis. Formal security and privacy proofs demonstrate the protocol's resilience against various threats and its ability to provide strong privacy guarantees. Additionally, a comprehensive performance evaluation reveals that the computational and communication overheads are manageable, making the protocol practical for real-world deployment. By adhering to data protection regulations such as GDPR and CCPA, our protocol not only enhances security but also fosters user trust and compliance with legal standards.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20568",
        "abstract url": "https://arxiv.org/abs/2410.20568",
        "title": "Detection of adrenal anomalous findings in spinal CT images using multi model graph aggregation",
        "rating": "-2",
        "keywords": [
            [
                "graph"
            ],
            [
                "MRI",
                "CT"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Low back pain is the symptom that is the second most frequently reported to primary care physicians, effecting 50 to 80 percent of the population in a lifetime, resulting in multiple referrals of patients suffering from back problems, to CT and MRI scans, which are then examined by radiologists. The radiologists examining these spinal scans naturally focus on spinal pathologies and might miss other types of abnormalities, and in particular, abdominal ones, such as malignancies. Nevertheless, the patients whose spine was scanned might as well have malignant and other abdominal pathologies. Thus, clinicians have suggested the need for computerized assistance and decision support in screening spinal scans for additional abnormalities. In the current study, We have addressed the important case of detecting suspicious lesions in the adrenal glands as an example for the overall methodology we have developed. A patient CT scan is integrated from multiple slices with an axial orientation. Our method determines whether a patient has an abnormal adrenal gland, and localises the abnormality if it exists. Our method is composed of three deep learning models; each model has a different task for achieving the final goal. We call our compound method the Multi Model Graph Aggregation MMGA method. The novelty in this study is twofold. First, the use, for an important screening task, of CT scans that are originally focused and tuned for imaging the spine, which were acquired from patients with potential spinal disorders, for detection of a totally different set of abnormalities such as abdominal Adrenal glands pathologies. Second, we have built a complex pipeline architecture composed from three deep learning models that can be utilized for other organs (such as the pancreas or the kidney), or for similar applications, but using other types of imaging, such as MRI.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20575",
        "abstract url": "https://arxiv.org/abs/2410.20575",
        "title": "Encrypted system identification as-a-service via reliable encrypted matrix inversion",
        "rating": "-2",
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "Encrypted computation opens up promising avenues across a plethora of application domains, including machine learning, health-care, finance, and control. Arithmetic homomorphic encryption, in particular, is a natural fit for cloud-based computational services. However, computations are essentially limited to polynomial circuits, while comparisons, transcendental functions, and iterative algorithms are notoriously hard to realize. Against this background, the paper presents an encrypted system identification service enabled by a reliable encrypted solution to least squares problems. More precisely, we devise an iterative algorithm for matrix inversion and present reliable initializations as well as certificates for the achieved accuracy without compromising the privacy of provided I/O-data. The effectiveness of the approach is illustrated with three popular identification tasks.",
        "subjects": [
            "cs.CR",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20580",
        "abstract url": "https://arxiv.org/abs/2410.20580",
        "title": "Coherence-guided Preference Disentanglement for Cross-domain Recommendations",
        "rating": "-2",
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "Discovering user preferences across different domains is pivotal in cross-domain recommendation systems, particularly when platforms lack comprehensive user-item interactive data. The limited presence of shared users often hampers the effective modeling of common preferences. While leveraging shared items' attributes, such as category and popularity, can enhance cross-domain recommendation performance, the scarcity of shared items between domains has limited research in this area. To address this, we propose a Coherence-guided Preference Disentanglement (CoPD) method aimed at improving cross-domain recommendation by i) explicitly extracting shared item attributes to guide the learning of shared user preferences and ii) disentangling these preferences to identify specific user interests transferred between domains. CoPD introduces coherence constraints on item embeddings of shared and specific domains, aiding in extracting shared attributes. Moreover, it utilizes these attributes to guide the disentanglement of user preferences into separate embeddings for interest and conformity through a popularity-weighted loss. Experiments conducted on real-world datasets demonstrate the superior performance of our proposed CoPD over existing competitive baselines, highlighting its effectiveness in enhancing cross-domain recommendation performance.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "28 pages"
    },
    {
        "paper id": "2410.20584",
        "abstract url": "https://arxiv.org/abs/2410.20584",
        "title": "Aerodynamics and Sensing Analysis for Efficient Drone-Based Parcel Delivery",
        "rating": "-2",
        "keywords": [
            [
                "Drone"
            ]
        ],
        "abstract": "In an era of rapid urbanization and e-commerce growth, efficient parcel delivery methods are crucial. This paper presents a detailed study of the aerodynamics and sensing analysis of drones for parcel delivery. Utilizing Computational Fluid Dynamics (CFD), the study offers a comprehensive airflow analysis, revealing the aerodynamic forces affecting drone stability due to payload capacity. A multidisciplinary approach is employed, integrating mechanical design, control theory, and sensing systems to address the complex issue of parcel positioning. The experimental validation section rigorously tests different size payloads and their positions and impact on drones with maximum thrusts of 2000 gf. The findings prove the drone's capacity to lift a large payload that covers up to 50 percent of the propeller, thereby contributing to optimizing drone designs and sustainable parcel delivery systems. It has been observed that the drone can lift a large payload smoothly when placed above the drone, with an error rate as low as 0.1 percent for roll, pitch, and yaw. This work paved the way for more versatile, real-world applications of drone technology, setting a new standard in the field.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": "6 pages, 9 figures"
    },
    {
        "paper id": "2410.20611",
        "abstract url": "https://arxiv.org/abs/2410.20611",
        "title": "Zero-Trust Network Access (ZTNA)",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "Zero-Trust Network Access (ZTNA) marks a significant shift in network security by adopting a \"never trust, always verify\" approach. This work provides an in-depth analysis of ZTNA, offering a comprehensive framework for understanding its principles, architectures, and applications. We discuss its role in securing modern, complex network environments, which include cloud platforms, Internet of Things (IoT) devices, and hybrid enterprise networks. Our objective is to create a key resource for researchers and practitioners by reviewing critical methodologies, analyzing current implementations, and highlighting open challenges and research directions.",
        "subjects": [
            "cs.CR",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20643",
        "abstract url": "https://arxiv.org/abs/2410.20643",
        "title": "GenUP: Generative User Profilers as In-Context Learners for Next POI Recommender Systems",
        "rating": "-2",
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "Traditional POI recommendation systems often lack transparency, interpretability, and scrutability due to their reliance on dense vector-based user embeddings. Furthermore, the cold-start problem -- where systems have insufficient data for new users -- limits their ability to generate accurate recommendations. Existing methods often address this by leveraging similar trajectories from other users, but this approach can be computationally expensive and increases the context length for LLM-based methods, making them difficult to scale. To address these limitations, we propose a method that generates natural language (NL) user profiles from large-scale, location-based social network (LBSN) check-ins, utilizing robust personality assessments and behavioral theories. These NL profiles capture user preferences, routines, and behaviors, improving POI prediction accuracy while offering enhanced transparency. By incorporating NL profiles as system prompts to LLMs, our approach reduces reliance on extensive historical data, while remaining flexible, easily updated, and computationally efficient. Our method is not only competitive with other LLM-based and complex agentic frameworks but is also more scalable for real-world scenarios and on-device POI recommendations. Results demonstrate that our approach consistently outperforms baseline methods, offering a more interpretable and resource-efficient solution for POI recommendation systems. Our source code is available at: \\url{https://github.com/w11wo/GenUP}.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20696",
        "abstract url": "https://arxiv.org/abs/2410.20696",
        "title": "\"So Am I Dr. Frankenstein? Or Were You a Monster the Whole Time?\": Mitigating Software Project Failure With Loss-Aversion-Aware Development Methodologies",
        "rating": "-2",
        "keywords": [
            [
                "psychological"
            ]
        ],
        "abstract": "Case studies have shown that software disasters snowball from technical issues to catastrophes through humans covering up problems rather than addressing them and empirical research has found the psychological safety of software engineers to discuss and address problems to be foundational to improving project success. However, the failure to do so can be attributed to psychological factors like loss aversion. We conduct a large-scale study of the experiences of 600 software engineers in the UK and USA on project success experiences. Empirical evaluation finds that approaches like ensuring clear requirements before the start of development, when loss aversion is at its lowest, correlated to 97\\% higher project success. The freedom of software engineers to discuss and address problems correlates with 87\\% higher success rates. The findings support the development of software development methodologies with a greater focus on human factors in preventing failure.",
        "subjects": [
            "cs.SE",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20697",
        "abstract url": "https://arxiv.org/abs/2410.20697",
        "title": "Narrow Passage Path Planning using Collision Constraint Interpolation",
        "rating": "-2",
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "Narrow passage path planning is a prevalent problem from industrial to household sites, often facing difficulties in finding feasible paths or requiring excessive computational resources. Given that deep penetration into the environment can cause optimization failure, we propose a framework to ensure feasibility throughout the process using a series of subproblems tailored for narrow passage problem. We begin by decomposing the environment into convex objects and initializing collision constraints with a subset of these objects. By continuously interpolating the collision constraints through the process of sequentially introducing remaining objects, our proposed framework generates subproblems that guide the optimization toward solving the narrow passage problem. Several examples are presented to demonstrate how the proposed framework addresses narrow passage path planning problems.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages, 7 figure"
    },
    {
        "paper id": "2410.21330",
        "abstract url": "https://arxiv.org/abs/2410.21330",
        "title": "LLM Robustness Against Misinformation in Biomedical Question Answering",
        "rating": "-2",
        "keywords": [
            [
                "attacks"
            ],
            [
                "Biomedical"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The retrieval-augmented generation (RAG) approach is used to reduce the confabulation of large language models (LLMs) for question answering by retrieving and providing additional context coming from external knowledge sources (e.g., by adding the context to the prompt). However, injecting incorrect information can mislead the LLM to generate an incorrect answer. In this paper, we evaluate the effectiveness and robustness of four LLMs against misinformation - Gemma 2, GPT-4o-mini, Llama~3.1, and Mixtral - in answering biomedical questions. We assess the answer accuracy on yes-no and free-form questions in three scenarios: vanilla LLM answers (no context is provided), \"perfect\" augmented generation (correct context is provided), and prompt-injection attacks (incorrect context is provided). Our results show that Llama 3.1 (70B parameters) achieves the highest accuracy in both vanilla (0.651) and \"perfect\" RAG (0.802) scenarios. However, the accuracy gap between the models almost disappears with \"perfect\" RAG, suggesting its potential to mitigate the LLM's size-related effectiveness differences. We further evaluate the ability of the LLMs to generate malicious context on one hand and the LLM's robustness against prompt-injection attacks on the other hand, using metrics such as attack success rate (ASR), accuracy under attack, and accuracy drop. As adversaries, we use the same four LLMs (Gemma 2, GPT-4o-mini, Llama 3.1, and Mixtral) to generate incorrect context that is injected in the target model's prompt. Interestingly, Llama is shown to be the most effective adversary, causing accuracy drops of up to 0.48 for vanilla answers and 0.63 for \"perfect\" RAG across target models. Our analysis reveals that robustness rankings vary depending on the evaluation measure, highlighting the complexity of assessing LLM resilience to adversarial attacks.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.22363",
        "abstract url": "https://arxiv.org/abs/2410.22363",
        "title": "Branch-and-bound algorithm for efficient reliability analysis of general coherent systems",
        "rating": "-2",
        "keywords": [
            [
                "survival"
            ]
        ],
        "abstract": "Branch and bound algorithms have been developed for reliability analysis of coherent systems. They exhibit a set of advantages; in particular, they can find a computationally efficient representation of a system failure or survival event, which can be re-used when the input probability distributions change over time or when new data is available. However, existing branch-and-bound algorithms can handle only a limited set of system performance functions, mostly network connectivity and maximum flow. Furthermore, they run redundant analyses on component vector states whose system state can be inferred from previous analysis results. This study addresses these limitations by proposing branch and bound for reliability analysis of general coherent systems} (BRC) algorithm: an algorithm that automatically finds minimal representations of failure/survival events of general coherent systems. Computational efficiency is attained by dynamically inferring importance of component events from hitherto obtained results. We demonstrate advantages of the BRC method as a real-time risk management tool by application to the Eastern Massachusetts highway benchmark network.",
        "subjects": [
            "math.OC",
            "eess.SY"
        ],
        "comment": "Preprint for peer-reviewed article"
    },
    {
        "paper id": "2410.20626",
        "abstract url": "https://arxiv.org/abs/2410.20626",
        "title": "TabDiff: a Multi-Modal Diffusion Model for Tabular Data Generation",
        "rating": "-2.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Tabular"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Synthesizing high-quality tabular data is an important topic in many data science tasks, ranging from dataset augmentation to privacy protection. However, developing expressive generative models for tabular data is challenging due to its inherent heterogeneous data types, complex inter-correlations, and intricate column-wise distributions. In this paper, we introduce TabDiff, a joint diffusion framework that models all multi-modal distributions of tabular data in one model. Our key innovation is the development of a joint continuous-time diffusion process for numerical and categorical data, where we propose feature-wise learnable diffusion processes to counter the high disparity of different feature distributions. TabDiff is parameterized by a transformer handling different input types, and the entire framework can be efficiently optimized in an end-to-end fashion. We further introduce a multi-modal stochastic sampler to automatically correct the accumulated decoding error during sampling, and propose classifier-free guidance for conditional missing column value imputation. Comprehensive experiments on seven datasets demonstrate that TabDiff achieves superior average performance over existing competitive baselines across all eight metrics, with up to $22.5\\%$ improvement over the state-of-the-art model on pair-wise column correlation estimations. Code is available at https://github.com/MinkaiXu/TabDiff.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20735",
        "abstract url": "https://arxiv.org/abs/2410.20735",
        "title": "Murine AI excels at cats and cheese: Structural differences between human and mouse neurons and their implementation in generative AIs",
        "rating": "-2.5",
        "keywords": [
            [
                "diffusion",
                "GAN"
            ],
            [
                "biological"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Mouse and human brains have different functions that depend on their neuronal networks. In this study, we analyzed nanometer-scale three-dimensional structures of brain tissues of the mouse medial prefrontal cortex and compared them with structures of the human anterior cingulate cortex. The obtained results indicated that mouse neuronal somata are smaller and neurites are thinner than those of human neurons. These structural features allow mouse neurons to be integrated in the limited space of the brain, though thin neurites should suppress distal connections according to cable theory. We implemented this mouse-mimetic constraint in convolutional layers of a generative adversarial network (GAN) and a denoising diffusion implicit model (DDIM), which were then subjected to image generation tasks using photo datasets of cat faces, cheese, human faces, and birds. The mouse-mimetic GAN outperformed a standard GAN in the image generation task using the cat faces and cheese photo datasets, but underperformed for human faces and birds. The mouse-mimetic DDIM gave similar results, suggesting that the nature of the datasets affected the results. Analyses of the four datasets indicated differences in their image entropy, which should influence the number of parameters required for image generation. The preferences of the mouse-mimetic AIs coincided with the impressions commonly associated with mice. The relationship between the neuronal network and brain function should be investigated by implementing other biological findings in artificial neural networks.",
        "subjects": [
            "q-bio.NC",
            "cs.AI",
            "physics.bio-ph"
        ],
        "comment": "41 pages, 4 figures"
    },
    {
        "paper id": "2410.21335",
        "abstract url": "https://arxiv.org/abs/2410.21335",
        "title": "E(3)-invariant diffusion model for pocket-aware peptide generation",
        "rating": "-2.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "Biologists",
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Biologists frequently desire protein inhibitors for a variety of reasons, including use as research tools for understanding biological processes and application to societal problems in agriculture, healthcare, etc. Immunotherapy, for instance, relies on immune checkpoint inhibitors to block checkpoint proteins, preventing their binding with partner proteins and boosting immune cell function against abnormal cells. Inhibitor discovery has long been a tedious process, which in recent years has been accelerated by computational approaches. Advances in artificial intelligence now provide an opportunity to make inhibitor discovery smarter than ever before. While extensive research has been conducted on computer-aided inhibitor discovery, it has mainly focused on either sequence-to-structure mapping, reverse mapping, or bio-activity prediction, making it unrealistic for biologists to utilize such tools. Instead, our work proposes a new method of computer-assisted inhibitor discovery: de novo pocket-aware peptide structure and sequence generation network. Our approach consists of two sequential diffusion models for end-to-end structure generation and sequence prediction. By leveraging angle and dihedral relationships between backbone atoms, we ensure an E(3)-invariant representation of peptide structures. Our results demonstrate that our method achieves comparable performance to state-of-the-art models, highlighting its potential in pocket-aware peptide design. This work offers a new approach for precise drug discovery using receptor-specific peptide generation.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "q-bio.BM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20348",
        "abstract url": "https://arxiv.org/abs/2410.20348",
        "title": "UTSRMorph: A Unified Transformer and Superresolution Network for Unsupervised Medical Image Registration",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "Superresolution"
            ],
            [
                "Medical",
                "CT"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Complicated image registration is a key issue in medical image analysis, and deep learning-based methods have achieved better results than traditional methods. The methods include ConvNet-based and Transformer-based methods. Although ConvNets can effectively utilize local information to reduce redundancy via small neighborhood convolution, the limited receptive field results in the inability to capture global dependencies. Transformers can establish long-distance dependencies via a self-attention mechanism; however, the intense calculation of the relationships among all tokens leads to high redundancy. We propose a novel unsupervised image registration method named the unified Transformer and superresolution (UTSRMorph) network, which can enhance feature representation learning in the encoder and generate detailed displacement fields in the decoder to overcome these problems. We first propose a fusion attention block to integrate the advantages of ConvNets and Transformers, which inserts a ConvNet-based channel attention module into a multihead self-attention module. The overlapping attention block, a novel cross-attention method, uses overlapping windows to obtain abundant correlations with match information of a pair of images. Then, the blocks are flexibly stacked into a new powerful encoder. The decoder generation process of a high-resolution deformation displacement field from low-resolution features is considered as a superresolution process. Specifically, the superresolution module was employed to replace interpolation upsampling, which can overcome feature degradation. UTSRMorph was compared to state-of-the-art registration methods in the 3D brain MR (OASIS, IXI) and MR-CT datasets. The qualitative and quantitative results indicate that UTSRMorph achieves relatively better performance. The code and datasets are publicly available at https://github.com/Runshi-Zhang/UTSRMorph.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "13pages,10 figures"
    },
    {
        "paper id": "2410.20423",
        "abstract url": "https://arxiv.org/abs/2410.20423",
        "title": "A Deconfounding Framework for Human Behavior Prediction: Enhancing Robotic Systems in Dynamic Environments",
        "rating": "-3",
        "keywords": [
            [
                "robot"
            ],
            [
                "forecasting"
            ]
        ],
        "abstract": "Accurate prediction of human behavior is crucial for effective human-robot interaction (HRI) systems, especially in dynamic environments where real-time decisions are essential. This paper addresses the challenge of forecasting future human behavior using multivariate time series data from wearable sensors, which capture various aspects of human movement. The presence of hidden confounding factors in this data often leads to biased predictions, limiting the reliability of traditional models. To overcome this, we propose a robust predictive model that integrates deconfounding techniques with advanced time series prediction methods, enhancing the model's ability to isolate true causal relationships and improve prediction accuracy. Evaluation on real-world datasets demonstrates that our approach significantly outperforms traditional methods, providing a more reliable foundation for responsive and adaptive HRI systems.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages, Under review"
    },
    {
        "paper id": "2410.20660",
        "abstract url": "https://arxiv.org/abs/2410.20660",
        "title": "TurboHopp: Accelerated Molecule Scaffold Hopping with Consistency Models",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "chemical"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Navigating the vast chemical space of druggable compounds is a formidable challenge in drug discovery, where generative models are increasingly employed to identify viable candidates. Conditional 3D structure-based drug design (3D-SBDD) models, which take into account complex three-dimensional interactions and molecular geometries, are particularly promising. Scaffold hopping is an efficient strategy that facilitates the identification of similar active compounds by strategically modifying the core structure of molecules, effectively narrowing the wide chemical space and enhancing the discovery of drug-like products. However, the practical application of 3D-SBDD generative models is hampered by their slow processing speeds. To address this bottleneck, we introduce TurboHopp, an accelerated pocket-conditioned 3D scaffold hopping model that merges the strategic effectiveness of traditional scaffold hopping with rapid generation capabilities of consistency models. This synergy not only enhances efficiency but also significantly boosts generation speeds, achieving up to 30 times faster inference speed as well as superior generation quality compared to existing diffusion-based models, establishing TurboHopp as a powerful tool in drug discovery. Supported by faster inference speed, we further optimize our model, using Reinforcement Learning for Consistency Models (RLCM), to output desirable molecules. We demonstrate the broad applicability of TurboHopp across multiple drug discovery scenarios, underscoring its potential in diverse molecular settings.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "q-bio.BM"
        ],
        "comment": "22 pages, 11 figures, 8 tables. Presented at NeurIPS 2024"
    },
    {
        "paper id": "2410.20695",
        "abstract url": "https://arxiv.org/abs/2410.20695",
        "title": "Combining Domain-Specific Models and LLMs for Automated Disease Phenotyping from Survey Data",
        "rating": "-3",
        "keywords": [
            [
                "biomedical",
                "Disease"
            ],
            [
                "named entity recognition"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This exploratory pilot study investigated the potential of combining a domain-specific model, BERN2, with large language models (LLMs) to enhance automated disease phenotyping from research survey data. Motivated by the need for efficient and accurate methods to harmonize the growing volume of survey data with standardized disease ontologies, we employed BERN2, a biomedical named entity recognition and normalization model, to extract disease information from the ORIGINS birth cohort survey data. After rigorously evaluating BERN2's performance against a manually curated ground truth dataset, we integrated various LLMs using prompt engineering, Retrieval-Augmented Generation (RAG), and Instructional Fine-Tuning (IFT) to refine the model's outputs. BERN2 demonstrated high performance in extracting and normalizing disease mentions, and the integration of LLMs, particularly with Few Shot Inference and RAG orchestration, further improved accuracy. This approach, especially when incorporating structured examples, logical reasoning prompts, and detailed context, offers a promising avenue for developing tools to enable efficient cohort profiling and data harmonization across large, heterogeneous research datasets.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20351",
        "abstract url": "https://arxiv.org/abs/2410.20351",
        "title": "Leveraging Auxiliary Task Relevance for Enhanced Industrial Fault Diagnosis through Curriculum Meta-learning",
        "rating": "-3.5",
        "keywords": [
            [
                "Diagnosis"
            ],
            [
                "Industrial"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The accurate diagnosis of machine breakdowns is crucial for maintaining operational safety in smart manufacturing. Despite the promise shown by deep learning in automating fault identification, the scarcity of labeled training data, particularly for equipment failure instances, poses a significant challenge. This limitation hampers the development of robust classification models. Existing methods like model-agnostic meta-learning (MAML) do not adequately address variable working conditions, affecting knowledge transfer. To address these challenges, a Related Task Aware Curriculum Meta-learning (RT-ACM) enhanced fault diagnosis framework is proposed in this paper, inspired by human cognitive learning processes. RT-ACM improves training by considering the relevance of auxiliary working conditions, adhering to the principle of ``paying more attention to more relevant knowledge\", and focusing on ``easier first, harder later\" curriculum sampling. This approach aids the meta-learner in achieving a superior convergence state. Extensive experiments on two real-world datasets demonstrate the superiority of RT-ACM framework.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20398",
        "abstract url": "https://arxiv.org/abs/2410.20398",
        "title": "Evaluation of uncertainty estimations for Gaussian process regression based machine learning interatomic potentials",
        "rating": "-3.5",
        "keywords": [
            [
                "chemical"
            ],
            [
                "quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine learning interatomic potentials (MLIPs) have seen significant advances as efficient replacement of expensive quantum chemical calculations. Uncertainty estimations for MLIPs are crucial to quantify the additional model error they introduce and to leverage this information in active learning strategies. MLIPs that are based on Gaussian process regression provide a standard deviation as a possible uncertainty measure. An alternative approach are ensemble-based uncertainties. Although these uncertainty measures have been applied to active learning, it has rarely been studied how they correlate with the error, and it is not always clear whether active learning actually outperforms random sampling strategies. We consider GPR models with Coulomb and SOAP representations as inputs to predict potential energy surfaces and excitation energies of molecules. We evaluate, how the GPR variance and ensemble-based uncertainties relate to the error and whether model performance improves by selecting the most uncertain samples from a fixed configuration space. For the ensemble based uncertainty estimations, we find that they often do not provide any information about the error. For the GPR standard deviation, we find that often predictions with an increasing standard deviation also have an increasing systematical bias, which is not captured by the uncertainty. In these cases, selecting training samples with the highest uncertainty leads to a model with a worse test error compared to random sampling. We conclude that confidence intervals, which are derived from the predictive standard deviation, can be highly overconfident. Selecting samples with high GPR standard deviation leads to a model that overemphasizes the borders of the configuration space represented in the fixed dataset. This may result in worse performance in more densely sampled areas but better generalization for extrapolation tasks.",
        "subjects": [
            "cs.LG",
            "physics.chem-ph",
            "physics.comp-ph",
            "q-bio.BM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20627",
        "abstract url": "https://arxiv.org/abs/2410.20627",
        "title": "DHPrep: Deep Hawkes Process based Dynamic Network Representation",
        "rating": "-3.5",
        "keywords": [
            [
                "biological"
            ],
            [
                "recommendation"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Networks representation aims to encode vertices into a low-dimensional space, while preserving the original network structures and properties. Most existing methods focus on static network structure without considering temporal dynamics. However, in real world, most networks (e.g., social and biological networks) are dynamic in nature and are constantly evolving over time. Such temporal dynamics are critical in representations learning, especially for predicting dynamic networks behaviors. To this end, a Deep Hawkes Process based Dynamic Networks Representation algorithm (DHPrep) is proposed in this paper, which is capable of capturing temporal dynamics of dynamic networks. Specifically, DHPrep incorporates both structural information and temporal dynamics to learn vertices representations that can model the edge formation process for a vertex pair, where the structural information is used to capture the historical impact from their neighborhood, and the temporal dynamics utilize this historical information and apply Hawkes point process to model the edges formation process. Moreover, a temporal smoother is further imposed to ensure the representations evolve smoothly over time. To evaluate the effectiveness of DHPrep, extensive experiments are carried out using four real-world datasets. Experimental results reveal that our DHPrep algorithm outperforms state-of-the-art baseline methods in various tasks including link prediction and vertices recommendation.",
        "subjects": [
            "cs.SI",
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20411",
        "abstract url": "https://arxiv.org/abs/2410.20411",
        "title": "Trajectory Reconstruction through a Gaussian Adaptive Selective Outlier Rejecting Smoother",
        "rating": "-4",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "robotics"
            ],
            [
                "biomechanics"
            ]
        ],
        "abstract": "Trajectory Reconstruction (TR) is vital for accurately mapping movement patterns and validating analyses, especially in fields like robotics, biomechanics, and environmental tracking, where data might be missing or affected by outliers. Improving Trajectory estimation by employing Gaussian smoothing techniques in the presence of non-Gaussian noise is the subject of this work. We consider the case where data is collected from independent sensors. A variational Bayesian (VB) based Unscented Raunch-Tung-Striebel smoothing (URTSS) scheme is proposed which adopts a vectorized weighing mechanism for the measurement covariance matrix to selectively remove contaminated measurements at each time step. To improve our outlier mitigation, we model our outlier characteristics as a Gamma distribution and dynamically learn the parameters of this distribution from data. We verify the performance of our proposed smoother by a range of simulations and experimental data. We also propose a robustness criterion for smoothers based on the Kullback-Leibler (KL) divergence and show that our proposed method complies with this criterion.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20466",
        "abstract url": "https://arxiv.org/abs/2410.20466",
        "title": "Guidance Disentanglement Network for Optics-Guided Thermal UAV Image Super-Resolution",
        "rating": "-4",
        "keywords": [
            [
                "Super-Resolution"
            ],
            [
                "Thermal"
            ],
            [
                "UAV",
                "agricultural"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Optics-guided Thermal UAV image Super-Resolution (OTUAV-SR) has attracted significant research interest due to its potential applications in security inspection, agricultural measurement, and object detection. Existing methods often employ single guidance model to generate the guidance features from optical images to assist thermal UAV images super-resolution. However, single guidance models make it difficult to generate effective guidance features under favorable and adverse conditions in UAV scenarios, thus limiting the performance of OTUAV-SR. To address this issue, we propose a novel Guidance Disentanglement network (GDNet), which disentangles the optical image representation according to typical UAV scenario attributes to form guidance features under both favorable and adverse conditions, for robust OTUAV-SR. Moreover, we design an attribute-aware fusion module to combine all attribute-based optical guidance features, which could form a more discriminative representation and fit the attribute-agnostic guidance process. To facilitate OTUAV-SR research in complex UAV scenarios, we introduce VGTSR2.0, a large-scale benchmark dataset containing 3,500 aligned optical-thermal image pairs captured under diverse conditions and scenes. Extensive experiments on VGTSR2.0 demonstrate that GDNet significantly improves OTUAV-SR performance over state-of-the-art methods, especially in the challenging low-light and foggy environments commonly encountered in UAV scenarios. The dataset and code will be publicly available at https://github.com/Jocelyney/GDNet.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "18 pages, 19 figures, 8 tables"
    },
    {
        "paper id": "2410.20688",
        "abstract url": "https://arxiv.org/abs/2410.20688",
        "title": "Reprogramming Pretrained Target-Specific Diffusion Models for Dual-Target Drug Design",
        "rating": "-4",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "graphs"
            ],
            [
                "cancer"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Dual-target therapeutic strategies have become a compelling approach and attracted significant attention due to various benefits, such as their potential in overcoming drug resistance in cancer therapy. Considering the tremendous success that deep generative models have achieved in structure-based drug design in recent years, we formulate dual-target drug design as a generative task and curate a novel dataset of potential target pairs based on synergistic drug combinations. We propose to design dual-target drugs with diffusion models that are trained on single-target protein-ligand complex pairs. Specifically, we align two pockets in 3D space with protein-ligand binding priors and build two complex graphs with shared ligand nodes for SE(3)-equivariant composed message passing, based on which we derive a composed drift in both 3D and categorical probability space in the generative process. Our algorithm can well transfer the knowledge gained in single-target pretraining to dual-target scenarios in a zero-shot manner. We also repurpose linker design methods as strong baselines for this task. Extensive experiments demonstrate the effectiveness of our method compared with various baselines.",
        "subjects": [
            "cs.LG",
            "q-bio.BM"
        ],
        "comment": "Accepted to NeurIPS 2024"
    },
    {
        "paper id": "2410.20354",
        "abstract url": "https://arxiv.org/abs/2410.20354",
        "title": "FoldMark: Protecting Protein Generative Models with Watermarking",
        "rating": "-4.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "bioengineering"
            ],
            [
                "Watermarking"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Protein structure is key to understanding protein function and is essential for progress in bioengineering, drug discovery, and molecular biology. Recently, with the incorporation of generative AI, the power and accuracy of computational protein structure prediction/design have been improved significantly. However, ethical concerns such as copyright protection and harmful content generation (biosecurity) pose challenges to the wide implementation of protein generative models. Here, we investigate whether it is possible to embed watermarks into protein generative models and their outputs for copyright authentication and the tracking of generated structures. As a proof of concept, we propose a two-stage method FoldMark as a generalized watermarking strategy for protein generative models. FoldMark first pretrain watermark encoder and decoder, which can minorly adjust protein structures to embed user-specific information and faithfully recover the information from the encoded structure. In the second step, protein generative models are fine-tuned with watermark Low-Rank Adaptation (LoRA) modules to preserve generation quality while learning to generate watermarked structures with high recovery rates. Extensive experiments are conducted on open-source protein structure prediction models (e.g., ESMFold and MultiFlow) and de novo structure design models (e.g., FrameDiff and FoldFlow) and we demonstrate that our method is effective across all these generative models. Meanwhile, our watermarking framework only exerts a negligible impact on the original protein structure quality and is robust under potential post-processing and adaptive attacks.",
        "subjects": [
            "cs.CR",
            "cs.LG",
            "q-bio.BM"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20599",
        "abstract url": "https://arxiv.org/abs/2410.20599",
        "title": "Sensor Fusion for Autonomous Indoor UAV Navigation in Confined Spaces",
        "rating": "-5",
        "keywords": [
            [
                "depth"
            ],
            [
                "LiDAR",
                "Flight"
            ],
            [
                "Robot",
                "Navigation"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "In this paper, we address the challenge of navigating through unknown indoor environments using autonomous aerial robots within confined spaces. The core of our system involves the integration of key sensor technologies, including depth sensing from the ZED 2i camera, IMU data, and LiDAR measurements, facilitated by the Robot Operating System (ROS) and RTAB-Map. Through custom designed experiments, we demonstrate the robustness and effectiveness of this approach. Our results showcase a promising navigation accuracy, with errors as low as 0.4 meters, and mapping quality characterized by a Root Mean Square Error (RMSE) of just 0.13 m. Notably, this performance is achieved while maintaining energy efficiency and balanced resource allocation, addressing a crucial concern in UAV applications. Flight tests further underscore the precision of our system in maintaining desired flight orientations, with a remarkable error rate of only 0.1%. This work represents a significant stride in the development of autonomous indoor UAV navigation systems, with potential applications in search and rescue, facility inspection, and environmental monitoring within GPS-denied indoor environments.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "6 pages, 8 figures"
    },
    {
        "paper id": "2410.20375",
        "abstract url": "https://arxiv.org/abs/2410.20375",
        "title": "On performance bounds for topology optimization",
        "rating": "-10",
        "keywords": [],
        "abstract": "Topology optimization has matured to become a powerful engineering design tool that is capable of designing extraordinary structures and materials taking into account various physical phenomena. Despite the method's great advancements in recent years, several unanswered questions remain. This paper takes a step towards answering one of the larger questions, namely: How far from the global optimum is a given topology optimized design? Typically this is a hard question to answer, as almost all interesting topology optimization problems are non-convex. Unfortunately, this non-convexity implies that local minima may plague the design space, resulting in optimizers ending up in suboptimal designs. In this work, we investigate performance bounds for topology optimization via a computational framework that utilizes Lagrange duality theory. This approach provides a viable measure of how \\say{close} a given design is to the global optimum for a subset of optimization formulations. The method's capabilities are exemplified via several numerical examples, including the design of mode converters and resonating plates.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20400",
        "abstract url": "https://arxiv.org/abs/2410.20400",
        "title": "MPLS Network Actions: Technological Overview and P4-Based Implementation on a High-Speed Switching ASIC",
        "rating": "-10",
        "keywords": [],
        "abstract": "The MPLS protocol, traditionally focused on packet forwarding using labels, has evolved to include advanced mechanisms such as Service Function Chaining (SFC), Alternate-Marking Method (AMM), and in-situ OAM (IOAM). However, many of those mechanisms require extensions to existing specifications in MPLS making them difficult to deploy. To bridge this gap, the IETF MPLS WG proposed the MPLS Network Actions (MNA) framework which provides a unified encoding for signaling network actions and their data within the MPLS stack. Network actions in the MNA framework serve a similar role for MPLS as extension headers (EH) do for IPv6. The network actions can be encoded within the label stack (in-stack) or following the stack (post-stack). In this work, we give a comprehensive overview of the design principles of network actions in the MNA framework and the mechanisms that benefit from this framework. We summarize and explain use cases in the MNA framework. Building on this, we implement the MNA framework in P4 on the Intel Tofino 2 switching ASIC. Our work explores an in-stack data (ISD) implementation of the MNA framework. The implementation can process 51 label stack entries containing 32 network actions at a line rate of 400 Gb/s per port. Additionally, we implement and evaluate an exemplary network action for performance measurement with AMM. Finally, we identify challenges with an MNA in-stack implementation and propose an extension to the signaling procedure.",
        "subjects": [
            "cs.NI",
            "cs.ET"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20412",
        "abstract url": "https://arxiv.org/abs/2410.20412",
        "title": "On the doubly generalized conjugacy problem",
        "rating": "-10",
        "keywords": [],
        "abstract": "We prove that the doubly generalized conjugacy problem is decidable for rational subsets of finitely generated virtually free groups: there is an algorithm taking as input two rational subsets $K_1$ and $K_2$ of a virtually free group that decides whether there is one element of $K_1$ conjugate to an element of $K_2$. For free groups, we prove that the same problem is decidable with rational constraints on the set of conjugators.",
        "subjects": [
            "math.GR",
            "cs.FL"
        ],
        "comment": "16 pages, comments are welcome"
    },
    {
        "paper id": "2410.20429",
        "abstract url": "https://arxiv.org/abs/2410.20429",
        "title": "MARS: Multi-sample Allocation through Russian roulette and Splitting",
        "rating": "-10",
        "keywords": [],
        "abstract": "Multiple importance sampling (MIS) is an indispensable tool in rendering that constructs robust sampling strategies by combining the respective strengths of individual distributions. Its efficiency can be greatly improved by carefully selecting the number of samples drawn from each distribution, but automating this process remains a challenging problem. Existing works are mostly limited to mixture sampling, in which only a single sample is drawn in total, and the works that do investigate multi-sample MIS only optimize the sample counts at a per-pixel level, which cannot account for variations beyond the first bounce. Recent work on Russian roulette and splitting has demonstrated how fixed-point schemes can be used to spatially vary sample counts to optimize image efficiency but is limited to choosing the same number of samples across all sampling strategies. Our work proposes a highly flexible sample allocation strategy that bridges the gap between these areas of work. We show how to iteratively optimize the sample counts to maximize the efficiency of the rendered image using a lightweight data structure, which allows us to make local and individual decisions per technique. We demonstrate the benefits of our approach in two applications, path guiding and bidirectional path tracing, in both of which we achieve consistent and substantial speedups over the respective previous state-of-the-art.",
        "subjects": [
            "cs.GR"
        ],
        "comment": "18 pages, 13 figures, to be published in SIGGRAPH Asia 2024 Conference Papers"
    },
    {
        "paper id": "2410.20468",
        "abstract url": "https://arxiv.org/abs/2410.20468",
        "title": "Understanding Communication Preferences of Information Workers in Engagement with Text-Based Conversational Agents",
        "rating": "-10",
        "keywords": [],
        "abstract": "Communication traits in text-based human-AI conversations play pivotal roles in shaping user experiences and perceptions of systems. With the advancement of large language models (LLMs), it is now feasible to analyze these traits at a more granular level. In this study, we explore the preferences of information workers regarding chatbot communication traits across seven applications. Participants were invited to participate in an interactive survey, which featured adjustable sliders, allowing them to adjust and express their preferences for five key communication traits: formality, personification, empathy, sociability, and humor. Our findings reveal distinct communication preferences across different applications; for instance, there was a preference for relatively high empathy in wellbeing contexts and relatively low personification in coding. Similarities in preferences were also noted between applications such as chatbots for customer service and scheduling. These insights offer crucial design guidelines for future chatbots, emphasizing the need for nuanced trait adjustments for each application.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20475",
        "abstract url": "https://arxiv.org/abs/2410.20475",
        "title": "Optimal Hardening Strategy for Electricity-Hydrogen Networks with Hydrogen Leakage Risk Control against Extreme Weather",
        "rating": "-10",
        "keywords": [],
        "abstract": "Defense hardening can effectively enhance the resilience of distribution networks against extreme weather disasters. Currently, most existing hardening strategies focus on reducing load shedding. However, for electricity-hydrogen distribution networks (EHDNs), the leakage risk of hydrogen should be controlled to avoid severe incidents such as explosions. To this end, this paper proposes an optimal hardening strategy for EHDNs under extreme weather, aiming to minimize load shedding while limiting the leakage risk of hydrogen pipelines. Specifically, modified failure uncertainty models for power lines and hydrogen pipelines are developed. These models characterize not only the effect of hardening, referred to as decision-dependent uncertainties (DDUs), but also the influence of disaster intensity correlations on failure probability distributions. Subsequently, a hardening decision framework is established, based on the two-stage distributionally robust optimization incorporating a hydrogen leakage chance constraint (HLCC). To enhance the computational efficiency of HLCC under discrete DDUs, an efficient second-order-cone transformation is introduced. Moreover, to address the intractable inverse of the second-order moment under DDUs, lifted variables are adopted to refine the main-cross moments. These reformulate the hardening problem as a two-stage mixed-integer second-order-cone programming, and finally solved by the column-and-constraint generation algorithm. Case studies demonstrate the effectiveness and superiority of the proposed method.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20484",
        "abstract url": "https://arxiv.org/abs/2410.20484",
        "title": "Smart Space Environments: Key Challenges and Innovative Solutions",
        "rating": "-10",
        "keywords": [],
        "abstract": "The integration of LoRaWAN (Long Range Wide Area Network) technology with both active and passive sensors presents a transformative opportunity for the development of smart home systems. This paper explores how active sensors, such as motion detectors and ultrasonic sensors, and passive sensors, including temperature and humidity sensors, work together to enhance connectivity and efficiency within diverse environments while addressing the challenges of modern living. By leveraging LoRaWAN long-range capabilities and low power consumption, the proposed framework enables effective data transmission from remote sensors, facilitating applications such as smart agriculture, environmental monitoring, and comprehensive home automation. Active sensors emit energy to detect changes in their surroundings, providing real-time data crucial for security and automation, while passive sensors capture ambient energy to monitor environmental conditions, ensuring resource efficiency and user comfort. The synergy between LoRaWAN and these various sensor types promotes innovation, contributing to a more responsive and sustainable living experience. Furthermore, this research highlights the adaptability of the proposed system, allowing for seamless integration of new devices and advanced functionalities. As the landscape of smart home technology continues to evolve, ongoing research in this area will yield advanced solutions tailored to user needs, ultimately paving the way for smarter, safer, and more efficient living environments.",
        "subjects": [
            "cs.ET",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20485",
        "abstract url": "https://arxiv.org/abs/2410.20485",
        "title": "A Risk-Averse Just-In-Time Scheme for Learning-Based Operation of Microgrids with Coupled Electricity-Hydrogen-Ammonia under Uncertainties",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper proposes a Risk-Averse Just-In-Time (RAJIT) operation scheme for Ammonia-Hydrogen-based Micro-Grids (AHMGs) to boost electricity-hydrogen-ammonia coupling under uncertainties. First, an off-grid AHMG model is developed, featuring a novel multi-mode ammonia synthesis process and a hydrogen-ammonia dual gas turbine with tunable feed-in ratios. Subsequently, a state-behavior mapping strategy linking hydrogen storage levels with the operation modes of ammonia synthesis is established to prevent cost-ineffective shutdowns. The proposed model substantially improves operational flexibility but results in a challenging nonlinear fractional program. Based upon this model, a data-driven RAJIT scheme is developed for the real-time rolling optimization of AHMGs. Unlike conventional one-size-fits-all schemes using one optimization method throughout, the data driven RAJIT intelligently switches between cost-effective deterministic optimization and risk-averse online-learning distributionally robust optimization depending on actual risk profiles, thus capitalizing on the respective strengths of these two optimization methods. To facilitate the solution of the resulting nonlinear program, we develop an equivalent-reformulation-based solution methodology by leveraging a constraint-tightening technique. Numerical simulations demonstrate that the proposed scheme guarantees safety and yields an overall cost reduction up to 14.6% compared with several state-of-the-art methods.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20503",
        "abstract url": "https://arxiv.org/abs/2410.20503",
        "title": "Realization of Reconfigurable Intelligent Surfaces with Space-Time Coded Metasurfaces",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents experimental realization of a reconfigurable intelligent surface (RIS) using space-time coding metasurfaces to enable concurrent beam steering and data modulation. The proposed approach harnesses the capabilities of metasurfaces, allowing precise temporal control over individual unit cells of the RIS. We show that by employing proper binary codes manipulating the state of unit cells, the RIS can act as a digital data modulator with beam steering capability. We describe the experimental setup and computational tools, followed by validation through harmonic generation and investigation of beam steering and data modulation. Additionally, four digital modulation schemes are evaluated. By implementing customized binary codes, constellations under varying conditions are compared, showcasing the potential for real-world applications. This study offers new insights into the practical implementation of RIS for advanced wireless communication systems.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20505",
        "abstract url": "https://arxiv.org/abs/2410.20505",
        "title": "Wireless Localization with Space-Time Coded Reconfigurable Intelligent Surfaces",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, a novel approach for wireless localization is proposed and experimentally validated that leverages space-time coded reconfigurable intelligent surfaces (RIS). It is demonstrated that applying proper single-bit codes to each RIS element, enables accurate determination of the direction of arrival (AOA) at the receiver. Moreover, we introduce different scenarios that such technique can be used for localization. By incorporating RIS, a passive component, the method significantly reduces the complexity found in previous localization techniques. Additionally, the use of 1-bit codes minimizes hardware requirements, offering a reliable, low-cost solution for localization in advanced telecommunications networks.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20538",
        "abstract url": "https://arxiv.org/abs/2410.20538",
        "title": "Improving the Leading Constant of Matrix Multiplication",
        "rating": "-10",
        "keywords": [],
        "abstract": "Algebraic matrix multiplication algorithms are designed by bounding the rank of matrix multiplication tensors, and then using a recursive method. However, designing algorithms in this way quickly leads to large constant factors: if one proves that the tensor for multiplying $n \\times n$ matrices has rank $\\leq t$, then the resulting recurrence shows that $M \\times M$ matrices can be multiplied using $O(n^2 \\cdot M^{\\log_n t})$ operations, where the leading constant scales proportionally to $n^2$. Even modest increases in $n$ can blow up the leading constant too much to be worth the slight decrease in the exponent of $M$. Meanwhile, the asymptotically best algorithms use very large $n$, such that $n^2$ is larger than the number of atoms in the visible universe! In this paper, we give new ways to use tensor rank bounds to design matrix multiplication algorithms, which lead to smaller leading constants than the standard recursive method. Our main result shows that, if the tensor for multiplying $n \\times n$ matrices has rank $\\leq t$, then $M \\times M$ matrices can be multiplied using only $n^{O(1/(\\log n)^{0.33})} \\cdot M^{\\log_n t}$ operations. In other words, we improve the leading constant in general from $O(n^2)$ to $n^{O(1/(\\log n)^{0.33})} < n^{o(1)}$. We then apply this and further improve the leading constant in a number of situations of interest. We show that, in the popularly-conjectured case where $\u03c9=2$, a new, different recursive approach can lead to an improvement. We also show that the leading constant of the current asymptotically fastest matrix multiplication algorithm, and any algorithm designed using the group-theoretic method, can be further improved by taking advantage of additional structure of the underlying tensor identities.",
        "subjects": [
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20541",
        "abstract url": "https://arxiv.org/abs/2410.20541",
        "title": "Data-driven Analysis of T-Product-based Dynamical Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "A wide variety of data can be represented using third-order tensors, spanning applications in chemometrics, psychometrics, and image processing. However, traditional data-driven frameworks are not naturally equipped to process tensors without first unfolding or flattening the data, which can result in a loss of crucial higher-order structural information. In this article, we introduce a novel framework for the data-driven analysis of T-product-based dynamical systems (TPDSs), where the system evolution is governed by the T-product between a third-order dynamic tensor and a third-order state tensor. In particular, we examine the data informativity of TPDSs concerning system identification, stability, controllability, and stabilizability and illustrate significant computational improvements over traditional approaches by leveraging the unique properties of the T-product. The effectiveness of our framework is demonstrated through numerical examples.",
        "subjects": [
            "eess.SY",
            "math.DS",
            "math.OC"
        ],
        "comment": "11 pages, 3 figures"
    },
    {
        "paper id": "2410.20560",
        "abstract url": "https://arxiv.org/abs/2410.20560",
        "title": "Scaling Effects of Transistor Leakage Current and IR Drop on 1T1R Memory Arrays",
        "rating": "-10",
        "keywords": [],
        "abstract": "1T1R (1-transistor-1-resistor) memory crossbar arrays represent a promising solution for compute-in-memory matrix-vector multiplication accelerators and embedded or storage-class memory. However, the size and scaling of these arrays are hindered by critical challenges, such as the IR drop on metal lines and the accumulation of leakage current from the transistors. Although the IR drop issue has been extensively investigated, the impact of transistor leakage current has received limited attention. In this work, we investigate both issues and highlight how transistor leakage in 1T1R arrays has effects similar to IR drop, which degrades the memory cell sensing margin, especially as the technology node scales down. This degradation could pose reliability concerns, particularly where the on/off ratio or sensing margin of memristors is critical. We characterized the joint effects of transistor read resistance, transistor leakage current, and IR drop as the array size scales up and the fabrication node scales down. Based on a model developed using specifications of a 22nm FDSOI technology, we found that an optimal resistance range of memristors exists for good array scaling capability, where the transistor read resistance and the IR drop issue establish a lower resistance boundary, while the transistor leakage issue sets an upper resistance boundary. This work provides valuable scaling guidelines for engineering the properties of memristor devices in 1T1R memory arrays.",
        "subjects": [
            "cs.ET"
        ],
        "comment": "5 pages, 6 figures"
    },
    {
        "paper id": "2410.20561",
        "abstract url": "https://arxiv.org/abs/2410.20561",
        "title": "A New Method for Inserting Train Paths into a Timetable",
        "rating": "-10",
        "keywords": [],
        "abstract": "A seemingly simple, yet widely applicable subroutine in automated train scheduling is the insertion of a new train path to a timetable in a railway network. We believe it to be the first step towards a new train-rerouting framework in case of large disturbances or maintenance works. Other applications include handling ad-hoc requests and modifying train paths upon request from railway undertakings. We propose a fast and scalable path-insertion algorithm based on dynamic programming that is able to output multiple suitable paths. Our algorithm uses macroscopic data and can run on railway networks with any number of tracks. We apply the algorithm on the line from G\u00f6teborg S\u00e4ven\u00e4s to the Norwegian border at Kornsj\u00f6. For a time window of seven hours, we obtain eight suitable paths for a freight train within 0.3 seconds after preprocessing.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20565",
        "abstract url": "https://arxiv.org/abs/2410.20565",
        "title": "A Fast Algorithm for Computing Zigzag Representatives",
        "rating": "-10",
        "keywords": [],
        "abstract": "Zigzag filtrations of simplicial complexes generalize the usual filtrations by allowing simplex deletions in addition to simplex insertions. The barcodes computed from zigzag filtrations encode the evolution of homological features. Although one can locate a particular feature at any index in the filtration using existing algorithms, the resulting representatives may not be compatible with the zigzag: a representative cycle at one index may not map into a representative cycle at its neighbor. For this, one needs to compute compatible representative cycles along each bar in the barcode. Even though it is known that the barcode for a zigzag filtration with $m$ insertions and deletions can be computed in $O(m^\u03c9)$ time, it is not known how to compute the compatible representatives so efficiently. For a non-zigzag filtration, the classical matrix-based algorithm provides representatives in $O(m^3)$ time, which can be improved to $O(m^\u03c9)$. However, no known algorithm for zigzag filtrations computes the representatives with the $O(m^3)$ time bound. We present an $O(m^2n)$ time algorithm for this problem, where $n\\leq m$ is the size of the largest complex in the filtration.",
        "subjects": [
            "cs.CG",
            "math.AT"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20571",
        "abstract url": "https://arxiv.org/abs/2410.20571",
        "title": "Making Urban Art Accessible: Current Art Access Techniques, Design Considerations, and the Role of AI",
        "rating": "-10",
        "keywords": [],
        "abstract": "Public artwork, from vibrant wall murals to captivating sculptures, can enhance the aesthetic of urban spaces, foster a sense of community and cultural identity, and help attract visitors. Despite its benefits, most public art is visual, making it often inaccessible to blind and low vision (BLV) people. In this workshop paper, we first draw on art literature to help define the space of public art, identify key differences with curated art shown in museums or galleries, and discuss implications for accessibility. We then enumerate how existing art accessibility techniques may (or may not) transfer to urban art spaces. We close by presenting future research directions and reflecting on the growing role of AI in making art accessible.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "ASSETS 2024 Workshop Submission (The Future of Urban Accessibility: The Role of AI)"
    },
    {
        "paper id": "2410.20581",
        "abstract url": "https://arxiv.org/abs/2410.20581",
        "title": "A Comprehensive Survey on Green Blockchain: Developing the Next Generation of Energy Efficient and Sustainable Blockchain Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Although Blockchain has been successfully used in many different fields and applications, it has been traditionally regarded as an energy-intensive technology, essentially due to the past use of inefficient consensus algorithms that prioritized security over sustainability. However, in the last years, thanks to the significant progress made on key blockchain components, their energy consumption can be decreased noticeably. To achieve this objective, this article analyzes the main components of blockchains and explores strategies to reduce their energy consumption. In this way, this article delves into each component of a blockchain system, including consensus mechanisms, network architecture, data storage and validation, smart contract execution, mining and block creation, and outlines specific strategies to decrease their energy consumption. For such a purpose, consensus mechanisms are compared, recommendations for reducing network communications energy consumption are provided, techniques for data storage and validation are suggested and diverse optimizations are proposed both for software and hardware components. Moreover, the main challenges and limitations of reducing power consumption in blockchain systems are analyzed. As a consequence, this article provides a guideline for the future researchers and developers who aim to develop the next generation of Green Blockchain solutions.",
        "subjects": [
            "cs.DC",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20598",
        "abstract url": "https://arxiv.org/abs/2410.20598",
        "title": "R^3AG: First Workshop on Refined and Reliable Retrieval Augmented Generation",
        "rating": "-10",
        "keywords": [],
        "abstract": "Retrieval-augmented generation (RAG) has gained wide attention as the key component to improve generative models with external knowledge augmentation from information retrieval. It has shown great prominence in enhancing the functionality and performance of large language model (LLM)-based applications. However, with the comprehensive application of RAG, more and more problems and limitations have been identified, thus urgently requiring further fundamental exploration to improve current RAG frameworks. This workshop aims to explore in depth how to conduct refined and reliable RAG for downstream AI tasks. To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024 to call for participants to re-examine and formulate the basic principles and practical implementation of refined and reliable RAG. The workshop serves as a platform for both academia and industry researchers to conduct discussions, share insights, and foster research to build the next generation of RAG systems. Participants will engage in discussions and presentations focusing on fundamental challenges, cutting-edge research, and potential pathways to improve RAG. At the end of the workshop, we aim to have a clearer understanding of how to improve the reliability and applicability of RAG with more robust information retrieval and language generation.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "R^3AG workshop overview at SIGIR-AP 2024"
    },
    {
        "paper id": "2410.20602",
        "abstract url": "https://arxiv.org/abs/2410.20602",
        "title": "ReACKed QUICer: Measuring the Performance of Instant Acknowledgments in QUIC Handshakes",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we present a detailed performance analysis of QUIC instant ACK, a standard-compliant approach to reduce waiting times during the QUIC connection setup in common CDN deployments. To understand the root causes of the performance properties, we combine numerical analysis and the emulation of eight QUIC implementations using the QUIC Interop Runner. Our experiments comprehensively cover packet loss and non-loss scenarios, different round trip times, and TLS certificate sizes. To clarify instant ACK deployments in the wild, we conduct active measurements of 1M popular domain names. For almost all domain names under control of Cloudflare, Cloudflare uses instant ACK, which in fact improves performance. We also find, however, that instant ACK may lead to unnecessary retransmissions or longer waiting times under some network conditions, raising awareness of drawbacks of instant ACK in the future.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "camera-ready"
    },
    {
        "paper id": "2410.20690",
        "abstract url": "https://arxiv.org/abs/2410.20690",
        "title": "KANsformer for Scalable Beamforming",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper proposes an unsupervised deep-learning (DL) approach by integrating transformer and Kolmogorov-Arnold networks (KAN) termed KANsformer to realize scalable beamforming for mobile communication systems. Specifically, we consider a classic multi-input-single-output energy efficiency maximization problem subject to the total power budget. The proposed KANsformer first extracts hidden features via a multi-head self-attention mechanism and then reads out the desired beamforming design via KAN. Numerical results are provided to evaluate the KANsformer in terms of generalization performance, transfer learning and ablation experiment. Overall, the KANsformer outperforms existing benchmark DL approaches, and is adaptable to the change in the number of mobile users with real-time and near-optimal inference.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20698",
        "abstract url": "https://arxiv.org/abs/2410.20698",
        "title": "Aqua-Sim Fourth Generation: Towards General and Intelligent Simulation for Underwater Acoustic Networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "Simulators are essential to troubleshoot and optimize Underwater Acoustic Network (UAN) schemes (network protocols and communication technologies) before real field experiments. However, due to programming differences between the above two contents, most existing simulators concentrate on one while weakening the other, leading to non-generic simulations and biased performance results. Moreover, novel UAN schemes increasingly integrate Artificial Intelligence (AI) techniques, yet existing simulators lack support for necessary AI frameworks, failing to train and evaluate these intelligent methods. On the other hand, these novel schemes consider more UAN characteristics involving more complex parameter configurations, which also challenges simulators in flexibility and fineness. To keep abreast of advances in UANs, we propose the Fourth Generation (FG) ns-3-based simulator Aqua-Sim~FG, enhancing the general and intelligent simulation ability. On the basis of retaining previous generations' functions, we design a new general architecture, which is compatible with various programming languages, including MATLAB, C++, and Python. In this way, Aqua-Sim~FG provides a general environment to simulate communication technologies, network protocols, and AI models simultaneously. In addition, we expand six new features from node and communication levels by considering the latest UAN methods' requirements, which enhances the simulation flexibility and fineness of Aqua-Sim~FG. Experimental results show that Aqua-Sim~FG can simulate UANs' performance realistically, reflect intelligent methods' problems in real-ocean scenarios, and provide more effective troubleshooting and optimization for actual UANs. The basic simulator is available at https://github.com/JLU-smartocean/aqua-sim-fg.",
        "subjects": [
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2410.20703",
        "abstract url": "https://arxiv.org/abs/2410.20703",
        "title": "Optimal Sensor Placement for TDOA-Based Source Localization with Sensor Location Errors",
        "rating": "-10",
        "keywords": [],
        "abstract": "The accuracy of time difference of arrival (TDOA)-based source localization is influenced by sensor location deployment. Many studies focus on optimal sensor placement (OSP) for TDOA-based localization without sensor location noises (OSP-WSLN). In practice, there are sensor location errors due to installation deviations, etc, which implies the necessity of studying OSP under sensor location noises (OSP-SLN). There are two fundamental problems: What is the OSP-SLN strategy? To what extent do sensor location errors affect the performance of OSP-SLN? For the first one, under the assumption of the near-field and full set of TDOA, minimizing the trace of the Cramer-Rao bound is used as optimization criteria. Based on this, a concise equality, namely Eq. (18), is proven to show that OSP-SLN is equivalent to OSP-WSLN. Extensive simulations validate both equality and equivalence and respond to the second problem: not large sensor position errors give an ignorable negative impact on the performance of OSP-SLN quantified by the trace of CRB. Also, simulations show source localization accuracy with OSP-SLN outperforms that with random placement. These simulations validate our derived OSP-SLN and its effectiveness. We have open-sourced the code for community use.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.00815",
        "abstract url": "https://arxiv.org/abs/2411.00815",
        "title": "Exploiting long vectors with a CFD code: a co-design show case",
        "rating": "-10",
        "keywords": [],
        "abstract": "A current trend in HPC systems is the utilization of architectures with SIMD or vector extensions to exploit data parallelism. There are several ways to take advantage of such modern vector architectures, each with a different impact on the code and its portability. For example, the use of intrinsics, guided vectorization via pragmas, or compiler autovectorization. Our objectives are to maximize vectorization efficiency and minimize code specialization. To achieve these objectives, we rely on compiler autovectorization. We leverage a set of hardware and software tools that allow us to analyze in detail where autovectorization is suboptimal. Thus, we apply an iterative methodology that allows us to incrementally improve the efficient use of the underlying hardware. In this paper, we apply this methodology to a CFD production code. We evaluate the performance on an innovative configurable platform powered by a RISC-V core coupled with a wide vector unit capable of operating with up to 256 double precision elements. Following the vectorization process, we demonstrate a single-core speedup of 7.6$\\times$ compared to its scalar implementation. Furthermore, we show that code portability is not compromised, as our solution continues to exhibit performance benefits, or at the very least, no drawbacks, on other HPC architectures such as Intel x86 and NEC SX-Aurora.",
        "subjects": [
            "cs.DC",
            "cs.AR",
            "cs.PF"
        ],
        "comment": "Main track paper, presented at IPDPS 2024"
    }
]