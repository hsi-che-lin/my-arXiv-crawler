[
    {
        "paper id": "2411.02860",
        "abstract url": "https://arxiv.org/abs/2411.02860",
        "title": "Continual Audio-Visual Sound Separation",
        "rating": "2.5",
        "keywords": [
            [
                "Audio-Visual"
            ],
            [
                "cs.LG",
                "cs.CV",
                "cs.SD",
                "eess.AS"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In this paper, we introduce a novel continual audio-visual sound separation task, aiming to continuously separate sound sources for new classes while preserving performance on previously learned classes, with the aid of visual guidance. This problem is crucial for practical visually guided auditory perception as it can significantly enhance the adaptability and robustness of audio-visual sound separation models, making them more applicable for real-world scenarios where encountering new sound sources is commonplace. The task is inherently challenging as our models must not only effectively utilize information from both modalities in current tasks but also preserve their cross-modal association in old tasks to mitigate catastrophic forgetting during audio-visual continual learning. To address these challenges, we propose a novel approach named ContAV-Sep (\\textbf{Cont}inual \\textbf{A}udio-\\textbf{V}isual Sound \\textbf{Sep}aration). ContAV-Sep presents a novel Cross-modal Similarity Distillation Constraint (CrossSDC) to uphold the cross-modal semantic similarity through incremental tasks and retain previously acquired knowledge of semantic similarity in old models, mitigating the risk of catastrophic forgetting. The CrossSDC can seamlessly integrate into the training process of different audio-visual sound separation frameworks. Experiments demonstrate that ContAV-Sep can effectively mitigate catastrophic forgetting and achieve significantly better performance compared to other continual learning baselines for audio-visual sound separation. Code is available at: \\url{https://github.com/weiguoPian/ContAV-Sep_NeurIPS2024}.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "cs.MM",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "NeurIPS 2024"
    },
    {
        "paper id": "2411.03313",
        "abstract url": "https://arxiv.org/abs/2411.03313",
        "title": "Classification Done Right for Vision-Language Pre-Training",
        "rating": "2.5",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data. Unlike its contrastive counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised classification labels, without the need for additional text filtering or selection. Due to the absence of the text encoding as contrastive target, SuperClass does not require a text encoder and does not need to maintain a large batch size as CLIP does. SuperClass demonstrated superior performance on various downstream tasks, including classic computer vision benchmarks and vision language downstream tasks. We further explored the scaling behavior of SuperClass on model size, training length, or data size, and reported encouraging results and comparisons to CLIP. https://github.com/x-cls/superclass",
        "subjects": [
            "cs.CV"
        ],
        "comment": "NeurIPS 2024"
    },
    {
        "paper id": "2411.02818",
        "abstract url": "https://arxiv.org/abs/2411.02818",
        "title": "LiVOS: Light Video Object Segmentation with Gated Linear Matching",
        "rating": "2",
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Semi-supervised video object segmentation (VOS) has been largely driven by space-time memory (STM) networks, which store past frame features in a spatiotemporal memory to segment the current frame via softmax attention. However, STM networks face memory limitations due to the quadratic complexity of softmax matching, restricting their applicability as video length and resolution increase. To address this, we propose LiVOS, a lightweight memory network that employs linear matching via linear attention, reformulating memory matching into a recurrent process that reduces the quadratic attention matrix to a constant-size, spatiotemporal-agnostic 2D state. To enhance selectivity, we introduce gated linear matching, where a data-dependent gate matrix is multiplied with the state matrix to control what information to retain or discard. Experiments on diverse benchmarks demonstrated the effectiveness of our method. It achieved 64.8 J&F on MOSE and 85.1 J&F on DAVIS, surpassing all non-STM methods and narrowing the gap with STM-based approaches. For longer and higher-resolution videos, it matched STM-based methods with 53% less GPU memory and supports 4096p inference on a 32G consumer-grade GPU--a previously cost-prohibitive capability--opening the door for long and high-resolution video foundation models.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Code&models: https://github.com/uncbiag/LiVOS"
    },
    {
        "paper id": "2411.02851",
        "abstract url": "https://arxiv.org/abs/2411.02851",
        "title": "Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual Visual Answer Localization",
        "rating": "2",
        "keywords": [
            [
                "Audio-Visual"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The goal of Multilingual Visual Answer Localization (MVAL) is to locate a video segment that answers a given multilingual question. Existing methods either focus solely on visual modality or integrate visual and subtitle modalities. However, these methods neglect the audio modality in videos, consequently leading to incomplete input information and poor performance in the MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span Localization (AVTSL) method that incorporates audio modality to augment both visual and textual representations for the MVAL task. Specifically, we integrate features from three modalities and develop three predictors, each tailored to the unique contributions of the fused modalities: an audio-visual predictor, a visual predictor, and a textual predictor. Each predictor generates predictions based on its respective modality. To maintain consistency across the predicted results, we introduce an Audio-Visual-Textual Consistency module. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing each modality's predictor to dynamically learn from the others. This collaborative learning ensures that the model generates consistent and comprehensive answers. Extensive experiments show that our proposed method outperforms several state-of-the-art (SOTA) methods, which demonstrates the effectiveness of the audio modality.",
        "subjects": [
            "cs.MM",
            "cs.AI",
            "cs.CL",
            "cs.HC",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03171",
        "abstract url": "https://arxiv.org/abs/2411.03171",
        "title": "Navigating Extremes: Dynamic Sparsity in Large Output Space",
        "rating": "2",
        "keywords": [
            [
                "memory efficient"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In recent years, Dynamic Sparse Training (DST) has emerged as an alternative to post-training pruning for generating efficient models. In principle, DST allows for a more memory efficient training process, as it maintains sparsity throughout the entire training run. However, current DST implementations fail to capitalize on this in practice. Because sparse matrix multiplication is much less efficient than dense matrix multiplication on GPUs, most implementations simulate sparsity by masking weights. In this paper, we leverage recent advances in semi-structured sparse training to apply DST in the domain of classification with large output spaces, where memory-efficiency is paramount. With a label space of possibly millions of candidates, the classification layer alone will consume several gigabytes of memory. Switching from a dense to a fixed fan-in sparse layer updated with sparse evolutionary training (SET); however, severely hampers training convergence, especially at the largest label spaces. We find that poor gradient flow from the sparse classifier to the dense text encoder make it difficult to learn good input representations. By employing an intermediate layer or adding an auxiliary training objective, we recover most of the generalisation performance of the dense model. Overall, we demonstrate the applicability and practical benefits of DST in a challenging domain -- characterized by a highly skewed label distribution that differs substantially from typical DST benchmark datasets -- which enables end-to-end training with millions of labels on commodity hardware.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "20 pages, 7 figures, NeurIPS 2024"
    },
    {
        "paper id": "2411.03312",
        "abstract url": "https://arxiv.org/abs/2411.03312",
        "title": "Inference Optimal VLMs Need Only One Visual Token but Larger Models",
        "rating": "2",
        "keywords": [
            [
                "Vision Language",
                "VLMs"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks. However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM. To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression. However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance. We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., $5-10\\times$), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take some initial steps towards building approaches tailored for high token compression settings. Code is available at https://github.com/locuslab/llava-token-compression.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03491",
        "abstract url": "https://arxiv.org/abs/2411.03491",
        "title": "An Application-Agnostic Automatic Target Recognition System Using Vision Language Models",
        "rating": "2",
        "keywords": [
            [
                "Vision Language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present a novel Automatic Target Recognition (ATR) system using open-vocabulary object detection and classification models. A primary advantage of this approach is that target classes can be defined just before runtime by a non-technical end user, using either a few natural language text descriptions of the target, or a few image exemplars, or both. Nuances in the desired targets can be expressed in natural language, which is useful for unique targets with little or no training data. We also implemented a novel combination of several techniques to improve performance, such as leveraging the additional information in the sequence of overlapping frames to perform tubelet identification (i.e., sequential bounding box matching), bounding box re-scoring, and tubelet linking. Additionally, we developed a technique to visualize the aggregate output of many overlapping frames as a mosaic of the area scanned during the aerial surveillance or reconnaissance, and a kernel density estimate (or heatmap) of the detected targets. We initially applied this ATR system to the use case of detecting and clearing unexploded ordinance on airfield runways and we are currently extending our research to other real-world applications.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted to the Thirty-Seventh Annual Conference on Innovative Applications of Artificial Intelligence (IAAI-25)"
    },
    {
        "paper id": "2411.03531",
        "abstract url": "https://arxiv.org/abs/2411.03531",
        "title": "Personalized Video Summarization by Multimodal Video Understanding",
        "rating": "2",
        "keywords": [
            [
                "visual language",
                "VLMs"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Video summarization techniques have been proven to improve the overall user experience when it comes to accessing and comprehending video content. If the user's preference is known, video summarization can identify significant information or relevant content from an input video, aiding them in obtaining the necessary information or determining their interest in watching the original video. Adapting video summarization to various types of video and user preferences requires significant training data and expensive human labeling. To facilitate such research, we proposed a new benchmark for video summarization that captures various user preferences. Also, we present a pipeline called Video Summarization with Language (VSL) for user-preferred video summarization that is based on pre-trained visual language models (VLMs) to avoid the need to train a video summarization system on a large training dataset. The pipeline takes both video and closed captioning as input and performs semantic analysis at the scene level by converting video frames into text. Subsequently, the user's genre preference was used as the basis for selecting the pertinent textual scenes. The experimental results demonstrate that our proposed pipeline outperforms current state-of-the-art unsupervised video summarization models. We show that our method is more adaptable across different datasets compared to supervised query-based video summarization models. In the end, the runtime analysis demonstrates that our pipeline is more suitable for practical use when scaling up the number of user preferences and videos.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "In Proceedings of CIKM 2024 Applied Research Track"
    },
    {
        "paper id": "2411.02840",
        "abstract url": "https://arxiv.org/abs/2411.02840",
        "title": "Test-Time Dynamic Image Fusion",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "The inherent challenge of image fusion lies in capturing the correlation of multi-source images and comprehensively integrating effective information from different sources. Most existing techniques fail to perform dynamic image fusion while notably lacking theoretical guarantees, leading to potential deployment risks in this field. Is it possible to conduct dynamic image fusion with a clear theoretical justification? In this paper, we give our solution from a generalization perspective. We proceed to reveal the generalized form of image fusion and derive a new test-time dynamic image fusion paradigm. It provably reduces the upper bound of generalization error. Specifically, we decompose the fused image into multiple components corresponding to its source data. The decomposed components represent the effective information from the source data, thus the gap between them reflects the Relative Dominability (RD) of the uni-source data in constructing the fusion image. Theoretically, we prove that the key to reducing generalization error hinges on the negative correlation between the RD-based fusion weight and the uni-source reconstruction loss. Intuitively, RD dynamically highlights the dominant regions of each source and can be naturally converted to the corresponding fusion weight, achieving robust results. Extensive experiments and discussions with in-depth analysis on multiple benchmarks confirm our findings and superiority. Our code is available at https://github.com/Yinan-Xia/TTD.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by NeurIPS 2024"
    },
    {
        "paper id": "2411.02858",
        "abstract url": "https://arxiv.org/abs/2411.02858",
        "title": "OLAF: A Plug-and-Play Framework for Enhanced Multi-object Multi-part Scene Parsing",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Multi-object multi-part scene segmentation is a challenging task whose complexity scales exponentially with part granularity and number of scene objects. To address the task, we propose a plug-and-play approach termed OLAF. First, we augment the input (RGB) with channels containing object-based structural cues (fg/bg mask, boundary edge mask). We propose a weight adaptation technique which enables regular (RGB) pre-trained models to process the augmented (5-channel) input in a stable manner during optimization. In addition, we introduce an encoder module termed LDF to provide low-level dense feature guidance. This assists segmentation, particularly for smaller parts. OLAF enables significant mIoU gains of $\\mathbf{3.3}$ (Pascal-Parts-58), $\\mathbf{3.5}$ (Pascal-Parts-108) over the SOTA model. On the most challenging variant (Pascal-Parts-201), the gain is $\\mathbf{4.0}$. Experimentally, we show that OLAF's broad applicability enables gains across multiple architectures (CNN, U-Net, Transformer) and datasets. The code is available at olafseg.github.io",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted in The European Conference on Computer Vision (ECCV) 2024"
    },
    {
        "paper id": "2411.03033",
        "abstract url": "https://arxiv.org/abs/2411.03033",
        "title": "Rethinking Decoders for Transformer-based Semantic Segmentation: Compression is All You Need",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "State-of-the-art methods for Transformer-based semantic segmentation typically adopt Transformer decoders that are used to extract additional embeddings from image embeddings via cross-attention, refine either or both types of embeddings via self-attention, and project image embeddings onto the additional embeddings via dot-product. Despite their remarkable success, these empirical designs still lack theoretical justifications or interpretations, thus hindering potentially principled improvements. In this paper, we argue that there are fundamental connections between semantic segmentation and compression, especially between the Transformer decoders and Principal Component Analysis (PCA). From such a perspective, we derive a white-box, fully attentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the interpretations as follows: 1) the self-attention operator refines image embeddings to construct an ideal principal subspace that aligns with the supervision and retains most information; 2) the cross-attention operator seeks to find a low-rank approximation of the refined image embeddings, which is expected to be a set of orthonormal bases of the principal subspace and corresponds to the predefined classes; 3) the dot-product operation yields compact representation for image embeddings as segmentation masks. Experiments conducted on dataset ADE20K find that DEPICT consistently outperforms its black-box counterpart, Segmenter, and it is light weight and more robust.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "NeurIPS2024. Code:https://github.com/QishuaiWen/DEPICT/"
    },
    {
        "paper id": "2411.03034",
        "abstract url": "https://arxiv.org/abs/2411.03034",
        "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model",
        "rating": "1.5",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Human-scene vision-language tasks are increasingly prevalent in diverse social applications, yet recent advancements predominantly rely on models specifically tailored to individual tasks. Emerging research indicates that large vision-language models (VLMs) can enhance performance across various downstream vision-language understanding tasks. However, general-domain models often underperform in specialized fields. This study introduces a domain-specific Large Vision-Language Model, Human-Scene Vision-Language Model (HumanVLM), designed to provide a foundation for human-scene Vision-Language tasks. Specifically, (1) we create a large-scale human-scene multimodal image-text dataset (HumanCaption-10M) sourced from the Internet to facilitate domain-specific alignment; (2) develop a captioning approach for human-centered images, capturing human faces, bodies, and backgrounds, and construct a high-quality Human-Scene image-text dataset (HumanCaptionHQ, about 311k pairs) that contain as much detailed information as possible about human; (3) Using HumanCaption-10M and HumanCaptionHQ, we train a HumanVLM. In the experiments, we then evaluate our HumanVLM across varous downstream tasks, where it demonstrates superior overall performance among multimodal models of comparable scale, particularly excelling in human-related tasks and significantly outperforming similar models, including Qwen2VL and ChatGPT-4o. HumanVLM, alongside the data introduced, will stimulate the research in human-around fields.",
        "subjects": [
            "cs.AI",
            "cs.MM"
        ],
        "comment": "34 pages,11 figures"
    },
    {
        "paper id": "2411.03042",
        "abstract url": "https://arxiv.org/abs/2411.03042",
        "title": "Predictor-Corrector Enhanced Transformers with Exponential Moving Average Coefficient Learning",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Residual networks, as discrete approximations of Ordinary Differential Equations (ODEs), have inspired significant advancements in neural network design, including multistep methods, high-order methods, and multi-particle dynamical systems. The precision of the solution to ODEs significantly affects parameter optimization, thereby impacting model performance. In this work, we present a series of advanced explorations of Transformer architecture design to minimize the error compared to the true ``solution.'' First, we introduce a predictor-corrector learning framework to minimize truncation errors, which consists of a high-order predictor and a multistep corrector. Second, we propose an exponential moving average-based coefficient learning method to strengthen our higher-order predictor. Extensive experiments on large-scale machine translation, abstractive summarization, language modeling, and natural language understanding benchmarks demonstrate the superiority of our approach. On the WMT'14 English-German and English-French tasks, our model achieved BLEU scores of 30.95 and 44.27, respectively. Furthermore, on the OPUS multilingual machine translation task, our model surpasses a robust 3.8B DeepNet by an average of 2.9 SacreBLEU, using only 1/3 parameters. Notably, it also beats LLama models by 5.7 accuracy points on the LM Harness Evaluation.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by NeurIPS 2024"
    },
    {
        "paper id": "2411.03177",
        "abstract url": "https://arxiv.org/abs/2411.03177",
        "title": "On Improved Conditioning Mechanisms and Pre-training Strategies for Diffusion Models",
        "rating": "1.5",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "Diffusion",
                "text-to-image"
            ],
            [
                "cs.AI",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Large-scale training of latent diffusion models (LDMs) has enabled unprecedented quality in image generation. However, the key components of the best performing LDM training recipes are oftentimes not available to the research community, preventing apple-to-apple comparisons and hindering the validation of progress in the field. In this work, we perform an in-depth study of LDM training recipes focusing on the performance of models and their training efficiency. To ensure apple-to-apple comparisons, we re-implement five previously published models with their corresponding recipes. Through our study, we explore the effects of (i)~the mechanisms used to condition the generative model on semantic information (e.g., text prompt) and control metadata (e.g., crop size, random flip flag, etc.) on the model performance, and (ii)~the transfer of the representations learned on smaller and lower-resolution datasets to larger ones on the training efficiency and model performance. We then propose a novel conditioning mechanism that disentangles semantic and control metadata conditionings and sets a new state-of-the-art in class-conditional generation on the ImageNet-1k dataset -- with FID improvements of 7% on 256 and 8% on 512 resolutions -- as well as text-to-image generation on the CC12M dataset -- with FID improvements of 8% on 256 and 23% on 512 resolution.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Accepted as a conference paper (poster) for NeurIPS 2024"
    },
    {
        "paper id": "2411.03236",
        "abstract url": "https://arxiv.org/abs/2411.03236",
        "title": "Enhancing Transformer Training Efficiency with Dynamic Dropout",
        "rating": "1.5",
        "keywords": [
            [
                "Training Efficiency"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We introduce Dynamic Dropout, a novel regularization technique designed to enhance the training efficiency of Transformer models by dynamically adjusting the dropout rate based on training epochs or validation loss improvements. This approach addresses the challenge of balancing regularization and model capacity, which is crucial for achieving fast convergence and high performance. Our method involves modifying the GPT model to accept a variable dropout rate and updating dropout layers during training using schedules such as linear decay, exponential decay, and validation loss-based adjustments. Extensive experiments on the Shakespeare\\_char dataset demonstrate that Dynamic Dropout significantly accelerates training and improves inference efficiency compared to a baseline model with a fixed dropout rate. The validation loss-based adjustment schedule provided the best overall performance, highlighting the potential of Dynamic Dropout as a valuable technique for training large-scale Transformer models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03493",
        "abstract url": "https://arxiv.org/abs/2411.03493",
        "title": "LASER: Attention with Exponential Transformation",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Transformers have had tremendous impact for several sequence related tasks, largely due to their ability to retrieve from any part of the sequence via softmax based dot-product attention. This mechanism plays a crucial role in Transformer's performance. We analyze the gradients backpropagated through the softmax operation in the attention mechanism and observe that these gradients can often be small. This poor gradient signal backpropagation can lead to inefficient learning of parameters preceeding the attention operations. To this end, we introduce a new attention mechanism called LASER, which we analytically show to admit a larger gradient signal. We show that LASER Attention can be implemented by making small modifications to existing attention implementations. We conduct experiments on autoregressive large language models (LLMs) with upto 2.2 billion parameters where we show upto 3.38% and an average of ~1% improvement over standard attention on downstream evaluations. Using LASER gives the following relative improvements in generalization performance across a variety of tasks (vision, text and speech): 4.67% accuracy in Vision Transformer (ViT) on Imagenet, 2.25% error rate in Conformer on the Librispeech speech-to-text and 0.93% fraction of incorrect predictions in BERT with 2.2 billion parameters.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": "15 pages, under review in ICLR 2025"
    },
    {
        "paper id": "2411.03495",
        "abstract url": "https://arxiv.org/abs/2411.03495",
        "title": "Automatic Generation of Question Hints for Mathematics Problems using Large Language Models in Educational Technology",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "The automatic generation of hints by Large Language Models (LLMs) within Intelligent Tutoring Systems (ITSs) has shown potential to enhance student learning. However, generating pedagogically sound hints that address student misconceptions and adhere to specific educational objectives remains challenging. This work explores using LLMs (GPT-4o and Llama-3-8B-instruct) as teachers to generate effective hints for students simulated through LLMs (GPT-3.5-turbo, Llama-3-8B-Instruct, or Mistral-7B-instruct-v0.3) tackling math exercises designed for human high-school students, and designed using cognitive science principles. We present here the study of several dimensions: 1) identifying error patterns made by simulated students on secondary-level math exercises; 2) developing various prompts for GPT-4o as a teacher and evaluating their effectiveness in generating hints that enable simulated students to self-correct; and 3) testing the best-performing prompts, based on their ability to produce relevant hints and facilitate error correction, with Llama-3-8B-Instruct as the teacher, allowing for a performance comparison with GPT-4o. The results show that model errors increase with higher temperature settings. Notably, when hints are generated by GPT-4o, the most effective prompts include prompts tailored to specific errors as well as prompts providing general hints based on common mathematical errors. Interestingly, Llama-3-8B-Instruct as a teacher showed better overall performance than GPT-4o. Also the problem-solving and response revision capabilities of the LLMs as students, particularly GPT-3.5-turbo, improved significantly after receiving hints, especially at lower temperature settings. However, models like Mistral-7B-Instruct demonstrated a decline in performance as the temperature increased.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Accepted at NeurIPS 2024 Workshop on Large Foundation Models for Educational Assessment (FM-Assess)"
    },
    {
        "paper id": "2411.03513",
        "abstract url": "https://arxiv.org/abs/2411.03513",
        "title": "Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "This paper introduces a novel model compression approach through dynamic layer-specific pruning in Large Language Models (LLMs), enhancing the traditional methodology established by SliceGPT. By transitioning from constant to dynamic slicing, our method leverages the newly proposed Layer Redundancy (LR) score, which assesses how much change each layer changes its input by measuring the cosine similarity of the input to the output of the layer. We use this score to prune parts of individual layers based on redundancy in such a way that the average pruned percentage for all layers is a fixed value. We conducted extensive experiments using models like Llama3-8B and Mistral-7B on multiple datasets, evaluating different slicing bases and percentages to determine optimal configurations that balance efficiency and performance. Our findings show that our dynamic slicing approach not only maintains but, in many cases, enhances model performance compared to the baseline established by constant slicing methods. For instance, in several settings, we see performance improvements of up to 5% over the SliceGPT baseline. Additionally, a perplexity decrease by as much as 7% was observed across multiple benchmarks, validating the effectiveness of our method. The code, model weights, and datasets are open-sourced at https://github.com/RazvanDu/DynamicSlicing.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted at EMNLP Findings 2024"
    },
    {
        "paper id": "2411.03538",
        "abstract url": "https://arxiv.org/abs/2411.03538",
        "title": "Long Context RAG Performance of Large Language Models",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Retrieval Augmented Generation (RAG) has emerged as a crucial technique for enhancing the accuracy of Large Language Models (LLMs) by incorporating external information. With the advent of LLMs that support increasingly longer context lengths, there is a growing interest in understanding how these models perform in RAG scenarios. Can these new long context models improve RAG performance? This paper presents a comprehensive study of the impact of increased context length on RAG performance across 20 popular open source and commercial LLMs. We ran RAG workflows while varying the total context length from 2,000 to 128,000 tokens (and 2 million tokens when possible) on three domain-specific datasets, and report key insights on the benefits and limitations of long context in RAG applications. Our findings reveal that while retrieving more documents can improve performance, only a handful of the most recent state of the art LLMs can maintain consistent accuracy at long context above 64k tokens. We also identify distinct failure modes in long context scenarios, suggesting areas for future research.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": "2024 NeurIPS workshop on Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning"
    },
    {
        "paper id": "2411.03550",
        "abstract url": "https://arxiv.org/abs/2411.03550",
        "title": "Learning to Write Rationally: How Information Is Distributed in Non-Native Speakers' Essays",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "People tend to distribute information evenly in language production for better and clearer communication. In this study, we compared essays written by second language learners with various native language (L1) backgrounds to investigate how they distribute information in their non-native language (L2) production. Analyses of surprisal and constancy of entropy rate indicated that writers with higher L2 proficiency can reduce the expected uncertainty of language production while still conveying informative content. However, the uniformity of information distribution showed less variability among different groups of L2 speakers, suggesting that this feature may be universal in L2 essay writing and less affected by L2 writers' variability in L1 background and L2 proficiency.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "To appear in main of Conference on Empirical Methods in Natural Language Processing; EMNLP 2024"
    },
    {
        "paper id": "2411.02810",
        "abstract url": "https://arxiv.org/abs/2411.02810",
        "title": "Leveraging Vision-Language Models for Manufacturing Feature Recognition in CAD Designs",
        "rating": "1",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ]
        ],
        "abstract": "Automatic feature recognition (AFR) is essential for transforming design knowledge into actionable manufacturing information. Traditional AFR methods, which rely on predefined geometric rules and large datasets, are often time-consuming and lack generalizability across various manufacturing features. To address these challenges, this study investigates vision-language models (VLMs) for automating the recognition of a wide range of manufacturing features in CAD designs without the need for extensive training datasets or predefined rules. Instead, prompt engineering techniques, such as multi-view query images, few-shot learning, sequential reasoning, and chain-of-thought, are applied to enable recognition. The approach is evaluated on a newly developed CAD dataset containing designs of varying complexity relevant to machining, additive manufacturing, sheet metal forming, molding, and casting. Five VLMs, including three closed-source models (GPT-4o, Claude-3.5-Sonnet, and Claude-3.0-Opus) and two open-source models (LLava and MiniCPM), are evaluated on this dataset with ground truth features labelled by experts. Key metrics include feature quantity accuracy, feature name matching accuracy, hallucination rate, and mean absolute error (MAE). Results show that Claude-3.5-Sonnet achieves the highest feature quantity accuracy (74%) and name-matching accuracy (75%) with the lowest MAE (3.2), while GPT-4o records the lowest hallucination rate (8%). In contrast, open-source models have higher hallucination rates (>30%) and lower accuracies (<40%). This study demonstrates the potential of VLMs to automate feature recognition in CAD designs within diverse manufacturing scenarios.",
        "subjects": [
            "cs.CE",
            "cs.IR"
        ],
        "comment": "Paper has been submitted to The ASME Journal of Computing and Information Science in Engineering (JCISE)"
    },
    {
        "paper id": "2411.02816",
        "abstract url": "https://arxiv.org/abs/2411.02816",
        "title": "ChatGPT in Research and Education: Exploring Benefits and Threats",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In recent years, advanced artificial intelligence technologies, such as ChatGPT, have significantly impacted various fields, including education and research. Developed by OpenAI, ChatGPT is a powerful language model that presents numerous opportunities for students and educators. It offers personalized feedback, enhances accessibility, enables interactive conversations, assists with lesson preparation and evaluation, and introduces new methods for teaching complex subjects. However, ChatGPT also poses challenges to traditional education and research systems. These challenges include the risk of cheating on online exams, the generation of human-like text that may compromise academic integrity, a potential decline in critical thinking skills, and difficulties in assessing the reliability of information generated by AI. This study examines both the opportunities and challenges ChatGPT brings to education from the perspectives of students and educators. Specifically, it explores the role of ChatGPT in helping students develop their subjective skills. To demonstrate its effectiveness, we conducted several subjective experiments using ChatGPT, such as generating solutions from subjective problem descriptions. Additionally, surveys were conducted with students and teachers to gather insights into how ChatGPT supports subjective learning and teaching. The results and analysis of these surveys are presented to highlight the impact of ChatGPT in this context.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02817",
        "abstract url": "https://arxiv.org/abs/2411.02817",
        "title": "Conditional Vendi Score: An Information-Theoretic Approach to Diversity Evaluation of Prompt-based Generative Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Text-conditioned generation models are commonly evaluated based on the quality of the generated data and its alignment with the input text prompt. On the other hand, several applications of prompt-based generative models require sufficient diversity in the generated data to ensure the models' capability of generating image and video samples possessing a variety of features. However, most existing diversity metrics are designed for unconditional generative models, and thus cannot distinguish the diversity arising from variations in text prompts and that contributed by the generative model itself. In this work, our goal is to quantify the prompt-induced and model-induced diversity in samples generated by prompt-based models. We propose an information-theoretic approach for internal diversity quantification, where we decompose the kernel-based entropy $H(X)$ of the generated data $X$ into the sum of the conditional entropy $H(X|T)$, given text variable $T$, and the mutual information $I(X; T)$ between the text and data variables. We introduce the \\emph{Conditional-Vendi} score based on $H(X|T)$ to quantify the internal diversity of the model and the \\emph{Information-Vendi} score based on $I(X; T)$ to measure the statistical relevance between the generated data and text prompts. We provide theoretical results to statistically interpret these scores and relate them to the unconditional Vendi score. We conduct several numerical experiments to show the correlation between the Conditional-Vendi score and the internal diversity of text-conditioned generative models. The codebase is available at \\href{https://github.com/mjalali/conditional-vendi}{https://github.com/mjalali/conditional-vendi}.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02820",
        "abstract url": "https://arxiv.org/abs/2411.02820",
        "title": "DroidSpeak: Enhancing Cross-LLM Communication",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "In multi-agent systems utilizing Large Language Models (LLMs), communication between agents traditionally relies on natural language. This communication often includes the full context of the query so far, which can introduce significant prefill-phase latency, especially with long contexts. We introduce DroidSpeak, a novel framework to target this cross-LLM communication by leveraging the reuse of intermediate data, such as input embeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the need to reprocess entire contexts for fine-tuned versions of the same foundational model. This approach allows faster context integration while maintaining the quality of task performance. Experimental evaluations demonstrate DroidSpeak's ability to significantly accelerate inter-agent communication, achieving up to a 2.78x speedup in prefill latency with negligible loss in accuracy. Our findings underscore the potential to create more efficient and scalable multi-agent systems.",
        "subjects": [
            "cs.MA",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02830",
        "abstract url": "https://arxiv.org/abs/2411.02830",
        "title": "Mixtures of In-Context Learners",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "In-context learning (ICL) adapts LLMs by providing demonstrations without fine-tuning the model parameters; however, it does not differentiate between demonstrations and quadratically increases the complexity of Transformer LLMs, exhausting the memory. As a solution, we propose Mixtures of In-Context Learners (MoICL), a novel approach to treat subsets of demonstrations as experts and learn a weighting function to merge their output distributions based on a training set. In our experiments, we show performance improvements on 5 out of 7 classification datasets compared to a set of strong baselines (up to +13\\% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of ICL by reducing the inference time needed to achieve the same performance with fewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to +11\\%), imbalanced (up to +49\\%), or noisy demonstrations (up to +38\\%) or can filter these out from datasets. Overall, MoICL is a more expressive approach to learning from demonstrations without exhausting the context window or memory.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02832",
        "abstract url": "https://arxiv.org/abs/2411.02832",
        "title": "PersianRAG: A Retrieval-Augmented Generation System for Persian Language",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Retrieval augmented generation (RAG) models, which integrate large-scale pre-trained generative models with external retrieval mechanisms, have shown significant success in various natural language processing (NLP) tasks. However, applying RAG models in Persian language as a low-resource language, poses distinct challenges. These challenges primarily involve the preprocessing, embedding, retrieval, prompt construction, language modeling, and response evaluation of the system. In this paper, we address the challenges towards implementing a real-world RAG system for Persian language called PersianRAG. We propose novel solutions to overcome these obstacles and evaluate our approach using several Persian benchmark datasets. Our experimental results demonstrate the capability of the PersianRAG framework to enhance question answering task in Persian.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02833",
        "abstract url": "https://arxiv.org/abs/2411.02833",
        "title": "Lost in Context: The Influence of Context on Feature Attribution Methods for Object Recognition",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Contextual information plays a critical role in object recognition models within computer vision, where changes in context can significantly affect accuracy, underscoring models' dependence on contextual cues. This study investigates how context manipulation influences both model accuracy and feature attribution, providing insights into the reliance of object recognition models on contextual information as understood through the lens of feature attribution methods. We employ a range of feature attribution techniques to decipher the reliance of deep neural networks on context in object recognition tasks. Using the ImageNet-9 and our curated ImageNet-CS datasets, we conduct experiments to evaluate the impact of contextual variations, analyzed through feature attribution methods. Our findings reveal several key insights: (a) Correctly classified images predominantly emphasize object volume attribution over context volume attribution. (b) The dependence on context remains relatively stable across different context modifications, irrespective of classification accuracy. (c) Context change exerts a more pronounced effect on model performance than Context perturbations. (d) Surprisingly, context attribution in `no-information' scenarios is non-trivial. Our research moves beyond traditional methods by assessing the implications of broad-level modifications on object recognition, either in the object or its context.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Published in ICVGIP 2024"
    },
    {
        "paper id": "2411.02853",
        "abstract url": "https://arxiv.org/abs/2411.02853",
        "title": "ADOPT: Modified Adam Can Converge with Any $\u03b2_2$ with the Optimal Rate",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Adam is one of the most popular optimization algorithms in deep learning. However, it is known that Adam does not converge in theory unless choosing a hyperparameter, i.e., $\u03b2_2$, in a problem-dependent manner. There have been many attempts to fix the non-convergence (e.g., AMSGrad), but they require an impractical assumption that the gradient noise is uniformly bounded. In this paper, we propose a new adaptive gradient method named ADOPT, which achieves the optimal convergence rate of $\\mathcal{O} ( 1 / \\sqrt{T} )$ with any choice of $\u03b2_2$ without depending on the bounded noise assumption. ADOPT addresses the non-convergence issue of Adam by removing the current gradient from the second moment estimate and changing the order of the momentum update and the normalization by the second moment estimate. We also conduct intensive numerical experiments, and verify that our ADOPT achieves superior results compared to Adam and its variants across a wide range of tasks, including image classification, generative modeling, natural language processing, and deep reinforcement learning. The implementation is available at https://github.com/iShohei220/adopt.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "Accepted at Neural Information Processing Systems (NeurIPS 2024)"
    },
    {
        "paper id": "2411.02862",
        "abstract url": "https://arxiv.org/abs/2411.02862",
        "title": "The Unreasonable Effectiveness of LLMs for Query Optimization",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Recent work in database query optimization has used complex machine learning strategies, such as customized reinforcement learning schemes. Surprisingly, we show that LLM embeddings of query text contain useful semantic information for query optimization. Specifically, we show that a simple binary classifier deciding between alternative query plans, trained only on a small number of labeled embedded query vectors, can outperform existing heuristic systems. Although we only present some preliminary results, an LLM-powered query optimizer could provide significant benefits, both in terms of performance and simplicity.",
        "subjects": [
            "cs.DB",
            "cs.LG"
        ],
        "comment": "To appear in the Machine Learning for Systems Workshop at NeurIPS 2024"
    },
    {
        "paper id": "2411.02871",
        "abstract url": "https://arxiv.org/abs/2411.02871",
        "title": "Enhancing Adversarial Robustness via Uncertainty-Aware Distributional Adversarial Training",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Despite remarkable achievements in deep learning across various domains, its inherent vulnerability to adversarial examples still remains a critical concern for practical deployment. Adversarial training has emerged as one of the most effective defensive techniques for improving model robustness against such malicious inputs. However, existing adversarial training schemes often lead to limited generalization ability against underlying adversaries with diversity due to their overreliance on a point-by-point augmentation strategy by mapping each clean example to its adversarial counterpart during training. In addition, adversarial examples can induce significant disruptions in the statistical information w.r.t. the target model, thereby introducing substantial uncertainty and challenges to modeling the distribution of adversarial examples. To circumvent these issues, in this paper, we propose a novel uncertainty-aware distributional adversarial training method, which enforces adversary modeling by leveraging both the statistical information of adversarial examples and its corresponding uncertainty estimation, with the goal of augmenting the diversity of adversaries. Considering the potentially negative impact induced by aligning adversaries to misclassified clean examples, we also refine the alignment reference based on the statistical proximity to clean examples during adversarial training, thereby reframing adversarial training within a distribution-to-distribution matching framework interacted between the clean and adversarial domains. Furthermore, we design an introspective gradient alignment approach via matching input gradients between these domains without introducing external models. Extensive experiments across four benchmark datasets and various network architectures demonstrate that our approach achieves state-of-the-art adversarial robustness and maintains natural performance.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02886",
        "abstract url": "https://arxiv.org/abs/2411.02886",
        "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "With the development of large language models (LLMs), the ability to handle longer contexts has become a key capability for Web applications such as cross-document understanding and LLM-powered search systems. However, this progress faces two major challenges: performance degradation due to sequence lengths out-of-distribution, and excessively long inference times caused by the quadratic computational complexity of attention. These issues hinder the application of LLMs in long-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic, training-free method for efficient and accurate long-context inference. TokenSelect builds upon the observation of non-contiguous attention sparsity, using Query-Key dot products to measure per-head KV Cache criticality at token-level. By per-head soft voting mechanism, TokenSelect selectively involves a small number of critical KV cache tokens in the attention calculation without sacrificing accuracy. To further accelerate TokenSelect, we designed the Selection Cache based on observations of consecutive Query similarity and implemented efficient dot product kernel, significantly reducing the overhead of token selection. A comprehensive evaluation of TokenSelect demonstrates up to 23.84x speedup in attention computation and up to 2.28x acceleration in end-to-end latency, while providing superior performance compared to state-of-the-art long-context inference methods.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02887",
        "abstract url": "https://arxiv.org/abs/2411.02887",
        "title": "The Translation of Circumlocution in Arabic Short Stories into English",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This study investigates the translation of circumlocution from Arabic to English in a corpus of short stories by renowned Arabic authors. By analyzing the source and target texts, the study aims to identify and categorize circumlocution instances in Arabic and their corresponding renditions in English. The study employs Nida's (1964) translation theory as a framework to assess the appropriateness of the translation strategies employed. It examines the extent to which translators successfully rendered Arabic circumlocution into English, identifying potential challenges and limitations in the translation process. The findings reveal significant similarities between Arabic circumlocution categories and English metadiscourse categories, particularly in terms of textual and interpersonal functions. However, the study also highlights instances where translators encountered difficulties in accurately conveying the nuances of circumlocution, often resorting to strategies like addition, subtraction, and alteration.https://ntu.edu.iq/",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02889",
        "abstract url": "https://arxiv.org/abs/2411.02889",
        "title": "Turbulence stabilization",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We recently developed a new approach to get a stabilized image from a sequence of frames acquired through atmospheric turbulence. The goal of this algorihtm is to remove the geometric distortions due by the atmosphere movements. This method is based on a variational formulation and is efficiently solved by the use of Bregman iterations and the operator splitting method. In this paper we propose to study the influence of the choice of the regularizing term in the model. Then we proposed to experiment some of the most used regularization constraints available in the litterature.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02890",
        "abstract url": "https://arxiv.org/abs/2411.02890",
        "title": "Fried deconvolution",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper we present a new approach to deblur the effect of atmospheric turbulence in the case of long range imaging. Our method is based on an analytical formulation, the Fried kernel, of the atmosphere modulation transfer function (MTF) and a framelet based deconvolution algorithm. An important parameter is the refractive index structure which requires specific measurements to be known. Then we propose a method which provides a good estimation of this parameter from the input blurred image. The final algorithms are very easy to implement and show very good results on both simulated blur and real images.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02920",
        "abstract url": "https://arxiv.org/abs/2411.02920",
        "title": "Domain Expansion and Boundary Growth for Open-Set Single-Source Domain Generalization",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Open-set single-source domain generalization aims to use a single-source domain to learn a robust model that can be generalized to unknown target domains with both domain shifts and label shifts. The scarcity of the source domain and the unknown data distribution of the target domain pose a great challenge for domain-invariant feature learning and unknown class recognition. In this paper, we propose a novel learning approach based on domain expansion and boundary growth to expand the scarce source samples and enlarge the boundaries across the known classes that indirectly broaden the boundary between the known and unknown classes. Specifically, we achieve domain expansion by employing both background suppression and style augmentation on the source data to synthesize new samples. Then we force the model to distill consistent knowledge from the synthesized samples so that the model can learn domain-invariant information. Furthermore, we realize boundary growth across classes by using edge maps as an additional modality of samples when training multi-binary classifiers. In this way, it enlarges the boundary between the inliers and outliers, and consequently improves the unknown class recognition during open-set generalization. Extensive experiments show that our approach can achieve significant improvements and reach state-of-the-art performance on several cross-domain image classification datasets.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "TMM 2024"
    },
    {
        "paper id": "2411.02930",
        "abstract url": "https://arxiv.org/abs/2411.02930",
        "title": "Textual Aesthetics in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Image aesthetics is a crucial metric in the field of image generation. However, textual aesthetics has not been sufficiently explored. With the widespread application of large language models (LLMs), previous work has primarily focused on the correctness of content and the helpfulness of responses. Nonetheless, providing responses with textual aesthetics is also an important factor for LLMs, which can offer a cleaner layout and ensure greater consistency and coherence in content. In this work, we introduce a pipeline for aesthetics polishing and help construct a textual aesthetics dataset named TexAes. We propose a textual aesthetics-powered fine-tuning method based on direct preference optimization, termed TAPO, which leverages textual aesthetics without compromising content correctness. Additionally, we develop two evaluation methods for textual aesthetics based on text and image analysis, respectively. Our experiments demonstrate that using textual aesthetics data and employing the TAPO fine-tuning method not only improves aesthetic scores but also enhances performance on general evaluation datasets such as AlpacalEval and Anera-hard.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02937",
        "abstract url": "https://arxiv.org/abs/2411.02937",
        "title": "Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Multimodal Retrieval Augmented Generation (mRAG) plays an important role in mitigating the \"hallucination\" issue inherent in multimodal large language models (MLLMs). Although promising, existing heuristic mRAGs typically predefined fixed retrieval processes, which causes two issues: (1) Non-adaptive Retrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws cannot be adequately reflected by current knowledge-seeking visual question answering (VQA) datasets, since the most required knowledge can be readily obtained with a standard two-step retrieval. To bridge the dataset gap, we first construct Dyn-VQA dataset, consisting of three types of \"dynamic\" questions, which require complex knowledge retrieval strategies variable in query, tool, and time: (1) Questions with rapidly changing answers. (2) Questions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments on Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient and precisely relevant knowledge for dynamic questions due to their rigid retrieval processes. Hence, we further propose the first self-adaptive planning agent for multimodal retrieval, OmniSearch. The underlying idea is to emulate the human behavior in question solution which dynamically decomposes complex multimodal questions into sub-question chains with retrieval action. Extensive experiments prove the effectiveness of our OmniSearch, also provide direction for advancing mRAG. The code and dataset will be open-sourced at https://github.com/Alibaba-NLP/OmniSearch.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02939",
        "abstract url": "https://arxiv.org/abs/2411.02939",
        "title": "A Post-Training Enhanced Optimization Approach for Small Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "This paper delves into the continuous post-training optimization methods for small language models, and proposes a continuous post-training alignment data construction method for small language models. The core of this method is based on the data guidance of large models, optimizing the diversity and accuracy of alignment data. In addition, to verify the effectiveness of the methods in this paper, we used Qwen2-0.5B-Instruct model as the baseline model for small language models, using the alignment dataset constructed by our proposed method, we trained and compared several groups of experiments, including SFT (Supervised Fine Tuning) post-training experiment and KTO (Kahneman Tversky optimization) post-training experiment, as well as SFT-KTO two-stage post-training experiment and model weight fusion experiment. Finally, we evaluated and analyzed the performance of post-training models, and confirmed that the continuous post-training optimization method proposed by us can significantly improve the performance of small language models.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02943",
        "abstract url": "https://arxiv.org/abs/2411.02943",
        "title": "Capturing research literature attitude towards Sustainable Development Goals: an LLM-based topic modeling approach",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The world is facing a multitude of challenges that hinder the development of human civilization and the well-being of humanity on the planet. The Sustainable Development Goals (SDGs) were formulated by the United Nations in 2015 to address these global challenges by 2030. Natural language processing techniques can help uncover discussions on SDGs within research literature. We propose a completely automated pipeline to 1) fetch content from the Scopus database and prepare datasets dedicated to five groups of SDGs; 2) perform topic modeling, a statistical technique used to identify topics in large collections of textual data; and 3) enable topic exploration through keywords-based search and topic frequency time series extraction. For topic modeling, we leverage the stack of BERTopic scaled up to be applied on large corpora of textual documents (we find hundreds of topics on hundreds of thousands of documents), introducing i) a novel LLM-based embeddings computation for representing scientific abstracts in the continuous space and ii) a hyperparameter optimizer to efficiently find the best configuration for any new big datasets. We additionally produce the visualization of results on interactive dashboards reporting topics' temporal evolution. Results are made inspectable and explorable, contributing to the interpretability of the topic modeling process. Our proposed LLM-based topic modeling pipeline for big-text datasets allows users to capture insights on the evolution of the attitude toward SDGs within scientific abstracts in the 2006-2023 time span. All the results are reproducible by using our system; the workflow can be generalized to be applied at any point in time to any big corpus of textual documents.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "27 pages, 8 figures, 5 tables"
    },
    {
        "paper id": "2411.02988",
        "abstract url": "https://arxiv.org/abs/2411.02988",
        "title": "Confidence Calibration of Classifiers with Many Classes",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "For classification models based on neural networks, the maximum predicted class probability is often used as a confidence score. This score rarely predicts well the probability of making a correct prediction and requires a post-processing calibration step. However, many confidence calibration methods fail for problems with many classes. To address this issue, we transform the problem of calibrating a multiclass classifier into calibrating a single surrogate binary classifier. This approach allows for more efficient use of standard calibration methods. We evaluate our approach on numerous neural networks used for image or text classification and show that it significantly enhances existing calibration methods.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "NeurIPS 2024; code available at https://github.com/allglc/tva-calibration"
    },
    {
        "paper id": "2411.02989",
        "abstract url": "https://arxiv.org/abs/2411.02989",
        "title": "Growing a Tail: Increasing Output Diversity in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "How diverse are the outputs of large language models when diversity is desired? We examine the diversity of responses of various models to questions with multiple possible answers, comparing them with human responses. Our findings suggest that models' outputs are highly concentrated, reflecting a narrow, mainstream 'worldview', in comparison to humans, whose responses exhibit a much longer-tail. We examine three ways to increase models' output diversity: 1) increasing generation randomness via temperature sampling; 2) prompting models to answer from diverse perspectives; 3) aggregating outputs from several models. A combination of these measures significantly increases models' output diversity, reaching that of humans. We discuss implications of these findings for AI policy that wishes to preserve cultural diversity, an essential building block of a democratic social fabric.",
        "subjects": [
            "cs.CL",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02999",
        "abstract url": "https://arxiv.org/abs/2411.02999",
        "title": "Precise Drive with VLM: First Prize Solution for PRCV 2024 Drive LM challenge",
        "rating": "1",
        "keywords": [
            [
                "VLM"
            ],
            [
                "autonomous driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This technical report outlines the methodologies we applied for the PRCV Challenge, focusing on cognition and decision-making in driving scenarios. We employed InternVL-2.0, a pioneering open-source multi-modal model, and enhanced it by refining both the model input and training methodologies. For the input data, we strategically concatenated and formatted the multi-view images. It is worth mentioning that we utilized the coordinates of the original images without transformation. In terms of model training, we initially pre-trained the model on publicly available autonomous driving scenario datasets to bolster its alignment capabilities of the challenge tasks, followed by fine-tuning on the DriveLM-nuscenes Dataset. During the fine-tuning phase, we innovatively modified the loss function to enhance the model's precision in predicting coordinate values. These approaches ensure that our model possesses advanced cognitive and decision-making capabilities in driving scenarios. Consequently, our model achieved a score of 0.6064, securing the first prize on the competition's final results.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03008",
        "abstract url": "https://arxiv.org/abs/2411.03008",
        "title": "Hierarchical Orchestra of Policies",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Continual reinforcement learning poses a major challenge due to the tendency of agents to experience catastrophic forgetting when learning sequential tasks. In this paper, we introduce a modularity-based approach, called Hierarchical Orchestra of Policies (HOP), designed to mitigate catastrophic forgetting in lifelong reinforcement learning. HOP dynamically forms a hierarchy of policies based on a similarity metric between the current observations and previously encountered observations in successful tasks. Unlike other state-of-the-art methods, HOP does not require task labelling, allowing for robust adaptation in environments where boundaries between tasks are ambiguous. Our experiments, conducted across multiple tasks in a procedurally generated suite of environments, demonstrate that HOP significantly outperforms baseline methods in retaining knowledge across tasks and performs comparably to state-of-the-art transfer methods that require task labelling. Moreover, HOP achieves this without compromising performance when tasks remain constant, highlighting its versatility.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Accepted as a poster. NeurIPS IMOL"
    },
    {
        "paper id": "2411.03016",
        "abstract url": "https://arxiv.org/abs/2411.03016",
        "title": "Real-Time Scream Detection and Position Estimation for Worker Safety in Construction Sites",
        "rating": "1",
        "keywords": [
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "The construction industry faces high risks due to frequent accidents, often leaving workers in perilous situations where rapid response is critical. Traditional safety monitoring methods, including wearable sensors and GPS, often fail under obstructive or indoor conditions. This research introduces a novel real-time scream detection and localization system tailored for construction sites, especially in low-resource environments. Integrating Wav2Vec2 and Enhanced ConvNet models for accurate scream detection, coupled with the GCC-PHAT algorithm for robust time delay estimation under reverberant conditions, followed by a gradient descent-based approach to achieve precise position estimation in noisy environments. Our approach combines these concepts to achieve high detection accuracy and rapid localization, thereby minimizing false alarms and optimizing emergency response. Preliminary results demonstrate that the system not only accurately detects distress calls amidst construction noise but also reliably identifies the caller's location. This solution represents a substantial improvement in worker safety, with the potential for widespread application across high-risk occupational environments. The scripts used for training, evaluation of scream detection, position estimation, and integrated framework will be released at: https://github.com/Anmol2059/construction_safety.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": "12 pages, 14 figures, 1 table, submitted to AIRISE conference"
    },
    {
        "paper id": "2411.03039",
        "abstract url": "https://arxiv.org/abs/2411.03039",
        "title": "Self-Compositional Data Augmentation for Scientific Keyphrase Generation",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "State-of-the-art models for keyphrase generation require large amounts of training data to achieve good performance. However, obtaining keyphrase-labeled documents can be challenging and costly. To address this issue, we present a self-compositional data augmentation method. More specifically, we measure the relatedness of training documents based on their shared keyphrases, and combine similar documents to generate synthetic samples. The advantage of our method lies in its ability to create additional training samples that keep domain coherence, without relying on external data or resources. Our results on multiple datasets spanning three different domains, demonstrate that our method consistently improves keyphrase generation. A qualitative analysis of the generated keyphrases for the Computer Science domain confirms this improvement towards their representativity property.",
        "subjects": [
            "cs.CL",
            "cs.IR"
        ],
        "comment": "Accepted to JCDL 2024. This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive version was published in the proceedings of the 2024 ACM/IEEE Joint Conference on Digital Libraries (JCDL 24) https://doi.org/10.1145/3677389.3702504"
    },
    {
        "paper id": "2411.03055",
        "abstract url": "https://arxiv.org/abs/2411.03055",
        "title": "ATM: Improving Model Merging by Alternating Tuning and Merging",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Model merging has recently emerged as a cost-efficient paradigm for multi-task learning. Among current approaches, task arithmetic stands out for its simplicity and effectiveness. In this paper, we motivate the effectiveness of task vectors by linking them to multi-task gradients. We show that in a single-epoch scenario, task vectors are mathematically equivalent to the gradients obtained via gradient descent in a multi-task setting, and still approximate these gradients in subsequent epochs. Furthermore, we show that task vectors perform optimally when equality is maintained, and their effectiveness is largely driven by the first epoch's gradient. Building on this insight, we propose viewing model merging as a single step in an iterative process that Alternates between Tuning and Merging (ATM). This method acts as a bridge between model merging and multi-task gradient descent, achieving state-of-the-art results with the same data and computational requirements. We extensively evaluate ATM across diverse settings, achieving up to 20% higher accuracy in computer vision and NLP tasks, compared to the best baselines. Finally, we provide both empirical and theoretical support for its effectiveness, demonstrating increased orthogonality between task vectors and proving that ATM minimizes an upper bound on the loss obtained by jointly finetuning all tasks.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "Main paper: 10 Pages, 11 figures, 2 tables"
    },
    {
        "paper id": "2411.03085",
        "abstract url": "https://arxiv.org/abs/2411.03085",
        "title": "Speech Separation with Pretrained Frontend to Minimize Domain Mismatch",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Speech separation seeks to separate individual speech signals from a speech mixture. Typically, most separation models are trained on synthetic data due to the unavailability of target reference in real-world cocktail party scenarios. As a result, there exists a domain gap between real and synthetic data when deploying speech separation models in real-world applications. In this paper, we propose a self-supervised domain-invariant pretrained (DIP) frontend that is exposed to mixture data without the need for target reference speech. The DIP frontend utilizes a Siamese network with two innovative pretext tasks, mixture predictive coding (MPC) and mixture invariant coding (MIC), to capture shared contextual cues between real and synthetic unlabeled mixtures. Subsequently, we freeze the DIP frontend as a feature extractor when training the downstream speech separation models on synthetic data. By pretraining the DIP frontend with the contextual cues, we expect that the speech separation skills learned from synthetic data can be effectively transferred to real data. To benefit from the DIP frontend, we introduce a novel separation pipeline to align the feature resolution of the separation models. We evaluate the speech separation quality on standard benchmarks and real-world datasets. The results confirm the superiority of our DIP frontend over existing speech separation models. This study underscores the potential of large-scale pretraining to enhance the quality and intelligibility of speech separation in real-world applications.",
        "subjects": [
            "cs.SD",
            "cs.LG",
            "cs.MM",
            "eess.AS"
        ],
        "comment": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
        "paper id": "2411.03087",
        "abstract url": "https://arxiv.org/abs/2411.03087",
        "title": "On Differentially Private Linear Algebra",
        "rating": "1",
        "keywords": [
            [
                "time efficient"
            ]
        ],
        "abstract": "We introduce efficient differentially private (DP) algorithms for several linear algebraic tasks, including solving linear equalities over arbitrary fields, linear inequalities over the reals, and computing affine spans and convex hulls. As an application, we obtain efficient DP algorithms for learning halfspaces and affine subspaces. Our algorithms addressing equalities are strongly polynomial, whereas those addressing inequalities are weakly polynomial. Furthermore, this distinction is inevitable: no DP algorithm for linear programming can be strongly polynomial-time efficient.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03097",
        "abstract url": "https://arxiv.org/abs/2411.03097",
        "title": "Correlating Variational Autoencoders Natively For Multi-View Imputation",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Multi-view data from the same source often exhibit correlation. This is mirrored in correlation between the latent spaces of separate variational autoencoders (VAEs) trained on each data-view. A multi-view VAE approach is proposed that incorporates a joint prior with a non-zero correlation structure between the latent spaces of the VAEs. By enforcing such correlation structure, more strongly correlated latent spaces are uncovered. Using conditional distributions to move between these latent spaces, missing views can be imputed and used for downstream analysis. Learning this correlation structure involves maintaining validity of the prior distribution, as well as a successful parameterization that allows end-to-end learning.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "Accepted at 'UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models', a workshop at NeurIPS 2024"
    },
    {
        "paper id": "2411.03107",
        "abstract url": "https://arxiv.org/abs/2411.03107",
        "title": "Near-Optimal Dynamic Regret for Adversarial Linear Mixture MDPs",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We study episodic linear mixture MDPs with the unknown transition and adversarial rewards under full-information feedback, employing dynamic regret as the performance measure. We start with in-depth analyses of the strengths and limitations of the two most popular methods: occupancy-measure-based and policy-based methods. We observe that while the occupancy-measure-based method is effective in addressing non-stationary environments, it encounters difficulties with the unknown transition. In contrast, the policy-based method can deal with the unknown transition effectively but faces challenges in handling non-stationary environments. Building on this, we propose a novel algorithm that combines the benefits of both methods. Specifically, it employs (i) an occupancy-measure-based global optimization with a two-layer structure to handle non-stationary environments; and (ii) a policy-based variance-aware value-targeted regression to tackle the unknown transition. We bridge these two parts by a novel conversion. Our algorithm enjoys an $\\widetilde{\\mathcal{O}}(d \\sqrt{H^3 K} + \\sqrt{HK(H + \\bar{P}_K)})$ dynamic regret, where $d$ is the feature dimension, $H$ is the episode length, $K$ is the number of episodes, $\\bar{P}_K$ is the non-stationarity measure. We show it is minimax optimal up to logarithmic factors by establishing a matching lower bound. To the best of our knowledge, this is the first work that achieves near-optimal dynamic regret for adversarial linear mixture MDPs with the unknown transition without prior knowledge of the non-stationarity measure.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": "NeurIPS 2024"
    },
    {
        "paper id": "2411.03109",
        "abstract url": "https://arxiv.org/abs/2411.03109",
        "title": "pTSE-T: Presentation Target Speaker Extraction using Unaligned Text Cues",
        "rating": "1",
        "keywords": [
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "TSE aims to extract the clean speech of the target speaker in an audio mixture, thus eliminating irrelevant background noise and speech. While prior work has explored various auxiliary cues including pre-recorded speech, visual information (e.g., lip motions and gestures), and spatial information, the acquisition and selection of such strong cues are infeasible in many practical scenarios. Unlike all existing work, in this paper, we condition the TSE algorithm on semantic cues extracted from limited and unaligned text content, such as condensed points from a presentation slide. This method is particularly useful in scenarios like meetings, poster sessions, or lecture presentations, where acquiring other cues in real-time is challenging. To this end, we design two different networks. Specifically, our proposed TPE fuses audio features with content-based semantic cues to facilitate time-frequency mask generation to filter out extraneous noise, while another proposal, namely TSR, employs the contrastive learning technique to associate blindly separated speech signals with semantic cues. The experimental results show the efficacy in accurately identifying the target speaker by utilizing semantic cues derived from limited and unaligned text, resulting in SI-SDRi of 12.16 dB, SDRi of 12.66 dB, PESQi of 0.830 and STOIi of 0.150, respectively. Dataset and source code will be publicly available. Project demo page: https://slideTSE.github.io/.",
        "subjects": [
            "cs.SD",
            "cs.MM",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03150",
        "abstract url": "https://arxiv.org/abs/2411.03150",
        "title": "Noise-Robust Hearing Aid Voice Control",
        "rating": "1",
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "Advancing the design of robust hearing aid (HA) voice control is crucial to increase the HA use rate among hard of hearing people as well as to improve HA users' experience. In this work, we contribute towards this goal by, first, presenting a novel HA speech dataset consisting of noisy own voice captured by 2 behind-the-ear (BTE) and 1 in-ear-canal (IEC) microphones. Second, we provide baseline HA voice control results from the evaluation of light, state-of-the-art keyword spotting models utilizing different combinations of HA microphone signals. Experimental results show the benefits of exploiting bandwidth-limited bone-conducted speech (BCS) from the IEC microphone to achieve noise-robust HA voice control. Furthermore, results also demonstrate that voice control performance can be boosted by assisting BCS by the broader-bandwidth BTE microphone signals. Aiming at setting a baseline upon which the scientific community can continue to progress, the HA noisy speech dataset has been made publicly available.",
        "subjects": [
            "eess.AS"
        ],
        "comment": "Submitted to IEEE Signal Processing Letters"
    },
    {
        "paper id": "2411.03168",
        "abstract url": "https://arxiv.org/abs/2411.03168",
        "title": "Reference Microphone Selection for the Weighted Prediction Error Algorithm using the Normalized L-p Norm",
        "rating": "1",
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "Reverberation may severely degrade the quality of speech signals recorded using microphones in a room. For compact microphone arrays, the choice of the reference microphone for multi-microphone dereverberation typically does not have a large influence on the dereverberation performance. In contrast, when the microphones are spatially distributed, the choice of the reference microphone may significantly contribute to the dereverberation performance. In this paper, we propose to perform reference microphone selection for the weighted prediction error (WPE) dereverberation algorithm based on the normalized $\\ell_p$-norm of the dereverberated output signal. Experimental results for different source positions in a reverberant laboratory show that the proposed method yields a better dereverberation performance than reference microphone selection based on the early-to-late reverberation ratio or signal power.",
        "subjects": [
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03226",
        "abstract url": "https://arxiv.org/abs/2411.03226",
        "title": "Kernel Orthogonality does not necessarily imply a Decrease in Feature Map Redundancy in CNNs: Convolutional Similarity Minimization",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Convolutional Neural Networks (CNNs) have been heavily used in Deep Learning due to their success in various tasks. Nonetheless, it has been observed that CNNs suffer from redundancy in feature maps, leading to inefficient capacity utilization. Efforts to mitigate and solve this problem led to the emergence of multiple methods, amongst which is kernel orthogonality through variant means. In this work, we challenge the common belief that kernel orthogonality leads to a decrease in feature map redundancy, which is, supposedly, the ultimate objective behind kernel orthogonality. We prove, theoretically and empirically, that kernel orthogonality has an unpredictable effect on feature map similarity and does not necessarily decrease it. Based on our theoretical result, we propose an effective method to reduce feature map similarity independently of the input of the CNN. This is done by minimizing a novel loss function we call Convolutional Similarity. Empirical results show that minimizing the Convolutional Similarity increases the performance of classification models and can accelerate their convergence. Furthermore, using our proposed method pushes towards a more efficient use of the capacity of models, allowing the use of significantly smaller models to achieve the same levels of performance.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03260",
        "abstract url": "https://arxiv.org/abs/2411.03260",
        "title": "ShadowMamba: State-Space Model with Boundary-Region Selective Scan for Shadow Removal",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Image shadow removal is a typical low-level vision problem, where the presence of shadows leads to abrupt changes in brightness in certain regions, affecting the accuracy of upstream tasks. Current shadow removal methods still face challenges such as residual boundary artifacts, and capturing feature information at shadow boundaries is crucial for removing shadows and eliminating residual boundary artifacts. Recently, Mamba has achieved remarkable success in computer vision by globally modeling long-sequence information with linear complexity. However, when applied to image shadow removal, the original Mamba scanning method overlooks the semantic continuity of shadow boundaries as well as the continuity of semantics within the same region. Based on the unique characteristics of shadow images, this paper proposes a novel selective scanning method called boundary-region selective scanning. This method scans boundary regions, shadow regions, and non-shadow regions independently, bringing pixels of the same region type closer together in the long sequence, especially focusing on the local information at the boundaries, which is crucial for shadow removal. This method combines with global scanning and channel scanning to jointly accomplish the shadow removal. We name our model ShadowMamba, the first Mamba-based model for shadow removal. Extensive experimental results show that our method outperforms current state-of-the-art models across most metrics on multiple datasets. The code for ShadowMamba is available at (Code will be released upon acceptance).",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03284",
        "abstract url": "https://arxiv.org/abs/2411.03284",
        "title": "SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "While multi-agent systems have been shown to significantly enhance the performance of Large Language Models (LLMs) across various tasks and applications, the dense interaction between scaling agents potentially hampers their efficiency and diversity. To address these challenges, we draw inspiration from the sparse mixture-of-agents (SMoE) and propose a sparse mixture-of-agents (SMoA) framework to improve the efficiency and diversity of multi-agent LLMs. Unlike completely connected structures, SMoA introduces novel Response Selection and Early Stopping mechanisms to sparsify information flows among individual LLM agents, striking a balance between performance and efficiency. Additionally, inspired by the expert diversity principle in SMoE frameworks for workload balance between experts, we assign distinct role descriptions to each LLM agent, fostering diverse and divergent thinking. Extensive experiments on reasoning, alignment, and fairness benchmarks demonstrate that SMoA achieves performance comparable to traditional mixture-of-agents approaches but with significantly lower computational costs. Further analysis reveals that SMoA is more stable, has a greater capacity to scale, and offers considerable potential through hyper-parameter optimization. Code and data will be available at: https://github.com/David-Li0406/SMoA.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.MA"
        ],
        "comment": "Under Review"
    },
    {
        "paper id": "2411.03300",
        "abstract url": "https://arxiv.org/abs/2411.03300",
        "title": "VERITAS: A Unified Approach to Reliability Evaluation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) often fail to synthesize information from their context to generate an accurate response. This renders them unreliable in knowledge intensive settings where reliability of the output is key. A critical component for reliable LLMs is the integration of a robust fact-checking system that can detect hallucinations across various formats. While several open-access fact-checking models are available, their functionality is often limited to specific tasks, such as grounded question-answering or entailment verification, and they perform less effectively in conversational settings. On the other hand, closed-access models like GPT-4 and Claude offer greater flexibility across different contexts, including grounded dialogue verification, but are hindered by high costs and latency. In this work, we introduce VERITAS, a family of hallucination detection models designed to operate flexibly across diverse contexts while minimizing latency and costs. VERITAS achieves state-of-the-art results considering average performance on all major hallucination detection benchmarks, with $10\\%$ increase in average performance when compared to similar-sized models and get close to the performance of GPT4 turbo with LLM-as-a-judge setting.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03307",
        "abstract url": "https://arxiv.org/abs/2411.03307",
        "title": "LLMs for Domain Generation Algorithm Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This work analyzes the use of large language models (LLMs) for detecting domain generation algorithms (DGAs). We perform a detailed evaluation of two important techniques: In-Context Learning (ICL) and Supervised Fine-Tuning (SFT), showing how they can improve detection. SFT increases performance by using domain-specific data, whereas ICL helps the detection model to quickly adapt to new threats without requiring much retraining. We use Meta's Llama3 8B model, on a custom dataset with 68 malware families and normal domains, covering several hard-to-detect schemes, including recent word-based DGAs. Results proved that LLM-based methods can achieve competitive results in DGA detection. In particular, the SFT-based LLM DGA detector outperforms state-of-the-art models using attention layers, achieving 94% accuracy with a 4% false positive rate (FPR) and excelling at detecting word-based DGA domains.",
        "subjects": [
            "cs.CL",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03314",
        "abstract url": "https://arxiv.org/abs/2411.03314",
        "title": "MME-Finance: A Multimodal Finance Benchmark for Expert-level Understanding and Reasoning",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "In recent years, multimodal benchmarks for general domains have guided the rapid development of multimodal models on general tasks. However, the financial field has its peculiarities. It features unique graphical images (e.g., candlestick charts, technical indicator charts) and possesses a wealth of specialized financial knowledge (e.g., futures, turnover rate). Therefore, benchmarks from general fields often fail to measure the performance of multimodal models in the financial domain, and thus cannot effectively guide the rapid development of large financial models. To promote the development of large financial multimodal models, we propose MME-Finance, an bilingual open-ended and practical usage-oriented Visual Question Answering (VQA) benchmark. The characteristics of our benchmark are finance and expertise, which include constructing charts that reflect the actual usage needs of users (e.g., computer screenshots and mobile photography), creating questions according to the preferences in financial domain inquiries, and annotating questions by experts with 10+ years of experience in the financial industry. Additionally, we have developed a custom-designed financial evaluation system in which visual information is first introduced in the multi-modal evaluation process. Extensive experimental evaluations of 19 mainstream MLLMs are conducted to test their perception, reasoning, and cognition capabilities. The results indicate that models performing well on general benchmarks cannot do well on MME-Finance; for instance, the top-performing open-source and closed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o), respectively. Their performance is particularly poor in categories most relevant to finance, such as candlestick charts and technical indicator charts. In addition, we propose a Chinese version, which helps compare performance of MLLMs under a Chinese context.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": "Project Page: https://hithink-research.github.io/MME-Finance/"
    },
    {
        "paper id": "2411.03362",
        "abstract url": "https://arxiv.org/abs/2411.03362",
        "title": "Derivation and physical interpretation of the general solutions to the wave equations for electromagnetic potentials",
        "rating": "1",
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "The inhomogeneous wave equations for the scalar, vector, and Hertz potentials are derived starting from retarded charge, current, and polarization densities and then solved in the reciprocal (or k-) space to obtain general solutions, which are formulated as nested integrals of such densities over the source volume, k-space, and time. The solutions thus obtained are inherently free of spatial singularities and do not require introduction by fiat of combinations of advanced and retarded terms as done previously to cure such singularities for the point-charge model. Physical implications of these general solutions are discussed in the context of specific examples involving either the real or reciprocal space forms of the different potentials. The present approach allows for k-space expansions of the potentials for arbitrary distributions of charges and may lead to applications in condensed matter and fluorescence-based imaging.",
        "subjects": [
            "physics.class-ph",
            "eess.IV"
        ],
        "comment": "Includes five appendices"
    },
    {
        "paper id": "2411.03397",
        "abstract url": "https://arxiv.org/abs/2411.03397",
        "title": "SAUCE: Synchronous and Asynchronous User-Customizable Environment for Multi-Agent LLM Interaction",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Many human interactions, such as political debates, are carried out in group settings, where there are arbitrarily many participants, each with different views and agendas. To explore such complex social settings, we present SAUCE: a customizable Python platform, allowing researchers to plug-and-play various LLMs participating in discussions on any topic chosen by the user. Our platform takes care of instantiating the models, scheduling their responses, managing the discussion history, and producing a comprehensive output log, all customizable through configuration files, requiring little to no coding skills. A novel feature of SAUCE is our asynchronous communication feature, where models decide when to speak in addition to what to say, thus modeling an important facet of human communication. We show SAUCE's attractiveness in two initial experiments, and invite the community to use it in simulating various group simulations.",
        "subjects": [
            "cs.CL",
            "cs.HC"
        ],
        "comment": "https://github.com/Deep-Cognition-Lab/SAUCE"
    },
    {
        "paper id": "2411.03417",
        "abstract url": "https://arxiv.org/abs/2411.03417",
        "title": "Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS'24 Experiment",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) represent a promising, but controversial, tool in aiding scientific peer review. This study evaluates the usefulness of LLMs in a conference setting as a tool for vetting paper submissions against submission standards. We conduct an experiment at the 2024 Neural Information Processing Systems (NeurIPS) conference, where 234 papers were voluntarily submitted to an \"LLM-based Checklist Assistant.\" This assistant validates whether papers adhere to the author checklist used by NeurIPS, which includes questions to ensure compliance with research and manuscript preparation standards. Evaluation of the assistant by NeurIPS paper authors suggests that the LLM-based assistant was generally helpful in verifying checklist completion. In post-usage surveys, over 70% of authors found the assistant useful, and 70% indicate that they would revise their papers or checklist responses based on its feedback. While causal attribution to the assistant is not definitive, qualitative evidence suggests that the LLM contributed to improving some submissions. Survey responses and analysis of re-submissions indicate that authors made substantive revisions to their submissions in response to specific feedback from the LLM. The experiment also highlights common issues with LLMs: inaccuracy (20/52) and excessive strictness (14/52) were the most frequent issues flagged by authors. We also conduct experiments to understand potential gaming of the system, which reveal that the assistant could be manipulated to enhance scores through fabricated justifications, highlighting potential vulnerabilities of automated review tools.",
        "subjects": [
            "cs.CL",
            "cs.DL",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03445",
        "abstract url": "https://arxiv.org/abs/2411.03445",
        "title": "Solving Trojan Detection Competitions with Linear Weight Classification",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Neural networks can conceal malicious Trojan backdoors that allow a trigger to covertly change the model behavior. Detecting signs of these backdoors, particularly without access to any triggered data, is the subject of ongoing research and open challenges. In one common formulation of the problem, we are given a set of clean and poisoned models and need to predict whether a given test model is clean or poisoned. In this paper, we introduce a detector that works remarkably well across many of the existing datasets and domains. It is obtained by training a binary classifier on a large number of models' weights after performing a few different pre-processing steps including feature selection and standardization, reference model weights subtraction, and model alignment prior to detection. We evaluate this algorithm on a diverse set of Trojan detection benchmarks and domains and examine the cases where the approach is most and least effective.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CR",
            "cs.CV"
        ],
        "comment": "9 pages, 4 Figures"
    },
    {
        "paper id": "2411.03471",
        "abstract url": "https://arxiv.org/abs/2411.03471",
        "title": "MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have been applied to various hardware design tasks, including Verilog code generation, EDA tool scripting, and RTL bug fixing. Despite this extensive exploration, LLMs are yet to be used for the task of post-synthesis metric reasoning and estimation of HDL designs. In this paper, we assess the ability of LLMs to reason about post-synthesis metrics of Verilog designs. We introduce MetRex, a large-scale dataset comprising 25,868 Verilog HDL designs and their corresponding post-synthesis metrics, namely area, delay, and static power. MetRex incorporates a Chain of Thought (CoT) template to enhance LLMs' reasoning about these metrics. Extensive experiments show that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities on average by 37.0\\%, 25.3\\%, and 25.7\\% on the area, delay, and static power, respectively. While SFT improves performance on our benchmark, it remains far from achieving optimal results, especially on complex problems. Comparing to state-of-the-art regression models, our approach delivers accurate post-synthesis predictions for 17.4\\% more designs (within a 5\\% error margin), in addition to offering a 1.7x speedup by eliminating the need for pre-processing. This work lays the groundwork for advancing LLM-based Verilog code metric reasoning.",
        "subjects": [
            "cs.AR",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03486",
        "abstract url": "https://arxiv.org/abs/2411.03486",
        "title": "LLM Generated Distribution-Based Prediction of US Electoral Results, Part I",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces distribution-based prediction, a novel approach to using Large Language Models (LLMs) as predictive tools by interpreting output token probabilities as distributions representing the models' learned representation of the world. This distribution-based nature offers an alternative perspective for analyzing algorithmic fidelity, complementing the approach used in silicon sampling. We demonstrate the use of distribution-based prediction in the context of recent United States presidential election, showing that this method can be used to determine task specific bias, prompt noise, and algorithmic fidelity. This approach has significant implications for assessing the reliability and increasing transparency of LLM-based predictions across various domains.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": "17 pages, 10 Figures, Pre-print"
    },
    {
        "paper id": "2411.03524",
        "abstract url": "https://arxiv.org/abs/2411.03524",
        "title": "Mitigating Metric Bias in Minimum Bayes Risk Decoding",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "While Minimum Bayes Risk (MBR) decoding using metrics such as COMET or MetricX has outperformed traditional decoding methods such as greedy or beam search, it introduces a challenge we refer to as metric bias. As MBR decoding aims to produce translations that score highly according to a specific utility metric, this very process makes it impossible to use the same metric for both decoding and evaluation, as improvements might simply be due to reward hacking rather than reflecting real quality improvements. In this work we find that compared to human ratings, neural metrics not only overestimate the quality of MBR decoding when the same metric is used as the utility metric, but they also overestimate the quality of MBR/QE decoding with other neural utility metrics as well. We also show that the metric bias issue can be mitigated by using an ensemble of utility metrics during MBR decoding: human evaluations show that MBR decoding using an ensemble of utility metrics outperforms a single utility metric.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "To appear at WMT2024"
    },
    {
        "paper id": "2411.03628",
        "abstract url": "https://arxiv.org/abs/2411.03628",
        "title": "StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video Understanding",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The rapid development of Multimodal Large Language Models (MLLMs) has expanded their capabilities from image comprehension to video understanding. However, most of these MLLMs focus primarily on offline video comprehension, necessitating extensive processing of all video frames before any queries can be made. This presents a significant gap compared to the human ability to watch, listen, think, and respond to streaming inputs in real time, highlighting the limitations of current MLLMs. In this paper, we introduce StreamingBench, the first comprehensive benchmark designed to evaluate the streaming video understanding capabilities of MLLMs. StreamingBench assesses three core aspects of streaming video understanding: (1) real-time visual understanding, (2) omni-source understanding, and (3) contextual understanding. The benchmark consists of 18 tasks, featuring 900 videos and 4,500 human-curated QA pairs. Each video features five questions presented at different time points to simulate a continuous streaming scenario. We conduct experiments on StreamingBench with 13 open-source and proprietary MLLMs and find that even the most advanced proprietary MLLMs like Gemini 1.5 Pro and GPT-4o perform significantly below human-level streaming video understanding capabilities. We hope our work can facilitate further advancements for MLLMs, empowering them to approach human-level video comprehension and interaction in more realistic scenarios.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03630",
        "abstract url": "https://arxiv.org/abs/2411.03630",
        "title": "RTify: Aligning Deep Neural Networks with Human Behavioral Decisions",
        "rating": "1",
        "keywords": [
            [
                "cs.AI"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Current neural network models of primate vision focus on replicating overall levels of behavioral accuracy, often neglecting perceptual decisions' rich, dynamic nature. Here, we introduce a novel computational framework to model the dynamics of human behavioral choices by learning to align the temporal dynamics of a recurrent neural network (RNN) to human reaction times (RTs). We describe an approximation that allows us to constrain the number of time steps an RNN takes to solve a task with human RTs. The approach is extensively evaluated against various psychophysics experiments. We also show that the approximation can be used to optimize an \"ideal-observer\" RNN model to achieve an optimal tradeoff between speed and accuracy without human data. The resulting model is found to account well for human RT data. Finally, we use the approximation to train a deep learning implementation of the popular Wong-Wang decision-making model. The model is integrated with a convolutional neural network (CNN) model of visual processing and evaluated using both artificial and natural image stimuli. Overall, we present a novel framework that helps align current vision models with human behavior, bringing us closer to an integrated model of human vision.",
        "subjects": [
            "cs.AI",
            "q-bio.NC"
        ],
        "comment": "Published at NeurIPS 2024"
    },
    {
        "paper id": "2411.03644",
        "abstract url": "https://arxiv.org/abs/2411.03644",
        "title": "Deploying Multi-task Online Server with Large Language Model",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In the industry, numerous tasks are deployed online. Traditional approaches often tackle each task separately by its own network, which leads to excessive costs for developing and scaling models, especially in the context of large language models. Although multi-task methods can save costs through parameter sharing, they often struggle to outperform single-task methods in real-world applications. To tackle these challenges, we present a three-stage multi-task learning framework for large language models. It involves task filtering, followed by fine-tuning on high-resource tasks, and finally fine-tuning on all tasks. We conducted comprehensive experiments in single-task and multi-task settings. Our approach, exemplified on different benchmarks, demonstrates that it is able to achieve performance comparable to the single-task method while reducing up to 90.9\\% of its overhead.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Accepted by COLING 2025 Industry Track"
    },
    {
        "paper id": "2411.03665",
        "abstract url": "https://arxiv.org/abs/2411.03665",
        "title": "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Proper moral beliefs are fundamental for language models, yet assessing these beliefs poses a significant challenge. This study introduces a novel three-module framework to evaluate the moral beliefs of four prominent large language models. Initially, we constructed a dataset containing 472 moral choice scenarios in Chinese, derived from moral words. The decision-making process of the models in these scenarios reveals their moral principle preferences. By ranking these moral choices, we discern the varying moral beliefs held by different language models. Additionally, through moral debates, we investigate the firmness of these models to their moral choices. Our findings indicate that English language models, namely ChatGPT and Gemini, closely mirror moral decisions of the sample of Chinese university students, demonstrating strong adherence to their choices and a preference for individualistic moral beliefs. In contrast, Chinese models such as Ernie and ChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their moral choices and debates. This study also uncovers gender bias embedded within the moral beliefs of all examined language models. Our methodology offers an innovative means to assess moral beliefs in both artificial and human intelligence, facilitating a comparison of moral values across different cultures.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.04141",
        "abstract url": "https://arxiv.org/abs/2411.04141",
        "title": "A Comparative Study on the Impact of Test-Driven Development (TDD) and Behavior-Driven Development (BDD) on Enterprise Software Delivery Effectiveness",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper compares the impact of Test-Driven Development (TDD) and Behavior-Driven Development (BDD) on software delivery effectiveness within enterprise environments. Using a qualitative research design, data were collected through in-depth interviews with developers and project managers from enterprises adopting TDD or BDD. Moreover, the findings reveal distinct effects of each model on delivery speed, software quality, and team collaboration. Specifically, TDD emphasizes early testing and iterative development, leading to enhanced code quality and fewer defects, while BDD improves cross-functional communication by focusing on behavior specifications that involve stakeholders directly. However, TDD may create a higher initial time investment, and BDD might encounter challenges in requirement clarity. These differences highlight gaps in understanding how each model aligns with varying project types and stakeholder needs, which can guide enterprises in selecting the most suitable model for their unique requirements. The study contributes to the literature by providing insights into the practical application and challenges of TDD and BDD, suggesting future research on their long-term impacts in diverse settings.",
        "subjects": [
            "cs.SE",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.04143",
        "abstract url": "https://arxiv.org/abs/2411.04143",
        "title": "Software Design Pattern Model and Data Structure Algorithm Abilities on Microservices Architecture Design in High-tech Enterprises",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "This study investigates the impact of software design model capabilities and data structure algorithm abilities on microservices architecture design within enterprises. Utilizing a qualitative methodology, the research involved in-depth interviews with software architects and developers who possess extensive experience in microservices implementation. The findings reveal that organizations emphasizing robust design models and efficient algorithms achieve superior scalability, performance, and flexibility in their microservices architecture. Notably, participants highlighted that a strong foundation in these areas facilitates better service decomposition, optimizes data processing, and enhances system responsiveness. Despite these insights, gaps remain regarding the integration of emerging technologies and the evolving nature of software design practices. This paper contributes to the existing literature by underscoring the critical role of these competencies in fostering effective microservices architectures and suggests avenues for future research to address identified gaps",
        "subjects": [
            "cs.SE",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02813",
        "abstract url": "https://arxiv.org/abs/2411.02813",
        "title": "Sparse Orthogonal Parameters Tuning for Continual Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Continual learning methods based on pre-trained models (PTM) have recently gained attention which adapt to successive downstream tasks without catastrophic forgetting. These methods typically refrain from updating the pre-trained parameters and instead employ additional adapters, prompts, and classifiers. In this paper, we from a novel perspective investigate the benefit of sparse orthogonal parameters for continual learning. We found that merging sparse orthogonality of models learned from multiple streaming tasks has great potential in addressing catastrophic forgetting. Leveraging this insight, we propose a novel yet effective method called SoTU (Sparse Orthogonal Parameters TUning). We hypothesize that the effectiveness of SoTU lies in the transformation of knowledge learned from multiple domains into the fusion of orthogonal delta parameters. Experimental evaluations on diverse CL benchmarks demonstrate the effectiveness of the proposed approach. Notably, SoTU achieves optimal feature representation for streaming data without necessitating complex classifier designs, making it a Plug-and-Play solution.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02824",
        "abstract url": "https://arxiv.org/abs/2411.02824",
        "title": "Layer-Adaptive State Pruning for Deep State Space Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Due to the lack of state dimension optimization methods, deep state space models (SSMs) have sacrificed model capacity, training search space, or stability to alleviate computational costs caused by high state dimensions. In this work, we provide a structured pruning method for SSMs, Layer-Adaptive STate pruning (LAST), which reduces the state dimension of each layer in minimizing model-level energy loss by extending modal truncation for a single system. LAST scores are evaluated using $\\mathcal{H}_{\\infty}$ norms of subsystems for each state and layer-wise energy normalization. The scores serve as global pruning criteria, enabling cross-layer comparison of states and layer-adaptive pruning. Across various sequence benchmarks, LAST optimizes previous SSMs, revealing the redundancy and compressibility of their state spaces. Notably, we demonstrate that, on average, pruning 33% of states still maintains performance with 0.52% accuracy loss in multi-input multi-output SSMs without retraining. Code is available at $\\href{https://github.com/msgwak/LAST}{\\text{this https URL}}$.",
        "subjects": [
            "cs.LG",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02829",
        "abstract url": "https://arxiv.org/abs/2411.02829",
        "title": "CE-CoLLM: Efficient and Adaptive Large Language Models Through Cloud-Edge Collaboration",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have achieved remarkable success in serving end-users with human-like intelligence. However, LLMs demand high computational resources, making it challenging to deploy them to satisfy various performance objectives, such as meeting the resource constraints on edge devices close to end-users or achieving high accuracy with ample resources. In this paper, we introduce CE-CoLLM, a novel cloud-edge collaboration framework that supports efficient and adaptive LLM inference for end-users at the edge with two modes, (1) low-latency edge standalone inference and (2) highly accurate cloud-edge collaborative inference. First, we show that the inherent high communication costs for transmitting LLM contextual information between the edge and cloud dominate the overall latency, making it inefficient and costly to deploy LLMs using cloud-edge collaboration. Second, we propose several critical techniques to address this challenge, including early-exit mechanism, cloud context manager, and quantization in cloud-edge collaboration to enable not only low-latency standalone edge inference but also efficient and adaptive cloud-edge collaborative inference for LLMs. Third, we perform comprehensive experimental analysis, which demonstrates that CE-CoLLM significantly reduces inference time by up to 13.81% and cloud computation costs by up to 84.55% compared to the popular cloud-based LLM deployment, while maintaining comparable model accuracy. The proposed approach effectively shifts the computational load to the edge, reduces the communication overhead, scales efficiently with multiple edge clients, and provides reliable LLM deployment using cloud-edge collaboration.",
        "subjects": [
            "cs.DC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02850",
        "abstract url": "https://arxiv.org/abs/2411.02850",
        "title": "WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African clean water access, sanitation and hygiene",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate rural African communities on clean water access, sanitation, and hygiene (WASH) principles. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach to address the limitations of previous approaches with limited reach or missing contextualization. The paper details the development process, employing Design Science Research Methodology. The evaluation consisted of two phases: content validation by four WASH experts and community validation by potential users. Content validation confirmed WASHtsApp's ability to provide accurate and relevant WASH-related information. Community validation indicated high user acceptance and perceived usefulness of the chatbot. The paper concludes by discussing the potential for further development, including incorporating local languages and user data analysis for targeted interventions. It also proposes future research cycles focused on wider deployment and leveraging user data for educational purposes.",
        "subjects": [
            "cs.CY",
            "cs.AI",
            "cs.HC",
            "cs.IR"
        ],
        "comment": "Working Paper"
    },
    {
        "paper id": "2411.02854",
        "abstract url": "https://arxiv.org/abs/2411.02854",
        "title": "SpiDR: A Reconfigurable Digital Compute-in-Memory Spiking Neural Network Accelerator for Event-based Perception",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Spiking Neural Networks (SNNs), with their inherent recurrence, offer an efficient method for processing the asynchronous temporal data generated by Dynamic Vision Sensors (DVS), making them well-suited for event-based vision applications. However, existing SNN accelerators suffer from limitations in adaptability to diverse neuron models, bit precisions and network sizes, inefficient membrane potential (Vmem) handling, and limited sparse optimizations. In response to these challenges, we propose a scalable and reconfigurable digital compute-in-memory (CIM) SNN accelerator \\chipname with a set of key features: 1) It uses in-memory computations and reconfigurable operating modes to minimize data movement associated with weight and Vmem data structures while efficiently adapting to different workloads. 2) It supports multiple weight/Vmem bit precision values, enabling a trade-off between accuracy and energy efficiency and enhancing adaptability to diverse application demands. 3) A zero-skipping mechanism for sparse inputs significantly reduces energy usage by leveraging the inherent sparsity of spikes without introducing high overheads for low sparsity. 4) Finally, the asynchronous handshaking mechanism maintains the computational efficiency of the pipeline for variable execution times of different computation units. We fabricated \\chipname in 65 nm Taiwan Semiconductor Manufacturing Company (TSMC) low-power (LP) technology. It demonstrates competitive performance (scaled to the same technology node) to other digital SNN accelerators proposed in the recent literature and supports advanced reconfigurability. It achieves up to 5 TOPS/W energy efficiency at 95% input sparsity with 4-bit weights and 7-bit Vmem precision.",
        "subjects": [
            "cs.AR",
            "cs.LG",
            "cs.NE"
        ],
        "comment": "9 pages, 17 figures"
    },
    {
        "paper id": "2411.02904",
        "abstract url": "https://arxiv.org/abs/2411.02904",
        "title": "Gradient Descent Finds Over-Parameterized Neural Networks with Sharp Generalization for Nonparametric Regression: A Distribution-Free Analysis",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study nonparametric regression by an over-parameterized two-layer neural network trained by gradient descent (GD) in this paper. We show that, if the neural network is trained by GD with early stopping, then the trained network renders a sharp rate of the nonparametric regression risk of $\\cO(\\eps_n^2)$, which is the same rate as that for the classical kernel regression trained by GD with early stopping, where $\\eps_n$ is the critical population rate of the Neural Tangent Kernel (NTK) associated with the network and $n$ is the size of the training data. It is remarked that our result does not require distributional assumptions on the training data, in a strong contrast with many existing results which rely on specific distributions such as the spherical uniform data distribution or distributions satisfying certain restrictive conditions. The rate $\\cO(\\eps_n^2)$ is known to be minimax optimal for specific cases, such as the case that the NTK has a polynomial eigenvalue decay rate which happens under certain distributional assumptions. Our result formally fills the gap between training a classical kernel regression model and training an over-parameterized but finite-width neural network by GD for nonparametric regression without distributional assumptions. We also provide confirmative answers to certain open questions or address particular concerns in the literature of training over-parameterized neural networks by GD with early stopping for nonparametric regression, including the characterization of the stopping time, the lower bound for the network width, and the constant learning rate used in GD.",
        "subjects": [
            "stat.ML",
            "cs.IT",
            "cs.LG"
        ],
        "comment": "This article draws results with revisions from the first author's other work in arXiv:2407.11353"
    },
    {
        "paper id": "2411.02933",
        "abstract url": "https://arxiv.org/abs/2411.02933",
        "title": "P-MOSS: Learned Scheduling For Indexes Over NUMA Servers Using Low-Level Hardware Statistics",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Ever since the Dennard scaling broke down in the early 2000s and the frequency of the CPU stalled, vendors have started to increase the core count in each CPU chip at the expense of introducing heterogeneity, thus ushering the era of NUMA processors. Since then, the heterogeneity in the design space of hardware has only increased to the point that DBMS performance may vary significantly up to an order of magnitude in modern servers. An important factor that affects performance includes the location of the logical cores where the DBMS queries are scheduled, and the locations of the data that the queries access. This paper introduces P-MOSS, a learned spatial scheduling framework that schedules query execution to certain logical cores, and places data accordingly to certain integrated memory controllers (IMC), to integrate hardware consciousness into the system. In the spirit of hardware-software synergy, P-MOSS solely guides its scheduling decision based on low-level hardware statistics collected by performance monitoring counters with the aid of a Decision Transformer. Experimental evaluation is performed in the context of the B-tree and R-tree indexes. Performance results demonstrate that P-MOSS has up to 6x improvement over traditional schedules in terms of query throughput.",
        "subjects": [
            "cs.DB",
            "cs.LG",
            "cs.PF"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02947",
        "abstract url": "https://arxiv.org/abs/2411.02947",
        "title": "Time-Causal VAE: Robust Financial Time Series Generator",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We build a time-causal variational autoencoder (TC-VAE) for robust generation of financial time series data. Our approach imposes a causality constraint on the encoder and decoder networks, ensuring a causal transport from the real market time series to the fake generated time series. Specifically, we prove that the TC-VAE loss provides an upper bound on the causal Wasserstein distance between market distributions and generated distributions. Consequently, the TC-VAE loss controls the discrepancy between optimal values of various dynamic stochastic optimization problems under real and generated distributions. To further enhance the model's ability to approximate the latent representation of the real market distribution, we integrate a RealNVP prior into the TC-VAE framework. Finally, extensive numerical experiments show that TC-VAE achieves promising results on both synthetic and real market data. This is done by comparing real and generated distributions according to various statistical distances, demonstrating the effectiveness of the generated data for downstream financial optimization tasks, as well as showcasing that the generated data reproduces stylized facts of real financial market data.",
        "subjects": [
            "cs.LG",
            "q-fin.CP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02957",
        "abstract url": "https://arxiv.org/abs/2411.02957",
        "title": "Embedding Safety into RL: A New Take on Trust Region Methods",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Reinforcement Learning (RL) agents are able to solve a wide variety of tasks but are prone to producing unsafe behaviors. Constrained Markov Decision Processes (CMDPs) provide a popular framework for incorporating safety constraints. However, common solution methods often compromise reward maximization by being overly conservative or allow unsafe behavior during training. We propose Constrained Trust Region Policy Optimization (C-TRPO), a novel approach that modifies the geometry of the policy space based on the safety constraints and yields trust regions composed exclusively of safe policies, ensuring constraint satisfaction throughout training. We theoretically study the convergence and update properties of C-TRPO and highlight connections to TRPO, Natural Policy Gradient (NPG), and Constrained Policy Optimization (CPO). Finally, we demonstrate experimentally that C-TRPO significantly reduces constraint violations while achieving competitive reward maximization compared to state-of-the-art CMDP algorithms.",
        "subjects": [
            "cs.LG",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02985",
        "abstract url": "https://arxiv.org/abs/2411.02985",
        "title": "Sparse Reconstruction of Wavefronts using an Over-Complete Phase Dictionary",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Wavefront reconstruction is a critical component in various optical systems, including adaptive optics, interferometry, and phase contrast imaging. Traditional reconstruction methods often employ either the Cartesian (pixel) basis or the Zernike polynomial basis. While the Cartesian basis is adept at capturing high-frequency features, it is susceptible to overfitting and inefficiencies due to the high number of degrees of freedom. The Zernike basis efficiently represents common optical aberrations but struggles with complex or non-standard wavefronts such as optical vortices, Bessel beams, or wavefronts with sharp discontinuities. This paper introduces a novel approach to wavefront reconstruction using an over-complete phase dictionary combined with sparse representation techniques. By constructing a dictionary that includes a diverse set of basis functions - ranging from Zernike polynomials to specialized functions representing optical vortices and other complex modes - we enable a more flexible and efficient representation of complex wavefronts. Furthermore, a trainable affine transform is implemented to account for misalignment. Utilizing principles from compressed sensing and sparse coding, we enforce sparsity in the coefficient space to avoid overfitting and enhance robustness to noise.",
        "subjects": [
            "physics.optics",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02995",
        "abstract url": "https://arxiv.org/abs/2411.02995",
        "title": "SUDS: A Strategy for Unsupervised Drift Sampling",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Supervised machine learning often encounters concept drift, where the data distribution changes over time, degrading model performance. Existing drift detection methods focus on identifying these shifts but often overlook the challenge of acquiring labeled data for model retraining after a shift occurs. We present the Strategy for Drift Sampling (SUDS), a novel method that selects homogeneous samples for retraining using existing drift detection algorithms, thereby enhancing model adaptability to evolving data. SUDS seamlessly integrates with current drift detection techniques. We also introduce the Harmonized Annotated Data Accuracy Metric (HADAM), a metric that evaluates classifier performance in relation to the quantity of annotated data required to achieve the stated performance, thereby taking into account the difficulty of acquiring labeled data. Our contributions are twofold: SUDS combines drift detection with strategic sampling to improve the retraining process, and HADAM provides a metric that balances classifier performance with the amount of labeled data, ensuring efficient resource utilization. Empirical results demonstrate the efficacy of SUDS in optimizing labeled data use in dynamic environments, significantly improving the performance of machine learning applications in real-world scenarios. Our code is open source and available at https://github.com/cfellicious/SUDS/",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "9 pages, 5 tables, 3 figures"
    },
    {
        "paper id": "2411.03006",
        "abstract url": "https://arxiv.org/abs/2411.03006",
        "title": "Neural Networks and (Virtual) Extended Formulations",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Neural networks with piecewise linear activation functions, such as rectified linear units (ReLU) or maxout, are among the most fundamental models in modern machine learning. We make a step towards proving lower bounds on the size of such neural networks by linking their representative capabilities to the notion of the extension complexity $\\mathrm{xc}(P)$ of a polytope $P$, a well-studied quantity in combinatorial optimization and polyhedral geometry. To this end, we propose the notion of virtual extension complexity $\\mathrm{vxc}(P)=\\min\\{\\mathrm{xc}(Q)+\\mathrm{xc}(R)\\mid P+Q=R\\}$. This generalizes $\\mathrm{xc}(P)$ and describes the number of inequalities needed to represent the linear optimization problem over $P$ as a difference of two linear programs. We prove that $\\mathrm{vxc}(P)$ is a lower bound on the size of a neural network that optimizes over $P$. While it remains open to derive strong lower bounds on virtual extension complexity, we show that powerful results on the ordinary extension complexity can be converted into lower bounds for monotone neural networks, that is, neural networks with only nonnegative weights. Furthermore, we show that one can efficiently optimize over a polytope $P$ using a small virtual extended formulation. We therefore believe that virtual extension complexity deserves to be studied independently from neural networks, just like the ordinary extension complexity. As a first step in this direction, we derive an example showing that extension complexity can go down under Minkowski sum.",
        "subjects": [
            "math.CO",
            "cs.CC",
            "cs.DM",
            "cs.LG",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03007",
        "abstract url": "https://arxiv.org/abs/2411.03007",
        "title": "Data Quality Awareness: A Journey from Traditional Data Management to Data Science Systems",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Artificial intelligence (AI) has transformed various fields, significantly impacting our daily lives. A major factor in AI success is high-quality data. In this paper, we present a comprehensive review of the evolution of data quality (DQ) awareness from traditional data management systems to modern data-driven AI systems, which are integral to data science. We synthesize the existing literature, highlighting the quality challenges and techniques that have evolved from traditional data management to data science including big data and ML fields. As data science systems support a wide range of activities, our focus in this paper lies specifically in the analytics aspect driven by machine learning. We use the cause-effect connection between the quality challenges of ML and those of big data to allow a more thorough understanding of emerging DQ challenges and the related quality awareness techniques in data science systems. To the best of our knowledge, our paper is the first to provide a review of DQ awareness spanning traditional and emergent data science systems. We hope that readers will find this journey through the evolution of data quality awareness insightful and valuable.",
        "subjects": [
            "cs.DB",
            "cs.AI"
        ],
        "comment": "12 pages"
    },
    {
        "paper id": "2411.03021",
        "abstract url": "https://arxiv.org/abs/2411.03021",
        "title": "Testing Generalizability in Causal Inference",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Ensuring robust model performance across diverse real-world scenarios requires addressing both transportability across domains with covariate shifts and extrapolation beyond observed data ranges. However, there is no formal procedure for statistically evaluating generalizability in machine learning algorithms, particularly in causal inference. Existing methods often rely on arbitrary metrics like AUC or MSE and focus predominantly on toy datasets, providing limited insights into real-world applicability. To address this gap, we propose a systematic and quantitative framework for evaluating model generalizability under covariate distribution shifts, specifically within causal inference settings. Our approach leverages the frugal parameterization, allowing for flexible simulations from fully and semi-synthetic benchmarks, offering comprehensive evaluations for both mean and distributional regression methods. By basing simulations on real data, our method ensures more realistic evaluations, which is often missing in current work relying on simplified datasets. Furthermore, using simulations and statistical testing, our framework is robust and avoids over-reliance on conventional metrics. Grounded in real-world data, it provides realistic insights into model performance, bridging the gap between synthetic evaluations and practical applications.",
        "subjects": [
            "cs.LG",
            "stat.AP",
            "stat.ME",
            "stat.ML"
        ],
        "comment": "17 pages, 10 figures, Under review at AISTATS 2025"
    },
    {
        "paper id": "2411.03027",
        "abstract url": "https://arxiv.org/abs/2411.03027",
        "title": "Adaptive Genetic Selection based Pinning Control with Asymmetric Coupling for Multi-Network Heterogeneous Vehicular Systems",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "To alleviate computational load on RSUs and cloud platforms, reduce communication bandwidth requirements, and provide a more stable vehicular network service, this paper proposes an optimized pinning control approach for heterogeneous multi-network vehicular ad-hoc networks (VANETs). In such networks, vehicles participate in multiple task-specific networks with asymmetric coupling and dynamic topologies. We first establish a rigorous theoretical foundation by proving the stability of pinning control strategies under both single and multi-network conditions, deriving sufficient stability conditions using Lyapunov theory and linear matrix inequalities (LMIs). Building on this theoretical groundwork, we propose an adaptive genetic algorithm tailored to select optimal pinning nodes, effectively balancing LMI constraints while prioritizing overlapping nodes to enhance control efficiency. Extensive simulations across various network scales demonstrate that our approach achieves rapid consensus with a reduced number of control nodes, particularly when leveraging network overlaps. This work provides a comprehensive solution for efficient control node selection in complex vehicular networks, offering practical implications for deploying large-scale intelligent transportation systems.",
        "subjects": [
            "cs.AI",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03035",
        "abstract url": "https://arxiv.org/abs/2411.03035",
        "title": "Blending Ensemble for Classification with Genetic-algorithm generated Alpha factors and Sentiments (GAS)",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "With the increasing maturity and expansion of the cryptocurrency market, understanding and predicting its price fluctuations has become an important issue in the field of financial engineering. This article introduces an innovative Genetic Algorithm-generated Alpha Sentiment (GAS) blending ensemble model specifically designed to predict Bitcoin market trends. The model integrates advanced ensemble learning methods, feature selection algorithms, and in-depth sentiment analysis to effectively capture the complexity and variability of daily Bitcoin trading data. The GAS framework combines 34 Alpha factors with 8 news economic sentiment factors to provide deep insights into Bitcoin price fluctuations by accurately analyzing market sentiment and technical indicators. The core of this study is using a stacked model (including LightGBM, XGBoost, and Random Forest Classifier) for trend prediction which demonstrates excellent performance in traditional buy-and-hold strategies. In addition, this article also explores the effectiveness of using genetic algorithms to automate alpha factor construction as well as enhancing predictive models through sentiment analysis. Experimental results show that the GAS model performs competitively in daily Bitcoin trend prediction especially when analyzing highly volatile financial assets with rich data.",
        "subjects": [
            "q-fin.CP",
            "cs.LG",
            "q-fin.TR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03059",
        "abstract url": "https://arxiv.org/abs/2411.03059",
        "title": "Enhancing DP-SGD through Non-monotonous Adaptive Scaling Gradient Weight",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In the domain of deep learning, the challenge of protecting sensitive data while maintaining model utility is significant. Traditional Differential Privacy (DP) techniques such as Differentially Private Stochastic Gradient Descent (DP-SGD) typically employ strategies like direct or per-sample adaptive gradient clipping. These methods, however, compromise model accuracy due to their critical influence on gradient handling, particularly neglecting the significant contribution of small gradients during later training stages. In this paper, we introduce an enhanced version of DP-SGD, named Differentially Private Per-sample Adaptive Scaling Clipping (DP-PSASC). This approach replaces traditional clipping with non-monotonous adaptive gradient scaling, which alleviates the need for intensive threshold setting and rectifies the disproportionate weighting of smaller gradients. Our contribution is twofold. First, we develop a novel gradient scaling technique that effectively assigns proper weights to gradients, particularly small ones, thus improving learning under differential privacy. Second, we integrate a momentum-based method into DP-PSASC to reduce bias from stochastic sampling, enhancing convergence rates. Our theoretical and empirical analyses confirm that DP-PSASC preserves privacy and delivers superior performance across diverse datasets, setting new standards for privacy-sensitive applications.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03068",
        "abstract url": "https://arxiv.org/abs/2411.03068",
        "title": "Alpha and Prejudice: Improving $\u03b1$-sized Worst-case Fairness via Intrinsic Reweighting",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Worst-case fairness with off-the-shelf demographics achieves group parity by maximizing the model utility of the worst-off group. Nevertheless, demographic information is often unavailable in practical scenarios, which impedes the use of such a direct max-min formulation. Recent advances have reframed this learning problem by introducing the lower bound of minimal partition ratio, denoted as $\u03b1$, as side information, referred to as ``$\u03b1$-sized worst-case fairness'' in this paper. We first justify the practical significance of this setting by presenting noteworthy evidence from the data privacy perspective, which has been overlooked by existing research. Without imposing specific requirements on loss functions, we propose reweighting the training samples based on their intrinsic importance to fairness. Given the global nature of the worst-case formulation, we further develop a stochastic learning scheme to simplify the training process without compromising model performance. Additionally, we address the issue of outliers and provide a robust variant to handle potential outliers during model training. Our theoretical analysis and experimental observations reveal the connections between the proposed approaches and existing ``fairness-through-reweighting'' studies, with extensive experimental results on fairness benchmarks demonstrating the superiority of our methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03100",
        "abstract url": "https://arxiv.org/abs/2411.03100",
        "title": "Modeling sparsity in count-weighted networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.SI"
            ]
        ],
        "abstract": "Community detection methods have been extensively studied to recover communities structures in network data. While many models and methods focus on binary data, real-world networks also present the strength of connections, which could be considered in the network analysis. We propose a probabilistic model for generating weighted networks that allows us to control network sparsity and incorporates degree corrections for each node. We propose a community detection method based on the Variational Expectation-Maximization (VEM) algorithm. We show that the proposed method works well in practice for simulated networks. We analyze the Brazilian airport network to compare the community structures before and during the COVID-19 pandemic.",
        "subjects": [
            "stat.ME",
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03127",
        "abstract url": "https://arxiv.org/abs/2411.03127",
        "title": "User Centric Semantic Communications",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Current studies on semantic communications mainly focus on efficiently extracting semantic information to reduce bandwidth usage between a transmitter and a user. Although significant process has been made in the semantic communications, a fundamental design problem is that the semantic information is extracted based on certain criteria at the transmitter side along, without considering the user's actual requirements. As a result, critical information that is of primary concern to the user may be lost. In such cases, the semantic transmission becomes meaningless to the user, as all received information is irrelevant to the user's interests. To solve this problem, this paper presents a user centric semantic communication system, where the user sends its request for the desired semantic information to the transmitter at the start of each transmission. Then, the transmitter extracts the required semantic information accordingly. A key challenge is how the transmitter can understand the user's requests for semantic information and extract the required semantic information in a reasonable and robust manner. We solve this challenge by designing a well-structured framework and leveraging off-the-shelf products, such as GPT-4, along with several specialized tools for detection and estimation. Evaluation results demonstrate the feasibility and effectiveness of the proposed user centric semantic communication system.",
        "subjects": [
            "cs.IT",
            "cs.LG",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03147",
        "abstract url": "https://arxiv.org/abs/2411.03147",
        "title": "The Effect of Funding on Student Achievement: Evidence from District of Columbia, Virginia, and Maryland",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "The question of how to best serve the student populations of our country is a complex topic. Since public funding is limited, we must explore the best ways to direct the money to improve student outcomes. Previous research has suggested that socio-economic status is the best predictor of student achievement, while other studies suggest that the amount of money spent on the student is a more significant factor. In this paper, we explore this question and its impacts on Maryland, Virginia, and the District of Columbia schools. We conclude that the graduation rate has a direct relationship with unemployment, suggesting that funding towards improving out-of-school opportunities and quality of life will significantly improve students chances of success. We do not find a significant relationship between per-pupil spending and student achievement.",
        "subjects": [
            "cs.CY",
            "stat.AP"
        ],
        "comment": "https://github.com/araabe2/Data-601-Group-Project"
    },
    {
        "paper id": "2411.03169",
        "abstract url": "https://arxiv.org/abs/2411.03169",
        "title": "Pre-trained Visual Dynamics Representations for Efficient Policy Learning",
        "rating": "0.5",
        "keywords": [
            [
                "robotics"
            ],
            [
                "cs.LG",
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Pre-training for Reinforcement Learning (RL) with purely video data is a valuable yet challenging problem. Although in-the-wild videos are readily available and inhere a vast amount of prior world knowledge, the absence of action annotations and the common domain gap with downstream tasks hinder utilizing videos for RL pre-training. To address the challenge of pre-training with videos, we propose Pre-trained Visual Dynamics Representations (PVDR) to bridge the domain gap between videos and downstream tasks for efficient policy learning. By adopting video prediction as a pre-training task, we use a Transformer-based Conditional Variational Autoencoder (CVAE) to learn visual dynamics representations. The pre-trained visual dynamics representations capture the visual dynamics prior knowledge in the videos. This abstract prior knowledge can be readily adapted to downstream tasks and aligned with executable actions through online adaptation. We conduct experiments on a series of robotics visual control tasks and verify that PVDR is an effective form for pre-training with videos to promote policy learning.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "ECCV 2024"
    },
    {
        "paper id": "2411.03172",
        "abstract url": "https://arxiv.org/abs/2411.03172",
        "title": "Blind Estimation of Sub-band Acoustic Parameters from Ambisonics Recordings using Spectro-Spatial Covariance Features",
        "rating": "0.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ],
            [
                "ICASSP"
            ]
        ],
        "abstract": "Estimating frequency-varying acoustic parameters is essential for enhancing immersive perception in realistic spatial audio creation. In this paper, we propose a unified framework that blindly estimates reverberation time (T60), direct-to-reverberant ratio (DRR), and clarity (C50) across 10 frequency bands using first-order Ambisonics (FOA) speech recordings as inputs. The proposed framework utilizes a novel feature named Spectro-Spatial Covariance Vector (SSCV), efficiently representing temporal, spectral as well as spatial information of the FOA signal. Our models significantly outperform existing single-channel methods with only spectral information, reducing estimation errors by more than half for all three acoustic parameters. Additionally, we introduce FOA-Conv3D, a novel back-end network for effectively utilising the SSCV feature with a 3D convolutional encoder. FOA-Conv3D outperforms the convolutional neural network (CNN) and recurrent convolutional neural network (CRNN) backends, achieving lower estimation errors and accounting for a higher proportion of variance (PoV) for all 3 acoustic parameters.",
        "subjects": [
            "eess.AS",
            "cs.LG",
            "cs.SD",
            "eess.SP"
        ],
        "comment": "Submitted to ICASSP2025"
    },
    {
        "paper id": "2411.03195",
        "abstract url": "https://arxiv.org/abs/2411.03195",
        "title": "Online Data Collection for Efficient Semiparametric Inference",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "While many works have studied statistical data fusion, they typically assume that the various datasets are given in advance. However, in practice, estimation requires difficult data collection decisions like determining the available data sources, their costs, and how many samples to collect from each source. Moreover, this process is often sequential because the data collected at a given time can improve collection decisions in the future. In our setup, given access to multiple data sources and budget constraints, the agent must sequentially decide which data source to query to efficiently estimate a target parameter. We formalize this task using Online Moment Selection, a semiparametric framework that applies to any parameter identified by a set of moment conditions. Interestingly, the optimal budget allocation depends on the (unknown) true parameters. We present two online data collection policies, Explore-then-Commit and Explore-then-Greedy, that use the parameter estimates at a given time to optimally allocate the remaining budget in the future steps. We prove that both policies achieve zero regret (assessed by asymptotic MSE) relative to an oracle policy. We empirically validate our methods on both synthetic and real-world causal effect estimation tasks, demonstrating that the online data collection policies outperform their fixed counterparts.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03205",
        "abstract url": "https://arxiv.org/abs/2411.03205",
        "title": "GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Recent advancements in Generative AI offer promising capabilities for spatial analysis. Despite their potential, the integration of generative AI with established GIS platforms remains underexplored. In this study, we propose a framework for integrating LLMs directly into existing GIS platforms, using QGIS as an example. Our approach leverages the reasoning and programming capabilities of LLMs to autonomously generate spatial analysis workflows and code through an informed agent that has comprehensive documentation of key GIS tools and parameters. The implementation of this framework resulted in the development of a \"GIS Copilot\" that allows GIS users to interact with QGIS using natural language commands for spatial analysis. The GIS Copilot was evaluated based on three complexity levels: basic tasks that require one GIS tool and typically involve one data layer to perform simple operations; intermediate tasks involving multi-step processes with multiple tools, guided by user instructions; and advanced tasks which involve multi-step processes that require multiple tools but not guided by user instructions, necessitating the agent to independently decide on and executes the necessary steps. The evaluation reveals that the GIS Copilot demonstrates strong potential in automating foundational GIS operations, with a high success rate in tool selection and code generation for basic and intermediate tasks, while challenges remain in achieving full autonomy for more complex tasks. This study contributes to the emerging vision of Autonomous GIS, providing a pathway for non-experts to engage with geospatial analysis with minimal prior expertise. While full autonomy is yet to be achieved, the GIS Copilot demonstrates significant potential for simplifying GIS workflows and enhancing decision-making processes.",
        "subjects": [
            "cs.AI",
            "cs.ET",
            "cs.HC",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03217",
        "abstract url": "https://arxiv.org/abs/2411.03217",
        "title": "A Personal data Value at Risk Approach",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "What if the main data protection vulnerability is risk management? Data Protection merges three disciplines: data protection law, information security, and risk management. Nonetheless, very little research has been made on the field of data protection risk management, where subjectivity and superficiality are the dominant state of the art. Since the GDPR tells you what to do, but not how to do it, the solution for approaching GDPR compliance is still a gray zone, where the trend is using the rule of thumb. Considering that the most important goal of risk management is to reduce uncertainty in order to take informed decisions, risk management for the protection of the rights and freedoms of the data subjects cannot be disconnected from the impact materialization that data controllers and processors need to assess. This paper proposes a quantitative approach to data protection risk-based compliance from a data controllers perspective, with the aim of proposing a mindset change, where data protection impact assessments can be improved by using data protection analytics, quantitative risk analysis, and calibrating expert opinions.",
        "subjects": [
            "q-fin.RM",
            "cs.LG"
        ],
        "comment": "24 pages"
    },
    {
        "paper id": "2411.03237",
        "abstract url": "https://arxiv.org/abs/2411.03237",
        "title": "On the Detection of Non-Cooperative RISs: Scan B-Testing via Deep Support Vector Data Description",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we study the problem of promptly detecting the presence of non-cooperative activity from one or more Reconfigurable Intelligent Surfaces (RISs) with unknown characteristics lying in the vicinity of a Multiple-Input Multiple-Output (MIMO) communication system using Orthogonal Frequency-Division Multiplexing (OFDM) transmissions. We first present a novel wideband channel model incorporating RISs as well as non-reconfigurable stationary surfaces, which captures both the effect of the RIS actuation time on the channel in the frequency domain as well as the difference between changing phase configurations during or among transmissions. Considering that RISs may operate under the coordination of a third-party system, and thus, may negatively impact the communication of the intended MIMO OFDM system, we present a novel RIS activity detection framework that is unaware of the distribution of the phase configuration of any of the non-cooperative RISs. In particular, capitalizing on the knowledge of the data distribution at the multi-antenna receiver, we design a novel online change point detection statistic that combines a deep support vector data description model with the scan $B$-test. The presented numerical investigations demonstrate the improved detection accuracy as well as decreased computational complexity of the proposed RIS detection approach over existing change point detection schemes.",
        "subjects": [
            "cs.NI",
            "cs.AI",
            "cs.CR",
            "cs.LG"
        ],
        "comment": "6 pages, 4 figures, submitted to an IEEE conference"
    },
    {
        "paper id": "2411.03252",
        "abstract url": "https://arxiv.org/abs/2411.03252",
        "title": "Spontaneous Emergence of Agent Individuality through Social Interactions in LLM-Based Communities",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "We study the emergence of agency from scratch by using Large Language Model (LLM)-based agents. In previous studies of LLM-based agents, each agent's characteristics, including personality and memory, have traditionally been predefined. We focused on how individuality, such as behavior, personality, and memory, can be differentiated from an undifferentiated state. The present LLM agents engage in cooperative communication within a group simulation, exchanging context-based messages in natural language. By analyzing this multi-agent simulation, we report valuable new insights into how social norms, cooperation, and personality traits can emerge spontaneously. This paper demonstrates that autonomously interacting LLM-powered agents generate hallucinations and hashtags to sustain communication, which, in turn, increases the diversity of words within their interactions. Each agent's emotions shift through communication, and as they form communities, the personalities of the agents emerge and evolve accordingly. This computational modeling approach and its findings will provide a new method for analyzing collective artificial intelligence.",
        "subjects": [
            "cs.AI",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03253",
        "abstract url": "https://arxiv.org/abs/2411.03253",
        "title": "Discovering Data Structures: Nearest Neighbor Search and Beyond",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We propose a general framework for end-to-end learning of data structures. Our framework adapts to the underlying data distribution and provides fine-grained control over query and space complexity. Crucially, the data structure is learned from scratch, and does not require careful initialization or seeding with candidate data structures/algorithms. We first apply this framework to the problem of nearest neighbor search. In several settings, we are able to reverse-engineer the learned data structures and query algorithms. For 1D nearest neighbor search, the model discovers optimal distribution (in)dependent algorithms such as binary search and variants of interpolation search. In higher dimensions, the model learns solutions that resemble k-d trees in some regimes, while in others, they have elements of locality-sensitive hashing. The model can also learn useful representations of high-dimensional data and exploit them to design effective data structures. We also adapt our framework to the problem of estimating frequencies over a data stream, and believe it could also be a powerful discovery tool for new problems.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03263",
        "abstract url": "https://arxiv.org/abs/2411.03263",
        "title": "Proxy-informed Bayesian transfer learning with unknown sources",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Generalization outside the scope of one's training data requires leveraging prior knowledge about the effects that transfer, and the effects that don't, between different data sources. Bayesian transfer learning is a principled paradigm for specifying this knowledge, and refining it on the basis of data from the source (training) and target (prediction) tasks. We address the challenging transfer learning setting where the learner (i) cannot fine-tune in the target task, and (ii) does not know which source data points correspond to the same task (i.e., the data sources are unknown). We propose a proxy-informed robust method for probabilistic transfer learning (PROMPT), which provides a posterior predictive estimate tailored to the structure of the target task, without requiring the learner have access to any outcome information from the target task. Instead, PROMPT relies on the availability of proxy information. PROMPT uses the same proxy information for two purposes: (i) estimation of effects specific to the target task, and (ii) construction of a robust reweighting of the source data for estimation of effects that transfer between tasks. We provide theoretical results on the effect of this reweighting on the risk of negative transfer, and demonstrate application of PROMPT in two synthetic settings.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03270",
        "abstract url": "https://arxiv.org/abs/2411.03270",
        "title": "Stable Matching with Ties: Approximation Ratios and Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study the problem of matching markets with ties, where one side of the market does not necessarily have strict preferences over members at its other side. For example, workers do not always have strict preferences over jobs, students can give the same ranking for different schools and more. In particular, assume w.l.o.g. that workers' preferences are determined by their utility from being matched to each job, which might admit ties. Notably, in contrast to classical two-sided markets with strict preferences, there is no longer a single stable matching that simultaneously maximizes the utility for all workers. We aim to guarantee each worker the largest possible share from the utility in her best possible stable matching. We call the ratio between the worker's best possible stable utility and its assigned utility the \\emph{Optimal Stable Share} (OSS)-ratio. We first prove that distributions over stable matchings cannot guarantee an OSS-ratio that is sublinear in the number of workers. Instead, randomizing over possibly non-stable matchings, we show how to achieve a tight logarithmic OSS-ratio. Then, we analyze the case where the real utility is not necessarily known and can only be approximated. In particular, we provide an algorithm that guarantees a similar fraction of the utility compared to the best possible utility. Finally, we move to a bandit setting, where we select a matching at each round and only observe the utilities for matches we perform. We show how to utilize our results for approximate utilities to gracefully interpolate between problems without ties and problems with statistical ties (small suboptimality gaps).",
        "subjects": [
            "cs.GT",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03275",
        "abstract url": "https://arxiv.org/abs/2411.03275",
        "title": "Causal Responsibility Attribution for Human-AI Collaboration",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "As Artificial Intelligence (AI) systems increasingly influence decision-making across various fields, the need to attribute responsibility for undesirable outcomes has become essential, though complicated by the complex interplay between humans and AI. Existing attribution methods based on actual causality and Shapley values tend to disproportionately blame agents who contribute more to an outcome and rely on real-world measures of blameworthiness that may misalign with responsible AI standards. This paper presents a causal framework using Structural Causal Models (SCMs) to systematically attribute responsibility in human-AI systems, measuring overall blameworthiness while employing counterfactual reasoning to account for agents' expected epistemic levels. Two case studies illustrate the framework's adaptability in diverse human-AI collaboration scenarios.",
        "subjects": [
            "cs.AI",
            "cs.HC",
            "stat.AP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03292",
        "abstract url": "https://arxiv.org/abs/2411.03292",
        "title": "Interaction2Code: How Far Are We From Automatic Interactive Webpage Generation?",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Converting webpage design into functional UI code is a critical step for building websites, which can be labor-intensive and time-consuming. To automate this design-to-code transformation process, various automated methods using learning-based networks and multi-modal large language models (MLLMs) have been proposed. However, these studies were merely evaluated on a narrow range of static web pages and ignored dynamic interaction elements, making them less practical for real-world website deployment. To fill in the blank, we present the first systematic investigation of MLLMs in generating interactive webpages. Specifically, we first formulate the Interaction-to-Code task and build the Interaction2Code benchmark that contains 97 unique web pages and 213 distinct interactions, spanning 15 webpage types and 30 interaction categories. We then conduct comprehensive experiments on three state-of-the-art (SOTA) MLLMs using both automatic metrics and human evaluations, thereby summarizing six findings accordingly. Our experimental results highlight the limitations of MLLMs in generating fine-grained interactive features and managing interactions with complex transformations and subtle visual modifications. We further analyze failure cases and their underlying causes, identifying 10 common failure types and assessing their severity. Additionally, our findings reveal three critical influencing factors, i.e., prompts, visual saliency, and textual descriptions, that can enhance the interaction generation performance of MLLMs. Based on these findings, we elicit implications for researchers and developers, providing a foundation for future advancements in this field. Datasets and source code are available at https://github.com/WebPAI/Interaction2Code.",
        "subjects": [
            "cs.SE",
            "cs.AI",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03375",
        "abstract url": "https://arxiv.org/abs/2411.03375",
        "title": "Kernel Approximation using Analog In-Memory Computing",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Kernel functions are vital ingredients of several machine learning algorithms, but often incur significant memory and computational costs. We introduce an approach to kernel approximation in machine learning algorithms suitable for mixed-signal Analog In-Memory Computing (AIMC) architectures. Analog In-Memory Kernel Approximation addresses the performance bottlenecks of conventional kernel-based methods by executing most operations in approximate kernel methods directly in memory. The IBM HERMES Project Chip, a state-of-the-art phase-change memory based AIMC chip, is utilized for the hardware demonstration of kernel approximation. Experimental results show that our method maintains high accuracy, with less than a 1% drop in kernel-based ridge classification benchmarks and within 1% accuracy on the Long Range Arena benchmark for kernelized attention in Transformer neural networks. Compared to traditional digital accelerators, our approach is estimated to deliver superior energy efficiency and lower power consumption. These findings highlight the potential of heterogeneous AIMC architectures to enhance the efficiency and scalability of machine learning applications.",
        "subjects": [
            "cs.LG",
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03376",
        "abstract url": "https://arxiv.org/abs/2411.03376",
        "title": "An Open API Architecture to Discover the Trustworthy Explanation of Cloud AI Services",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "This article presents the design of an open-API-based explainable AI (XAI) service to provide feature contribution explanations for cloud AI services. Cloud AI services are widely used to develop domain-specific applications with precise learning metrics. However, the underlying cloud AI services remain opaque on how the model produces the prediction. We argue that XAI operations are accessible as open APIs to enable the consolidation of the XAI operations into the cloud AI services assessment. We propose a design using a microservice architecture that offers feature contribution explanations for cloud AI services without unfolding the network structure of the cloud models. We can also utilize this architecture to evaluate the model performance and XAI consistency metrics showing cloud AI services trustworthiness. We collect provenance data from operational pipelines to enable reproducibility within the XAI service. Furthermore, we present the discovery scenarios for the experimental tests regarding model performance and XAI consistency metrics for the leading cloud vision AI services. The results confirm that the architecture, based on open APIs, is cloud-agnostic. Additionally, data augmentations result in measurable improvements in XAI consistency metrics for cloud AI services.",
        "subjects": [
            "cs.DC",
            "cs.AI",
            "cs.NI"
        ],
        "comment": "Published in: IEEE Transactions on Cloud Computing ( Volume: 12, Issue: 2, April-June 2024)"
    },
    {
        "paper id": "2411.03384",
        "abstract url": "https://arxiv.org/abs/2411.03384",
        "title": "Solving stochastic partial differential equations using neural networks in the Wiener chaos expansion",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this paper, we solve stochastic partial differential equations (SPDEs) numerically by using (possibly random) neural networks in the truncated Wiener chaos expansion of their corresponding solution. Moreover, we provide some approximation rates for learning the solution of SPDEs with additive and/or multiplicative noise. Finally, we apply our results in numerical examples to approximate the solution of three SPDEs: the stochastic heat equation, the Heath-Jarrow-Morton equation, and the Zakai equation.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.NA",
            "math.PR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03402",
        "abstract url": "https://arxiv.org/abs/2411.03402",
        "title": "Climate AI for Corporate Decarbonization Metrics Extraction",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Corporate Greenhouse Gas (GHG) emission targets are important metrics in sustainable investing [12, 16]. To provide a comprehensive view of company emission objectives, we propose an approach to source these metrics from company public disclosures. Without automation, curating these metrics manually is a labor-intensive process that requires combing through lengthy corporate sustainability disclosures that often do not follow a standard format. Furthermore, the resulting dataset needs to be validated thoroughly by Subject Matter Experts (SMEs), further lengthening the time-to-market. We introduce the Climate Artificial Intelligence for Corporate Decarbonization Metrics Extraction (CAI) model and pipeline, a novel approach utilizing Large Language Models (LLMs) to extract and validate linked metrics from corporate disclosures. We demonstrate that the process improves data collection efficiency and accuracy by automating data curation, validation, and metric scoring from public corporate disclosures. We further show that our results are agnostic to the choice of LLMs. This framework can be applied broadly to information extraction from textual data.",
        "subjects": [
            "q-fin.PM",
            "cs.CY",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03405",
        "abstract url": "https://arxiv.org/abs/2411.03405",
        "title": "Fine-Grained Spatial and Verbal Losses for 3D Visual Grounding",
        "rating": "0.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ],
            [
                "WACV"
            ]
        ],
        "abstract": "3D visual grounding consists of identifying the instance in a 3D scene which is referred by an accompanying language description. While several architectures have been proposed within the commonly employed grounding-by-selection framework, the utilized losses are comparatively under-explored. In particular, most methods rely on a basic supervised cross-entropy loss on the predicted distribution over candidate instances, which fails to model both spatial relations between instances and the internal fine-grained word-level structure of the verbal referral. Sparse attempts to additionally supervise verbal embeddings globally by learning the class of the referred instance from the description or employing verbo-visual contrast to better separate instance embeddings do not fundamentally lift the aforementioned limitations. Responding to these shortcomings, we introduce two novel losses for 3D visual grounding: a visual-level offset loss on regressed vector offsets from each instance to the ground-truth referred instance and a language-related span loss on predictions for the word-level span of the referred instance in the description. In addition, we equip the verbo-visual fusion module of our new 3D visual grounding architecture AsphaltNet with a top-down bidirectional attentive fusion block, which enables the supervisory signals from our two losses to propagate to the respective converse branches of the network and thus aid the latter to learn context-aware instance embeddings and grounding-aware verbal embeddings. AsphaltNet proposes novel auxiliary losses to aid 3D visual grounding with competitive results compared to the state-of-the-art on the ReferIt3D benchmark.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at WACV 2025"
    },
    {
        "paper id": "2411.03409",
        "abstract url": "https://arxiv.org/abs/2411.03409",
        "title": "STEER: Flexible Robotic Manipulation via Dense Language Grounding",
        "rating": "0.5",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "robot",
                "Robotic Manipulation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The complexity of the real world demands robotic systems that can intelligently adapt to unseen situations. We present STEER, a robot learning framework that bridges high-level, commonsense reasoning with precise, flexible low-level control. Our approach translates complex situational awareness into actionable low-level behavior through training language-grounded policies with dense annotation. By structuring policy training around fundamental, modular manipulation skills expressed in natural language, STEER exposes an expressive interface for humans or Vision-Language Models (VLMs) to intelligently orchestrate the robot's behavior by reasoning about the task and context. Our experiments demonstrate the skills learned via STEER can be combined to synthesize novel behaviors to adapt to new situations or perform completely new tasks without additional data collection or training.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "Project website: https://lauramsmith.github.io/steer/"
    },
    {
        "paper id": "2411.03455",
        "abstract url": "https://arxiv.org/abs/2411.03455",
        "title": "Watson: A Cognitive Observability Framework for the Reasoning of Foundation Model-Powered Agents",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "As foundation models (FMs) play an increasingly prominent role in complex software systems, such as FM-powered agentic software (i.e., Agentware), they introduce significant challenges for developers regarding observability. Unlike traditional software, agents operate autonomously, using extensive data and opaque implicit reasoning, making it difficult to observe and understand their behavior during runtime, especially when they take unexpected actions or encounter errors. In this paper, we highlight the limitations of traditional operational observability in the context of FM-powered software, and introduce cognitive observability as a new type of required observability that has emerged for such innovative systems. We then propose a novel framework that provides cognitive observability into the implicit reasoning processes of agents (a.k.a. reasoning observability), and demonstrate the effectiveness of our framework in boosting the debuggability of Agentware and, in turn, the abilities of an Agentware through a case study on AutoCodeRover, a cuttingedge Agentware for autonomous program improvement.",
        "subjects": [
            "cs.AI",
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03517",
        "abstract url": "https://arxiv.org/abs/2411.03517",
        "title": "Understanding Contrastive Learning via Gaussian Mixture Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Contrastive learning attempts to learn representations from un-labeled data; it does so via a loss function that encourages the embedding of a point to be close to that of its augmentations, and far from the embeddings of random other points. This simple idea performs remarkably well, yet it is not precisely theoretically understood why this is the case. In this paper we analyze contrastive learning (specifically, the InfoNCE loss) in a natural context: dimensionality reduction in Gaussian Mixture Models. Crucially, we define an augmentation of a data point as being another independent draw from the same underlying mixture component. We show that vanilla InfoNCE is able to find the optimal lower-dimensional subspace even when the Gaussians are not isotropic -- something that vanilla spectral techniques cannot do. We further extend our analyses to multi-modal contrastive learning algorithms (e.g., CLIP). In this setting we show that contrastive learning learns the subset of fisher-optimal subspace, effectively filtering out all the noise from the learnt representations.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03519",
        "abstract url": "https://arxiv.org/abs/2411.03519",
        "title": "AI Metropolis: Scaling Large Language Model-based Multi-Agent Simulation with Out-of-order Execution",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "With more advanced natural language understanding and reasoning capabilities, large language model (LLM)-powered agents are increasingly developed in simulated environments to perform complex tasks, interact with other agents, and exhibit emergent behaviors relevant to social science and gaming. However, current multi-agent simulations frequently suffer from inefficiencies due to the limited parallelism caused by false dependencies, resulting in performance bottlenecks. In this paper, we introduce AI Metropolis, a simulation engine that improves the efficiency of LLM agent simulations by incorporating out-of-order execution scheduling. By dynamically tracking real dependencies between agents, AI Metropolis minimizes false dependencies, enhancing parallelism and enabling efficient hardware utilization. Our evaluations demonstrate that AI Metropolis achieves speedups from 1.3x to 4.15x over standard parallel simulation with global synchronization, approaching optimal performance as the number of agents increases.",
        "subjects": [
            "cs.DC",
            "cs.AI",
            "cs.LG",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03527",
        "abstract url": "https://arxiv.org/abs/2411.03527",
        "title": "PACE: Pacing Operator Learning to Accurate Optical Field Simulation for Complicated Photonic Devices",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Electromagnetic field simulation is central to designing, optimizing, and validating photonic devices and circuits. However, costly computation associated with numerical simulation poses a significant bottleneck, hindering scalability and turnaround time in the photonic circuit design process. Neural operators offer a promising alternative, but existing SOTA approaches, NeurOLight, struggle with predicting high-fidelity fields for real-world complicated photonic devices, with the best reported 0.38 normalized mean absolute error in NeurOLight. The inter-plays of highly complex light-matter interaction, e.g., scattering and resonance, sensitivity to local structure details, non-uniform learning complexity for full-domain simulation, and rich frequency information, contribute to the failure of existing neural PDE solvers. In this work, we boost the prediction fidelity to an unprecedented level for simulating complex photonic devices with a novel operator design driven by the above challenges. We propose a novel cross-axis factorized PACE operator with a strong long-distance modeling capacity to connect the full-domain complex field pattern with local device structures. Inspired by human learning, we further divide and conquer the simulation task for extremely hard cases into two progressively easy tasks, with a first-stage model learning an initial solution refined by a second model. On various complicated photonic device benchmarks, we demonstrate one sole PACE model is capable of achieving 73% lower error with 50% fewer parameters compared with various recent ML for PDE solvers. The two-stage setup further advances high-fidelity simulation for even more intricate cases. In terms of runtime, PACE demonstrates 154-577x and 11.8-12x simulation speedup over numerical solver using scipy or highly-optimized pardiso solver, respectively. We open sourced the code and dataset.",
        "subjects": [
            "cs.LG",
            "physics.optics"
        ],
        "comment": "Accepeted by Neurips 2024, 21 pages"
    },
    {
        "paper id": "2411.03535",
        "abstract url": "https://arxiv.org/abs/2411.03535",
        "title": "The Differentiable Feasibility Pump",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Although nearly 20 years have passed since its conception, the feasibility pump algorithm remains a widely used heuristic to find feasible primal solutions to mixed-integer linear problems. Many extensions of the initial algorithm have been proposed. Yet, its core algorithm remains centered around two key steps: solving the linear relaxation of the original problem to obtain a solution that respects the constraints, and rounding it to obtain an integer solution. This paper shows that the traditional feasibility pump and many of its follow-ups can be seen as gradient-descent algorithms with specific parameters. A central aspect of this reinterpretation is observing that the traditional algorithm differentiates the solution of the linear relaxation with respect to its cost. This reinterpretation opens many opportunities for improving the performance of the original algorithm. We study how to modify the gradient-update step as well as extending its loss function. We perform extensive experiments on MIPLIB instances and show that these modifications can substantially reduce the number of iterations needed to find a solution.",
        "subjects": [
            "math.OC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03537",
        "abstract url": "https://arxiv.org/abs/2411.03537",
        "title": "Two-Stage Pretraining for Molecular Property Prediction in the Wild",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Accurate property prediction is crucial for accelerating the discovery of new molecules. Although deep learning models have achieved remarkable success, their performance often relies on large amounts of labeled data that are expensive and time-consuming to obtain. Thus, there is a growing need for models that can perform well with limited experimentally-validated data. In this work, we introduce MoleVers, a versatile pretrained model designed for various types of molecular property prediction in the wild, i.e., where experimentally-validated molecular property labels are scarce. MoleVers adopts a two-stage pretraining strategy. In the first stage, the model learns molecular representations from large unlabeled datasets via masked atom prediction and dynamic denoising, a novel task enabled by a new branching encoder architecture. In the second stage, MoleVers is further pretrained using auxiliary labels obtained with inexpensive computational methods, enabling supervised learning without the need for costly experimental data. This two-stage framework allows MoleVers to learn representations that generalize effectively across various downstream datasets. We evaluate MoleVers on a new benchmark comprising 22 molecular datasets with diverse types of properties, the majority of which contain 50 or fewer training labels reflecting real-world conditions. MoleVers achieves state-of-the-art results on 20 out of the 22 datasets, and ranks second among the remaining two, highlighting its ability to bridge the gap between data-hungry models and real-world conditions where practically-useful labels are scarce.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "physics.chem-ph",
            "q-bio.BM"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03541",
        "abstract url": "https://arxiv.org/abs/2411.03541",
        "title": "Do Mice Grok? Glimpses of Hidden Progress During Overtraining in Sensory Cortex",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Does learning of task-relevant representations stop when behavior stops changing? Motivated by recent theoretical advances in machine learning and the intuitive observation that human experts continue to learn from practice even after mastery, we hypothesize that task-specific representation learning can continue, even when behavior plateaus. In a novel reanalysis of recently published neural data, we find evidence for such learning in posterior piriform cortex of mice following continued training on a task, long after behavior saturates at near-ceiling performance (\"overtraining\"). This learning is marked by an increase in decoding accuracy from piriform neural populations and improved performance on held-out generalization tests. We demonstrate that class representations in cortex continue to separate during overtraining, so that examples that were incorrectly classified at the beginning of overtraining can abruptly be correctly classified later on, despite no changes in behavior during that time. We hypothesize this hidden yet rich learning takes the form of approximate margin maximization; we validate this and other predictions in the neural data, as well as build and interpret a simple synthetic model that recapitulates these phenomena. We conclude by showing how this model of late-time feature learning implies an explanation for the empirical puzzle of overtraining reversal in animal learning, where task-specific representations are more robust to particular task changes because the learned features can be reused.",
        "subjects": [
            "cs.LG",
            "q-bio.NC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03561",
        "abstract url": "https://arxiv.org/abs/2411.03561",
        "title": "Estimating Ego-Body Pose from Doubly Sparse Egocentric Video Data",
        "rating": "0.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We study the problem of estimating the body movements of a camera wearer from egocentric videos. Current methods for ego-body pose estimation rely on temporally dense sensor data, such as IMU measurements from spatially sparse body parts like the head and hands. However, we propose that even temporally sparse observations, such as hand poses captured intermittently from egocentric videos during natural or periodic hand movements, can effectively constrain overall body motion. Naively applying diffusion models to generate full-body pose from head pose and sparse hand pose leads to suboptimal results. To overcome this, we develop a two-stage approach that decomposes the problem into temporal completion and spatial completion. First, our method employs masked autoencoders to impute hand trajectories by leveraging the spatiotemporal correlations between the head pose sequence and intermittent hand poses, providing uncertainty estimates. Subsequently, we employ conditional diffusion models to generate plausible full-body motions based on these temporally dense trajectories of the head and hands, guided by the uncertainty estimates from the imputation. The effectiveness of our method was rigorously tested and validated through comprehensive experiments conducted on various HMD setup with AMASS and Ego-Exo4D datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at NeurIPS 2024"
    },
    {
        "paper id": "2411.03611",
        "abstract url": "https://arxiv.org/abs/2411.03611",
        "title": "Designing a Linearized Potential Function in Neural Network Optimization Using Csisz\u00e1r Type of Tsallis Entropy",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In recent years, learning for neural networks can be viewed as optimization in the space of probability measures. To obtain the exponential convergence to the optimizer, the regularizing term based on Shannon entropy plays an important role. Even though an entropy function heavily affects convergence results, there is almost no result on its generalization, because of the following two technical difficulties: one is the lack of sufficient condition for generalized logarithmic Sobolev inequality, and the other is the distributional dependence of the potential function within the gradient flow equation. In this paper, we establish a framework that utilizes a linearized potential function via Csisz\u00e1r type of Tsallis entropy, which is one of the generalized entropies. We also show that our new framework enable us to derive an exponential convergence result.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.AP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03637",
        "abstract url": "https://arxiv.org/abs/2411.03637",
        "title": "Structure Consistent Gaussian Splatting with Matching Prior for Few-shot Novel View Synthesis",
        "rating": "0.5",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "NeRF",
                "Radiance Fields"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Despite the substantial progress of novel view synthesis, existing methods, either based on the Neural Radiance Fields (NeRF) or more recently 3D Gaussian Splatting (3DGS), suffer significant degradation when the input becomes sparse. Numerous efforts have been introduced to alleviate this problem, but they still struggle to synthesize satisfactory results efficiently, especially in the large scene. In this paper, we propose SCGaussian, a Structure Consistent Gaussian Splatting method using matching priors to learn 3D consistent scene structure. Considering the high interdependence of Gaussian attributes, we optimize the scene structure in two folds: rendering geometry and, more importantly, the position of Gaussian primitives, which is hard to be directly constrained in the vanilla 3DGS due to the non-structure property. To achieve this, we present a hybrid Gaussian representation. Besides the ordinary non-structure Gaussian primitives, our model also consists of ray-based Gaussian primitives that are bound to matching rays and whose optimization of their positions is restricted along the ray. Thus, we can utilize the matching correspondence to directly enforce the position of these Gaussian primitives to converge to the surface points where rays intersect. Extensive experiments on forward-facing, surrounding, and complex large scenes show the effectiveness of our approach with state-of-the-art performance and high efficiency. Code is available at https://github.com/prstrive/SCGaussian.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "NeurIPS 2024 Accepted"
    },
    {
        "paper id": "2411.03641",
        "abstract url": "https://arxiv.org/abs/2411.03641",
        "title": "Constrained Multi-objective Bayesian Optimization through Optimistic Constraints Estimation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Multi-objective Bayesian optimization has been widely adopted in scientific experiment design, including drug discovery and hyperparameter optimization. In practice, regulatory or safety concerns often impose additional thresholds on certain attributes of the experimental outcomes. Previous work has primarily focused on constrained single-objective optimization tasks or active search under constraints. We propose CMOBO, a sample-efficient constrained multi-objective Bayesian optimization algorithm that balances learning of the feasible region (defined on multiple unknowns) with multi-objective optimization within the feasible region in a principled manner. We provide both theoretical justification and empirical evidence, demonstrating the efficacy of our approach on various synthetic benchmarks and real-world applications.",
        "subjects": [
            "cs.LG",
            "stat.ME"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03651",
        "abstract url": "https://arxiv.org/abs/2411.03651",
        "title": "Policy Aggregation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We consider the challenge of AI value alignment with multiple individuals that have different reward functions and optimal policies in an underlying Markov decision process. We formalize this problem as one of policy aggregation, where the goal is to identify a desirable collective policy. We argue that an approach informed by social choice theory is especially suitable. Our key insight is that social choice methods can be reinterpreted by identifying ordinal preferences with volumes of subsets of the state-action occupancy polytope. Building on this insight, we demonstrate that a variety of methods--including approval voting, Borda count, the proportional veto core, and quantile fairness--can be practically applied to policy aggregation.",
        "subjects": [
            "cs.AI",
            "cs.GT",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03659",
        "abstract url": "https://arxiv.org/abs/2411.03659",
        "title": "Towards Scalable Automated Grading: Leveraging Large Language Models for Conceptual Question Evaluation in Engineering",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "This study explores the feasibility of using large language models (LLMs), specifically GPT-4o (ChatGPT), for automated grading of conceptual questions in an undergraduate Mechanical Engineering course. We compared the grading performance of GPT-4o with that of human teaching assistants (TAs) on ten quiz problems from the MEEN 361 course at Texas A&M University, each answered by approximately 225 students. Both the LLM and TAs followed the same instructor-provided rubric to ensure grading consistency. We evaluated performance using Spearman's rank correlation coefficient and Root Mean Square Error (RMSE) to assess the alignment between rankings and the accuracy of scores assigned by GPT-4o and TAs under zero- and few-shot grading settings. In the zero-shot setting, GPT-4o demonstrated a strong correlation with TA grading, with Spearman's rank correlation coefficient exceeding 0.6 in seven out of ten datasets and reaching a high of 0.9387. Our analysis reveals that GPT-4o performs well when grading criteria are straightforward but struggles with nuanced answers, particularly those involving synonyms not present in the rubric. The model also tends to grade more stringently in ambiguous cases compared to human TAs. Overall, ChatGPT shows promise as a tool for grading conceptual questions, offering scalability and consistency.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "21 pages, 21 figures"
    },
    {
        "paper id": "2411.02844",
        "abstract url": "https://arxiv.org/abs/2411.02844",
        "title": "Correlation of Object Detection Performance with Visual Saliency and Depth Estimation",
        "rating": "0",
        "keywords": [
            [
                "Depth"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "As object detection techniques continue to evolve, understanding their relationships with complementary visual tasks becomes crucial for optimising model architectures and computational resources. This paper investigates the correlations between object detection accuracy and two fundamental visual tasks: depth prediction and visual saliency prediction. Through comprehensive experiments using state-of-the-art models (DeepGaze IIE, Depth Anything, DPT-Large, and Itti's model) on COCO and Pascal VOC datasets, we find that visual saliency shows consistently stronger correlations with object detection accuracy (mA$\u03c1$ up to 0.459 on Pascal VOC) compared to depth prediction (mA$\u03c1$ up to 0.283). Our analysis reveals significant variations in these correlations across object categories, with larger objects showing correlation values up to three times higher than smaller objects. These findings suggest incorporating visual saliency features into object detection architectures could be more beneficial than depth information, particularly for specific object categories. The observed category-specific variations also provide insights for targeted feature engineering and dataset design improvements, potentially leading to more efficient and accurate object detection systems.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Code Available at: https://github.com/mbar0075/Object-Detection-Correlation-Saliency-vs-Depth"
    },
    {
        "paper id": "2411.02848",
        "abstract url": "https://arxiv.org/abs/2411.02848",
        "title": "Adversarial multi-task underwater acoustic target recognition: towards robustness against various influential factors",
        "rating": "0",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.LG",
                "cs.SD"
            ]
        ],
        "abstract": "Underwater acoustic target recognition based on passive sonar faces numerous challenges in practical maritime applications. One of the main challenges lies in the susceptibility of signal characteristics to diverse environmental conditions and data acquisition configurations, which can lead to instability in recognition systems. While significant efforts have been dedicated to addressing these influential factors in other domains of underwater acoustics, they are often neglected in the field of underwater acoustic target recognition. To overcome this limitation, this study designs auxiliary tasks that model influential factors (e.g., source range, water column depth, or wind speed) based on available annotations and adopts a multi-task framework to connect these factors to the recognition task. Furthermore, we integrate an adversarial learning mechanism into the multi-task framework to prompt the model to extract representations that are robust against influential factors. Through extensive experiments and analyses on the ShipsEar dataset, our proposed adversarial multi-task model demonstrates its capacity to effectively model the influential factors and achieve state-of-the-art performance on the 12-class recognition task.",
        "subjects": [
            "cs.SD",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02864",
        "abstract url": "https://arxiv.org/abs/2411.02864",
        "title": "Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document Relation Extraction with Graph-of-Thoughts Reasoning",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning capability on many NLP tasks. Recasting an NLP task into a text-to-text generation task is a common practice so that generative LLMs can be prompted to resolve it. However, performing document-level relation extraction (DocRE) tasks with generative LLM models is still challenging due to the structured output format of DocRE, which complicates the conversion to plain text. Limited information available in few-shot samples and prompt instructions induce further difficulties and challenges in relation extraction for mentioned entities in a document. In this paper, we represent the structured output as a graph-style triplet rather than natural language expressions and leverage generative LLMs for the DocRE task. Our approach, the Graph-DPEP framework is grounded in the reasoning behind triplet explanation thoughts presented in natural language. In this framework, we first introduce a ``decomposed-plug\" method for performing the generation from LLMs over prompts with type-space decomposition to alleviate the burden of distinguishing all relation types. Second, we employ a verifier for calibrating the generation and identifying overlooked query entity pairs. Third, we develop \"ensemble-play\", reapplying generation on the entire type list by leveraging the reasoning thoughts embedded in a sub-graph associated with the missing query pair to address the missingness issue. Through extensive comparisons with existing prompt techniques and alternative Language Models (LLMs), our framework demonstrates superior performance on publicly available benchmarks in experiments.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02979",
        "abstract url": "https://arxiv.org/abs/2411.02979",
        "title": "CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model Retrieval",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "NeRF",
                "radiance fields"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Reconstructing from multi-view images is a longstanding problem in 3D vision, where neural radiance fields (NeRFs) have shown great potential and get realistic rendered images of novel views. Currently, most NeRF methods either require accurate camera poses or a large number of input images, or even both. Reconstructing NeRF from few-view images without poses is challenging and highly ill-posed. To address this problem, we propose CAD-NeRF, a method reconstructed from less than 10 images without any known poses. Specifically, we build a mini library of several CAD models from ShapeNet and render them from many random views. Given sparse-view input images, we run a model and pose retrieval from the library, to get a model with similar shapes, serving as the density supervision and pose initializations. Here we propose a multi-view pose retrieval method to avoid pose conflicts among views, which is a new and unseen problem in uncalibrated NeRF methods. Then, the geometry of the object is trained by the CAD guidance. The deformation of the density field and camera poses are optimized jointly. Then texture and density are trained and fine-tuned as well. All training phases are in self-supervised manners. Comprehensive evaluations of synthetic and real images show that CAD-NeRF successfully learns accurate densities with a large deformation from retrieved CAD models, showing the generalization abilities.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The article has been accepted by Frontiers of Computer Science (FCS)"
    },
    {
        "paper id": "2411.02992",
        "abstract url": "https://arxiv.org/abs/2411.02992",
        "title": "Efficient and Effective Adaptation of Multimodal Foundation Models in Sequential Recommendation",
        "rating": "0",
        "keywords": [
            [
                "Parameter-efficient",
                "PEFT",
                "efficient Fine-tuning",
                "GPU memory"
            ],
            [
                "Recommendation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Multimodal foundation models (MFMs) have revolutionized sequential recommender systems through advanced representation learning. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt these models, studies often prioritize parameter efficiency, neglecting GPU memory and training speed. To address this, we introduced the IISAN framework, significantly enhancing efficiency. However, IISAN was limited to symmetrical MFMs and identical text and image encoders, preventing the use of state-of-the-art Large Language Models. To overcome this, we developed IISAN-Versa, a versatile plug-and-play architecture compatible with both symmetrical and asymmetrical MFMs. IISAN-Versa employs a Decoupled PEFT structure and utilizes both intra- and inter-modal adaptation. It effectively handles asymmetry through a simple yet effective combination of group layer-dropping and dimension transformation alignment. Our research demonstrates that IISAN-Versa effectively adapts large text encoders, and we further identify a scaling effect where larger encoders generally perform better. IISAN-Versa also demonstrates strong versatility in our defined multimodal scenarios, which include raw titles and captions generated from images and videos. Additionally, IISAN-Versa achieved state-of-the-art performance on the Microlens public benchmark. We will release our code and datasets to support future research.",
        "subjects": [
            "cs.IR",
            "cs.CV"
        ],
        "comment": "The extension of IISAN in SIGIR2024"
    },
    {
        "paper id": "2411.03053",
        "abstract url": "https://arxiv.org/abs/2411.03053",
        "title": "Gradient-Guided Conditional Diffusion Models for Private Image Reconstruction: Analyzing Adversarial Impacts of Differential Privacy and Denoising",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "We investigate the construction of gradient-guided conditional diffusion models for reconstructing private images, focusing on the adversarial interplay between differential privacy noise and the denoising capabilities of diffusion models. While current gradient-based reconstruction methods struggle with high-resolution images due to computational complexity and prior knowledge requirements, we propose two novel methods that require minimal modifications to the diffusion model's generation process and eliminate the need for prior knowledge. Our approach leverages the strong image generation capabilities of diffusion models to reconstruct private images starting from randomly generated noise, even when a small amount of differentially private noise has been added to the gradients. We also conduct a comprehensive theoretical analysis of the impact of differential privacy noise on the quality of reconstructed images, revealing the relationship among noise magnitude, the architecture of attacked models, and the attacker's reconstruction capability. Additionally, extensive experiments validate the effectiveness of our proposed methods and the accuracy of our theoretical findings, suggesting new directions for privacy risk auditing using conditional diffusion models.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03223",
        "abstract url": "https://arxiv.org/abs/2411.03223",
        "title": "Beyond Grid Data: Exploring Graph Neural Networks for Earth Observation",
        "rating": "0",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Earth Observation (EO) data analysis has been significantly revolutionized by deep learning (DL), with applications typically limited to grid-like data structures. Graph Neural Networks (GNNs) emerge as an important innovation, propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively tackle the challenges posed by diverse modalities, multiple sensors, and the heterogeneous nature of EO data. To introduce GNNs in the related domains, our review begins by offering fundamental knowledge on GNNs. Then, we summarize the generic problems in EO, to which GNNs can offer potential solutions. Following this, we explore a broad spectrum of GNNs' applications to scientific problems in Earth systems, covering areas such as weather and climate analysis, disaster management, air quality monitoring, agriculture, land cover classification, hydrological process modeling, and urban modeling. The rationale behind adopting GNNs in these fields is explained, alongside methodologies for organizing graphs and designing favorable architectures for various tasks. Furthermore, we highlight methodological challenges of implementing GNNs in these domains and possible solutions that could guide future research. While acknowledging that GNNs are not a universal solution, we conclude the paper by comparing them with other popular architectures like transformers and analyzing their potential synergies.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "Accepted for publication in Geoscience and Remote Sensing Magazine (GRSM)"
    },
    {
        "paper id": "2411.03228",
        "abstract url": "https://arxiv.org/abs/2411.03228",
        "title": "Topograph: An efficient Graph-Based Framework for Strictly Topology Preserving Image Segmentation",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Topological correctness plays a critical role in many image segmentation tasks, yet most networks are trained using pixel-wise loss functions, such as Dice, neglecting topological accuracy. Existing topology-aware methods often lack robust topological guarantees, are limited to specific use cases, or impose high computational costs. In this work, we propose a novel, graph-based framework for topologically accurate image segmentation that is both computationally efficient and generally applicable. Our method constructs a component graph that fully encodes the topological information of both the prediction and ground truth, allowing us to efficiently identify topologically critical regions and aggregate a loss based on local neighborhood information. Furthermore, we introduce a strict topological metric capturing the homotopy equivalence between the union and intersection of prediction-label pairs. We formally prove the topological guarantees of our approach and empirically validate its effectiveness on binary and multi-class datasets. Our loss demonstrates state-of-the-art performance with up to fivefold faster loss computation compared to persistent homology methods.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03286",
        "abstract url": "https://arxiv.org/abs/2411.03286",
        "title": "DiT4Edit: Diffusion Transformer for Image Editing",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "Image Editing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Despite recent advances in UNet-based image editing, methods for shape-aware object editing in high-resolution images are still lacking. Compared to UNet, Diffusion Transformers (DiT) demonstrate superior capabilities to effectively capture the long-range dependencies among patches, leading to higher-quality image generation. In this paper, we propose DiT4Edit, the first Diffusion Transformer-based image editing framework. Specifically, DiT4Edit uses the DPM-Solver inversion algorithm to obtain the inverted latents, reducing the number of steps compared to the DDIM inversion algorithm commonly used in UNet-based frameworks. Additionally, we design unified attention control and patches merging, tailored for transformer computation streams. This integration allows our framework to generate higher-quality edited images faster. Our design leverages the advantages of DiT, enabling it to surpass UNet structures in image editing, especially in high-resolution and arbitrary-size images. Extensive experiments demonstrate the strong performance of DiT4Edit across various editing scenarios, highlighting the potential of Diffusion Transformers in supporting image editing.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03475",
        "abstract url": "https://arxiv.org/abs/2411.03475",
        "title": "Self Supervised Networks for Learning Latent Space Representations of Human Body Scans and Motions",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper introduces self-supervised neural network models to tackle several fundamental problems in the field of 3D human body analysis and processing. First, we propose VariShaPE (Varifold Shape Parameter Estimator), a novel architecture for the retrieval of latent space representations of body shapes and poses. This network offers a fast and robust method to estimate the embedding of arbitrary unregistered meshes into the latent space. Second, we complement the estimation of latent codes with MoGeN (Motion Geometry Network) a framework that learns the geometry on the latent space itself. This is achieved by lifting the body pose parameter space into a higher dimensional Euclidean space in which body motion mini-sequences from a training set of 4D data can be approximated by simple linear interpolation. Using the SMPL latent space representation we illustrate how the combination of these network models, once trained, can be used to perform a variety of tasks with very limited computational cost. This includes operations such as motion interpolation, extrapolation and transfer as well as random shape and pose generation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "23 pages, 11 figures, 6 tables"
    },
    {
        "paper id": "2411.03480",
        "abstract url": "https://arxiv.org/abs/2411.03480",
        "title": "Rainfall regression from C-band Synthetic Aperture Radar using Multi-Task Generative Adversarial Networks",
        "rating": "0",
        "keywords": [
            [
                "Radar"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper introduces a data-driven approach to estimate precipitation rates from Synthetic Aperture Radar (SAR) at a spatial resolution of 200 meters per pixel. It addresses previous challenges related to the collocation of SAR and weather radar data, specifically the misalignment in collocations and the scarcity of rainfall examples under strong wind. To tackle these challenges, the paper proposes a multi-objective formulation, introducing patch-level components and an adversarial component. It exploits the full NEXRAD archive to look for potential co-locations with Sentinel-1 data. With additional enhancements to the training procedure and the incorporation of additional inputs, the resulting model demonstrates improved accuracy in rainfall estimates and the ability to extend its performance to scenarios up to 15 m/s.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "36 pages, 13 figures"
    },
    {
        "paper id": "2411.03511",
        "abstract url": "https://arxiv.org/abs/2411.03511",
        "title": "Beyond Complete Shapes: A Quantitative Evaluation of 3D Shape Matching Algorithms",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Finding correspondences between 3D shapes is an important and long-standing problem in computer vision, graphics and beyond. While approaches based on machine learning dominate modern 3D shape matching, almost all existing (learning-based) methods require that at least one of the involved shapes is complete. In contrast, the most challenging and arguably most practically relevant setting of matching partially observed shapes, is currently underexplored. One important factor is that existing datasets contain only a small number of shapes (typically below 100), which are unable to serve data-hungry machine learning approaches, particularly in the unsupervised regime. In addition, the type of partiality present in existing datasets is often artificial and far from realistic. To address these limitations and to encourage research on these relevant settings, we provide a generic and flexible framework for the procedural generation of challenging partial shape matching scenarios. Our framework allows for a virtually infinite generation of partial shape matching instances from a finite set of shapes with complete geometry. Further, we manually create cross-dataset correspondences between seven existing (complete geometry) shape matching datasets, leading to a total of 2543 shapes. Based on this, we propose several challenging partial benchmark settings, for which we evaluate respective state-of-the-art methods as baselines.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03555",
        "abstract url": "https://arxiv.org/abs/2411.03555",
        "title": "Object and Contact Point Tracking in Demonstrations Using 3D Gaussian Splatting",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper introduces a method to enhance Interactive Imitation Learning (IIL) by extracting touch interaction points and tracking object movement from video demonstrations. The approach extends current IIL systems by providing robots with detailed knowledge of both where and how to interact with objects, particularly complex articulated ones like doors and drawers. By leveraging cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for tracking, this method allows robots to better understand and manipulate objects in dynamic environments. The research lays the foundation for more effective task learning and execution in autonomous robotic systems.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "CoRL 2024, Workshop on Lifelong Learning for Home Robots, Munich, Germany"
    },
    {
        "paper id": "2411.03569",
        "abstract url": "https://arxiv.org/abs/2411.03569",
        "title": "Towards Personalized Federated Learning via Comprehensive Knowledge Distillation",
        "rating": "0",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Federated learning is a distributed machine learning paradigm designed to protect data privacy. However, data heterogeneity across various clients results in catastrophic forgetting, where the model rapidly forgets previous knowledge while acquiring new knowledge. To address this challenge, personalized federated learning has emerged to customize a personalized model for each client. However, the inherent limitation of this mechanism is its excessive focus on personalization, potentially hindering the generalization of those models. In this paper, we present a novel personalized federated learning method that uses global and historical models as teachers and the local model as the student to facilitate comprehensive knowledge distillation. The historical model represents the local model from the last round of client training, containing historical personalized knowledge, while the global model represents the aggregated model from the last round of server aggregation, containing global generalized knowledge. By applying knowledge distillation, we effectively transfer global generalized knowledge and historical personalized knowledge to the local model, thus mitigating catastrophic forgetting and enhancing the general performance of personalized models. Extensive experimental results demonstrate the significant advantages of our method.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CR",
            "cs.CV"
        ],
        "comment": "Accepted by IEEE SMC 2024"
    },
    {
        "paper id": "2411.03615",
        "abstract url": "https://arxiv.org/abs/2411.03615",
        "title": "ADMIRE: a locally adaptive single-image, non-uniformity correction and denoising algorithm: application to uncooled IR camera",
        "rating": "0",
        "keywords": [
            [
                "infrared"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "We propose a new way to correct for the non-uniformity (NU) and the noise in uncooled infrared-type images. This method works on static images, needs no registration, no camera motion and no model for the non uniformity. The proposed method uses an hybrid scheme including an automatic locally-adaptive contrast adjustment and a state-of-the-art image denoising method. It permits to correct for a fully non-linear NU and the noise efficiently using only one image. We compared it with total variation on real raw and simulated NU infrared images. The strength of this approach lies in its simplicity, low computational cost. It needs no test-pattern or calibration and produces no \"ghost-artefact\".",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.04142",
        "abstract url": "https://arxiv.org/abs/2411.04142",
        "title": "Unified Pathological Speech Analysis with Prompt Tuning",
        "rating": "0",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "disease",
                "Pathological"
            ],
            [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Pathological speech analysis has been of interest in the detection of certain diseases like depression and Alzheimer's disease and attracts much interest from researchers. However, previous pathological speech analysis models are commonly designed for a specific disease while overlooking the connection between diseases, which may constrain performance and lower training efficiency. Instead of fine-tuning deep models for different tasks, prompt tuning is a much more efficient training paradigm. We thus propose a unified pathological speech analysis system for as many as three diseases with the prompt tuning technique. This system uses prompt tuning to adjust only a small part of the parameters to detect different diseases from speeches of possible patients. Our system leverages a pre-trained spoken language model and demonstrates strong performance across multiple disorders while only fine-tuning a fraction of the parameters. This efficient training approach leads to faster convergence and improved F1 scores by allowing knowledge to be shared across tasks. Our experiments on Alzheimer's disease, Depression, and Parkinson's disease show competitive results, highlighting the effectiveness of our method in pathological speech analysis.",
        "subjects": [
            "eess.AS",
            "cs.CL",
            "cs.SD"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication"
    },
    {
        "paper id": "2411.04709",
        "abstract url": "https://arxiv.org/abs/2411.04709",
        "title": "TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation",
        "rating": "0",
        "keywords": [
            [
                "text-to-video"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video generation models are revolutionizing content creation, with image-to-video models drawing increasing attention due to their enhanced controllability, visual consistency, and practical applications. However, despite their popularity, these models rely on user-provided text and image prompts, and there is currently no dedicated dataset for studying these prompts. In this paper, we introduce TIP-I2V, the first large-scale dataset of over 1.70 million unique user-provided Text and Image Prompts specifically for Image-to-Video generation. Additionally, we provide the corresponding generated videos from five state-of-the-art image-to-video models. We begin by outlining the time-consuming and costly process of curating this large-scale dataset. Next, we compare TIP-I2V to two popular prompt datasets, VidProM (text-to-video) and DiffusionDB (text-to-image), highlighting differences in both basic and semantic information. This dataset enables advancements in image-to-video research. For instance, to develop better models, researchers can use the prompts in TIP-I2V to analyze user preferences and evaluate the multi-dimensional performance of their trained models; and to enhance model safety, they may focus on addressing the misinformation issue caused by image-to-video models. The new research inspired by TIP-I2V and the differences with existing datasets emphasize the importance of a specialized image-to-video prompt dataset. The project is publicly available at https://tip-i2v.github.io.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The project is publicly available at https://tip-i2v.github.io"
    },
    {
        "paper id": "2411.04712",
        "abstract url": "https://arxiv.org/abs/2411.04712",
        "title": "SEE-DPO: Self Entropy Enhanced Direct Preference Optimization",
        "rating": "0",
        "keywords": [
            [
                "diffusion",
                "text-to-image"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Direct Preference Optimization (DPO) has been successfully used to align large language models (LLMs) according to human preferences, and more recently it has also been applied to improving the quality of text-to-image diffusion models. However, DPO-based methods such as SPO, Diffusion-DPO, and D3PO are highly susceptible to overfitting and reward hacking, especially when the generative model is optimized to fit out-of-distribution during prolonged training. To overcome these challenges and stabilize the training of diffusion models, we introduce a self-entropy regularization mechanism in reinforcement learning from human feedback. This enhancement improves DPO training by encouraging broader exploration and greater robustness. Our regularization technique effectively mitigates reward hacking, leading to improved stability and enhanced image quality across the latent space. Extensive experiments demonstrate that integrating human feedback with self-entropy regularization can significantly boost image diversity and specificity, achieving state-of-the-art results on key image generation metrics.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02837",
        "abstract url": "https://arxiv.org/abs/2411.02837",
        "title": "On the Comparison between Multi-modal and Single-modal Contrastive Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Multi-modal contrastive learning with language supervision has presented a paradigm shift in modern machine learning. By pre-training on a web-scale dataset, multi-modal contrastive learning can learn high-quality representations that exhibit impressive robustness and transferability. Despite its empirical success, the theoretical understanding is still in its infancy, especially regarding its comparison with single-modal contrastive learning. In this work, we introduce a feature learning theory framework that provides a theoretical foundation for understanding the differences between multi-modal and single-modal contrastive learning. Based on a data generation model consisting of signal and noise, our analysis is performed on a ReLU network trained with the InfoMax objective function. Through a trajectory-based optimization analysis and generalization characterization on downstream tasks, we identify the critical factor, which is the signal-to-noise ratio (SNR), that impacts the generalizability in downstream tasks of both multi-modal and single-modal contrastive learning. Through the cooperation between the two modalities, multi-modal learning can achieve better feature learning, leading to improvements in performance in downstream tasks compared to single-modal learning. Our analysis provides a unified framework that can characterize the optimization and generalization of both single-modal and multi-modal contrastive learning. Empirical experiments on both synthetic and real-world datasets further consolidate our theoretical findings.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "51pages, 1 figure, 1 table"
    },
    {
        "paper id": "2411.02847",
        "abstract url": "https://arxiv.org/abs/2411.02847",
        "title": "Dissecting the Failure of Invariant Learning on Graphs",
        "rating": "-0.5",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Enhancing node-level Out-Of-Distribution (OOD) generalization on graphs remains a crucial area of research. In this paper, we develop a Structural Causal Model (SCM) to theoretically dissect the performance of two prominent invariant learning methods -- Invariant Risk Minimization (IRM) and Variance-Risk Extrapolation (VREx) -- in node-level OOD settings. Our analysis reveals a critical limitation: due to the lack of class-conditional invariance constraints, these methods may struggle to accurately identify the structure of the predictive invariant ego-graph and consequently rely on spurious features. To address this, we propose Cross-environment Intra-class Alignment (CIA), which explicitly eliminates spurious features by aligning cross-environment representations conditioned on the same class, bypassing the need for explicit knowledge of the causal pattern structure. To adapt CIA to node-level OOD scenarios where environment labels are hard to obtain, we further propose CIA-LRA (Localized Reweighting Alignment) that leverages the distribution of neighboring labels to selectively align node representations, effectively distinguishing and preserving invariant features while removing spurious ones, all without relying on environment labels. We theoretically prove CIA-LRA's effectiveness by deriving an OOD generalization error bound based on PAC-Bayesian analysis. Experiments on graph OOD benchmarks validate the superiority of CIA and CIA-LRA, marking a significant advancement in node-level OOD generalization. The codes are available at https://github.com/NOVAglow646/NeurIPS24-Invariant-Learning-on-Graphs.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02902",
        "abstract url": "https://arxiv.org/abs/2411.02902",
        "title": "Membership Inference Attacks against Large Vision-Language Models",
        "rating": "-0.5",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "Attacks"
            ],
            [
                "medical"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxR\u00e9nyi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.CR",
            "cs.LG"
        ],
        "comment": "NeurIPS 2024"
    },
    {
        "paper id": "2411.02908",
        "abstract url": "https://arxiv.org/abs/2411.02908",
        "title": "Photon: Federated LLM Pre-Training",
        "rating": "-0.5",
        "keywords": [
            [
                "federated learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Scaling large language models (LLMs) demands extensive data and computing resources, which are traditionally constrained to data centers by the high-bandwidth requirements of distributed training. Low-bandwidth methods like federated learning (FL) could enable collaborative training of larger models across weakly-connected GPUs if they can effectively be used for pre-training. To achieve this, we introduce Photon, the first complete system for federated end-to-end LLM training, leveraging cross-silo FL for global-scale training with minimal communication overheads. Using Photon, we train the first federated family of decoder-only LLMs from scratch. We show that: (1) Photon can train model sizes up to 7B in a federated fashion while reaching an even better perplexity than centralized pre-training; (2) Photon model training time decreases with available compute, achieving a similar compute-time trade-off to centralized; and (3) Photon outperforms the wall-time of baseline distributed training methods by 35% via communicating 64x-512xless. Our proposal is robust to data heterogeneity and converges twice as fast as previous methods like DiLoCo. This surprising data efficiency stems from a unique approach combining small client batch sizes with extremely high learning rates, enabled by federated averaging's robustness to hyperparameters. Photon thus represents the first economical system for global internet-wide LLM pre-training.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": "13 pages, 9 appendix pages, 10 figures, 3 algorithms, 8 tables"
    },
    {
        "paper id": "2411.02921",
        "abstract url": "https://arxiv.org/abs/2411.02921",
        "title": "Theoretically Guaranteed Distribution Adaptable Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In many open environment applications, data are collected in the form of a stream, which exhibits an evolving distribution over time. How to design algorithms to track these evolving data distributions with provable guarantees, particularly in terms of the generalization ability, remains a formidable challenge. To handle this crucial but rarely studied problem and take a further step toward robust artificial intelligence, we propose a novel framework called Distribution Adaptable Learning (DAL). It enables the model to effectively track the evolving data distributions. By Encoding Feature Marginal Distribution Information (EFMDI), we broke the limitations of optimal transport to characterize the environmental changes and enable model reuse across diverse data distributions. It can enhance the reusable and evolvable properties of DAL in accommodating evolving distributions. Furthermore, to obtain the model interpretability, we not only analyze the generalization error bound of the local step in the evolution process, but also investigate the generalization error bound associated with the entire classifier trajectory of the evolution based on the Fisher-Rao distance. For demonstration, we also present two special cases within the framework, together with their optimizations and convergence analyses. Experimental results over both synthetic and real-world data distribution evolving tasks validate the effectiveness and practical utility of the proposed framework.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02926",
        "abstract url": "https://arxiv.org/abs/2411.02926",
        "title": "Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic Encryption for Collaborative Anti-Money Laundering",
        "rating": "-0.5",
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Combating money laundering has become increasingly complex with the rise of cybercrime and digitalization of financial transactions. Graph-based machine learning techniques have emerged as promising tools for Anti-Money Laundering (AML) detection, capturing intricate relationships within money laundering networks. However, the effectiveness of AML solutions is hindered by data silos within financial institutions, limiting collaboration and overall efficacy. This research presents a novel privacy-preserving approach for collaborative AML machine learning, facilitating secure data sharing across institutions and borders while preserving privacy and regulatory compliance. Leveraging Fully Homomorphic Encryption (FHE), computations are directly performed on encrypted data, ensuring the confidentiality of financial data. Notably, FHE over the Torus (TFHE) was integrated with graph-based machine learning using Zama Concrete ML. The research contributes two key privacy-preserving pipelines. First, the development of a privacy-preserving Graph Neural Network (GNN) pipeline was explored. Optimization techniques like quantization and pruning were used to render the GNN FHE-compatible. Second, a privacy-preserving graph-based XGBoost pipeline leveraging Graph Feature Preprocessor (GFP) was successfully developed. Experiments demonstrated strong predictive performance, with the XGBoost model consistently achieving over 99% accuracy, F1-score, precision, and recall on the balanced AML dataset in both unencrypted and FHE-encrypted inference settings. On the imbalanced dataset, the incorporation of graph-based features improved the F1-score by 8%. The research highlights the need to balance the trade-off between privacy and computational efficiency.",
        "subjects": [
            "cs.CR",
            "cs.LG"
        ],
        "comment": "14th International Conference on Security, Privacy, and Applied Cryptographic Engineering (SPACE) 2024"
    },
    {
        "paper id": "2411.02954",
        "abstract url": "https://arxiv.org/abs/2411.02954",
        "title": "IMUDiffusion: A Diffusion Model for Multivariate Time Series Synthetisation for Inertial Motion Capturing Systems",
        "rating": "-0.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Kinematic sensors are often used to analyze movement behaviors in sports and daily activities due to their ease of use and lack of spatial restrictions, unlike video-based motion capturing systems. Still, the generation, and especially the labeling of motion data for specific activities can be time-consuming and costly. Additionally, many models struggle with limited data, which limits their performance in recognizing complex movement patterns. To address those issues, generating synthetic data can help expand the diversity and variability. In this work, we propose IMUDiffusion, a probabilistic diffusion model specifically designed for multivariate time series generation. Our approach enables the generation of high-quality time series sequences which accurately capture the dynamics of human activities. Moreover, by joining our dataset with synthetic data, we achieve a significant improvement in the performance of our baseline human activity classifier. In some cases, we are able to improve the macro F1-score by almost 30%. IMUDiffusion provides a valuable tool for generating realistic human activity movements and enhance the robustness of models in scenarios with limited training data.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03022",
        "abstract url": "https://arxiv.org/abs/2411.03022",
        "title": "Flashy Backdoor: Real-world Environment Backdoor Attack on SNNs with DVS Cameras",
        "rating": "-0.5",
        "keywords": [
            [
                "Attack"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "While security vulnerabilities in traditional Deep Neural Networks (DNNs) have been extensively studied, the susceptibility of Spiking Neural Networks (SNNs) to adversarial attacks remains mostly underexplored. Until now, the mechanisms to inject backdoors into SNN models have been limited to digital scenarios; thus, we present the first evaluation of backdoor attacks in real-world environments. We begin by assessing the applicability of existing digital backdoor attacks and identifying their limitations for deployment in physical environments. To address each of the found limitations, we present three novel backdoor attack methods on SNNs, i.e., Framed, Strobing, and Flashy Backdoor. We also assess the effectiveness of traditional backdoor procedures and defenses adapted for SNNs, such as pruning, fine-tuning, and fine-pruning. The results show that while these procedures and defenses can mitigate some attacks, they often fail against stronger methods like Flashy Backdoor or sacrifice too much clean accuracy, rendering the models unusable. Overall, all our methods can achieve up to a 100% Attack Success Rate while maintaining high clean accuracy in every tested dataset. Additionally, we evaluate the stealthiness of the triggers with commonly used metrics, finding them highly stealthy. Thus, we propose new alternatives more suited for identifying poisoned samples in these scenarios. Our results show that further research is needed to ensure the security of SNN-based systems against backdoor attacks and their safe application in real-world scenarios. The code, experiments, and results are available in our repository.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03028",
        "abstract url": "https://arxiv.org/abs/2411.03028",
        "title": "Graph Agnostic Causal Bayesian Optimisation",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study the problem of globally optimising a target variable of an unknown causal graph on which a sequence of soft or hard interventions can be performed. The problem of optimising the target variable associated with a causal graph is formalised as Causal Bayesian Optimisation (CBO). We study the CBO problem under the cumulative regret objective with unknown causal graphs for two settings, namely structural causal models with hard interventions and function networks with soft interventions. We propose Graph Agnostic Causal Bayesian Optimisation (GACBO), an algorithm that actively discovers the causal structure that contributes to achieving optimal rewards. GACBO seeks to balance exploiting the actions that give the best rewards against exploring the causal structures and functions. To the best of our knowledge, our work is the first to study causal Bayesian optimization with cumulative regret objectives in scenarios where the graph is unknown or partially known. We show our proposed algorithm outperforms baselines in simulated experiments and real-world applications.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03239",
        "abstract url": "https://arxiv.org/abs/2411.03239",
        "title": "Decoupling Fine Detail and Global Geometry for Compressed Depth Map Super-Resolution",
        "rating": "-0.5",
        "keywords": [
            [
                "Depth"
            ],
            [
                "Super-Resolution"
            ],
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Recovering high-quality depth maps from compressed sources has gained significant attention due to the limitations of consumer-grade depth cameras and the bandwidth restrictions during data transmission. However, current methods still suffer from two challenges. First, bit-depth compression produces a uniform depth representation in regions with subtle variations, hindering the recovery of detailed information. Second, densely distributed random noise reduces the accuracy of estimating the global geometric structure of the scene. To address these challenges, we propose a novel framework, termed geometry-decoupled network (GDNet), for compressed depth map super-resolution that decouples the high-quality depth map reconstruction process by handling global and detailed geometric features separately. To be specific, we propose the fine geometry detail encoder (FGDE), which is designed to aggregate fine geometry details in high-resolution low-level image features while simultaneously enriching them with complementary information from low-resolution context-level image features. In addition, we develop the global geometry encoder (GGE) that aims at suppressing noise and extracting global geometric information effectively via constructing compact feature representation in a low-rank space. We conduct experiments on multiple benchmark datasets, demonstrating that our GDNet significantly outperforms current methods in terms of geometric consistency and detail recovery. In the ECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st place award. Our codes will be available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "The 1st solution for the ECCV 2024 AIM Compressed Depth Upsampling Challenge"
    },
    {
        "paper id": "2411.03273",
        "abstract url": "https://arxiv.org/abs/2411.03273",
        "title": "Graph-Based Semi-Supervised Segregated Lipschitz Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper presents an approach to semi-supervised learning for the classification of data using the Lipschitz Learning on graphs. We develop a graph-based semi-supervised learning framework that leverages the properties of the infinity Laplacian to propagate labels in a dataset where only a few samples are labeled. By extending the theory of spatial segregation from the Laplace operator to the infinity Laplace operator, both in continuum and discrete settings, our approach provides a robust method for dealing with class imbalance, a common challenge in machine learning. Experimental validation on several benchmark datasets demonstrates that our method not only improves classification accuracy compared to existing methods but also ensures efficient label propagation in scenarios with limited labeled data.",
        "subjects": [
            "cs.LG",
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03279",
        "abstract url": "https://arxiv.org/abs/2411.03279",
        "title": "Oblivious Defense in ML Models: Backdoor Removal without Detection",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "As society grows more reliant on machine learning, ensuring the security of machine learning systems against sophisticated attacks becomes a pressing concern. A recent result of Goldwasser, Kim, Vaikuntanathan, and Zamir (2022) shows that an adversary can plant undetectable backdoors in machine learning models, allowing the adversary to covertly control the model's behavior. Backdoors can be planted in such a way that the backdoored machine learning model is computationally indistinguishable from an honest model without backdoors. In this paper, we present strategies for defending against backdoors in ML models, even if they are undetectable. The key observation is that it is sometimes possible to provably mitigate or even remove backdoors without needing to detect them, using techniques inspired by the notion of random self-reducibility. This depends on properties of the ground-truth labels (chosen by nature), and not of the proposed ML model (which may be chosen by an attacker). We give formal definitions for secure backdoor mitigation, and proceed to show two types of results. First, we show a \"global mitigation\" technique, which removes all backdoors from a machine learning model under the assumption that the ground-truth labels are close to a Fourier-heavy function. Second, we consider distributions where the ground-truth labels are close to a linear or polynomial function in $\\mathbb{R}^n$. Here, we show \"local mitigation\" techniques, which remove backdoors with high probability for every inputs of interest, and are computationally cheaper than global mitigation. All of our constructions are black-box, so our techniques work without needing access to the model's representation (i.e., its code or parameters). Along the way we prove a simple result for robust mean estimation.",
        "subjects": [
            "cs.LG",
            "cs.CC",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03294",
        "abstract url": "https://arxiv.org/abs/2411.03294",
        "title": "Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy For Visuomotor Imitation Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "We propose an object-centric recovery policy framework to address the challenges of out-of-distribution (OOD) scenarios in visuomotor policy learning. Previous behavior cloning (BC) methods rely heavily on a large amount of labeled data coverage, failing in unfamiliar spatial states. Without relying on extra data collection, our approach learns a recovery policy constructed by an inverse policy inferred from object keypoint manifold gradient in the original training data. The recovery policy serves as a simple add-on to any base visuomotor BC policy, agnostic to a specific method, guiding the system back towards the training distribution to ensure task success even in OOD situations. We demonstrate the effectiveness of our object-centric framework in both simulation and real robot experiments, achieving an improvement of 77.7% over the base policy in OOD. Project Website: https://sites.google.com/view/ocr-penn",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "Accepted for Spotlight (5 out of 21 papers) at CoRL 2024 Workshop on Lifelong Learning for Home Robots"
    },
    {
        "paper id": "2411.03389",
        "abstract url": "https://arxiv.org/abs/2411.03389",
        "title": "Neurons for Neutrons: A Transformer Model for Computation Load Estimation on Domain-Decomposed Neutron Transport Problems",
        "rating": "-0.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Domain decomposition is a technique used to reduce memory overhead on large neutron transport problems. Currently, the optimal load-balanced processor allocation for these domains is typically determined through small-scale simulations of the problem, which can be time-consuming for researchers and must be repeated anytime a problem input is changed. We propose a Transformer model with a unique 3D input embedding, and input representations designed for domain-decomposed neutron transport problems, which can predict the subdomain computation loads generated by small-scale simulations. We demonstrate that such a model trained on domain-decomposed Small Modular Reactor (SMR) simulations achieves 98.2% accuracy while being able to skip the small-scale simulation step entirely. Tests of the model's robustness on variant fuel assemblies, other problem geometries, and changes in simulation parameters are also discussed.",
        "subjects": [
            "physics.comp-ph",
            "cs.AI"
        ],
        "comment": "28 pages, 14 figures"
    },
    {
        "paper id": "2411.03449",
        "abstract url": "https://arxiv.org/abs/2411.03449",
        "title": "AI Horizon Scanning -- White Paper p3395, IEEE-SA. Part III: Technology Watch: a selection of key developments, emerging technologies, and industry trends in Artificial Intelligence",
        "rating": "-0.5",
        "keywords": [
            [
                "robotics"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "Generative Artificial Intelligence (AI) technologies are in a phase of unprecedented rapid development following the landmark release of Chat-GPT, which brought the phenomenon to wide public attention. As the deployment of AI products rises geometrically, considerable attention is being given to the threats and opportunities that AI technologies offer, and to the need for regulatory and standards initiatives to ensure that use of the technology aligns with societal needs and generates broad benefits while mitigating risks and threats. This manuscript is the third of a series of White Papers informing the development of IEEE-SA's p3995 {\\it `Standard for the Implementation of Safeguards, Controls, and Preventive Techniques for Artificial Intelligence Models'} \\cite{P3395}, Chair Marina Cort\u00eas. This part focuses on assessing calmly and objectively, as far as is possible, the current state of Artificial Intelligence (AI) technology development and identifying predominant trends, prospects, and ensuing risks. It necessarily forms a snapshot of the current instant of a rapidly-evolving landscape, with new products and innovations emerging continuously. While our main focus is on software and hardware developments and their corporate context, we also briefly review progress on robotics within the AI context and describe some implications of the substantial and growing AI energy demand.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "This is an interim version of our p3395 WG White Paper, Part III. We will update this version, until publication by IEEE-SA, Sponsor Committee - Artificial Intelligence Standards Committee (C/AISC); https://standards.ieee.org/ieee/3395/11378/ This White Paper is companion to Part I available at arXiv:2410.01808"
    },
    {
        "paper id": "2411.03494",
        "abstract url": "https://arxiv.org/abs/2411.03494",
        "title": "An Open-source Sim2Real Approach for Sensor-independent Robot Navigation in a Grid",
        "rating": "-0.5",
        "keywords": [
            [
                "Robot",
                "Navigation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper presents a Sim2Real (Simulation to Reality) approach to bridge the gap between a trained agent in a simulated environment and its real-world implementation in navigating a robot in a similar setting. Specifically, we focus on navigating a quadruped robot in a real-world grid-like environment inspired by the Gymnasium Frozen Lake -- a highly user-friendly and free Application Programming Interface (API) to develop and test Reinforcement Learning (RL) algorithms. We detail the development of a pipeline to transfer motion policies learned in the Frozen Lake simulation to a physical quadruped robot, thus enabling autonomous navigation and obstacle avoidance in a grid without relying on expensive localization and mapping sensors. The work involves training an RL agent in the Frozen Lake environment and utilizing the resulting Q-table to control a 12 Degrees-of-Freedom (DOF) quadruped robot. In addition to detailing the RL implementation, inverse kinematics-based quadruped gaits, and the transfer policy pipeline, we open-source the project on GitHub and include a demonstration video of our Sim2Real transfer approach. This work provides an accessible, straightforward, and low-cost framework for researchers, students, and hobbyists to explore and implement RL-based robot navigation in real-world grid environments.",
        "subjects": [
            "cs.RO",
            "cs.LG"
        ],
        "comment": "Accepted for publication at the 9th IEEE International Conference on Robotics and Automation Engineering (IEEE ICRAE 2024), Singapore"
    },
    {
        "paper id": "2411.03570",
        "abstract url": "https://arxiv.org/abs/2411.03570",
        "title": "Learning Constant-Depth Circuits in Malicious Noise Models",
        "rating": "-0.5",
        "keywords": [
            [
                "Depth"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The seminal work of Linial, Mansour, and Nisan gave a quasipolynomial-time algorithm for learning constant-depth circuits ($\\mathsf{AC}^0$) with respect to the uniform distribution on the hypercube. Extending their algorithm to the setting of malicious noise, where both covariates and labels can be adversarially corrupted, has remained open. Here we achieve such a result, inspired by recent work on learning with distribution shift. Our running time essentially matches their algorithm, which is known to be optimal assuming various cryptographic primitives. Our proof uses a simple outlier-removal method combined with Braverman's theorem for fooling constant-depth circuits. We attain the best possible dependence on the noise rate and succeed in the harshest possible noise model (i.e., contamination or so-called \"nasty noise\").",
        "subjects": [
            "cs.DS",
            "cs.CR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03598",
        "abstract url": "https://arxiv.org/abs/2411.03598",
        "title": "Open-Source High-Speed Flight Surrogate Modeling Framework",
        "rating": "-0.5",
        "keywords": [
            [
                "Flight"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "High-speed flight vehicles, which travel much faster than the speed of sound, are crucial for national defense and space exploration. However, accurately predicting their behavior under numerous, varied flight conditions is a challenge and often prohibitively expensive. The proposed approach involves creating smarter, more efficient machine learning models (also known as surrogate models or meta models) that can fuse data generated from a variety of fidelity levels -- to include engineering methods, simulation, wind tunnel, and flight test data -- to make more accurate predictions. These models are able to move the bulk of the computation from high performance computing (HPC) to single user machines (laptop, desktop, etc.). The project builds upon previous work but introduces code improvements and an informed perspective on the direction of the field. The new surrogate modeling framework is now modular and, by design, broadly applicable to many modeling problems. The new framework also has a more robust automatic hyperparameter tuning capability and abstracts away most of the pre- and post-processing tasks. The Gaussian process regression and deep neural network-based models included in the presented framework were able to model two datasets with high accuracy (R^2>0.99). The primary conclusion is that the framework is effective and has been delivered to the Air Force for integration into real-world projects. For future work, significant and immediate investment in continued research is crucial. The author recommends further testing and refining modeling methods that explicitly incorporate physical laws and are robust enough to handle simulation and test data from varying resolutions and sources, including coarse meshes, fine meshes, unstructured meshes, and limited experimental test points.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03622",
        "abstract url": "https://arxiv.org/abs/2411.03622",
        "title": "Fully Hyperbolic Rotation for Knowledge Graph Embedding",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Hyperbolic rotation is commonly used to effectively model knowledge graphs and their inherent hierarchies. However, existing hyperbolic rotation models rely on logarithmic and exponential mappings for feature transformation. These models only project data features into hyperbolic space for rotation, limiting their ability to fully exploit the hyperbolic space. To address this problem, we propose a novel fully hyperbolic model designed for knowledge graph embedding. Instead of feature mappings, we define the model directly in hyperbolic space with the Lorentz model. Our model considers each relation in knowledge graphs as a Lorentz rotation from the head entity to the tail entity. We adopt the Lorentzian version distance as the scoring function for measuring the plausibility of triplets. Extensive results on standard knowledge graph completion benchmarks demonstrated that our model achieves competitive results with fewer parameters. In addition, our model get the state-of-the-art performance on datasets of CoDEx-s and CoDEx-m, which are more diverse and challenging than before. Our code is available at https://github.com/llqy123/FHRE.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted by ECAI 2024"
    },
    {
        "paper id": "2411.03624",
        "abstract url": "https://arxiv.org/abs/2411.03624",
        "title": "SEGMN: A Structure-Enhanced Graph Matching Network for Graph Similarity Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph similarity computation (GSC) aims to quantify the similarity score between two graphs. Although recent GSC methods based on graph neural networks (GNNs) take advantage of intra-graph structures in message passing, few of them fully utilize the structures presented by edges to boost the representation of their connected nodes. Moreover, previous cross-graph node embedding matching lacks the perception of the overall structure of the graph pair, due to the fact that the node representations from GNNs are confined to the intra-graph structure, causing the unreasonable similarity score. Intuitively, the cross-graph structure represented in the assignment graph is helpful to rectify the inappropriate matching. Therefore, we propose a structure-enhanced graph matching network (SEGMN). Equipped with a dual embedding learning module and a structure perception matching module, SEGMN achieves structure enhancement in both embedding learning and cross-graph matching. The dual embedding learning module incorporates adjacent edge representation into each node to achieve a structure-enhanced representation. The structure perception matching module achieves cross-graph structure enhancement through assignment graph convolution. The similarity score of each cross-graph node pair can be rectified by aggregating messages from structurally relevant node pairs. Experimental results on benchmark datasets demonstrate that SEGMN outperforms the state-of-the-art GSC methods in the GED regression task, and the structure perception matching module is plug-and-play, which can further improve the performance of the baselines by up to 25%.",
        "subjects": [
            "cs.LG",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02812",
        "abstract url": "https://arxiv.org/abs/2411.02812",
        "title": "NEOviz: Uncertainty-Driven Visual Analysis of Asteroid Trajectories",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "trajectory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We introduce NEOviz, an interactive visualization system designed to assist planetary defense experts in the visual analysis of the movements of near-Earth objects in the Solar System that might prove hazardous to Earth. Asteroids are often discovered using optical telescopes and their trajectories are calculated from images, resulting in an inherent asymmetric uncertainty in their position and velocity. Consequently, we typically cannot determine the exact trajectory of an asteroid, and an ensemble of trajectories must be generated to estimate an asteroid's movement over time. When propagating these ensembles over decades, it is challenging to visualize the varying paths and determine their potential impact on Earth, which could cause catastrophic damage. NEOviz equips experts with the necessary tools to effectively analyze the existing catalog of asteroid observations. In particular, we present a novel approach for visualizing the 3D uncertainty region through which an asteroid travels, while providing accurate spatial context in relation to system-critical infrastructure such as Earth, the Moon, and artificial satellites. Furthermore, we use NEOviz to visualize the divergence of asteroid trajectories, capturing high-variance events in an asteroid's orbital properties. For potential impactors, we combine the 3D visualization with an uncertainty-aware impact map to illustrate the potential risks to human populations. NEOviz was developed with continuous input from members of the planetary defense community through a participatory design process. It is exemplified in three real-world use cases and evaluated via expert feedback interviews.",
        "subjects": [
            "astro-ph.EP",
            "astro-ph.IM",
            "cs.CV",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02821",
        "abstract url": "https://arxiv.org/abs/2411.02821",
        "title": "Faster Exact and Parameterized Algorithm for Feedback Vertex Set in Bipartite Tournaments",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "A {\\em bipartite tournament} is a directed graph $T:=(A \\cup B, E)$ such that every pair of vertices $(a,b), a\\in A,b\\in B$ are connected by an arc, and no arc connects two vertices of $A$ or two vertices of $B$. A {\\em feedback vertex set} is a set $S$ of vertices in $T$ such that $T - S$ is acyclic. In this article we consider the {\\sc Feedback Vertex Set} problem in bipartite tournaments. Here the input is a bipartite tournament $T$ on $n$ vertices together with an integer $k$, and the task is to determine whether $T$ has a feedback vertex set of size at most $k$. We give a new algorithm for {\\sc Feedback Vertex Set in Bipartite Tournaments}. The running time of our algorithm is upper-bounded by $O(1.6181^k + n^{O(1)})$, improving over the previously best known algorithm with running time $2^kk^{O(1)} + n^{O(1)}$ [Hsiao, ISAAC 2011]. As a by-product, we also obtain the fastest currently known exact exponential-time algorithm for the problem, with running time $O(1.3820^n)$.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "36th IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science (FSTTCS 2016)"
    },
    {
        "paper id": "2411.02855",
        "abstract url": "https://arxiv.org/abs/2411.02855",
        "title": "Analyzing Poverty through Intra-Annual Time-Series: A Wavelet Transform Approach",
        "rating": "-1",
        "keywords": [
            [
                "Health"
            ],
            [
                "cs.LG",
                "cs.CY",
                "cs.CV"
            ]
        ],
        "abstract": "Reducing global poverty is a key objective of the Sustainable Development Goals (SDGs). Achieving this requires high-frequency, granular data to capture neighborhood-level changes, particularly in data scarce regions such as low- and middle-income countries. To fill in the data gaps, recent computer vision methods combining machine learning (ML) with earth observation (EO) data to improve poverty estimation. However, while much progress have been made, they often omit intra-annual variations, which are crucial for estimating poverty in agriculturally dependent countries. We explored the impact of integrating intra-annual NDVI information with annual multi-spectral data on model accuracy. To evaluate our method, we created a simulated dataset using Landsat imagery and nighttime light data to evaluate EO-ML methods that use intra-annual EO data. Additionally, we evaluated our method against the Demographic and Health Survey (DHS) dataset across Africa. Our results indicate that integrating specific NDVI-derived features with multi-spectral data provides valuable insights for poverty analysis, emphasizing the importance of retaining intra-annual information.",
        "subjects": [
            "cs.LG",
            "cs.CV",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02861",
        "abstract url": "https://arxiv.org/abs/2411.02861",
        "title": "Centerness-based Instance-aware Knowledge Distillation with Task-wise Mutual Lifting for Object Detection on Drone Imagery",
        "rating": "-1",
        "keywords": [
            [
                "Drone"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Developing accurate and efficient detectors for drone imagery is challenging due to the inherent complexity of aerial scenes. While some existing methods aim to achieve high accuracy by utilizing larger models, their computational cost is prohibitive for drones. Recently, Knowledge Distillation (KD) has shown promising potential for maintaining satisfactory accuracy while significantly compressing models in general object detection. Considering the advantages of KD, this paper presents the first attempt to adapt it to object detection on drone imagery and addresses two intrinsic issues: (1) low foreground-background ratio and (2) small instances and complex backgrounds, which lead to inadequate training, resulting insufficient distillation. Therefore, we propose a task-wise Lightweight Mutual Lifting (Light-ML) module with a Centerness-based Instance-aware Distillation (CID) strategy. The Light-ML module mutually harmonizes the classification and localization branches by channel shuffling and convolution, integrating teacher supervision across different tasks during back-propagation, thus facilitating training the student model. The CID strategy extracts valuable regions surrounding instances through the centerness of proposals, enhancing distillation efficacy. Experiments on the VisDrone, UAVDT, and COCO benchmarks demonstrate that the proposed approach promotes the accuracies of existing state-of-the-art KD methods with comparable computational requirements. Codes will be available upon acceptance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02863",
        "abstract url": "https://arxiv.org/abs/2411.02863",
        "title": "LoopSCC: Towards Summarizing Multi-branch Loops within Determinate Cycles",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Analyzing programs with loops is a challenging task, suffering from potential issues such as indeterminate number of iterations and exponential growth of control flow complexity. Loop summarization, as a static analysis method for concrete semantic interpretation, receives increasing focuses. It produces symbolic expressions semantically equivalent to the loop program. However, current loop summarization methods are only suitable for single-branch loops or multi-branch loops with simple cycles, without supporting complex loops with irregular branch-to-branch transitions. In this paper, we proposed LoopSCC, a novel loop summarization technique, to achieve concrete semantic interpretation on complex loop. LoopSCC analyzes the control flow at the granularity of single-loop-path and applies the strongly connected components (SCC for short) for contraction and simplification, resulting in the contracted single-loop-path graph (CSG for short). Based on the control flow information provided by the CSG, we can convert the loop summary into a combination of SCC summaries. When an SCC contains irregular branch-to-branch transitions, we propose to explore a convergent range to identify the determinate cycles of different execution paths, referred as oscillatory interval. The loop summarization composed of both iteration conditions and execution operations can eventually be derived recursively. Extensive experiments compared to six state-of-the-art loop interpretation methods are conducted to evaluate the effectiveness of LoopSCC. From the results, LoopSCC outperforms comparative methods in both interpretation accuracy and application effectiveness. Especially, LoopSCC achieves a 100% interpretation accuracy on public common-used benchmark. A systematical study for loop properties on three large-scale programs illustrates that LoopSCC presents outstanding scalability for real-world loop programs.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02867",
        "abstract url": "https://arxiv.org/abs/2411.02867",
        "title": "AtlasSeg: Atlas Prior Guided Dual-U-Net for Cortical Segmentation in Fetal Brain MRI",
        "rating": "-1",
        "keywords": [
            [
                "MRI"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Accurate tissue segmentation in fetal brain MRI remains challenging due to the dynamically changing anatomical anatomy and contrast during fetal development. To enhance segmentation accuracy throughout gestation, we introduced AtlasSeg, a dual-U-shape convolution network incorporating gestational age (GA) specific information as guidance. By providing a publicly available fetal brain atlas with segmentation label at the corresponding GA, AtlasSeg effectively extracted the contextual features of age-specific patterns in atlas branch and generated tissue segmentation in segmentation branch. Multi-scale attentive atlas feature fusions were constructed in all stages during encoding and decoding, giving rise to a dual-U-shape network to assist feature flow and information interactions between two branches. AtlasSeg outperformed six well-known segmentation networks in both our internal fetal brain MRI dataset and the external FeTA dataset. Ablation experiments demonstrate the efficiency of atlas guidance and the attention mechanism. The proposed AtlasSeg demonstrated superior segmentation performance against other convolution networks with higher segmentation accuracy, and may facilitate fetal brain MRI analysis in large-scale fetal brain studies.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02882",
        "abstract url": "https://arxiv.org/abs/2411.02882",
        "title": "Approximation Algorithms for the Freeze Tag Problem inside Polygons",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "The freeze tag problem (FTP) aims to awaken a swarm of robots with one or more initial awake robots as soon as possible. Each awake robot must touch a sleeping robot to wake it up. Once a robot is awakened, it can assist in awakening other sleeping robots. We study this problem inside a polygonal domain and present approximation algorithms for it.",
        "subjects": [
            "cs.CG"
        ],
        "comment": "7 pages, 3 figures"
    },
    {
        "paper id": "2411.02900",
        "abstract url": "https://arxiv.org/abs/2411.02900",
        "title": "Distributed Graph Neural Network Design for Sum Ergodic Spectral Efficiency Maximization in Cell-Free Massive MIMO",
        "rating": "-1",
        "keywords": [
            [
                "GNN",
                "Graph"
            ]
        ],
        "abstract": "This paper proposes a distributed learning-based framework to tackle the sum ergodic rate maximization problem in cell-free massive multiple-input multiple-output (MIMO) systems by utilizing the graph neural network (GNN). Different from centralized schemes, which gather all the channel state information (CSI) at the central processing unit (CPU) for calculating the resource allocation, the local resource of access points (APs) is exploited in the proposed distributed GNN-based framework to allocate transmit powers. Specifically, APs can use a unique GNN model to allocate their power based on the local CSI. The GNN model is trained at the CPU using the local CSI of one AP, with partially exchanged information from other APs to calculate the loss function to reflect system characteristics, capturing comprehensive network information while avoiding computation burden. Numerical results show that the proposed distributed learning-based approach achieves a sum ergodic rate close to that of centralized learning while outperforming the model-based optimization.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "6 pages, 4 figures, and 4 tables. Accepted by IEEE TVT"
    },
    {
        "paper id": "2411.02935",
        "abstract url": "https://arxiv.org/abs/2411.02935",
        "title": "Mapping Africa Settlements: High Resolution Urban and Rural Map by Deep Learning and Satellite Imagery",
        "rating": "-1",
        "keywords": [
            [
                "Satellite"
            ],
            [
                "cs.LG",
                "cs.CY",
                "cs.CV"
            ]
        ],
        "abstract": "Accurate Land Use and Land Cover (LULC) maps are essential for understanding the drivers of sustainable development, in terms of its complex interrelationships between human activities and natural resources. However, existing LULC maps often lack precise urban and rural classifications, particularly in diverse regions like Africa. This study presents a novel construction of a high-resolution rural-urban map using deep learning techniques and satellite imagery. We developed a deep learning model based on the DeepLabV3 architecture, which was trained on satellite imagery from Landsat-8 and the ESRI LULC dataset, augmented with human settlement data from the GHS-SMOD. The model utilizes semantic segmentation to classify land into detailed categories, including urban and rural areas, at a 10-meter resolution. Our findings demonstrate that incorporating LULC along with urban and rural classifications significantly enhances the model's ability to accurately distinguish between urban, rural, and non-human settlement areas. Therefore, our maps can support more informed decision-making for policymakers, researchers, and stakeholders. We release a continent wide urban-rural map, covering the period 2016 and 2022.",
        "subjects": [
            "cs.CV",
            "cs.CY",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02948",
        "abstract url": "https://arxiv.org/abs/2411.02948",
        "title": "Grounding Natural Language to SQL Translation with Data-Based Self-Explanations",
        "rating": "-1",
        "keywords": [
            [
                "SQL"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Natural Language Interfaces for Databases empower non-technical users to interact with data using natural language (NL). Advanced approaches, utilizing either neural sequence-to-sequence or more recent sophisticated large-scale language models, typically implement NL to SQL (NL2SQL) translation in an end-to-end fashion. However, like humans, these end-to-end translation models may not always generate the best SQL output on their first try. In this paper, we propose CycleSQL, an iterative framework designed for end-to-end translation models to autonomously generate the best output through self-evaluation. The main idea of CycleSQL is to introduce data-grounded NL explanations of query results as self-provided feedback, and use the feedback to validate the correctness of the translation iteratively, hence improving the overall translation accuracy. Extensive experiments, including quantitative and qualitative evaluations, are conducted to study CycleSQL by applying it to seven existing translation models on five widely used benchmarks. The results show that 1) the feedback loop introduced in CycleSQL can consistently improve the performance of existing models, and in particular, by applying CycleSQL to RESDSQL, obtains a translation accuracy of 82.0% (+2.6%) on the validation set, and 81.6% (+3.2%) on the test set of Spider benchmark; 2) the generated NL explanations can also provide insightful information for users, aiding in the comprehension of translation results and consequently enhancing the interpretability of NL2SQL translation.",
        "subjects": [
            "cs.DB",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02949",
        "abstract url": "https://arxiv.org/abs/2411.02949",
        "title": "A scalable generative model for dynamical system reconstruction from neuroimaging data",
        "rating": "-1",
        "keywords": [
            [
                "biophysical",
                "fMRI"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Data-driven inference of the generative dynamics underlying a set of observed time series is of growing interest in machine learning and the natural sciences. In neuroscience, such methods promise to alleviate the need to handcraft models based on biophysical principles and allow to automatize the inference of inter-individual differences in brain dynamics. Recent breakthroughs in training techniques for state space models (SSMs) specifically geared toward dynamical systems (DS) reconstruction (DSR) enable to recover the underlying system including its geometrical (attractor) and long-term statistical invariants from even short time series. These techniques are based on control-theoretic ideas, like modern variants of teacher forcing (TF), to ensure stable loss gradient propagation while training. However, as it currently stands, these techniques are not directly applicable to data modalities where current observations depend on an entire history of previous states due to a signal's filtering properties, as common in neuroscience (and physiology more generally). Prominent examples are the blood oxygenation level dependent (BOLD) signal in functional magnetic resonance imaging (fMRI) or Ca$^{2+}$ imaging data. Such types of signals render the SSM's decoder model non-invertible, a requirement for previous TF-based methods. Here, exploiting the recent success of control techniques for training SSMs, we propose a novel algorithm that solves this problem and scales exceptionally well with model dimensionality and filter length. We demonstrate its efficiency in reconstructing dynamical systems, including their state space geometry and long-term temporal properties, from just short BOLD time series.",
        "subjects": [
            "cs.LG",
            "math.DS",
            "nlin.CD",
            "physics.data-an"
        ],
        "comment": "38th Conference on Neural Information Processing Systems (NeurIPS 2024)"
    },
    {
        "paper id": "2411.02969",
        "abstract url": "https://arxiv.org/abs/2411.02969",
        "title": "Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "voxel",
                "NeRF"
            ],
            [
                "autonomous driving",
                "LiDAR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "LiDAR Semantic Segmentation is a fundamental task in autonomous driving perception consisting of associating each LiDAR point to a semantic label. Fully-supervised models have widely tackled this task, but they require labels for each scan, which either limits their domain or requires impractical amounts of expensive annotations. Camera images, which are generally recorded alongside LiDAR pointclouds, can be processed by the widely available 2D foundation models, which are generic and dataset-agnostic. However, distilling knowledge from 2D data to improve LiDAR perception raises domain adaptation challenges. For example, the classical perspective projection suffers from the parallax effect produced by the position shift between both sensors at their respective capture times. We propose a Semi-Supervised Learning setup to leverage unlabeled LiDAR pointclouds alongside distilled knowledge from the camera images. To self-supervise our model on the unlabeled scans, we add an auxiliary NeRF head and cast rays from the camera viewpoint over the unlabeled voxel features. The NeRF head predicts densities and semantic logits at each sampled ray location which are used for rendering pixel semantics. Concurrently, we query the Segment-Anything (SAM) foundation model with the camera image to generate a set of unlabeled generic masks. We fuse the masks with the rendered pixel semantics from LiDAR to produce pseudo-labels that supervise the pixel predictions. During inference, we drop the NeRF head and run our model with only LiDAR. We show the effectiveness of our approach in three public LiDAR Semantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2024"
    },
    {
        "paper id": "2411.02973",
        "abstract url": "https://arxiv.org/abs/2411.02973",
        "title": "[Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for Diabetic Retinopathy using Chatbots and Generative AI",
        "rating": "-1",
        "keywords": [
            [
                "healthcare",
                "clinical"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "We present an outline of the first large language model (LLM) based chatbot application in the context of patient-reported outcome measures (PROMs) for diabetic retinopathy. By utilizing the capabilities of current LLMs, we enable patients to provide feedback about their quality of life and treatment progress via an interactive application. The proposed framework offers significant advantages over the current approach, which encompasses only qualitative collection of survey data or a static survey with limited answer options. Using the PROBot LLM-PROM application, patients will be asked tailored questions about their individual challenges, and can give more detailed feedback on the progress of their treatment. Based on this input, we will use machine learning to infer conventional PROM scores, which can be used by clinicians to evaluate the treatment status. The goal of the application is to improve adherence to the healthcare system and treatments, and thus ultimately reduce cases of subsequent vision impairment. The approach needs to be further validated using a survey and a clinical study.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02997",
        "abstract url": "https://arxiv.org/abs/2411.02997",
        "title": "PV-faultNet: Optimized CNN Architecture to detect defects resulting efficient PV production",
        "rating": "-1",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "The global shift towards renewable energy has pushed PV cell manufacturing as a pivotal point as they are the fundamental building block of green energy. However, the manufacturing process is complex enough to lose its purpose due to probable defects experienced during the time impacting the overall efficiency. However, at the moment, manual inspection is being conducted to detect the defects that can cause bias, leading to time and cost inefficiency. Even if automated solutions have also been proposed, most of them are resource-intensive, proving ineffective in production environments. In that context, this study presents PV-faultNet, a lightweight Convolutional Neural Network (CNN) architecture optimized for efficient and real-time defect detection in photovoltaic (PV) cells, designed to be deployable on resource-limited production devices. Addressing computational challenges in industrial PV manufacturing environments, the model includes only 2.92 million parameters, significantly reducing processing demands without sacrificing accuracy. Comprehensive data augmentation techniques were implemented to tackle data scarcity, thus enhancing model generalization and maintaining a balance between precision and recall. The proposed model achieved high performance with 91\\% precision, 89\\% recall, and a 90\\% F1 score, demonstrating its effectiveness for scalable quality control in PV production.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03003",
        "abstract url": "https://arxiv.org/abs/2411.03003",
        "title": "Fractional Chromatic Numbers from Exact Decision Diagrams",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Recently, Van Hoeve proposed an algorithm for graph coloring based on an integer flow formulation on decision diagrams for stable sets. We prove that the solution to the linear flow relaxation on exact decision diagrams determines the fractional chromatic number of a graph. This settles the question whether the decision diagram formulation or the fractional chromatic number establishes a stronger lower bound. It also establishes that the integrality gap of the linear programming relaxation is O(log n), where n represents the number of vertices in the graph. We also conduct experiments using exact decision diagrams and could determine the chromatic number of r1000.1c from the DIMACS benchmark set. It was previously unknown and is one of the few newly solved DIMACS instances in the last 10 years.",
        "subjects": [
            "math.CO",
            "cs.DM"
        ],
        "comment": "11 pages, 1 Figure, 1 Table, source code published at https://github.com/trewes/ddcolors, Scripts, logfiles, results and instructions for the experiments are archived at https://bonndata.uni-bonn.de/dataset.xhtml?persistentId=doi:10.60507/FK2/ZE9C3L"
    },
    {
        "paper id": "2411.03004",
        "abstract url": "https://arxiv.org/abs/2411.03004",
        "title": "Controlling for Unobserved Confounding with Large Language Model Classification of Patient Smoking Status",
        "rating": "-1",
        "keywords": [
            [
                "clinical"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Causal understanding is a fundamental goal of evidence-based medicine. When randomization is impossible, causal inference methods allow the estimation of treatment effects from retrospective analysis of observational data. However, such analyses rely on a number of assumptions, often including that of no unobserved confounding. In many practical settings, this assumption is violated when important variables are not explicitly measured in the clinical record. Prior work has proposed to address unobserved confounding with machine learning by imputing unobserved variables and then correcting for the classifier's mismeasurement. When such a classifier can be trained and the necessary assumptions are met, this method can recover an unbiased estimate of a causal effect. However, such work has been limited to synthetic data, simple classifiers, and binary variables. This paper extends this methodology by using a large language model trained on clinical notes to predict patients' smoking status, which would otherwise be an unobserved confounder. We then apply a measurement error correction on the categorical predicted smoking status to estimate the causal effect of transthoracic echocardiography on mortality in the MIMIC dataset.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Advancements In Medical Foundation Models: Explainability, Robustness, Security, and Beyond (AIM-FM) at NeurIPS 2024"
    },
    {
        "paper id": "2411.03010",
        "abstract url": "https://arxiv.org/abs/2411.03010",
        "title": "Learning-based Lossless Event Data Compression",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "event cameras"
            ]
        ],
        "abstract": "Emerging event cameras acquire visual information by detecting time domain brightness changes asynchronously at the pixel level and, unlike conventional cameras, are able to provide high temporal resolution, very high dynamic range, low latency, and low power consumption. Considering the huge amount of data involved, efficient compression solutions are very much needed. In this context, this paper presents a novel deep-learning-based lossless event data compression scheme based on octree partitioning and a learned hyperprior model. The proposed method arranges the event stream as a 3D volume and employs an octree structure for adaptive partitioning. A deep neural network-based entropy model, using a hyperprior, is then applied. Experimental results demonstrate that the proposed method outperforms traditional lossless data compression techniques in terms of compression ratio and bits per event.",
        "subjects": [
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03012",
        "abstract url": "https://arxiv.org/abs/2411.03012",
        "title": "Leveraging Large Language Models in Code Question Answering: Baselines and Issues",
        "rating": "-1",
        "keywords": [
            [
                "grammar"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Question answering over source code provides software engineers and project managers with helpful information about the implemented features of a software product. This paper presents a work devoted to using large language models for question answering over source code in Python. The proposed method for implementing a source code question answering system involves fine-tuning a large language model on a unified dataset of questions and answers for Python code. To achieve the highest quality answers, we tested various models trained on datasets preprocessed in different ways: a dataset without grammar correction, a dataset with grammar correction, and a dataset augmented with the generated summaries. The model answers were also analyzed for errors manually. We report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along with the conclusions from the manual error analysis. The obtained experimental results highlight the current problems of the research area, such as poor quality of the public genuine question-answering datasets. In addition, the findings include the positive effect of the grammar correction of the training data on the testing metric values. The addressed findings and issues could be important for other researchers who attempt to improve the quality of source code question answering solutions. The training and evaluation code is publicly available at https://github.com/IU-AES-AI4Code/CodeQuestionAnswering.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "15 pages, 3 figures, Accepted to NLP (CCIS) @ AIST'24"
    },
    {
        "paper id": "2411.03017",
        "abstract url": "https://arxiv.org/abs/2411.03017",
        "title": "Rozproszone Wykrywanie Zaj\u0119to\u015bci Widma Oparte na Uczeniu Federacyjnym",
        "rating": "-1",
        "keywords": [
            [
                "federated learning"
            ]
        ],
        "abstract": "Spectrum occupancy detection is a key enabler for dynamic spectrum access, where machine learning algorithms are successfully utilized for detection improvement. However, the main challenge is limited access to labeled data about users transmission presence needed in supervised learning models. We present a distributed federated learning approach that addresses this challenge for sensors without access to learning data. The paper discusses the results of the conducted hardware experiment, where FL has been applied for DVB-T signal detection.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "4 pages, in Polish language, 10 figures"
    },
    {
        "paper id": "2411.03019",
        "abstract url": "https://arxiv.org/abs/2411.03019",
        "title": "FEDLAD: Federated Evaluation of Deep Leakage Attacks and Defenses",
        "rating": "-1",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Attacks"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Federated Learning is a privacy preserving decentralized machine learning paradigm designed to collaboratively train models across multiple clients by exchanging gradients to the server and keeping private data local. Nevertheless, recent research has revealed that the security of Federated Learning is compromised, as private ground truth data can be recovered through a gradient inversion technique known as Deep Leakage. While these attacks are crafted with a focus on applications in Federated Learning, they generally are not evaluated in realistic scenarios. This paper introduces the FEDLAD Framework (Federated Evaluation of Deep Leakage Attacks and Defenses), a comprehensive benchmark for evaluating Deep Leakage attacks and defenses within a realistic Federated context. By implementing a unified benchmark that encompasses multiple state-of-the-art Deep Leakage techniques and various defense strategies, our framework facilitates the evaluation and comparison of the efficacy of these methods across different datasets and training states. This work highlights a crucial trade-off between privacy and model accuracy in Federated Learning and aims to advance the understanding of security challenges in decentralized machine learning systems, stimulate future research, and enhance reproducibility in evaluating Deep Leakage attacks and defenses.",
        "subjects": [
            "cs.CR",
            "cs.CV"
        ],
        "comment": "9 pages"
    },
    {
        "paper id": "2411.03038",
        "abstract url": "https://arxiv.org/abs/2411.03038",
        "title": "Can Transformers Smell Like Humans?",
        "rating": "-1",
        "keywords": [
            [
                "chemical"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "The human brain encodes stimuli from the environment into representations that form a sensory perception of the world. Despite recent advances in understanding visual and auditory perception, olfactory perception remains an under-explored topic in the machine learning community due to the lack of large-scale datasets annotated with labels of human olfactory perception. In this work, we ask the question of whether pre-trained transformer models of chemical structures encode representations that are aligned with human olfactory perception, i.e., can transformers smell like humans? We demonstrate that representations encoded from transformers pre-trained on general chemical structures are highly aligned with human olfactory perception. We use multiple datasets and different types of perceptual representations to show that the representations encoded by transformer models are able to predict: (i) labels associated with odorants provided by experts; (ii) continuous ratings provided by human participants with respect to pre-defined descriptors; and (iii) similarity ratings between odorants provided by human participants. Finally, we evaluate the extent to which this alignment is associated with physicochemical features of odorants known to be relevant for olfactory decoding.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Spotlight paper at NeurIPS 2024"
    },
    {
        "paper id": "2411.03041",
        "abstract url": "https://arxiv.org/abs/2411.03041",
        "title": "Judge Like a Real Doctor: Dual Teacher Sample Consistency Framework for Semi-supervised Medical Image Classification",
        "rating": "-1",
        "keywords": [
            [
                "Medical",
                "diagnosing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Semi-supervised learning (SSL) is a popular solution to alleviate the high annotation cost in medical image classification. As a main branch of SSL, consistency regularization engages in imposing consensus between the predictions of a single sample from different views, termed as Absolute Location consistency (AL-c). However, only AL-c may be insufficient. Just like when diagnosing a case in practice, besides the case itself, the doctor usually refers to certain related trustworthy cases to make more reliable decisions.Therefore, we argue that solely relying on AL-c may ignore the relative differences across samples, which we interpret as relative locations, and only exploit limited information from one perspective. To address this issue, we propose a Sample Consistency Mean Teacher (SCMT) which not only incorporates AL c but also additionally enforces consistency between the samples' relative similarities to its related samples, called Relative Location consistency (RL c). AL c and RL c conduct consistency regularization from two different perspectives, jointly extracting more diverse semantic information for classification. On the other hand, due to the highly similar structures in medical images, the sample distribution could be overly dense in feature space, making their relative locations susceptible to noise. To tackle this problem, we further develop a Sample Scatter Mean Teacher (SSMT) by utilizing contrastive learning to sparsify the sample distribution and obtain robust and effective relative locations. Extensive experiments on different datasets demonstrate the superiority of our method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by IEEE Transactions on Emerging Topics in Computational Intelligence"
    },
    {
        "paper id": "2411.03047",
        "abstract url": "https://arxiv.org/abs/2411.03047",
        "title": "GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Neural implicit functions have brought impressive advances to the state-of-the-art of clothed human digitization from multiple or even single images. However, despite the progress, current arts still have difficulty generalizing to unseen images with complex cloth deformation and body poses. In this work, we present GarVerseLOD, a new dataset and framework that paves the way to achieving unprecedented robustness in high-fidelity 3D garment reconstruction from a single unconstrained image. Inspired by the recent success of large generative models, we believe that one key to addressing the generalization challenge lies in the quantity and quality of 3D garment data. Towards this end, GarVerseLOD collects 6,000 high-quality cloth models with fine-grained geometry details manually created by professional artists. In addition to the scale of training data, we observe that having disentangled granularities of geometry can play an important role in boosting the generalization capability and inference accuracy of the learned model. We hence craft GarVerseLOD as a hierarchical dataset with levels of details (LOD), spanning from detail-free stylized shape to pose-blended garment with pixel-aligned details. This allows us to make this highly under-constrained problem tractable by factorizing the inference into easier tasks, each narrowed down with smaller searching space. To ensure GarVerseLOD can generalize well to in-the-wild images, we propose a novel labeling paradigm based on conditional diffusion models to generate extensive paired images for each garment model with high photorealism. We evaluate our method on a massive amount of in-the-wild images. Experimental results demonstrate that GarVerseLOD can generate standalone garment pieces with significantly better quality than prior approaches. Project page: https://garverselod.github.io/",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "Project page: https://garverselod.github.io/"
    },
    {
        "paper id": "2411.03061",
        "abstract url": "https://arxiv.org/abs/2411.03061",
        "title": "Unsupervised detection and classification of heartbeats using the dissimilarity matrix in PCG signals",
        "rating": "-1",
        "keywords": [
            [
                "clinical",
                "cardiac"
            ],
            [
                "eess.AS"
            ]
        ],
        "abstract": "The proposed system consists of a two-stage cascade. The first stage performs a rough heartbeat detection while the second stage refines the previous one, improving the temporal localization and also classifying the heartbeats into types S1 and S2. The first contribution is a novel approach that combines the dissimilarity matrix with the frame-level spectral divergence to locate heartbeats using the repetitiveness shown by the heart sounds and the temporal relationships between the intervals defined by the events S1/S2 and non-S1/S2 (systole and diastole). The second contribution is a verification-correction-classification process based on a sliding window that allows the preservation of the temporal structure of the cardiac cycle in order to be applied in the heart sound classification. The proposed method has been assessed using the open access databases PASCAL, CirCor DigiScope Phonocardiogram and an additional sound mixing procedure considering both Additive White Gaussian Noise (AWGN) and different kinds of clinical ambient noises from a commercial database. The proposed method provides the best detection/classification performance in realistic scenarios where the presence of cardiac anomalies as well as different types of clinical environmental noises are active in the PCG signal. Of note, the promising modelling of the temporal structures of the heart provided by the dissimilarity matrix together with the frame-level spectral divergence, as well as the removal of a significant number of spurious heart events and recovery of missing heart events, both corrected by the proposed verification-correction-classification algorithm, suggest that our proposal is a successful tool to be applied in heart segmentation.",
        "subjects": [
            "eess.AS",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03064",
        "abstract url": "https://arxiv.org/abs/2411.03064",
        "title": "Exploiting the Segment Anything Model (SAM) for Lung Segmentation in Chest X-ray Images",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "healthcare",
                "X-ray"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Segment Anything Model (SAM), a new AI model from Meta AI released in April 2023, is an ambitious tool designed to identify and separate individual objects within a given image through semantic interpretation. The advanced capabilities of SAM are the result of its training with millions of images and masks, and a few days after its release, several researchers began testing the model on medical images to evaluate its performance in this domain. With this perspective in focus -- i.e., optimizing work in the healthcare field -- this work proposes the use of this new technology to evaluate and study chest X-ray images. The approach adopted for this work, with the aim of improving the model's performance for lung segmentation, involved a transfer learning process, specifically the fine-tuning technique. After applying this adjustment, a substantial improvement was observed in the evaluation metrics used to assess SAM's performance compared to the masks provided by the datasets. The results obtained by the model after the adjustments were satisfactory and similar to cutting-edge neural networks, such as U-Net.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03079",
        "abstract url": "https://arxiv.org/abs/2411.03079",
        "title": "Utilizing Precise and Complete Code Context to Guide LLM in Automatic False Positive Mitigation",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Static Application Security Testing(SAST) tools are crucial for early bug detection and code quality but often generate false positives that slow development. Automating false positive mitigation is thus essential for advancing SAST tools. Past efforts use static/dynamic analysis or machine learning. The advent of Large Language Models, adept at understanding natural language and code, offers promising ways to improve the accuracy and usability of SAST tools. However, existing LLM-based methods need improvement in two key areas: first, extracted code snippets related to warnings are often cluttered with irrelevant control and data flows, reducing precision; second, critical code contexts are often missing, leading to incomplete representations that can mislead LLMs and cause inaccurate assessments. To ensure the use of precise and complete code context, thereby avoiding misguidance and enabling LLMs to reach accurate conclusions, we propose LLM4FPM. One of its core components is eCPG-Slicer, which builds an extended code property graph and extracts line-level, precise code context. Moreover, LLM4FPM incorporates FARF algorithm, which builds a file reference graph and then efficiently detects all files related to a warning in linear time, enabling eCPG-Slicer to gather complete code context across these files. We evaluate LLM4FPM on Juliet dataset, where it comprehensively outperforms the baseline, achieving an F1 score above 99% across various CWEs. LLM4FPM leverages a free, open-source model, avoiding costly alternatives and reducing inspection costs by up to $2758 per run on Juliet, with an average inspection time of 4.7 seconds per warning. Our work emphasizes the critical impact of precise and complete code context and highlights the potential of combining program analysis with LLMs, improving the quality and efficiency of software development.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "21 pages"
    },
    {
        "paper id": "2411.03113",
        "abstract url": "https://arxiv.org/abs/2411.03113",
        "title": "Minimum Radiative Heat and Propellant Aerocapture Guidance with Attitude Kinematics Constraints",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "To maximize the payload mass, an aerocapture trajectory should be flown in such a way that both the final \u0394V and the total heat load are minimized. For some aerocapture missions, the heating due to radiation of high temperature gases in the shock-layer is so much larger than the heat due to convection, that the latter is negligible. This paper provides analytical proof and numerical validation that radiative heat is minimized by the same trajectory that minimizes the final \u0394V: a single switch bang-bang trajectory, starting with full lift-up, full lift-down commands. Further, a novel guidance that plans a bang-bang trajectory with constraints in the attitude kinematics is introduced. While achieving similar performance as the current state-of-the-art, the inclusion of constraints in attitude kinematics allows for much less tuning. Finally, a lateral guidance that makes use of information on the final inclination of the predicted trajectory is introduced. Such guidance allows for very high accuracy in the inclination requirements with only two reversals, by requiring a single parameter to be tuned.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Submitted to the Journal of Guidance, Control, and Dynamics"
    },
    {
        "paper id": "2411.03129",
        "abstract url": "https://arxiv.org/abs/2411.03129",
        "title": "MA^2: A Self-Supervised and Motion Augmenting Autoencoder for Gait-Based Automatic Disease Detection",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "diagnosis",
                "Disease",
                "pathological"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Ground reaction force (GRF) is the force exerted by the ground on a body in contact with it. GRF-based automatic disease detection (ADD) has become an emerging medical diagnosis method, which aims to learn and identify disease patterns corresponding to different gait pressures based on deep learning methods. Although existing ADD methods can save doctors time in making diagnoses, training deep models still struggles with the cost caused by the labeling engineering for a large number of gait diagnostic data for subjects. On the other hand, the accuracy of the deep model under the unified benchmark GRF dataset and the generalization ability on scalable gait datasets need to be further improved. To address these issues, we propose MA2, a GRF-based self-supervised and motion augmenting auto-encoder, which models the ADD task as an encoder-decoder paradigm. In the encoder, we introduce an embedding block including the 3-layer 1D convolution for extracting the token and a mask generator to randomly mask out the sequence of tokens to maximize the model's potential to capture high-level, discriminative, intrinsic representations. whereafter, the decoder utilizes this information to reconstruct the pixel sequence of the origin input and calculate the reconstruction loss to optimize the network. Moreover, the backbone of an auto-encoder is multi-head self-attention that can consider the global information of the token from the input, not just the local neighborhood. This allows the model to capture generalized contextual information. Extensive experiments demonstrate MA2 has SOTA performance of 90.91% accuracy on 1% limited pathological GRF samples with labels, and good generalization ability of 78.57% accuracy on scalable Parkinson disease dataset.",
        "subjects": [
            "physics.bio-ph",
            "cs.CV"
        ],
        "comment": "8 pages, 11 figures, article"
    },
    {
        "paper id": "2411.03133",
        "abstract url": "https://arxiv.org/abs/2411.03133",
        "title": "Reconstructing edge-deleted unicyclic graphs",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "The Harary reconstruction conjecture states that any graph with more than four edges can be uniquely reconstructed from its set of maximal edge-deleted subgraphs. In 1977, M\u00fcller verified the conjecture for graphs with $n$ vertices and $n \\log_2(n)$ edges, improving on Lov\u00e1s's bound of $\\log(n^2-n)/4$. Here, we show that the reconstruction conjecture holds for graphs which have exactly one cycle and and three non-isomorphic subtrees.",
        "subjects": [
            "math.CO",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03191",
        "abstract url": "https://arxiv.org/abs/2411.03191",
        "title": "Newtonized Orthogonal Matching Pursuit for High-Resolution Target Detection in Sparse OFDM ISAC Systems",
        "rating": "-1",
        "keywords": [
            [
                "radar",
                "vehicle"
            ]
        ],
        "abstract": "Integrated Sensing and Communication (ISAC) is a technology paradigm that combines sensing capabilities with communication functionalities in a single device or system. In vehicle-to-everything (V2X) sidelink, ISAC can provide enhanced safety by allowing vehicles to not only communicate with one another but also sense the surrounding environment by using sidelink signals. In ISAC-capable V2X sidelink, the random resource allocation results in an unstructured and sparse distribution of time and frequency resources in the received orthogonal frequency division multiplexing (OFDM) grid, leading to degraded radar detection performance when processed using the conventional 2D-FFT method. To address this challenge, this paper proposes a high-resolution off-grid radar target detection algorithm irrespective of the OFDM grid structure. The proposed method utilizes the Newtonized orthogonal matching pursuit (NOMP) algorithm to effectively detect weak targets masked by the sidelobes of stronger ones and accurately estimates off-grid range and velocity parameters with minimal resources through Newton refinements. Simulation results demonstrate the superior performance of the proposed NOMP-based target detection algorithm compared to existing compressed sensing (CS) methods in terms of detection probability, resolution, and accuracy. Additionally, experimental validation is performed using a bi-static radar setup in a semi-anechoic chamber. The measurement results validate the simulation findings, showing that the proposed algorithm significantly enhances target detection and parameter estimation accuracy in realistic scenarios.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03213",
        "abstract url": "https://arxiv.org/abs/2411.03213",
        "title": "What Makes an Educational Robot Game Fun? Framework Analysis of Children's Design Ideas",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "Fun acts as a catalyst for learning by enhancing motivation, active engagement and knowledge retention. As social robots gain traction as educational tools, understanding how their unique affordances can be leveraged to cultivate fun becomes crucial. This research investigates the concept of fun in educational games involving social robots to support the design of REMind:a robot-mediated role-play game aimed at encouraging bystander intervention against peer bullying among children. To incorporate fun elements into design of REMind, we conducted a user-centered Research through Design (RtD) study with focus groups of children to gain a deeper understanding of their perceptions of fun. We analyzed children's ideas by using Framework Analysis and leveraging LeBlanc's Taxonomy of Game Pleasures and identified 28 elements of fun that can be incorporated into robot-mediated games. We present our observations, discuss their impact on REMind's design, and offer recommendations for designing fun educational games using social robots.",
        "subjects": [
            "cs.HC",
            "cs.RO"
        ],
        "comment": "This is a pre-print of a manuscript that was accepted to International Conference on Social Robotics 2024 (ICSR'24 + AI), 2024, which was held in Odense, Denmark"
    },
    {
        "paper id": "2411.03225",
        "abstract url": "https://arxiv.org/abs/2411.03225",
        "title": "Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities of Neurosymbolic AI",
        "rating": "-1",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "Graphs"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "In the era of Generative AI, Neurosymbolic AI is emerging as a powerful approach for tasks spanning from perception to cognition. The use of Neurosymbolic AI has been shown to achieve enhanced capabilities, including improved grounding, alignment, explainability, and reliability. However, due to its nascent stage, there is a lack of widely available real-world benchmark datasets tailored to Neurosymbolic AI tasks. To address this gap and support the evaluation of current and future methods, we introduce DSceneKG -- a suite of knowledge graphs of driving scenes built from real-world, high-quality scenes from multiple open autonomous driving datasets. In this article, we detail the construction process of DSceneKG and highlight its application in seven different tasks. DSceneKG is publicly accessible at: https://github.com/ruwantw/DSceneKG",
        "subjects": [
            "cs.AI",
            "cs.CV"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2411.03230",
        "abstract url": "https://arxiv.org/abs/2411.03230",
        "title": "Fermionic Independent Set and Laplacian of an independence complex are QMA-hard",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "The Independent Set is a well known NP-hard optimization problem. In this work, we define a fermionic generalization of the Independent Set problem and prove that the optimization problem is QMA-hard in a $k$-particle subspace using perturbative gadgets. We discuss how the Fermionic Independent Set is related to the problem of computing the minimum eigenvalue of the $k^{\\text{th}}$-Laplacian of an independence complex of a vertex weighted graph. Consequently, we use the same perturbative gadget to prove QMA-hardness of the later problem resolving an open conjecture from arXiv:2311.17234 and give the first example of a natural topological data analysis problem that is QMA-hard.",
        "subjects": [
            "quant-ph",
            "cs.CC"
        ],
        "comment": "14 pages"
    },
    {
        "paper id": "2411.03271",
        "abstract url": "https://arxiv.org/abs/2411.03271",
        "title": "A Traffic Prediction-Based Individualized Driver Warning System to Reduce Red Light Violations",
        "rating": "-1",
        "keywords": [
            [
                "vehicle"
            ]
        ],
        "abstract": "Red light violation is a major cause of traffic collisions and resulting injuries and fatalities. Despite extensive prior work to reduce red light violations, they continue to be a major problem in practice, partly because existing systems suffer from the flaw of providing the same guidance to all drivers. As a result, some violations are avoided, but other drivers ignore or respond inappropriately to red light running systems, resulting in safety issues overall. We show a method of providing accurate warnings to individual drivers to avoid the broad guidance approach of most existing systems. Recognizing if a driver will run red lights is highly dependent on signal phase and timing, traffic conditions along the road, and individual driver behaviour, the proposed warning system contains three parts: a traffic prediction algorithm, an individual warning signal optimizer, and a driver warning display. The traffic prediction algorithm predicts future traffic states along the road towards the signalized intersections using the latest traffic conditions obtained through vehicle-to-vehicle and vehicle-to-infrastructure communications. Then, an optimization problem is formulated to compute the optimal warning signal based on predicted traffic states and driver reaction model. Finally, the optimal warning signal is shown on the display screen to advise driver on how much braking is needed to avoid running the red light. The system continuously updates the latest warning signal as the vehicle is approaching the intersection. Both numerical simulated driving scenarios and real-world road tests are used to demonstrate the proposed algorithm's performance under different conditions by comparing with previous work on red light running warning system. The results show that the system provides more effective and accurate warning signals to drivers, helping them avoid running red lights.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "submitted to TR-C"
    },
    {
        "paper id": "2411.03274",
        "abstract url": "https://arxiv.org/abs/2411.03274",
        "title": "Generalized Word-Representable Graphs",
        "rating": "-1",
        "keywords": [
            [
                "Graphs"
            ]
        ],
        "abstract": "The literature on word-representable graphs is quite rich, and a number of variations of the original definition have been proposed over the years. We are initiating a systematic study of such variations based on formal languages. In our framework, we can associate a graph class to each language over the binary alphabet \\{0,1\\}. All graph classes that are language-representable in this sense are hereditary and enjoy further common properties. Besides word-representable graphs and, more generally, 1^k- or k-11-representable graphs, we can identify many more graph classes in our framework, like (co)bipartite graphs, (co)comparability graphs, to name a few. It was already known that any graph is 111- or 2-11-representable. When such representations are considered for storing graphs, 111- or 2-11-representability bears the disadvantage of being significantly inferior to standard adjacency matrices or lists. We prove that quite famous languages like the palindromes, the copy language or the Lyndon words can match the efficiency of standard graph representations. The perspective of language theory allows us to prove general results that hold for all graph classes that can be defined in this way. This includes certain closure properties (e.g., all language-definable graph classes are hereditary) as well as certain limitations (e.g., all language-representable graph classes contain graphs of arbitrarily large treewidth and of arbitrarily large degeneracy, except a trivial case). As each language describes a graph class, we can also ask decidability questions concerning graph classes, given a concrete presentation of a formal language. We also present a systematic study of graph classes that can be represented by languages in which each letter occurs at most twice. Here, we find graph classes like interval, permutation, circle, bipartite chain, convex, and threshold graphs.",
        "subjects": [
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03289",
        "abstract url": "https://arxiv.org/abs/2411.03289",
        "title": "Data-Driven Sampling Based Stochastic MPC for Skid-Steer Mobile Robot Navigation",
        "rating": "-1",
        "keywords": [
            [
                "Robot",
                "Navigation"
            ]
        ],
        "abstract": "Traditional approaches to motion modeling for skid-steer robots struggle with capturing nonlinear tire-terrain dynamics, especially during high-speed maneuvers. In this paper, we tackle such nonlinearities by enhancing a dynamic unicycle model with Gaussian Process (GP) regression outputs. This enables us to develop an adaptive, uncertainty-informed navigation formulation. We solve the resultant stochastic optimal control problem using a chance-constrained Model Predictive Path Integral (MPPI) control method. This approach formulates both obstacle avoidance and path-following as chance constraints, accounting for residual uncertainties from the GP to ensure safety and reliability in control. Leveraging GPU acceleration, we efficiently manage the non-convex nature of the problem, ensuring real-time performance. Our approach unifies path-following and obstacle avoidance across different terrains, unlike prior works which typically focus on one or the other. We compare our GP-MPPI method against unicycle and data-driven kinematic models within the MPPI framework. In simulations, our approach shows superior tracking accuracy and obstacle avoidance. We further validate our approach through hardware experiments on a skid-steer robot platform, demonstrating its effectiveness in high-speed navigation. The GPU implementation of the proposed method and supplementary video footage are available at https: //stochasticmppi.github.io.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": "Currently under review for ICRA 2025"
    },
    {
        "paper id": "2411.03393",
        "abstract url": "https://arxiv.org/abs/2411.03393",
        "title": "A refined graph container lemma and applications to the hard-core model on bipartite expanders",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "We establish a refined version of a graph container lemma due to Galvin and discuss several applications related to the hard-core model on bipartite expander graphs. Given a graph $G$ and $\u03bb>0$, the hard-core model on $G$ at activity $\u03bb$ is the probability distribution $\u03bc_{G,\u03bb}$ on independent sets in $G$ given by $\u03bc_{G,\u03bb}(I)\\propto \u03bb^{|I|}$. As one of our main applications, we show that the hard-core model at activity $\u03bb$ on the hypercube $Q_d$ exhibits a `structured phase' for $\u03bb= \u03a9( \\log^2 d/d^{1/2})$ in the following sense: in a typical sample from $\u03bc_{Q_d,\u03bb}$, most vertices are contained in one side of the bipartition of $Q_d$. This improves upon a result of Galvin which establishes the same for $\u03bb=\u03a9(\\log d/ d^{1/3})$. As another application, we establish a fully polynomial-time approximation scheme (FPTAS) for the hard-core model on a $d$-regular bipartite $\u03b1$-expander, with $\u03b1>0$ fixed, when $\u03bb= \u03a9( \\log^2 d/d^{1/2})$. This improves upon the bound $\u03bb=\u03a9(\\log d/ d^{1/4})$ due to the first author, Perkins and Potukuchi. We discuss similar improvements to results of Galvin-Tetali, Balogh-Garcia-Li and Kronenberg-Spinka.",
        "subjects": [
            "math.CO",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03395",
        "abstract url": "https://arxiv.org/abs/2411.03395",
        "title": "Exploring Large Language Models for Specialist-level Oncology Care",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "surgery",
                "cancer",
                "clinical",
                "tumor"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have shown remarkable progress in encoding clinical knowledge and responding to complex medical queries with appropriate clinical reasoning. However, their applicability in subspecialist or complex medical settings remains underexplored. In this work, we probe the performance of AMIE, a research conversational diagnostic AI system, in the subspecialist domain of breast oncology care without specific fine-tuning to this challenging domain. To perform this evaluation, we curated a set of 50 synthetic breast cancer vignettes representing a range of treatment-naive and treatment-refractory cases and mirroring the key information available to a multidisciplinary tumor board for decision-making (openly released with this work). We developed a detailed clinical rubric for evaluating management plans, including axes such as the quality of case summarization, safety of the proposed care plan, and recommendations for chemotherapy, radiotherapy, surgery and hormonal therapy. To improve performance, we enhanced AMIE with the inference-time ability to perform web search retrieval to gather relevant and up-to-date clinical knowledge and refine its responses with a multi-stage self-critique pipeline. We compare response quality of AMIE with internal medicine trainees, oncology fellows, and general oncology attendings under both automated and specialist clinician evaluations. In our evaluations, AMIE outperformed trainees and fellows demonstrating the potential of the system in this challenging and important domain. We further demonstrate through qualitative examples, how systems such as AMIE might facilitate conversational interactions to assist clinicians in their decision making. However, AMIE's performance was overall inferior to attending oncologists suggesting that further research is needed prior to consideration of prospective uses.",
        "subjects": [
            "cs.HC",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03403",
        "abstract url": "https://arxiv.org/abs/2411.03403",
        "title": "Enhancing Maritime Situational Awareness through End-to-End Onboard Raw Data Analysis",
        "rating": "-1",
        "keywords": [
            [
                "Satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Satellite-based onboard data processing is crucial for time-sensitive applications requiring timely and efficient rapid response. Advances in edge artificial intelligence are shifting computational power from ground-based centers to on-orbit platforms, transforming the \"sensing-communication-decision-feedback\" cycle and reducing latency from acquisition to delivery. The current research presents a framework addressing the strict bandwidth, energy, and latency constraints of small satellites, focusing on maritime monitoring. The study contributes three main innovations. Firstly, it investigates the application of deep learning techniques for direct ship detection and classification from raw satellite imagery. By simplifying the onboard processing chain, our approach facilitates direct analyses without requiring computationally intensive steps such as calibration and ortho-rectification. Secondly, to address the scarcity of raw satellite data, we introduce two novel datasets, VDS2Raw and VDV2Raw, which are derived from raw data from Sentinel-2 and Vegetation and Environment Monitoring New Micro Satellite (VENuS) missions, respectively, and enriched with Automatic Identification System (AIS) records. Thirdly, we characterize the tasks' optimal single and multiple spectral band combinations through statistical and feature-based analyses validated on both datasets. In sum, we demonstrate the feasibility of the proposed method through a proof-of-concept on CubeSat-like hardware, confirming the models' potential for operational satellite-based maritime monitoring.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "38 pages"
    },
    {
        "paper id": "2411.03408",
        "abstract url": "https://arxiv.org/abs/2411.03408",
        "title": "Learning Few-Shot Object Placement with Intra-Category Transfer",
        "rating": "-1",
        "keywords": [
            [
                "robotics"
            ]
        ],
        "abstract": "Efficient learning from demonstration for long-horizon tasks remains an open challenge in robotics. While significant effort has been directed toward learning trajectories, a recent resurgence of object-centric approaches has demonstrated improved sample efficiency, enabling transferable robotic skills. Such approaches model tasks as a sequence of object poses over time. In this work, we propose a scheme for transferring observed object arrangements to novel object instances by learning these arrangements on canonical class frames. We then employ this scheme to enable a simple yet effective approach for training models from as few as five demonstrations to predict arrangements of a wide range of objects including tableware, cutlery, furniture, and desk spaces. We propose a method for optimizing the learned models to enables efficient learning of tasks such as setting a table or tidying up an office with intra-category transfer, even in the presence of distractors. We present extensive experimental results in simulation and on a real robotic system for table setting which, based on human evaluations, scored 73.3% compared to a human baseline. We make the code and trained models publicly available at http://oplict.cs.uni-freiburg.de.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "8 pages, 7 figures, 2 tables, submitted to RA-L"
    },
    {
        "paper id": "2411.03413",
        "abstract url": "https://arxiv.org/abs/2411.03413",
        "title": "Rapid Mixing at the Uniqueness Threshold",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "Over the past decades, a fascinating computational phase transition has been identified in sampling from Gibbs distributions. Though, the computational complexity at the critical point remains poorly understood, as previous algorithmic and hardness results all required a constant slack from this threshold. In this paper, we resolve this open question at the critical phase transition threshold, thus completing the picture of the computational phase transition. We show that for the hardcore model on graphs with maximum degree $\u0394\\ge 3$ at the uniqueness threshold $\u03bb= \u03bb_c(\u0394)$, the mixing time of Glauber dynamics is upper bounded by a polynomial in $n$, but is not nearly linear in the worst case. For the Ising model (either antiferromagnetic or ferromagnetic), we establish similar results. For the Ising model on graphs with maximum degree $\u0394\\ge 3$ at the critical temperature $\u03b2$ where $|\u03b2| = \u03b2_c(\u0394)$, with the tree-uniqueness threshold $\u03b2_c(\u0394)$, we show that the mixing time of Glauber dynamics is upper bounded by $\\tilde{O}\\left(n^{2 + O(1/\u0394)}\\right)$ and lower bounded by $\u03a9\\left(n^{3/2}\\right)$ in the worst case. For the Ising model specified by a critical interaction matrix $J$ with $\\left \\lVert J \\right \\rVert_2=1$, we obtain an upper bound $\\tilde{O}(n^{3/2})$ for the mixing time, matching the lower bound $\u03a9\\left(n^{3/2}\\right)$ on the complete graph up to a logarithmic factor. Our mixing time upper bounds are derived from a new interpretation and analysis of the localization scheme method introduced by Chen and Eldan (2022), applied to the field dynamics for the hardcore model and the proximal sampler for the Ising model. As key steps in both our upper and lower bounds, we establish sub-linear upper and lower bounds for spectral independence at the critical point for worst-case instances.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03451",
        "abstract url": "https://arxiv.org/abs/2411.03451",
        "title": "Redundancy Is All You Need",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "The seminal work of Bencz\u00far and Karger demonstrated cut sparsifiers of near-linear size, with several applications throughout theoretical computer science. Subsequent extensions have yielded sparsifiers for hypergraph cuts and more recently linear codes over Abelian groups. A decade ago, Kogan and Krauthgamer asked about the sparsifiability of arbitrary constraint satisfaction problems (CSPs). For this question, a trivial lower bound is the size of a non-redundant CSP instance, which admits, for each constraint, an assignment satisfying only that constraint (so that no constraint can be dropped by the sparsifier). For graph cuts, spanning trees are non-redundant instances. Our main result is that redundant clauses are sufficient for sparsification: for any CSP predicate R, every unweighted instance of CSP(R) has a sparsifier of size at most its non-redundancy (up to polylog factors). For weighted instances, we similarly pin down the sparsifiability to the so-called chain length of the predicate. These results precisely determine the extent to which any CSP can be sparsified. A key technical ingredient in our work is a novel application of the entropy method from Gilmer's recent breakthrough on the union-closed sets conjecture. As an immediate consequence of our main theorem, a number of results in the non-redundancy literature immediately extend to CSP sparsification. We also contribute new techniques for understanding the non-redundancy of CSP predicates. In particular, we give an explicit family of predicates whose non-redundancy roughly corresponds to the structure of matching vector families in coding theory. By adapting methods from the matching vector codes literature, we are able to construct an explicit predicate whose non-redundancy lies between $\u03a9(n^{1.5})$ and $\\widetilde{O}(n^{1.6})$, the first example with a provably non-integral exponent.",
        "subjects": [
            "cs.DS",
            "cs.DM",
            "cs.IT",
            "cs.LO",
            "math.CO"
        ],
        "comment": "66 pages"
    },
    {
        "paper id": "2411.03456",
        "abstract url": "https://arxiv.org/abs/2411.03456",
        "title": "BOston Neonatal Brain Injury Data for Hypoxic Ischemic Encephalopathy (BONBID-HIE): II. 2-year Neurocognitive Outcome and NICU Outcome",
        "rating": "-1",
        "keywords": [
            [
                "biomarkers",
                "MRI",
                "clinical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Hypoxic Ischemic Encephalopathy (HIE) affects approximately 1-5/1000 newborns globally and leads to adverse neurocognitive outcomes in 30% to 50% of cases by two years of age. Despite therapeutic advances with Therapeutic Hypothermia (TH), prognosis remains challenging, highlighting the need for improved biomarkers. This paper introduces the second release of the Boston Neonatal Brain Injury Dataset for Hypoxic-Ischemic Encephalopathy (BONBID-HIE), an open-source, comprehensive MRI and clinical dataset featuring 237 patients, including NICU outcomes and 2-year neurocognitive outcomes from Massachusetts General Hospital and Boston Children's Hospital.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "Data description for BONBID-HIE 2024 Challenge on MICCAI 2024"
    },
    {
        "paper id": "2411.03464",
        "abstract url": "https://arxiv.org/abs/2411.03464",
        "title": "TopoTxR: A topology-guided deep convolutional network for breast parenchyma learning on DCE-MRIs",
        "rating": "-1",
        "keywords": [
            [
                "MRI",
                "disease",
                "pathological"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Characterization of breast parenchyma in dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is a challenging task owing to the complexity of underlying tissue structures. Existing quantitative approaches, like radiomics and deep learning models, lack explicit quantification of intricate and subtle parenchymal structures, including fibroglandular tissue. To address this, we propose a novel topological approach that explicitly extracts multi-scale topological structures to better approximate breast parenchymal structures, and then incorporates these structures into a deep-learning-based prediction model via an attention mechanism. Our topology-informed deep learning model, \\emph{TopoTxR}, leverages topology to provide enhanced insights into tissues critical for disease pathophysiology and treatment response. We empirically validate \\emph{TopoTxR} using the VICTRE phantom breast dataset, showing that the topological structures extracted by our model effectively approximate the breast parenchymal structures. We further demonstrate \\emph{TopoTxR}'s efficacy in predicting response to neoadjuvant chemotherapy. Our qualitative and quantitative analyses suggest differential topological behavior of breast tissue in treatment-na\u00efve imaging, in patients who respond favorably to therapy as achieving pathological complete response (pCR) versus those who do not. In a comparative analysis with several baselines on the publicly available I-SPY 1 dataset (N=161, including 47 patients with pCR and 114 without) and the Rutgers proprietary dataset (N=120, with 69 patients achieving pCR and 51 not), \\emph{TopoTxR} demonstrates a notable improvement, achieving a 2.6\\% increase in accuracy and a 4.6\\% enhancement in AUC compared to the state-of-the-art method.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "22 pages, 8 figures, 8 tables, accepted by Medical Image Analysis ( https://www.sciencedirect.com/science/article/abs/pii/S1361841524002986 )"
    },
    {
        "paper id": "2411.03465",
        "abstract url": "https://arxiv.org/abs/2411.03465",
        "title": "Digital Twin for Autonomous Surface Vessels: Enabler for Safe Maritime Navigation",
        "rating": "-1",
        "keywords": [
            [
                "Navigation"
            ]
        ],
        "abstract": "Autonomous surface vessels (ASVs) are becoming increasingly significant in enhancing the safety and sustainability of maritime operations. To ensure the reliability of modern control algorithms utilized in these vessels, digital twins (DTs) provide a robust framework for conducting safe and effective simulations within a virtual environment. Digital twins are generally classified on a scale from 0 to 5, with each level representing a progression in complexity and functionality: Level 0 (Standalone) employs offline modeling techniques; Level 1 (Descriptive) integrates sensors and online modeling to enhance situational awareness; Level 2 (Diagnostic) focuses on condition monitoring and cybersecurity; Level 3 (Predictive) incorporates predictive analytics; Level 4 (Prescriptive) embeds decision-support systems; and Level 5 (Autonomous) enables advanced functionalities such as collision avoidance and path following. These digital representations not only provide insights into the vessel's current state and operational efficiency but also predict future scenarios and assess life endurance. By continuously updating with real-time sensor data, the digital twin effectively corrects modeling errors and enhances decision-making processes. Since DTs are key enablers for complex autonomous systems, this paper introduces a comprehensive methodology for establishing a digital twin framework specifically tailored for ASVs. Through a detailed literature survey, we explore existing state-of-the-art enablers across the defined levels, offering valuable recommendations for future research and development in this rapidly evolving field.",
        "subjects": [
            "eess.SY",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03477",
        "abstract url": "https://arxiv.org/abs/2411.03477",
        "title": "CrowdGenUI: Enhancing LLM-Based UI Widget Generation with a Crowdsourced Preference Library",
        "rating": "-1",
        "keywords": [
            [
                "image editing"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable skills across various design domains, including UI generation. However, current LLMs for UI generation tend to offer generic solutions that lack a deep understanding of task context and user preferences in specific scenarios. We present \\textit{CrowdGenUI}, a framework that enhances LLM-driven UI generation with a crowdsourced user preference library. This approach addresses the limitations of existing methods by guiding LLM reasoning with user preferences, enabling the generation of UI widgets that align more closely with user needs and task-specific requirements. Using image editing as a test domain, we built this library from 50 users, capturing 720 user preferences, which include the predictability, efficiency, and explorability of multiple UI widgets. In a user study with 72 additional participants, our framework outperformed standard LLM-generated widgets in meeting user preferences and task requirements. We discuss these findings to inform future opportunities for designing user-centered and customizable UIs by comprehensively analyzing the extendability of the proposed framework and crowdsourced library.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03481",
        "abstract url": "https://arxiv.org/abs/2411.03481",
        "title": "Chance-Constrained Convex MPC for Robust Quadruped Locomotion Under Parametric and Additive Uncertainties",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Recent advances in quadrupedal locomotion have focused on improving stability and performance across diverse environments. However, existing methods often lack adequate safety analysis and struggle to adapt to varying payloads and complex terrains, typically requiring extensive tuning. To overcome these challenges, we propose a Chance-Constrained Model Predictive Control (CCMPC) framework that explicitly models payload and terrain variability as distributions of parametric and additive disturbances within the single rigid body dynamics (SRBD) model. Our approach ensures safe and consistent performance under uncertain dynamics by expressing the model friction cone constraints, which define the feasible set of ground reaction forces, as chance constraints. Moreover, we solve the resulting stochastic control problem using a computationally efficient quadratic programming formulation. Extensive Monte Carlo simulations of quadrupedal locomotion across varying payloads and complex terrains demonstrate that CCMPC significantly outperforms two competitive benchmarks: Linear MPC (LMPC) and MPC with hand-tuned safety margins to maintain stability, reduce foot slippage, and track the center of mass. Hardware experiments on the Unitree Go1 robot show successful locomotion across various indoor and outdoor terrains with unknown loads exceeding 50% of the robot body weight, despite no additional parameter tuning. A video of the results and accompanying code can be found at: https://cc-mpc.github.io/.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": "Under review for Robotics and Automation Letters"
    },
    {
        "paper id": "2411.03497",
        "abstract url": "https://arxiv.org/abs/2411.03497",
        "title": "Uncertainty Quantification for Clinical Outcome Predictions with (Large) Language Models",
        "rating": "-1",
        "keywords": [
            [
                "health",
                "healthcare",
                "Clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "To facilitate healthcare delivery, language models (LMs) have significant potential for clinical prediction tasks using electronic health records (EHRs). However, in these high-stakes applications, unreliable decisions can result in high costs due to compromised patient safety and ethical concerns, thus increasing the need for good uncertainty modeling of automated clinical predictions. To address this, we consider the uncertainty quantification of LMs for EHR tasks in white- and black-box settings. We first quantify uncertainty in white-box models, where we can access model parameters and output logits. We show that an effective reduction of model uncertainty can be achieved by using the proposed multi-tasking and ensemble methods in EHRs. Continuing with this idea, we extend our approach to black-box settings, including popular proprietary LMs such as GPT-4. We validate our framework using longitudinal clinical data from more than 6,000 patients in ten clinical prediction tasks. Results show that ensembling methods and multi-task prediction prompts reduce uncertainty across different scenarios. These findings increase the transparency of the model in white-box and black-box settings, thus advancing reliable AI healthcare.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03501",
        "abstract url": "https://arxiv.org/abs/2411.03501",
        "title": "The Python LevelSet Toolbox (LevelSetPy)",
        "rating": "-1",
        "keywords": [
            [
                "robotics"
            ]
        ],
        "abstract": "This paper describes open-source scientific contributions in python surrounding the numerical solutions to hyperbolic Hamilton-Jacobi (HJ) partial differential equations viz., their implicit representation on co-dimension one surfaces; dynamics evolution with levelsets; spatial derivatives; total variation diminishing Runge-Kutta integration schemes; and their applications to the theory of reachable sets. They are increasingly finding applications in multiple research domains such as reinforcement learning, robotics, control engineering and automation. We describe the library components, illustrate usage with an example, and provide comparisons with existing implementations. This GPU-accelerated package allows for easy portability to many modern libraries for the numerical analyses of the HJ equations. We also provide a CPU implementation in python that is significantly faster than existing alternatives.",
        "subjects": [
            "cs.MS",
            "eess.SY",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03532",
        "abstract url": "https://arxiv.org/abs/2411.03532",
        "title": "A Behavior Architecture for Fast Humanoid Robot Door Traversals",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "Towards the role of humanoid robots as squad mates in urban operations and other domains, we identified doors as a major area lacking capability development. In this paper, we focus on the ability of humanoid robots to navigate and deal with doors. Human-sized doors are ubiquitous in many environment domains and the humanoid form factor is uniquely suited to operate and traverse them. We present an architecture which incorporates GPU accelerated perception and a tree based interactive behavior coordination system with a whole body motion and walking controller. Our system is capable of performing door traversals on a variety of door types. It supports rapid authoring of behaviors for unseen door types and techniques to achieve re-usability of those authored behaviors. The behaviors are modelled using trees and feature logical reactivity and action sequences that can be executed with layered concurrency to increase speed. Primitive actions are built on top of our existing whole body controller which supports manipulation while walking. We include a perception system using both neural networks and classical computer vision for door mechanism detection outside of the lab environment. We present operator-robot interdependence analysis charts to explore how human cognition is combined with artificial intelligence to produce complex robot behavior. Finally, we present and discuss real robot performances of fast door traversals on our Nadia humanoid robot. Videos online at https://www.youtube.com/playlist?list=PLXuyT8w3JVgMPaB5nWNRNHtqzRK8i68dy.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "15 pages, 23 figure, for submission to Elsevier RAS"
    },
    {
        "paper id": "2411.03533",
        "abstract url": "https://arxiv.org/abs/2411.03533",
        "title": "Shared Memory-Aware Latency-Sensitive Message Aggregation for Fine-Grained Communication",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Message aggregation is often used with a goal to reduce communication cost in HPC applications. The difference in the order of overhead of sending a message and cost of per byte transferred motivates the need for message aggregation, for several irregular fine-grained messaging applications like graph algorithms and parallel discrete event simulation (PDES). While message aggregation is frequently utilized in \"MPI-everywhere\" model, to coalesce messages between processes mapped to cores, such aggregation across threads in a process, say in MPI+X models or Charm++ SMP (Shared Memory Parallelism) mode, is often avoided. Within-process coalescing is likely to require synchronization across threads and lead to performance issues from contention. However, as a result, SMP-unaware aggregation mechanisms may not fully utilize aggregation opportunities available to applications in SMP mode. Additionally, while the benefit of message aggregation is often analyzed in terms of reducing the overhead, specifically the per message cost, we also analyze different schemes that can aid in reducing the message latency, ie. the time from when a message is sent to the time when it is received. Message latency can affect several applications like PDES with speculative execution where reducing message latency could result in fewer rollbacks. To address these challenges, in our work, we demonstrate the effectiveness of shared memory-aware message aggregation schemes for a range of proxy applications with respect to messaging overhead and latency.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "A shorter version of this paper has been accepted at IA^3 workshop at SC24"
    },
    {
        "paper id": "2411.03572",
        "abstract url": "https://arxiv.org/abs/2411.03572",
        "title": "Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge Reasoning and Text Generation",
        "rating": "-1",
        "keywords": [
            [
                "GNN",
                "Graph"
            ]
        ],
        "abstract": "This study aims to optimize the existing retrieval-augmented generation model (RAG) by introducing a graph structure to improve the performance of the model in dealing with complex knowledge reasoning tasks. The traditional RAG model has the problem of insufficient processing efficiency when facing complex graph structure information (such as knowledge graphs, hierarchical relationships, etc.), which affects the quality and consistency of the generated results. This study proposes a scheme to process graph structure data by combining graph neural network (GNN), so that the model can capture the complex relationship between entities, thereby improving the knowledge consistency and reasoning ability of the generated text. The experiment used the Natural Questions (NQ) dataset and compared it with multiple existing generation models. The results show that the graph-based RAG model proposed in this paper is superior to the traditional generation model in terms of quality, knowledge consistency, and reasoning ability, especially when dealing with tasks that require multi-dimensional reasoning. Through the combination of the enhancement of the retrieval module and the graph neural network, the model in this study can better handle complex knowledge background information and has broad potential value in multiple practical application scenarios.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03575",
        "abstract url": "https://arxiv.org/abs/2411.03575",
        "title": "Semantic Navigation for AI-assisted Ideation",
        "rating": "-1",
        "keywords": [
            [
                "Navigation"
            ]
        ],
        "abstract": "We present a novel AI-based ideation assistant and evaluate it in a user study with a group of innovators. The key contribution of our work is twofold: we propose a method of idea exploration in a constrained domain by means of LLM-supported semantic navigation of problem and solution spaces, and employ novel automated data input filtering to improve generations. We found that semantic exploration is preferred to the traditional prompt-output interactions, measured both in explicit survey rankings, and in terms of innovation assistant engagement, where 2.1x more generations were performed using semantic exploration. We also show that filtering input data with metrics such as relevancy, coherence and human alignment leads to improved generations in the same metrics as well as enhanced quality of experience among innovators.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2402.06053"
    },
    {
        "paper id": "2411.03581",
        "abstract url": "https://arxiv.org/abs/2411.03581",
        "title": "Can Robotic Cues Manipulate Human Decisions? Exploring Consensus Building via Bias-Controlled Non-linear Opinion Dynamics and Robotic Eye Gaze Mediated Interaction in Human-Robot Teaming",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "Although robots are becoming more advanced with human-like anthropomorphic features and decision-making abilities to improve collaboration, the active integration of humans into this process remains under-explored. This article presents the first experimental study exploring decision-making interactions between humans and robots with visual cues from robotic eyes, which can dynamically influence human opinion formation. The cues generated by robotic eyes gradually guide human decisions towards alignment with the robot's choices. Both human and robot decision-making processes are modeled as non-linear opinion dynamics with evolving biases. To examine these opinion dynamics under varying biases, we conduct numerical parametric and equilibrium continuation analyses using tuned parameters designed explicitly for the presented human-robot interaction experiment. Furthermore, to facilitate the transition from disagreement to agreement, we introduced a human opinion observation algorithm integrated with the formation of the robot's opinion, where the robot's behavior is controlled based on its formed opinion. The algorithms developed aim to enhance human involvement in consensus building, fostering effective collaboration between humans and robots. Experiments with 51 participants (N = 51) show that human-robot teamwork can be improved by guiding human decisions using robotic cues. Finally, we provide detailed insights on the effects of trust, cognitive load, and participant demographics on decision-making based on user feedback and post-experiment interviews.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": "35 pages, 14 figures"
    },
    {
        "paper id": "2411.03590",
        "abstract url": "https://arxiv.org/abs/2411.03590",
        "title": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond",
        "rating": "-1",
        "keywords": [
            [
                "Medical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Run-time steering strategies like Medprompt are valuable for guiding large language models (LLMs) to top performance on challenging tasks. Medprompt demonstrates that a general LLM can be focused to deliver state-of-the-art performance on specialized domains like medicine by using a prompt to elicit a run-time strategy involving chain of thought reasoning and ensembling. OpenAI's o1-preview model represents a new paradigm, where a model is designed to do run-time reasoning before generating final responses. We seek to understand the behavior of o1-preview on a diverse set of medical challenge problem benchmarks. Following on the Medprompt study with GPT-4, we systematically evaluate the o1-preview model across various medical benchmarks. Notably, even without prompting techniques, o1-preview largely outperforms the GPT-4 series with Medprompt. We further systematically study the efficacy of classic prompt engineering strategies, as represented by Medprompt, within the new paradigm of reasoning models. We found that few-shot prompting hinders o1's performance, suggesting that in-context learning may no longer be an effective steering approach for reasoning-native models. While ensembling remains viable, it is resource-intensive and requires careful cost-performance optimization. Our cost and accuracy analysis across run-time strategies reveals a Pareto frontier, with GPT-4o representing a more affordable option and o1-preview achieving state-of-the-art performance at higher cost. Although o1-preview offers top performance, GPT-4o with steering strategies like Medprompt retains value in specific contexts. Moreover, we note that the o1-preview model has reached near-saturation on many existing medical benchmarks, underscoring the need for new, challenging benchmarks. We close with reflections on general directions for inference-time computation with LLMs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "25 pages"
    },
    {
        "paper id": "2411.03591",
        "abstract url": "https://arxiv.org/abs/2411.03591",
        "title": "vMF-Contact: Uncertainty-aware Evidential Learning for Probabilistic Contact-grasp in Noisy Clutter",
        "rating": "-1",
        "keywords": [
            [
                "6-DoF"
            ]
        ],
        "abstract": "Grasp learning in noisy environments, such as occlusions, sensor noise, and out-of-distribution (OOD) objects, poses significant challenges. Recent learning-based approaches focus primarily on capturing aleatoric uncertainty from inherent data noise. The epistemic uncertainty, which represents the OOD recognition, is often addressed by ensembles with multiple forward paths, limiting real-time application. In this paper, we propose an uncertainty-aware approach for 6-DoF grasp detection using evidential learning to comprehensively capture both uncertainties in real-world robotic grasping. As a key contribution, we introduce vMF-Contact, a novel architecture for learning hierarchical contact grasp representations with probabilistic modeling of directional uncertainty as von Mises-Fisher (vMF) distribution. To achieve this, we derive and analyze the theoretical formulation of the second-order objective on the posterior parametrization, providing formal guarantees for the model's ability to quantify uncertainty and improve grasp prediction performance. Moreover, we enhance feature expressiveness by applying partial point reconstructions as an auxiliary task, improving the comprehension of uncertainty quantification as well as the generalization to unseen objects. In the real-world experiments, our method demonstrates a significant improvement by 39% in the overall clearance rate compared to the baselines. Video is under https://www.youtube.com/watch?v=4aQsrDgdV8Y&t=12s",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03595",
        "abstract url": "https://arxiv.org/abs/2411.03595",
        "title": "Investigating Conceptual Blending of a Diffusion Model for Improving Nonword-to-Image Generation",
        "rating": "-1",
        "keywords": [
            [
                "Diffusion",
                "Text-to-image"
            ]
        ],
        "abstract": "Text-to-image diffusion models sometimes depict blended concepts in the generated images. One promising use case of this effect would be the nonword-to-image generation task which attempts to generate images intuitively imaginable from a non-existing word (nonword). To realize nonword-to-image generation, an existing study focused on associating nonwords with similar-sounding words. Since each nonword can have multiple similar-sounding words, generating images containing their blended concepts would increase intuitiveness, facilitating creative activities and promoting computational psycholinguistics. Nevertheless, no existing study has quantitatively evaluated this effect in either diffusion models or the nonword-to-image generation paradigm. Therefore, this paper first analyzes the conceptual blending in a pretrained diffusion model, Stable Diffusion. The analysis reveals that a high percentage of generated images depict blended concepts when inputting an embedding interpolating between the text embeddings of two text prompts referring to different concepts. Next, this paper explores the best text embedding space conversion method of an existing nonword-to-image generation framework to ensure both the occurrence of conceptual blending and image generation quality. We compare the conventional direct prediction approach with the proposed method that combines $k$-nearest neighbor search and linear regression. Evaluation reveals that the enhanced accuracy of the embedding space conversion by the proposed method improves the image generation quality, while the emergence of conceptual blending could be attributed mainly to the specific dimensions of the high-dimensional text embedding space.",
        "subjects": [
            "cs.MM"
        ],
        "comment": "Paper accepted at ACM MM 2024 (doi: 10.1145/3664647.3681202) with supplementary materials concatenated"
    },
    {
        "paper id": "2411.03604",
        "abstract url": "https://arxiv.org/abs/2411.03604",
        "title": "Temporal-Difference Learning Using Distributed Error Signals",
        "rating": "-1",
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "A computational problem in biological reward-based learning is how credit assignment is performed in the nucleus accumbens (NAc). Much research suggests that NAc dopamine encodes temporal-difference (TD) errors for learning value predictions. However, dopamine is synchronously distributed in regionally homogeneous concentrations, which does not support explicit credit assignment (like used by backpropagation). It is unclear whether distributed errors alone are sufficient for synapses to make coordinated updates to learn complex, nonlinear reward-based learning tasks. We design a new deep Q-learning algorithm, Artificial Dopamine, to computationally demonstrate that synchronously distributed, per-layer TD errors may be sufficient to learn surprisingly complex RL tasks. We empirically evaluate our algorithm on MinAtar, the DeepMind Control Suite, and classic control tasks, and show it often achieves comparable performance to deep RL algorithms that use backpropagation.",
        "subjects": [
            "cs.LG",
            "cs.NE"
        ],
        "comment": "10 pages, to be published at NeurIPS 2024"
    },
    {
        "paper id": "2411.03610",
        "abstract url": "https://arxiv.org/abs/2411.03610",
        "title": "LCP-Fusion: A Neural Implicit SLAM with Enhanced Local Constraints and Computable Prior",
        "rating": "-1",
        "keywords": [
            [
                "voxel",
                "RGB-D",
                "SDF"
            ],
            [
                "SLAM"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recently the dense Simultaneous Localization and Mapping (SLAM) based on neural implicit representation has shown impressive progress in hole filling and high-fidelity mapping. Nevertheless, existing methods either heavily rely on known scene bounds or suffer inconsistent reconstruction due to drift in potential loop-closure regions, or both, which can be attributed to the inflexible representation and lack of local constraints. In this paper, we present LCP-Fusion, a neural implicit SLAM system with enhanced local constraints and computable prior, which takes the sparse voxel octree structure containing feature grids and SDF priors as hybrid scene representation, enabling the scalability and robustness during mapping and tracking. To enhance the local constraints, we propose a novel sliding window selection strategy based on visual overlap to address the loop-closure, and a practical warping loss to constrain relative poses. Moreover, we estimate SDF priors as coarse initialization for implicit features, which brings additional explicit constraints and robustness, especially when a light but efficient adaptive early ending is adopted. Experiments demonstrate that our method achieve better localization accuracy and reconstruction consistency than existing RGB-D implicit SLAM, especially in challenging real scenes (ScanNet) as well as self-captured scenes with unknown scene bounds. The code is available at https://github.com/laliwang/LCP-Fusion.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": "Accepted by 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2024)"
    },
    {
        "paper id": "2411.03614",
        "abstract url": "https://arxiv.org/abs/2411.03614",
        "title": "Robot Swarming over the internet",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "This paper considers cooperative control of robots involving two different testbed systems in remote locations with communication on the internet. This provides us the capability to exchange robots status like positions, velocities and directions needed for the swarming algorithm. The results show that all robots properly follow some leader defined one of the testbeds. Measurement of data exchange rates show no loss of packets, and average transfer delays stay within tolerance limits for practical applications. In our knowledge, the novelty of this paper concerns this kind of control over a large network like internet.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03618",
        "abstract url": "https://arxiv.org/abs/2411.03618",
        "title": "Cross Feature Fusion of Fundus Image and Generated Lesion Map for Referable Diabetic Retinopathy Classification",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "clinical",
                "Lesion"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Diabetic Retinopathy (DR) is a primary cause of blindness, necessitating early detection and diagnosis. This paper focuses on referable DR classification to enhance the applicability of the proposed method in clinical practice. We develop an advanced cross-learning DR classification method leveraging transfer learning and cross-attention mechanisms. The proposed method employs the Swin U-Net architecture to segment lesion maps from DR fundus images. The Swin U-Net segmentation model, enriched with DR lesion insights, is transferred to generate a lesion map. Both the fundus image and its segmented lesion map are used as complementary inputs for the classification model. A cross-attention mechanism is deployed to improve the model's ability to capture fine-grained details from the input pairs. Our experiments, utilizing two public datasets, FGADR and EyePACS, demonstrate a superior accuracy of 94.6%, surpassing current state-of-the-art methods by 4.4%. To this end, we aim for the proposed method to be seamlessly integrated into clinical workflows, enhancing accuracy and efficiency in identifying referable DR.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "ACCV 2024 accepted"
    },
    {
        "paper id": "2411.03619",
        "abstract url": "https://arxiv.org/abs/2411.03619",
        "title": "Real-Time Safe Bipedal Robot Navigation using Linear Discrete Control Barrier Functions",
        "rating": "-1",
        "keywords": [
            [
                "Robot",
                "Navigation"
            ]
        ],
        "abstract": "Safe navigation in real-time is an essential task for humanoid robots in real-world deployment. Since humanoid robots are inherently underactuated thanks to unilateral ground contacts, a path is considered safe if it is obstacle-free and respects the robot's physical limitations and underlying dynamics. Existing approaches often decouple path planning from gait control due to the significant computational challenge caused by the full-order robot dynamics. In this work, we develop a unified, safe path and gait planning framework that can be evaluated online in real-time, allowing the robot to navigate clustered environments while sustaining stable locomotion. Our approach uses the popular Linear Inverted Pendulum (LIP) model as a template model to represent walking dynamics. It incorporates heading angles in the model to evaluate kinematic constraints essential for physically feasible gaits properly. In addition, we leverage discrete control barrier functions (DCBF) for obstacle avoidance, ensuring that the subsequent foot placement provides a safe navigation path within clustered environments. To guarantee real-time computation, we use a novel approximation of the DCBF to produce linear DCBF (LDCBF) constraints. We validate the proposed approach in simulation using a Digit robot in randomly generated environments. The results demonstrate that our approach can generate safe gaits for a non-trivial humanoid robot to navigate environments with randomly generated obstacles in real-time.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": "7 pages, 10 figures"
    },
    {
        "paper id": "2411.03660",
        "abstract url": "https://arxiv.org/abs/2411.03660",
        "title": "Development of a Practical Articulated Wheeled In-pipe Robot for Both 3-4 in Force Main Inspection of Sewer Pipes",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "This paper reports a practical articulated wheeled in-pipe inspection robot \"AIRo-7.1\" which is waterproof and dustproof, and can adapt to 3 to 4 in inner diameters. The joint torque can be adjusted by a PWM open-loop control. The middle joint angle can be controlled by a position feedback control system while the other two joints are bent by torsional springs. Thanks to this simple and high-density design, not only downsizing of the robot but also wide range of the adaptive inner diameter were achieved. However, the relationship between the actual middle joint torque value and the PWM duty ratio should be pre-known because the reducer used in AIRo-7.1 was designed by ourselves. Therefore, preliminary experiments were conducted to clarify the relationship between them. To examine the adaptive movement, experiments in both 3 in and 4 in pipes with vertical, bend, and diameter change sections. Finally, field experiment was also conducted. From the results, high adaptability to different inner diameters of pipes and slippery environments were confirmed although waterproof and dustproof were not perfectly working.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "The Twenty-Ninth International Symposium on Artificial Life and Robotics 2024 (AROB 29th 2024), The Ninth International Symposium on BioComplexity 2024 (ISBC 9th 2024), The Seventh International Symposium on Swarm Behavior and Bio-Inspired Robotics 2024 (SWARM 7th 2024) B-Con Plaza, Beppu, Japan and ONLINE, January 24-26, 2024"
    },
    {
        "paper id": "2411.03663",
        "abstract url": "https://arxiv.org/abs/2411.03663",
        "title": "Can Graph Neural Networks Expose Training Data Properties? An Efficient Risk Assessment Approach",
        "rating": "-1",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "attack"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Graph neural networks (GNNs) have attracted considerable attention due to their diverse applications. However, the scarcity and quality limitations of graph data present challenges to their training process in practical settings. To facilitate the development of effective GNNs, companies and researchers often seek external collaboration. Yet, directly sharing data raises privacy concerns, motivating data owners to train GNNs on their private graphs and share the trained models. Unfortunately, these models may still inadvertently disclose sensitive properties of their training graphs (e.g., average default rate in a transaction network), leading to severe consequences for data owners. In this work, we study graph property inference attack to identify the risk of sensitive property information leakage from shared models. Existing approaches typically train numerous shadow models for developing such attack, which is computationally intensive and impractical. To address this issue, we propose an efficient graph property inference attack by leveraging model approximation techniques. Our method only requires training a small set of models on graphs, while generating a sufficient number of approximated shadow models for attacks. To enhance diversity while reducing errors in the approximated models, we apply edit distance to quantify the diversity within a group of approximated models and introduce a theoretically guaranteed criterion to evaluate each model's error. Subsequently, we propose a novel selection mechanism to ensure that the retained approximated models achieve high diversity and low error. Extensive experiments across six real-world scenarios demonstrate our method's substantial improvement, with average increases of 2.7% in attack accuracy and 4.1% in ROC-AUC, while being 6.5$\\times$ faster compared to the best baseline.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": "In NeurIPS'24"
    },
    {
        "paper id": "2411.02941",
        "abstract url": "https://arxiv.org/abs/2411.02941",
        "title": "A Mamba Foundation Model for Time Series Forecasting",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Time series foundation models have demonstrated strong performance in zero-shot learning, making them well-suited for predicting rapidly evolving patterns in real-world applications where relevant training data are scarce. However, most of these models rely on the Transformer architecture, which incurs quadratic complexity as input length increases. To address this, we introduce TSMamba, a linear-complexity foundation model for time series forecasting built on the Mamba architecture. The model captures temporal dependencies through both forward and backward Mamba encoders, achieving high prediction accuracy. To reduce reliance on large datasets and lower training costs, TSMamba employs a two-stage transfer learning process that leverages pretrained Mamba LLMs, allowing effective time series modeling with a moderate training set. In the first stage, the forward and backward backbones are optimized via patch-wise autoregressive prediction; in the second stage, the model trains a prediction head and refines other components for long-term forecasting. While the backbone assumes channel independence to manage varying channel numbers across datasets, a channel-wise compressed attention module is introduced to capture cross-channel dependencies during fine-tuning on specific multivariate datasets. Experiments show that TSMamba's zero-shot performance is comparable to state-of-the-art time series foundation models, despite using significantly less training data. It also achieves competitive or superior full-shot performance compared to task-specific prediction models. The code will be made publicly available.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02983",
        "abstract url": "https://arxiv.org/abs/2411.02983",
        "title": "Autonomous Decision Making for UAV Cooperative Pursuit-Evasion Game with Reinforcement Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "vehicle"
            ],
            [
                "UAV"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The application of intelligent decision-making in unmanned aerial vehicle (UAV) is increasing, and with the development of UAV 1v1 pursuit-evasion game, multi-UAV cooperative game has emerged as a new challenge. This paper proposes a deep reinforcement learning-based model for decision-making in multi-role UAV cooperative pursuit-evasion game, to address the challenge of enabling UAV to autonomously make decisions in complex game environments. In order to enhance the training efficiency of the reinforcement learning algorithm in UAV pursuit-evasion game environment that has high-dimensional state-action space, this paper proposes multi-environment asynchronous double deep Q-network with priority experience replay algorithm to effectively train the UAV's game policy. Furthermore, aiming to improve cooperation ability and task completion efficiency, as well as minimize the cost of UAVs in the pursuit-evasion game, this paper focuses on the allocation of roles and targets within multi-UAV environment. The cooperative game decision model with varying numbers of UAVs are obtained by assigning diverse tasks and roles to the UAVs in different scenarios. The simulation results demonstrate that the proposed method enables autonomous decision-making of the UAVs in pursuit-evasion game scenarios and exhibits significant capabilities in cooperation.",
        "subjects": [
            "cs.AI",
            "cs.MA",
            "cs.RO"
        ],
        "comment": "11 pages, 12 figures, 31 conference"
    },
    {
        "paper id": "2411.03025",
        "abstract url": "https://arxiv.org/abs/2411.03025",
        "title": "DA-MoE: Addressing Depth-Sensitivity in Graph-Level Analysis through Mixture of Experts",
        "rating": "-1.5",
        "keywords": [
            [
                "Depth"
            ],
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Graph neural networks (GNNs) are gaining popularity for processing graph-structured data. In real-world scenarios, graph data within the same dataset can vary significantly in scale. This variability leads to depth-sensitivity, where the optimal depth of GNN layers depends on the scale of the graph data. Empirically, fewer layers are sufficient for message passing in smaller graphs, while larger graphs typically require deeper networks to capture long-range dependencies and global features. However, existing methods generally use a fixed number of GNN layers to generate representations for all graphs, overlooking the depth-sensitivity issue in graph structure data. To address this challenge, we propose the depth adaptive mixture of expert (DA-MoE) method, which incorporates two main improvements to GNN backbone: \\textbf{1)} DA-MoE employs different GNN layers, each considered an expert with its own parameters. Such a design allows the model to flexibly aggregate information at different scales, effectively addressing the depth-sensitivity issue in graph data. \\textbf{2)} DA-MoE utilizes GNN to capture the structural information instead of the linear projections in the gating network. Thus, the gating network enables the model to capture complex patterns and dependencies within the data. By leveraging these improvements, each expert in DA-MoE specifically learns distinct graph patterns at different scales. Furthermore, comprehensive experiments on the TU dataset and open graph benchmark (OGB) have shown that DA-MoE consistently surpasses existing baselines on various tasks, including graph, node, and link-level analyses. The code are available at \\url{https://github.com/Celin-Yao/DA-MoE}.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "8pages"
    },
    {
        "paper id": "2411.03105",
        "abstract url": "https://arxiv.org/abs/2411.03105",
        "title": "Evaluating Machine Learning Models against Clinical Protocols for Enhanced Interpretability and Continuity of Care",
        "rating": "-1.5",
        "keywords": [
            [
                "medical",
                "Clinical"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In clinical practice, decision-making relies heavily on established protocols, often formalised as rules. Concurrently, Machine Learning (ML) models, trained on clinical data, aspire to integrate into medical decision-making processes. However, despite the growing number of ML applications, their adoption into clinical practice remains limited. Two critical concerns arise, relevant to the notions of consistency and continuity of care: (a) accuracy - the ML model, albeit more accurate, might introduce errors that would not have occurred by applying the protocol; (b) interpretability - ML models operating as black boxes might make predictions based on relationships that contradict established clinical knowledge. In this context, the literature suggests using ML models integrating domain knowledge for improved accuracy and interpretability. However, there is a lack of appropriate metrics for comparing ML models with clinical rules in addressing these challenges. Accordingly, in this article, we first propose metrics to assess the accuracy of ML models with respect to the established protocol. Secondly, we propose an approach to measure the distance of explanations provided by two rule sets, with the goal of comparing the explanation similarity between clinical rule-based systems and rules extracted from ML models. The approach is validated on the Pima Indians Diabetes dataset by training two neural networks - one exclusively on data, and the other integrating a clinical protocol. Our findings demonstrate that the integrated ML model achieves comparable performance to that of a fully data-driven model while exhibiting superior accuracy relative to the clinical protocol, ensuring enhanced continuity of care. Furthermore, we show that our integrated model provides explanations for predictions that align more closely with the clinical protocol compared to the data-driven model.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03140",
        "abstract url": "https://arxiv.org/abs/2411.03140",
        "title": "The Impact of Medicaid Expansion on Medicare Quality Measures",
        "rating": "-1.5",
        "keywords": [
            [
                "health",
                "surgery"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "The Affordable Care Act was signed into law in 2010, expanding Medicaid and improving access to care for millions of low-income Americans. Fewer uninsured individuals reduced the cost of uncompensated care, consequently improving the financial health of hospitals. We hypothesize that this amelioration in hospital finances resulted in a marked improvement of quality measures in states that chose to expand Medicaid. To our knowledge, the impact of Medicaid expansion on the Medicare population has not been investigated. Using a difference-in-difference analysis, we compare readmission rates for four measures from the Hospital Readmission Reduction Program: acute myocardial infarction, pneumonia, heart failure, and coronary artery bypass graft surgery. Our analysis provides evidence that between 2013 and 2021 expansion states improved hospital quality relative to non-expansion states as it relates to acute myocardial infarction readmissions (p = 0.015) and coronary artery bypass graft surgery readmissions (p = 0.039). Our analysis provides some evidence that expanding Medicaid improved hospital quality, as measured by a reduction in readmission rates. Using visualizations, we provide some evidence that hospital quality improved for the other two measures as well. We believe that a refinement of our estimation method and an improved dataset will increase our chances of finding significant results for these two other measures.",
        "subjects": [
            "cs.CY",
            "econ.GN"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03162",
        "abstract url": "https://arxiv.org/abs/2411.03162",
        "title": "A Machine Learning Approach for the Efficient Estimation of Ground-Level Air Temperature in Urban Areas",
        "rating": "-1.5",
        "keywords": [
            [
                "thermal"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The increasingly populated cities of the 21st Century face the challenge of being sustainable and resilient spaces for their inhabitants. However, climate change, among other problems, makes these objectives difficult to achieve. The Urban Heat Island (UHI) phenomenon that occurs in cities, increasing their thermal stress, is one of the stumbling blocks to achieve a more sustainable city. The ability to estimate temperatures with a high degree of accuracy allows for the identification of the highest priority areas in cities where urban improvements need to be made to reduce thermal discomfort. In this work we explore the usefulness of image-to-image deep neural networks (DNNs) for correlating spatial and meteorological variables of a urban area with street-level air temperature. The air temperature at street-level is estimated both spatially and temporally for a specific use case, and compared with existing, well-established numerical models. Based on the obtained results, deep neural networks are confirmed to be faster and less computationally expensive alternative for ground-level air temperature compared to numerical models.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "39 pages, 8 figures, 2 tables, under review"
    },
    {
        "paper id": "2411.03186",
        "abstract url": "https://arxiv.org/abs/2411.03186",
        "title": "Insights into Lunar Mineralogy: An Unsupervised Approach for Clustering of the Moon Mineral Mapper (M3) spectral data",
        "rating": "-1.5",
        "keywords": [
            [
                "hyperspectral data",
                "Mineral"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper presents a novel method for mapping spectral features of the Moon using machine learning-based clustering of hyperspectral data from the Moon Mineral Mapper (M3) imaging spectrometer. The method uses a convolutional variational autoencoder to reduce the dimensionality of the spectral data and extract features of the spectra. Then, a k-means algorithm is applied to cluster the latent variables into five distinct groups, corresponding to dominant spectral features, which are related to the mineral composition of the Moon's surface. The resulting global spectral cluster map shows the distribution of the five clusters on the Moon, which consist of a mixture of, among others, plagioclase, pyroxene, olivine, and Fe-bearing minerals across the Moon's surface. The clusters are compared to the mineral maps from the Kaguya mission, which showed that the locations of the clusters overlap with the locations of high wt% of minerals such as plagioclase, clinopyroxene, and olivine. The paper demonstrates the usefulness of unbiased unsupervised learning for lunar mineral exploration and provides a comprehensive analysis of lunar mineralogy.",
        "subjects": [
            "astro-ph.EP",
            "astro-ph.IM",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03187",
        "abstract url": "https://arxiv.org/abs/2411.03187",
        "title": "Design-Reality Gap Analysis of Health Information Systems Failure",
        "rating": "-1.5",
        "keywords": [
            [
                "Health",
                "healthcare"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "This study investigates the factors contributing to the failure of Health Information Systems (HIS) in a public hospital in South Africa. While HIS have the potential to improve healthcare delivery by integrating services and enhancing effectiveness, failures can lead to service interruptions, revenue loss, data loss, administrative difficulties, and reputational damage. Using semi-structured interviews with key stakeholders, we employed a hybrid data analysis approach combining deductive analysis based on the Design- Reality Gap Model and inductive thematic analysis. Our findings highlight several factors contributing to HIS failures, including system capacity constraints, inadequate IT risk management, and critical skills gaps. Despite these challenges, end users perceive HIS positively and recommend its implementation for streamlining daily processes. This study underscores the importance of addressing design-reality gaps to improve HIS outcomes in public healthcare settings.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03224",
        "abstract url": "https://arxiv.org/abs/2411.03224",
        "title": "Interpretable Predictive Models for Healthcare via Rational Logistic Regression",
        "rating": "-1.5",
        "keywords": [
            [
                "health",
                "Healthcare",
                "clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The healthcare sector has experienced a rapid accumulation of digital data recently, especially in the form of electronic health records (EHRs). EHRs constitute a precious resource that IS researchers could utilize for clinical applications (e.g., morbidity prediction). Deep learning seems like the obvious choice to exploit this surfeit of data. However, numerous studies have shown that deep learning does not enjoy the same kind of success on EHR data as it has in other domains; simple models like logistic regression are frequently as good as sophisticated deep learning ones. Inspired by this observation, we develop a novel model called rational logistic regression (RLR) that has standard logistic regression (LR) as its special case (and thus inherits LR's inductive bias that aligns with EHR data). RLR has rational series as its theoretical underpinnings, works on longitudinal time-series data, and learns interpretable patterns. Empirical comparisons on real-world clinical tasks demonstrate RLR's efficacy.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "ICIS 2021 Proceedings ( see https://aisel.aisnet.org/icis2021/is_health/is_health/18 )"
    },
    {
        "paper id": "2411.03231",
        "abstract url": "https://arxiv.org/abs/2411.03231",
        "title": "Formal Logic-guided Robust Federated Learning against Poisoning Attacks",
        "rating": "-1.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Attacks"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Federated Learning (FL) offers a promising solution to the privacy concerns associated with centralized Machine Learning (ML) by enabling decentralized, collaborative learning. However, FL is vulnerable to various security threats, including poisoning attacks, where adversarial clients manipulate the training data or model updates to degrade overall model performance. Recognizing this threat, researchers have focused on developing defense mechanisms to counteract poisoning attacks in FL systems. However, existing robust FL methods predominantly focus on computer vision tasks, leaving a gap in addressing the unique challenges of FL with time series data. In this paper, we present FLORAL, a defense mechanism designed to mitigate poisoning attacks in federated learning for time-series tasks, even in scenarios with heterogeneous client data and a large number of adversarial participants. Unlike traditional model-centric defenses, FLORAL leverages logical reasoning to evaluate client trustworthiness by aligning their predictions with global time-series patterns, rather than relying solely on the similarity of client updates. Our approach extracts logical reasoning properties from clients, then hierarchically infers global properties, and uses these to verify client updates. Through formal logic verification, we assess the robustness of each client contribution, identifying deviations indicative of adversarial behavior. Experimental results on two datasets demonstrate the superior performance of our approach compared to existing baseline methods, highlighting its potential to enhance the robustness of FL to time series applications. Notably, FLORAL reduced the prediction error by 93.27% in the best-case scenario compared to the second-best baseline. Our code is available at https://anonymous.4open.science/r/FLORAL-Robust-FTS.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.DC",
            "cs.LO"
        ],
        "comment": "12 pages, 4 figures, 6 tables"
    },
    {
        "paper id": "2411.03372",
        "abstract url": "https://arxiv.org/abs/2411.03372",
        "title": "Energy Price Modelling: A Comparative Evaluation of four Generations of Forecasting Methods",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Energy is a critical driver of modern economic systems. Accurate energy price forecasting plays an important role in supporting decision-making at various levels, from operational purchasing decisions at individual business organizations to policy-making. A significant body of literature has looked into energy price forecasting, investigating a wide range of methods to improve accuracy and inform these critical decisions. Given the evolving landscape of forecasting techniques, the literature lacks a thorough empirical comparison that systematically contrasts these methods. This paper provides an in-depth review of the evolution of forecasting modeling frameworks, from well-established econometric models to machine learning methods, early sequence learners such LSTMs, and more recent advancements in deep learning with transformer networks, which represent the cutting edge in forecasting. We offer a detailed review of the related literature and categorize forecasting methodologies into four model families. We also explore emerging concepts like pre-training and transfer learning, which have transformed the analysis of unstructured data and hold significant promise for time series forecasting. We address a gap in the literature by performing a comprehensive empirical analysis on these four family models, using data from the EU energy markets, we conduct a large-scale empirical study, which contrasts the forecasting accuracy of different approaches, focusing especially on alternative propositions for time series transformers.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03387",
        "abstract url": "https://arxiv.org/abs/2411.03387",
        "title": "Quantifying Aleatoric Uncertainty of the Treatment Effect: A Novel Orthogonal Learner",
        "rating": "-1.5",
        "keywords": [
            [
                "medical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Estimating causal quantities from observational data is crucial for understanding the safety and effectiveness of medical treatments. However, to make reliable inferences, medical practitioners require not only estimating averaged causal quantities, such as the conditional average treatment effect, but also understanding the randomness of the treatment effect as a random variable. This randomness is referred to as aleatoric uncertainty and is necessary for understanding the probability of benefit from treatment or quantiles of the treatment effect. Yet, the aleatoric uncertainty of the treatment effect has received surprisingly little attention in the causal machine learning community. To fill this gap, we aim to quantify the aleatoric uncertainty of the treatment effect at the covariate-conditional level, namely, the conditional distribution of the treatment effect (CDTE). Unlike average causal quantities, the CDTE is not point identifiable without strong additional assumptions. As a remedy, we employ partial identification to obtain sharp bounds on the CDTE and thereby quantify the aleatoric uncertainty of the treatment effect. We then develop a novel, orthogonal learner for the bounds on the CDTE, which we call AU-learner. We further show that our AU-learner has several strengths in that it satisfies Neyman-orthogonality and is doubly robust. Finally, we propose a fully-parametric deep learning instantiation of our AU-learner.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03450",
        "abstract url": "https://arxiv.org/abs/2411.03450",
        "title": "Fourier Analysis of Variational Quantum Circuits for Supervised Learning",
        "rating": "-1.5",
        "keywords": [
            [
                "Quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "VQC can be understood through the lens of Fourier analysis. It is already well-known that the function space represented by any circuit architecture can be described through a truncated Fourier sum. We show that the spectrum available to that truncated Fourier sum is not entirely determined by the encoding gates of the circuit, since the variational part of the circuit can constrain certain coefficients to zero, effectively removing that frequency from the spectrum. To the best of our knowledge, we give the first description of the functional dependence of the Fourier coefficients on the variational parameters as trigonometric polynomials. This allows us to provide an algorithm which computes the exact spectrum of any given circuit and the corresponding Fourier coefficients. Finally, we demonstrate that by comparing the Fourier transform of the dataset to the available spectra, it is possible to predict which VQC out of a given list of choices will be able to best fit the data.",
        "subjects": [
            "cs.LG",
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03460",
        "abstract url": "https://arxiv.org/abs/2411.03460",
        "title": "Pathway-Guided Optimization of Deep Generative Molecular Design Models for Cancer Therapy",
        "rating": "-1.5",
        "keywords": [
            [
                "Cancer"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The data-driven drug design problem can be formulated as an optimization task of a potentially expensive black-box objective function over a huge high-dimensional and structured molecular space. The junction tree variational autoencoder (JTVAE) has been shown to be an efficient generative model that can be used for suggesting legitimate novel drug-like small molecules with improved properties. While the performance of the generative molecular design (GMD) scheme strongly depends on the initial training data, one can improve its sampling efficiency for suggesting better molecules with enhanced properties by optimizing the latent space. In this work, we propose how mechanistic models - such as pathway models described by differential equations - can be used for effective latent space optimization(LSO) of JTVAEs and other similar models for GMD. To demonstrate the potential of our proposed approach, we show how a pharmacodynamic model, assessing the therapeutic efficacy of a drug-like small molecule by predicting how it modulates a cancer pathway, can be incorporated for effective LSO of data-driven models for GMD.",
        "subjects": [
            "cs.LG",
            "q-bio.QM"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03520",
        "abstract url": "https://arxiv.org/abs/2411.03520",
        "title": "Forecasting Outside the Box: Application-Driven Optimal Pointwise Forecasts for Stochastic Optimization",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The exponential growth in data availability in recent years has led to new formulations of data-driven optimization problems. One such formulation is that of stochastic optimization problems with contextual information, where the goal is to optimize the expected value of a certain function given some contextual information (also called features) that accompany the main data of interest. The contextual information then allows for a better estimation of the quantity of interest via machine learning methods, thereby leading to better solutions. Oftentimes, however, machine learning methods yield just a pointwise estimate instead of an entire distribution. In this paper we show that, when the problem to be solved is a class of two-stage stochastic programs (namely, those with fixed recourse matrix and fixed costs), under mild assumptions the problem can be solved with just one scenario. While such a scenario - which does not have be unique - is usually unknown, we present an integrated learning and optimization procedure that yields the best approximation of that scenario within the modeler's pre-specified set of parameterized forecast functions. Numerical results conducted with inventory problems from the literature (with synthetic data) as well as a bike-sharing problem with real data demonstrate that the proposed approach performs well when compared to benchmark methods from the literature.",
        "subjects": [
            "math.OC",
            "cs.LG"
        ],
        "comment": "Submitted for publication"
    },
    {
        "paper id": "2411.03522",
        "abstract url": "https://arxiv.org/abs/2411.03522",
        "title": "Exploring the Potentials and Challenges of Using Large Language Models for the Analysis of Transcriptional Regulation of Long Non-coding RNAs",
        "rating": "-1.5",
        "keywords": [
            [
                "biological",
                "disease"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Research on long non-coding RNAs (lncRNAs) has garnered significant attention due to their critical roles in gene regulation and disease mechanisms. However, the complexity and diversity of lncRNA sequences, along with the limited knowledge of their functional mechanisms and the regulation of their expressions, pose significant challenges to lncRNA studies. Given the tremendous success of large language models (LLMs) in capturing complex dependencies in sequential data, this study aims to systematically explore the potential and limitations of LLMs in the sequence analysis related to the transcriptional regulation of lncRNA genes. Our extensive experiments demonstrated promising performance of fine-tuned genome foundation models on progressively complex tasks. Furthermore, we conducted an insightful analysis of the critical impact of task complexity, model selection, data quality, and biological interpretability for the studies of the regulation of lncRNA gene expression.",
        "subjects": [
            "q-bio.GN",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03562",
        "abstract url": "https://arxiv.org/abs/2411.03562",
        "title": "Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level",
        "rating": "-1.5",
        "keywords": [
            [
                "tabular"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We introduce Agent K v1.0, an end-to-end autonomous data science agent designed to automate, optimise, and generalise across diverse data science tasks. Fully automated, Agent K v1.0 manages the entire data science life cycle by learning from experience. It leverages a highly flexible structured reasoning framework to enable it to dynamically process memory in a nested structure, effectively learning from accumulated experience stored to handle complex reasoning tasks. It optimises long- and short-term memory by selectively storing and retrieving key information, guiding future decisions based on environmental rewards. This iterative approach allows it to refine decisions without fine-tuning or backpropagation, achieving continuous improvement through experiential learning. We evaluate our agent's apabilities using Kaggle competitions as a case study. Following a fully automated protocol, Agent K v1.0 systematically addresses complex and multimodal data science tasks, employing Bayesian optimisation for hyperparameter tuning and feature engineering. Our new evaluation framework rigorously assesses Agent K v1.0's end-to-end capabilities to generate and send submissions starting from a Kaggle competition URL. Results demonstrate that Agent K v1.0 achieves a 92.5\\% success rate across tasks, spanning tabular, computer vision, NLP, and multimodal domains. When benchmarking against 5,856 human Kaggle competitors by calculating Elo-MMR scores for each, Agent K v1.0 ranks in the top 38\\%, demonstrating an overall skill level comparable to Expert-level users. Notably, its Elo-MMR score falls between the first and third quartiles of scores achieved by human Grandmasters. Furthermore, our results indicate that Agent K v1.0 has reached a performance level equivalent to Kaggle Grandmaster, with a record of 6 gold, 3 silver, and 7 bronze medals, as defined by Kaggle's progression system.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03585",
        "abstract url": "https://arxiv.org/abs/2411.03585",
        "title": "Potential Use of IoT Distance Measurement Tool in Boule Sports",
        "rating": "-1.5",
        "keywords": [
            [
                "IoT"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "In Petanque, each player aims to throw the boule closer to the jack. The closest boule to the jack among players will score the point. Currently, the distance of the boule to the jack is still measured using manual measurement tools such as measuring tape, string, and calipers. The manual measurement method is considered time-consuming and prone to inconsistent reading, which the ordinary referees and players conduct. A steady hand is required to hold the tape at two ends while squatting or kneeling. The technique of reading the measurement is also important to determine the accuracy of the length. This project aims to design and develop a prototype device that can measure the distance between jack and boule using a microcontroller and ultrasonic sensor technology. The device is expected to provide an instant measurement of the distance between the jack and the boule. The measurement data can be displayed on the mobile device to ease the user to view the result. This prototype device also counts the score points and determines the winner.",
        "subjects": [
            "cs.NI",
            "cs.CY"
        ],
        "comment": "10 pages"
    },
    {
        "paper id": "2411.03588",
        "abstract url": "https://arxiv.org/abs/2411.03588",
        "title": "An Experimental Study on Decomposition-Based Deep Ensemble Learning for Traffic Flow Forecasting",
        "rating": "-1.5",
        "keywords": [
            [
                "Forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Traffic flow forecasting is a crucial task in intelligent transport systems. Deep learning offers an effective solution, capturing complex patterns in time-series traffic flow data to enable the accurate prediction. However, deep learning models are prone to overfitting the intricate details of flow data, leading to poor generalisation. Recent studies suggest that decomposition-based deep ensemble learning methods may address this issue by breaking down a time series into multiple simpler signals, upon which deep learning models are built and ensembled to generate the final prediction. However, few studies have compared the performance of decomposition-based ensemble methods with non-decomposition-based ones which directly utilise raw time-series data. This work compares several decomposition-based and non-decomposition-based deep ensemble learning methods. Experimental results on three traffic datasets demonstrate the superiority of decomposition-based ensemble methods, while also revealing their sensitivity to aggregation strategies and forecasting horizons.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.NE"
        ],
        "comment": "This work has been accepted by the 2024 Australasian Joint Conference on Artificial Intelligence (AJCAI 2024)"
    },
    {
        "paper id": "2411.03620",
        "abstract url": "https://arxiv.org/abs/2411.03620",
        "title": "A Subsampling Based Neural Network for Spatial Data",
        "rating": "-1.5",
        "keywords": [
            [
                "satellite"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The application of deep neural networks in geospatial data has become a trending research problem in the present day. A significant amount of statistical research has already been introduced, such as generalized least square optimization by incorporating spatial variance-covariance matrix, considering basis functions in the input nodes of the neural networks, and so on. However, for lattice data, there is no available literature about the utilization of asymptotic analysis of neural networks in regression for spatial data. This article proposes a consistent localized two-layer deep neural network-based regression for spatial data. We have proved the consistency of this deep neural network for bounded and unbounded spatial domains under a fixed sampling design of mixed-increasing spatial regions. We have proved that its asymptotic convergence rate is faster than that of \\cite{zhan2024neural}'s neural network and an improved generalization of \\cite{shen2023asymptotic}'s neural network structure. We empirically observe the rate of convergence of discrepancy measures between the empirical probability distribution of observed and predicted data, which will become faster for a less smooth spatial surface. We have applied our asymptotic analysis of deep neural networks to the estimation of the monthly average temperature of major cities in the USA from its satellite image. This application is an effective showcase of non-linear spatial regression. We demonstrate our methodology with simulated lattice data in various scenarios.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02815",
        "abstract url": "https://arxiv.org/abs/2411.02815",
        "title": "Artificial Intelligence-Enhanced Couinaud Segmentation for Precision Liver Cancer Therapy",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "surgery",
                "survival",
                "CT",
                "Cancer"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Precision therapy for liver cancer necessitates accurately delineating liver sub-regions to protect healthy tissue while targeting tumors, which is essential for reducing recurrence and improving survival rates. However, the segmentation of hepatic segments, known as Couinaud segmentation, is challenging due to indistinct sub-region boundaries and the need for extensive annotated datasets. This study introduces LiverFormer, a novel Couinaud segmentation model that effectively integrates global context with low-level local features based on a 3D hybrid CNN-Transformer architecture. Additionally, a registration-based data augmentation strategy is equipped to enhance the segmentation performance with limited labeled data. Evaluated on CT images from 123 patients, LiverFormer demonstrated high accuracy and strong concordance with expert annotations across various metrics, allowing for enhanced treatment planning for surgery and radiation therapy. It has great potential to reduces complications and minimizes potential damages to surrounding tissue, leading to improved outcomes for patients undergoing complex liver cancer treatments.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02843",
        "abstract url": "https://arxiv.org/abs/2411.02843",
        "title": "Advances in Photoacoustic Imaging Reconstruction and Quantitative Analysis for Biomedical Applications",
        "rating": "-2",
        "keywords": [
            [
                "depth"
            ],
            [
                "Biomedical",
                "clinical",
                "physiological"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Photoacoustic imaging (PAI) represents an innovative biomedical imaging modality that harnesses the advantages of optical resolution and acoustic penetration depth while ensuring enhanced safety. Despite its promising potential across a diverse array of preclinical and clinical applications, the clinical implementation of PAI faces significant challenges, including the trade-off between penetration depth and spatial resolution, as well as the demand for faster imaging speeds. This paper explores the fundamental principles underlying PAI, with a particular emphasis on three primary implementations: photoacoustic computed tomography (PACT), photoacoustic microscopy (PAM), and photoacoustic endoscopy (PAE). We undertake a critical assessment of their respective strengths and practical limitations. Furthermore, recent developments in utilizing conventional or deep learning (DL) methodologies for image reconstruction and artefact mitigation across PACT, PAM, and PAE are outlined, demonstrating considerable potential to enhance image quality and accelerate imaging processes. Furthermore, this paper examines the recent developments in quantitative analysis within PAI, including the quantification of haemoglobin concentration, oxygen saturation, and other physiological parameters within tissues. Finally, our discussion encompasses current trends and future directions in PAI research while emphasizing the transformative impact of deep learning on advancing PAI.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02888",
        "abstract url": "https://arxiv.org/abs/2411.02888",
        "title": "A Symmetric Dynamic Learning Framework for Diffeomorphic Medical Image Registration",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Medical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Diffeomorphic image registration is crucial for various medical imaging applications because it can preserve the topology of the transformation. This study introduces DCCNN-LSTM-Reg, a learning framework that evolves dynamically and learns a symmetrical registration path by satisfying a specified control increment system. This framework aims to obtain symmetric diffeomorphic deformations between moving and fixed images. To achieve this, we combine deep learning networks with diffeomorphic mathematical mechanisms to create a continuous and dynamic registration architecture, which consists of multiple Symmetric Registration (SR) modules cascaded on five different scales. Specifically, our method first uses two U-nets with shared parameters to extract multiscale feature pyramids from the images. We then develop an SR-module comprising a sequential CNN-LSTM architecture to progressively correct the forward and reverse multiscale deformation fields using control increment learning and the homotopy continuation technique. Through extensive experiments on three 3D registration tasks, we demonstrate that our method outperforms existing approaches in both quantitative and qualitative evaluations.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "12 pages,7 figures"
    },
    {
        "paper id": "2411.02914",
        "abstract url": "https://arxiv.org/abs/2411.02914",
        "title": "Exploring the Interplay Between Video Generation and World Models in Autonomous Driving: A Survey",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "Autonomous Driving"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "World models and video generation are pivotal technologies in the domain of autonomous driving, each playing a critical role in enhancing the robustness and reliability of autonomous systems. World models, which simulate the dynamics of real-world environments, and video generation models, which produce realistic video sequences, are increasingly being integrated to improve situational awareness and decision-making capabilities in autonomous vehicles. This paper investigates the relationship between these two technologies, focusing on how their structural parallels, particularly in diffusion-based models, contribute to more accurate and coherent simulations of driving scenarios. We examine leading works such as JEPA, Genie, and Sora, which exemplify different approaches to world model design, thereby highlighting the lack of a universally accepted definition of world models. These diverse interpretations underscore the field's evolving understanding of how world models can be optimized for various autonomous driving tasks. Furthermore, this paper discusses the key evaluation metrics employed in this domain, such as Chamfer distance for 3D scene reconstruction and Fr\u00e9chet Inception Distance (FID) for assessing the quality of generated video content. By analyzing the interplay between video generation and world models, this survey identifies critical challenges and future research directions, emphasizing the potential of these technologies to jointly advance the performance of autonomous driving systems. The findings presented in this paper aim to provide a comprehensive understanding of how the integration of video generation and world models can drive innovation in the development of safer and more reliable autonomous vehicles.",
        "subjects": [
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02938",
        "abstract url": "https://arxiv.org/abs/2411.02938",
        "title": "Multi-Modal 3D Scene Graph Updater for Shared and Dynamic Environments",
        "rating": "-2",
        "keywords": [
            [
                "VLMs"
            ],
            [
                "3D"
            ],
            [
                "robot"
            ],
            [
                "Graph"
            ]
        ],
        "abstract": "The advent of generalist Large Language Models (LLMs) and Large Vision Models (VLMs) have streamlined the construction of semantically enriched maps that can enable robots to ground high-level reasoning and planning into their representations. One of the most widely used semantic map formats is the 3D Scene Graph, which captures both metric (low-level) and semantic (high-level) information. However, these maps often assume a static world, while real environments, like homes and offices, are dynamic. Even small changes in these spaces can significantly impact task performance. To integrate robots into dynamic environments, they must detect changes and update the scene graph in real-time. This update process is inherently multimodal, requiring input from various sources, such as human agents, the robot's own perception system, time, and its actions. This work proposes a framework that leverages these multimodal inputs to maintain the consistency of scene graphs during real-time operation, presenting promising initial results and outlining a roadmap for future research.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "This paper has been accepted at the Workshop on Lifelong Learning for Home Robots at the 8th Conference on Robot Learning (CoRL 2024), Munich, Germany"
    },
    {
        "paper id": "2411.02951",
        "abstract url": "https://arxiv.org/abs/2411.02951",
        "title": "LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent Diffusion Prior",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "MRI"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Diffusion model, as a powerful generative model, has found a wide range of applications including MRI reconstruction. However, most existing diffusion model-based MRI reconstruction methods operate directly in pixel space, which makes their optimization and inference computationally expensive. Latent diffusion models were introduced to address this problem in natural image processing, but directly applying them to MRI reconstruction still faces many challenges, including the lack of control over the generated results, the adaptability of Variational AutoEncoder (VAE) to MRI, and the exploration of applicable data consistency in latent space. To address these challenges, a Latent Diffusion Prior based undersampled MRI reconstruction (LDPM) method is proposed. A sketcher module is utilized to provide appropriate control and balance the quality and fidelity of the reconstructed MR images. A VAE adapted for MRI tasks (MR-VAE) is explored, which can serve as the backbone for future MR-related tasks. Furthermore, a variation of the DDIM sampler, called the Dual-Stage Sampler, is proposed to achieve high-fidelity reconstruction in the latent space. The proposed method achieves competitive results on fastMRI datasets, and the effectiveness of each module is demonstrated in ablation experiments.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02964",
        "abstract url": "https://arxiv.org/abs/2411.02964",
        "title": "Speaker Emotion Recognition: Leveraging Self-Supervised Models for Feature Extraction Using Wav2Vec2 and HuBERT",
        "rating": "-2",
        "keywords": [
            [
                "SVM"
            ],
            [
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Speech is the most natural way of expressing ourselves as humans. Identifying emotion from speech is a nontrivial task due to the ambiguous definition of emotion itself. Speaker Emotion Recognition (SER) is essential for understanding human emotional behavior. The SER task is challenging due to the variety of speakers, background noise, complexity of emotions, and speaking styles. It has many applications in education, healthcare, customer service, and Human-Computer Interaction (HCI). Previously, conventional machine learning methods such as SVM, HMM, and KNN have been used for the SER task. In recent years, deep learning methods have become popular, with convolutional neural networks and recurrent neural networks being used for SER tasks. The input of these methods is mostly spectrograms and hand-crafted features. In this work, we study the use of self-supervised transformer-based models, Wav2Vec2 and HuBERT, to determine the emotion of speakers from their voice. The models automatically extract features from raw audio signals, which are then used for the classification task. The proposed solution is evaluated on reputable datasets, including RAVDESS, SHEMO, SAVEE, AESDD, and Emo-DB. The results show the effectiveness of the proposed method on different datasets. Moreover, the model has been used for real-world applications like call center conversations, and the results demonstrate that the model accurately predicts emotions.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "cs.LG",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02972",
        "abstract url": "https://arxiv.org/abs/2411.02972",
        "title": "Exploring Seasonal Variability in the Context of Neural Radiance Fields for 3D Reconstruction on Satellite Imagery",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "NeRF",
                "Radiance Fields"
            ],
            [
                "Satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this work, the seasonal predictive capabilities of Neural Radiance Fields (NeRF) applied to satellite images are investigated. Focusing on the utilization of satellite data, the study explores how Sat-NeRF, a novel approach in computer vision, performs in predicting seasonal variations across different months. Through comprehensive analysis and visualization, the study examines the model's ability to capture and predict seasonal changes, highlighting specific challenges and strengths. Results showcase the impact of the sun direction on predictions, revealing nuanced details in seasonal transitions, such as snow cover, color accuracy, and texture representation in different landscapes. Given these results, we propose Planet-NeRF, an extension to Sat-NeRF capable of incorporating seasonal variability through a set of month embedding vectors. Comparative evaluations reveal that Planet-NeRF outperforms prior models in the case where seasonal changes are present. The extensive evaluation combined with the proposed method offers promising avenues for future research in this domain.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02993",
        "abstract url": "https://arxiv.org/abs/2411.02993",
        "title": "Empowering Library Users: Creative Strategies for Engagement and Innovation",
        "rating": "-2",
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "This study investigated the integration of cutting-edge technologies and methodologies for creating dynamic, user-centered library environments. In creative strategies for engagement and innovation, library users must be empowered to undertake the new role of modernizing library services and enhancing user experiences. It also enhances the information management and user engagement. This can be attained from personalized approaches, such as recommendation systems to interactive platforms that will have effective experiences tailored to users of different natures. It investigates the consumer engagement practices of enthusiasm, sharing, and learning about their roles in cognitive, affective, and behavioural engagements. Combined, these new approaches will help promote learning, interaction, and growth, add value, and have a more positive impact on users. The challenge for libraries in this rapidly changing, technologically advancing, and digitally networked world, with a base of expectant users, is to remain relevant and engaging. This study discusses innovative strategies for empowering library users and enhancing their engagement through creative and technological approaches. This investigation was conducted to integrate cutting-edge technologies and methodologies into creating dynamic library settings that are user-centered and foster learning, interaction, and personal growth.",
        "subjects": [
            "cs.DL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02998",
        "abstract url": "https://arxiv.org/abs/2411.02998",
        "title": "Accelerating Task Generalisation with Multi-Level Hierarchical Options",
        "rating": "-2",
        "keywords": [
            [
                "depth"
            ],
            [
                "tabular"
            ],
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICLR"
            ]
        ],
        "abstract": "Creating reinforcement learning agents that generalise effectively to new tasks is a key challenge in AI research. This paper introduces Fracture Cluster Options (FraCOs), a multi-level hierarchical reinforcement learning method that achieves state-of-the-art performance on difficult generalisation tasks. FraCOs identifies patterns in agent behaviour and forms options based on the expected future usefulness of those patterns, enabling rapid adaptation to new tasks. In tabular settings, FraCOs demonstrates effective transfer and improves performance as it grows in hierarchical depth. We evaluate FraCOs against state-of-the-art deep reinforcement learning algorithms in several complex procedurally generated environments. Our results show that FraCOs achieves higher in-distribution and out-of-distribution performance than competitors.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": "10 pages, under review for ICLR 2025"
    },
    {
        "paper id": "2411.03015",
        "abstract url": "https://arxiv.org/abs/2411.03015",
        "title": "Constitutive Models for Active Skeletal Muscle: Review, Comparison, and Application in a Novel Continuum Shoulder Model",
        "rating": "-2",
        "keywords": [
            [
                "biomechanics",
                "physiological"
            ]
        ],
        "abstract": "The shoulder joint is one of the functionally and anatomically most sophisticated articular systems in the human body. Both complex movement patterns and the stabilization of the highly mobile joint rely on intricate three-dimensional interactions among various components. Continuum-based finite element models can capture such complexity, and are thus particularly relevant in shoulder biomechanics. Considering their role as active joint stabilizers and force generators, skeletal muscles require special attention regarding their constitutive description. In this contribution, we propose a constitutive description to model active skeletal muscle within complex musculoskeletal systems, focusing on a novel continuum shoulder model. We thoroughly review existing material models before analyzing three selected ones in detail: an active-stress, an active-strain, and a generalized active-strain approach. To establish a basis for comparison, we identify the material parameters based on experimental stress-strain data obtained under various active and passive loading conditions. We discuss the concepts to incorporate active contractile behavior from a mathematical and physiological perspective, address analytical and numerical challenges arising from the mathematical formulations, and analyze the included biophysical principles of force generation in terms of physiological correctness and relevance for human shoulder modeling. Based on these insights, we present an improved constitutive model combining the studied models' most promising and relevant features. Using the example of a fusiform muscle, we investigate force generation, deformation, and kinematics during active isometric and free contractions. Eventually, we demonstrate the applicability of the suggested material model in a novel continuum mechanical model of the human shoulder.",
        "subjects": [
            "cs.CE",
            "physics.bio-ph",
            "physics.med-ph"
        ],
        "comment": "Submitted to the International Journal for Numerical Methods in Biomedical Engineering. Supplementary material is provided as an ancillary file"
    },
    {
        "paper id": "2411.03044",
        "abstract url": "https://arxiv.org/abs/2411.03044",
        "title": "Evaluation of handwriting kinematics and pressure for differential diagnosis of Parkinson's disease",
        "rating": "-2",
        "keywords": [
            [
                "SVM"
            ],
            [
                "diagnosis",
                "disease"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Objective: We present the PaHaW Parkinson's disease handwriting database, consisting of handwriting samples from Parkinson's disease (PD) patients and healthy controls. Our goal is to show that kinematic features and pressure features in handwriting can be used for the differential diagnosis of PD. Methods and Material: The database contains records from 37 PD patients and 38 healthy controls performing eight different handwriting tasks. The tasks include drawing an Archimedean spiral, repetitively writing orthographically simple syllables and words, and writing of a sentence. In addition to the conventional kinematic features related to the dynamics of handwriting, we investigated new pressure features based on the pressure exerted on the writing surface. To discriminate between PD patients and healthy subjects, three different classifiers were compared: K-nearest neighbors (K-NN), ensemble AdaBoost classifier, and support vector machines (SVM). Results: For predicting PD based on kinematic and pressure features of handwriting, the best performing model was SVM with classification accuracy of Pacc = 81.3% (sensitivity Psen = 87.4% and specificity of Pspe = 80.9%). When evaluated separately, pressure features proved to be relevant for PD diagnosis, yielding Pacc = 82.5% compared to Pacc = 75.4% using kinematic features. Conclusion: Experimental results showed that an analysis of kinematic and pressure features during handwriting can help assess subtle characteristics of handwriting and discriminate between PD patients and healthy controls.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "23 pages"
    },
    {
        "paper id": "2411.03048",
        "abstract url": "https://arxiv.org/abs/2411.03048",
        "title": "UNet: A Generic and Reliable Multi-UAV Communication and Networking Architecture for Heterogeneous Applications",
        "rating": "-2",
        "keywords": [
            [
                "UAV"
            ]
        ],
        "abstract": "The rapid growth of UAV applications necessitates a robust communication and networking architecture capable of addressing the diverse requirements of various applications concurrently, rather than relying on application-specific solutions. This paper proposes a generic and reliable multi-UAV communication and networking architecture designed to support the varying demands of heterogeneous applications, including short-range and long-range communication, star and mesh topologies, different data rates, and multiple wireless standards. Our architecture accommodates both adhoc and infrastructure networks, ensuring seamless connectivity throughout the network. Additionally, we present the design of a multi-protocol UAV gateway that enables interoperability among various communication protocols. Furthermore, we introduce a data processing and service layer framework with a graphical user interface of a ground control station that facilitates remote control and monitoring from any location at any time. We practically implemented the proposed architecture and evaluated its performance using different metrics, demonstrating its effectiveness.",
        "subjects": [
            "cs.NI",
            "cs.RO"
        ],
        "comment": "11 pages, 20 figures, Journal paper"
    },
    {
        "paper id": "2411.03086",
        "abstract url": "https://arxiv.org/abs/2411.03086",
        "title": "HFGaussian: Learning Generalizable Gaussian Human with Integrated Human Features",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "Gaussian splatting",
                "skeleton"
            ],
            [
                "biomechanical"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in radiance field rendering show promising results in 3D scene representation, where Gaussian splatting-based techniques emerge as state-of-the-art due to their quality and efficiency. Gaussian splatting is widely used for various applications, including 3D human representation. However, previous 3D Gaussian splatting methods either use parametric body models as additional information or fail to provide any underlying structure, like human biomechanical features, which are essential for different applications. In this paper, we present a novel approach called HFGaussian that can estimate novel views and human features, such as the 3D skeleton, 3D key points, and dense pose, from sparse input images in real time at 25 FPS. The proposed method leverages generalizable Gaussian splatting technique to represent the human subject and its associated features, enabling efficient and generalizable reconstruction. By incorporating a pose regression network and the feature splatting technique with Gaussian splatting, HFGaussian demonstrates improved capabilities over existing 3D human methods, showcasing the potential of 3D human representations with integrated biomechanics. We thoroughly evaluate our HFGaussian method against the latest state-of-the-art techniques in human Gaussian splatting and pose estimation, demonstrating its real-time, state-of-the-art performance.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03098",
        "abstract url": "https://arxiv.org/abs/2411.03098",
        "title": "Local Lesion Generation is Effective for Capsule Endoscopy Image Data Augmentation in a Limited Data Setting",
        "rating": "-2",
        "keywords": [
            [
                "GAN",
                "Inpainting",
                "Image Editing"
            ],
            [
                "medical",
                "Lesion"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Limited medical imaging datasets challenge deep learning models by increasing risks of overfitting and reduced generalization, particularly in Generative Adversarial Networks (GANs), where discriminators may overfit, leading to training divergence. This constraint also impairs classification models trained on small datasets. Generative Data Augmentation (GDA) addresses this by expanding training datasets with synthetic data, although it requires training a generative model. We propose and evaluate two local lesion generation approaches to address the challenge of augmenting small medical image datasets. The first approach employs the Poisson Image Editing algorithm, a classical image processing technique, to create realistic image composites that outperform current state-of-the-art methods. The second approach introduces a novel generative method, leveraging a fine-tuned Image Inpainting GAN to synthesize realistic lesions within specified regions of real training images. A comprehensive comparison of the two proposed methods demonstrates that effective local lesion generation in a data-constrained setting allows for reaching new state-of-the-art results in capsule endoscopy lesion classification. Combination of our techniques achieves a macro F1-score of 33.07%, surpassing the previous best result by 7.84 percentage points (p.p.) on the highly imbalanced Kvasir Capsule Dataset, a benchmark for capsule endoscopy. To the best of our knowledge, this work is the first to apply a fine-tuned Image Inpainting GAN for GDA in medical imaging, demonstrating that an image-conditional GAN can be adapted effectively to limited datasets to generate high-quality examples, facilitating effective data augmentation. Additionally, we show that combining this GAN-based approach with classical image processing techniques further enhances the results.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "45 pages, 27 figures"
    },
    {
        "paper id": "2411.03149",
        "abstract url": "https://arxiv.org/abs/2411.03149",
        "title": "Hardware for converting floating-point to the microscaling (MX) format",
        "rating": "-2",
        "keywords": [
            [
                "FPGA"
            ]
        ],
        "abstract": "This paper proposes hardware converters for the microscaling format (MX-format), a reduced representation of floating-point numbers. We present an algorithm and a memory-free hardware model for converting 32 single-precision floating-point numbers to MX-format. The proposed model supports six different types of MX-format: E5M2, E4M3, E3M2, E2M3, E2M1, and INT8. The conversion process consists of three steps: calculating the maximum absolute value among 32 inputs, generating a shared scale, and producing 32 outputs in the selected MX-format type. The hardware converters were implemented in FPGA, and experimental results demonstrate.",
        "subjects": [
            "cs.AR"
        ],
        "comment": "this 6-page paper was never published or submitted anywhere. Submitted 05.11.2024"
    },
    {
        "paper id": "2411.03173",
        "abstract url": "https://arxiv.org/abs/2411.03173",
        "title": "A Stochastic Dynamic Network Model of the Space Environment",
        "rating": "-2",
        "keywords": [
            [
                "forecast"
            ]
        ],
        "abstract": "This work proposes to model the space environment as a stochastic dynamic network where each node is a group of objects of a given class, or species, and their relationship is represented by stochastic links. A set of stochastic dynamic equations, governing the evolution of the network, are derived from the network structure and topology. It will be shown that the proposed system of stochastic dynamic equations well reproduces existing results on the evolution of the space environment. The analysis of the structure of the network and relationships among node can help to understand which species of objects and orbit regimes are more critical and affect the most the future evolution of the space environment. In analogy with ecological networks, we develop a theory of the carrying capacity of space based on the stability of equilibria of the network dynamics. Some examples are presented starting from the current population of resident objects and different launch traffic forecast models. It will be shown how the proposed network model can be used to study the effect of the adoption of different policies on the execution of collision avoidance and post mission disposal manoeuvres.",
        "subjects": [
            "math.DS",
            "cs.CE",
            "physics.comp-ph"
        ],
        "comment": "40 pages, 28 figures, 8 tables"
    },
    {
        "paper id": "2411.03203",
        "abstract url": "https://arxiv.org/abs/2411.03203",
        "title": "Statistical Analysis to Support CSI-Based Sensing Methods",
        "rating": "-2",
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "Building upon the foundational work of the Bachelor's Degree Thesis titled \"Analysis and Characterization of Wi-Fi Channel State Information'', this thesis significantly advances the research by conducting an in-depth analysis of CSIs, offering new insights that extend well beyond the original study. The goal of this work is to broaden the mathematical and statistical representation of a wireless channel through the study of CSI behavior and evolution over time and frequency. CSI provides a high-level description of the behavior of a signal propagating from a transmitter to a receiver, thereby representing the structure of the environment where the signal propagates. This knowledge can be used to perform ambvient sensing, a technique that extracts relevant information about the surroundings of the receiver from the properties of the received signal, which are affected by interactions with the surfaces of the objects within the analyzed environment. Ambient sensing already plays an essential role in new wireless networks such as 5G and Beyond 5G; its use Joint Communication and Sensing applications and for the optimization of signal propagation through beamforming could also enable the implementation of efficient cooperative ambient sensing in vehicular networks, facilitating Cooperative Perception and, consequently, increasing road safety. Due to the lack of research on CSI characterization, this study aims to begin analyzing the structure of CSI traces collected in a controlled experimental environment and to describe their statistical properties. The results of such characterization could provide mathematical support for environment classification and movement recognition tasks that are currently performed only through Machine Learning techniques, introducing instead efficient, dedicated algorithms.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "Master's Degree Thesis. Academic Year 2023/24. University of Brescia (Italy)"
    },
    {
        "paper id": "2411.03206",
        "abstract url": "https://arxiv.org/abs/2411.03206",
        "title": "Statistical Radar Cross Section Characterization for Indoor Factory Targets",
        "rating": "-2",
        "keywords": [
            [
                "Radar"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "In this work, we statistically analyze the radar cross section (RCS) of different test targets present in an indoor factory (InF) scenario specified by 3rd Generation Partnership Project considering bistatic configuration. The test targets that we consider are drones, humans, quadruped robot and a robotic arm. We consider two drones of different sizes and five human subjects for RCS characterization. For the drones, we measure the RCS when they are are flying over a given point and while they are rotating over the same point. For human subjects, we measure the RCS while standing still, sitting still and walking. For quadruped robot and robotic arm, we consider a continuous random motion emulating different tasks which they are supposed to perfom in typical InF scenario. We employ different distributions, such as Normal, Lognormal, Gamma, Rician, Weibull, Rayleigh and Exponential to fit the measurement data. From the statistical analysis, we gather that Lognormal distribution can fit all the considered targets in the InF scenario.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03250",
        "abstract url": "https://arxiv.org/abs/2411.03250",
        "title": "DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Tabular"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Recent advancements in large language models (LLMs) have significantly enhanced their knowledge and generative capabilities, leading to a surge of interest in leveraging LLMs for high-quality data synthesis. However, synthetic data generation via prompting LLMs remains challenging due to LLMs' limited understanding of target data distributions and the complexity of prompt engineering, especially for structured formatted data. To address these issues, we introduce DiffLM, a controllable data synthesis framework based on variational autoencoder (VAE), which further (1) leverages diffusion models to reserve more information of original distribution and format structure in the learned latent distribution and (2) decouples the learning of target distribution knowledge from the LLM's generative objectives via a plug-and-play latent feature injection module. As we observed significant discrepancies between the VAE's latent representations and the real data distribution, the latent diffusion module is introduced into our framework to learn a fully expressive latent distribution. Evaluations on seven real-world datasets with structured formatted data (i.e., Tabular, Code and Tool data) demonstrate that DiffLM generates high-quality data, with performance on downstream tasks surpassing that of real data by 2-7 percent in certain cases. The data and code will be publicly available upon completion of internal review.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "17 pages, 8 figures"
    },
    {
        "paper id": "2411.03255",
        "abstract url": "https://arxiv.org/abs/2411.03255",
        "title": "Error Interference in Quantum Simulation",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Understanding algorithmic error accumulation in quantum simulation is crucial due to its fundamental significance and practical applications in simulating quantum many-body system dynamics. Conventional theories typically apply the triangle inequality to provide an upper bound for the error. However, these often yield overly conservative and inaccurate estimates as they neglect error interference -- a phenomenon where errors in different segments can destructively interfere. Here, we introduce a novel method that directly estimates the long-time algorithmic errors with multiple segments, thereby establishing a comprehensive framework for characterizing algorithmic error interference. We identify the sufficient and necessary condition for strict error interference and introduce the concept of approximate error interference, which is more broadly applicable to scenarios such as power-law interaction models, the Fermi-Hubbard model, and higher-order Trotter formulas. Our work demonstrates significant improvements over prior ones and opens new avenues for error analysis in quantum simulation, offering potential advancements in both theoretical algorithm design and experimental implementation of Hamiltonian simulation.",
        "subjects": [
            "quant-ph",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03296",
        "abstract url": "https://arxiv.org/abs/2411.03296",
        "title": "Quantum Communication Advantage in TFNP",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "We exhibit a total search problem whose communication complexity in the quantum SMP (simultaneous message passing) model is exponentially smaller than in the classical two-way randomized model. Moreover, the quantum protocol is computationally efficient and its solutions are classically verifiable, that is, the problem lies in the communication analogue of the class TFNP. Our problem is a bipartite version of a query complexity problem recently introduced by Yamakawa and Zhandry (JACM 2024). We prove the classical lower bound using the structure-vs-randomness paradigm for analyzing communication protocols.",
        "subjects": [
            "quant-ph",
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03303",
        "abstract url": "https://arxiv.org/abs/2411.03303",
        "title": "Monocular Event-Based Vision for Obstacle Avoidance with a Quadrotor",
        "rating": "-2",
        "keywords": [
            [
                "depth",
                "event camera"
            ],
            [
                "flight"
            ]
        ],
        "abstract": "We present the first static-obstacle avoidance method for quadrotors using just an onboard, monocular event camera. Quadrotors are capable of fast and agile flight in cluttered environments when piloted manually, but vision-based autonomous flight in unknown environments is difficult in part due to the sensor limitations of traditional onboard cameras. Event cameras, however, promise nearly zero motion blur and high dynamic range, but produce a very large volume of events under significant ego-motion and further lack a continuous-time sensor model in simulation, making direct sim-to-real transfer not possible. By leveraging depth prediction as a pretext task in our learning framework, we can pre-train a reactive obstacle avoidance events-to-control policy with approximated, simulated events and then fine-tune the perception component with limited events-and-depth real-world data to achieve obstacle avoidance in indoor and outdoor settings. We demonstrate this across two quadrotor-event camera platforms in multiple settings and find, contrary to traditional vision-based works, that low speeds (1m/s) make the task harder and more prone to collisions, while high speeds (5m/s) result in better event-based depth estimation and avoidance. We also find that success rates in outdoor scenes can be significantly higher than in certain indoor scenes.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "18 pages with supplementary"
    },
    {
        "paper id": "2411.03305",
        "abstract url": "https://arxiv.org/abs/2411.03305",
        "title": "Quantum One-Time Protection of any Randomized Algorithm",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "The meteoric rise in power and popularity of machine learning models dependent on valuable training data has reignited a basic tension between the power of running a program locally and the risk of exposing details of that program to the user. At the same time, fundamental properties of quantum states offer new solutions to data and program security that can require strikingly few quantum resources to exploit, and offer advantages outside of mere computational run time. In this work, we demonstrate such a solution with quantum one-time tokens. A quantum one-time token is a quantum state that permits a certain program to be evaluated exactly once. One-time security guarantees, roughly, that the token cannot be used to evaluate the program more than once. We propose a scheme for building quantum one-time tokens for any randomized classical program, which include generative AI models. We prove that the scheme satisfies an interesting definition of one-time security as long as outputs of the classical algorithm have high enough min-entropy, in a black box model. Importantly, the classical program being protected does not need to be implemented coherently on a quantum computer. In fact, the size and complexity of the quantum one-time token is independent of the program being protected, and additional quantum resources serve only to increase the security of the protocol. Due to this flexibility in adjusting the security, we believe that our proposal is parsimonious enough to serve as a promising candidate for a near-term useful demonstration of quantum computing in either the NISQ or early fault tolerant regime.",
        "subjects": [
            "quant-ph",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03368",
        "abstract url": "https://arxiv.org/abs/2411.03368",
        "title": "Personal Data Protection in AI-Native 6G Systems",
        "rating": "-2",
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "As 6G evolves into an AI-native technology, the integration of artificial intelligence (AI) and Generative AI into cellular communication systems presents unparalleled opportunities for enhancing connectivity, network optimization, and personalized services. However, these advancements also introduce significant data protection challenges, as AI models increasingly depend on vast amounts of personal data for training and decision-making. In this context, ensuring compliance with stringent data protection regulations, such as the General Data Protection Regulation (GDPR), becomes critical for the design and operational integrity of 6G networks. These regulations shape key system architecture aspects, including transparency, accountability, fairness, bias mitigation, and data security. This paper identifies and examines the primary data protection risks associated with AI-driven 6G networks, focusing on the complex data flows and processing activities throughout the 6G lifecycle. By exploring these risks, we provide a comprehensive analysis of the potential privacy implications and propose effective mitigation strategies. Our findings stress the necessity of embedding privacy-by-design and privacy-by-default principles in the development of 6G standards to ensure both regulatory compliance and the protection of individual rights.",
        "subjects": [
            "cs.CR",
            "cs.ET",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03416",
        "abstract url": "https://arxiv.org/abs/2411.03416",
        "title": "Accelerating Gaussian Variational Inference for Motion Planning Under Uncertainty",
        "rating": "-2",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "This work addresses motion planning under uncertainty as a stochastic optimal control problem. The path distribution induced by the optimal controller corresponds to a posterior path distribution with a known form. To approximate this posterior, we frame an optimization problem in the space of Gaussian distributions, which aligns with the Gaussian Variational Inference Motion Planning (GVIMP) paradigm introduced in \\cite{yu2023gaussian}. In this framework, the computation bottleneck lies in evaluating the expectation of collision costs over a dense discretized trajectory and computing the marginal covariances. This work exploits the sparse motion planning factor graph, which allows for parallel computing collision costs and Gaussian Belief Propagation (GBP) marginal covariance computation, to introduce a computationally efficient approach to solving GVIMP. We term the novel paradigm as the Parallel Gaussian Variational Inference Motion Planning (P-GVIMP). We validate the proposed framework on various robotic systems, demonstrating significant speed acceleration achieved by leveraging Graphics Processing Units (GPUs) for parallel computation. An open-sourced implementation is presented at https://github.com/hzyu17/VIMP.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages"
    },
    {
        "paper id": "2411.03439",
        "abstract url": "https://arxiv.org/abs/2411.03439",
        "title": "Towards Entropic Constraints on Quantum Speedups",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Some quantum algorithms have \"quantum speedups\": improved time complexity as compared with the best-known classical algorithms for solving the same tasks. Can we understand what fuels these speedups from an entropic perspective? Information theory gives us a multitude of metrics we might choose from to measure how fundamentally 'quantum' is the behavior of a quantum computer running an algorithm. The entanglement entropies for subsystems of a quantum state can be analyzed for subsystems of qubits in a quantum computer throughout the running of an algorithm. Here, a framework for making this entropic analysis is constructed, and performed on a selection of quantum circuits implementing known fast quantum algorithms and subroutines: Grover search, the quantum Fourier transform, and phase estimation. Our results are largely unsatisfactory: known entropy inequalities do not suffice to identify the presence or absence of quantum speedups. Although we know our algorithms must have quantum \"magic\", the Ingleton inequality, which holds for all entropies of subsystems of stabilizer states, is not violated in any of our examples. In some cases, however, monogamy of mutual information, which is obeyed for product states but violated for highly entangled bipartite states such as the $GHZ$ states, fails at some point in the course of our quantum circuits.",
        "subjects": [
            "quant-ph",
            "cs.IT",
            "hep-th"
        ],
        "comment": "18 pages, 11 figures; Submitted for QIP 2025, comments welcome"
    },
    {
        "paper id": "2411.03484",
        "abstract url": "https://arxiv.org/abs/2411.03484",
        "title": "Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature",
        "rating": "-2",
        "keywords": [
            [
                "chemical"
            ]
        ],
        "abstract": "Automated knowledge extraction from scientific literature can potentially accelerate materials discovery. We have investigated an approach for extracting synthesis protocols for reticular materials from scientific literature using large language models (LLMs). To that end, we introduce a Knowledge Extraction Pipeline (KEP) that automatizes LLM-assisted paragraph classification and information extraction. By applying prompt engineering with in-context learning (ICL) to a set of open-source LLMs, we demonstrate that LLMs can retrieve chemical information from PDF documents, without the need for fine-tuning or training and at a reduced risk of hallucination. By comparing the performance of five open-source families of LLMs in both paragraph classification and information extraction tasks, we observe excellent model performance even if only few example paragraphs are included in the ICL prompts. The results show the potential of the KEP approach for reducing human annotations and data curation efforts in automated scientific knowledge extraction.",
        "subjects": [
            "cond-mat.mtrl-sci",
            "cs.IR"
        ],
        "comment": "16 pages"
    },
    {
        "paper id": "2411.03487",
        "abstract url": "https://arxiv.org/abs/2411.03487",
        "title": "Enhancing Exploratory Capability of Visual Navigation Using Uncertainty of Implicit Scene Representation",
        "rating": "-2",
        "keywords": [
            [
                "NeRF"
            ],
            [
                "robot",
                "Navigation"
            ]
        ],
        "abstract": "In the context of visual navigation in unknown scenes, both \"exploration\" and \"exploitation\" are equally crucial. Robots must first establish environmental cognition through exploration and then utilize the cognitive information to accomplish target searches. However, most existing methods for image-goal navigation prioritize target search over the generation of exploratory behavior. To address this, we propose the Navigation with Uncertainty-driven Exploration (NUE) pipeline, which uses an implicit and compact scene representation, NeRF, as a cognitive structure. We estimate the uncertainty of NeRF and augment the exploratory ability by the uncertainty to in turn facilitate the construction of implicit representation. Simultaneously, we extract memory information from NeRF to enhance the robot's reasoning ability for determining the location of the target. Ultimately, we seamlessly combine the two generated abilities to produce navigational actions. Our pipeline is end-to-end, with the environmental cognitive structure being constructed online. Extensive experimental results on image-goal navigation demonstrate the capability of our pipeline to enhance exploratory behaviors, while also enabling a natural transition from the exploration to exploitation phase. This enables our model to outperform existing memory-based cognitive navigation structures in terms of navigation performance.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03500",
        "abstract url": "https://arxiv.org/abs/2411.03500",
        "title": "\u03bb-Tune: Harnessing Large Language Models for Automated Database System Tuning",
        "rating": "-2",
        "keywords": [
            [
                "MySQL"
            ]
        ],
        "abstract": "We introduce \u03bb-Tune, a framework that leverages Large Language Models (LLMs) for automated database system tuning. The design of \u03bb-Tune is motivated by the capabilities of the latest generation of LLMs. Different from prior work, leveraging LLMs to extract tuning hints for single parameters, \u03bb-Tune generates entire configuration scripts, based on a large input document, describing the tuning context. \u03bb-Tune generates alternative configurations, using a principled approach to identify the best configuration, out of a small set of candidates. In doing so, it minimizes reconfiguration overheads and ensures that evaluation costs are bounded as a function of the optimal run time. By treating prompt generation as a cost-based optimization problem, \u03bb-Tune conveys the most relevant context to the LLM while bounding the number of input tokens and, therefore, monetary fees for LLM invocations. We compare \u03bb-Tune to various baselines, using multiple benchmarks and PostgreSQL and MySQL as target systems for tuning, showing that \u03bb-Tune is significantly more robust than prior approaches.",
        "subjects": [
            "cs.DB"
        ],
        "comment": "To be presented at SIGMOD 2025"
    },
    {
        "paper id": "2411.03505",
        "abstract url": "https://arxiv.org/abs/2411.03505",
        "title": "SynthSet: Generative Diffusion Model for Semantic Segmentation in Precision Agriculture",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion",
                "super-resolution"
            ],
            [
                "agricultural"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper introduces a methodology for generating synthetic annotated data to address data scarcity in semantic segmentation tasks within the precision agriculture domain. Utilizing Denoising Diffusion Probabilistic Models (DDPMs) and Generative Adversarial Networks (GANs), we propose a dual diffusion model architecture for synthesizing realistic annotated agricultural data, without any human intervention. We employ super-resolution to enhance the phenotypic characteristics of the synthesized images and their coherence with the corresponding generated masks. We showcase the utility of the proposed method for wheat head segmentation. The high quality of synthesized data underscores the effectiveness of the proposed methodology in generating image-mask pairs. Furthermore, models trained on our generated data exhibit promising performance when tested on an external, diverse dataset of real wheat fields. The results show the efficacy of the proposed methodology for addressing data scarcity for semantic segmentation tasks. Moreover, the proposed approach can be readily adapted for various segmentation tasks in precision agriculture and beyond.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03540",
        "abstract url": "https://arxiv.org/abs/2411.03540",
        "title": "VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation",
        "rating": "-2",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "3D"
            ],
            [
                "Navigation"
            ],
            [
                "graphs"
            ]
        ],
        "abstract": "With the recent rise of Large Language Models (LLMs), Vision-Language Models (VLMs), and other general foundation models, there is growing potential for multimodal, multi-task embodied agents that can operate in diverse environments given only natural language as input. One such application area is indoor navigation using natural language instructions. However, despite recent progress, this problem remains challenging due to the spatial reasoning and semantic understanding required, particularly in arbitrary scenes that may contain many objects belonging to fine-grained classes. To address this challenge, we curate the largest real-world dataset for Vision and Language-guided Action in 3D Scenes (VLA-3D), consisting of over 11.5K scanned 3D indoor rooms from existing datasets, 23.5M heuristically generated semantic relations between objects, and 9.7M synthetically generated referential statements. Our dataset consists of processed 3D point clouds, semantic object and room annotations, scene graphs, navigable free space annotations, and referential language statements that specifically focus on view-independent spatial relations for disambiguating objects. The goal of these features is to aid the downstream task of navigation, especially on real-world systems where some level of robustness must be guaranteed in an open world of changing scenes and imperfect language. We benchmark our dataset with current state-of-the-art models to obtain a performance baseline. All code to generate and visualize the dataset is publicly released, see https://github.com/HaochenZ11/VLA-3D. With the release of this dataset, we hope to provide a resource for progress in semantic 3D scene understanding that is robust to changes and one which will aid the development of interactive indoor navigation systems.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted and presented at the 1st Workshop on Semantic Reasoning and Goal Understanding in Robotics (SemRob), Robotics Science and Systems Conference (RSS 2024)"
    },
    {
        "paper id": "2411.03551",
        "abstract url": "https://arxiv.org/abs/2411.03551",
        "title": "Enhancing Weakly Supervised Semantic Segmentation for Fibrosis via Controllable Image Generation",
        "rating": "-2",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "diagnosing",
                "Disease"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Fibrotic Lung Disease (FLD) is a severe condition marked by lung stiffening and scarring, leading to respiratory decline. High-resolution computed tomography (HRCT) is critical for diagnosing and monitoring FLD; however, fibrosis appears as irregular, diffuse patterns with unclear boundaries, leading to high inter-observer variability and time-intensive manual annotation. To tackle this challenge, we propose DiffSeg, a novel weakly supervised semantic segmentation (WSSS) method that uses image-level annotations to generate pixel-level fibrosis segmentation, reducing the need for fine-grained manual labeling. Additionally, our DiffSeg incorporates a diffusion-based generative model to synthesize HRCT images with different levels of fibrosis from healthy slices, enabling the generation of the fibrosis-injected slices and their paired fibrosis location. Experiments indicate that our method significantly improves the accuracy of pseudo masks generated by existing WSSS methods, greatly reducing the complexity of manual labeling and enhancing the consistency of the generated masks.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03554",
        "abstract url": "https://arxiv.org/abs/2411.03554",
        "title": "Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset",
        "rating": "-2",
        "keywords": [
            [
                "Vision Language",
                "VLMs"
            ],
            [
                "Unlearning"
            ],
            [
                "attacks"
            ],
            [
                "Facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Machine unlearning has emerged as an effective strategy for forgetting specific information in the training data. However, with the increasing integration of visual data, privacy concerns in Vision Language Models (VLMs) remain underexplored. To address this, we introduce Facial Identity Unlearning Benchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly evaluate the effectiveness of unlearning algorithms under the Right to be Forgotten setting. Specifically, we formulate the VLM unlearning task via constructing the Fictitious Facial Identity VQA dataset and apply a two-stage evaluation pipeline that is designed to precisely control the sources of information and their exposure levels. In terms of evaluation, since VLM supports various forms of ways to ask questions with the same semantic meaning, we also provide robust evaluation metrics including membership inference attacks and carefully designed adversarial privacy attacks to evaluate the performance of algorithms. Through the evaluation of four baseline VLM unlearning algorithms within FIUBench, we find that all methods remain limited in their unlearning performance, with significant trade-offs between model utility and forget quality. Furthermore, our findings also highlight the importance of privacy attacks for robust evaluations. We hope FIUBench will drive progress in developing more effective VLM unlearning algorithms.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03557",
        "abstract url": "https://arxiv.org/abs/2411.03557",
        "title": "Shem: A Hardware-Aware Optimization Framework for Analog Computing Systems",
        "rating": "-2",
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "As the demand for efficient data processing escalates, reconfigurable analog hardware which implements novel analog compute paradigms, is promising for energy-efficient computing at the sensing and actuation boundaries. These analog computing platforms embed information in physical properties and then use the physics of materials, devices, and circuits to perform computation. These hardware platforms are more sensitive to nonidealities, such as noise and fabrication variations, than their digital counterparts and accrue high resource costs when programmable elements are introduced. Identifying resource-efficient analog system designs that mitigate these nonidealities is done manually today. While design optimization frameworks have been enormously successful in other fields, such as photonics, they typically either target linear dynamical systems that have closed-form solutions or target a specific differential equation system and then derive the solution through hand analysis. In both cases, time-domain simulation is no longer needed to predict hardware behavior. In contrast, described analog hardware platforms have nonlinear time-evolving dynamics that vary substantially from design to design, lack closed-form solutions, and require the optimizer to consider time explicitly. We present Shem, an optimization framework for analog systems. Shem leverages differentiation methods recently popularized to train neural ODEs to enable the optimization of analog systems that exhibit nonlinear dynamics, noise and mismatch, and discrete behavior. We evaluate Shem on oscillator-based pattern recognizer, CNN edge detector, and transmission-line security primitive design case studies and demonstrate it can improve designs. To our knowledge, the latter two design problems have not been optimized with automated methods before.",
        "subjects": [
            "cs.ET",
            "cs.CE"
        ],
        "comment": "14 pages"
    },
    {
        "paper id": "2411.03565",
        "abstract url": "https://arxiv.org/abs/2411.03565",
        "title": "Upper Mid-Band Channel Measurements and Characterization at 6.75 GHz FR1(C) and 16.95 GHz FR3 in an Indoor Factory Scenario",
        "rating": "-2",
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "This paper presents detailed radio propagation measurements for an indoor factory (InF) environment at 6.75 GHz and 16.95 GHz using a 1 GHz bandwidth channel sounder. Conducted at the NYU MakerSpace in the NYU Tandon School of Engineering campus in Brooklyn, NY, USA, our measurement campaign characterizes a representative small factory with diverse machinery and open workspaces across 12 locations, comprising 5 line-of-sight (LOS) and 7 non-line-of-sight (NLOS) scenarios. Analysis using the close-in (CI) path loss model with a 1 m reference distance reveals path loss exponents (PLE) below 2 in LOS at 6.75 GHz and 16.95 GHz, while in NLOS PLE is similar to free-space propagation. The RMS delay spread (DS) decreases at higher frequencies with a clear frequency dependence. Similarly, RMS angular spread (AS) measurements show wider spreads in NLOS compared to LOS at both frequency bands, with a decreasing trend as frequency increases. These observations in a dense-scatterer environment demonstrate frequency-dependent behavior that deviate from existing industry-standard models. Our findings provide crucial insights into complex propagation mechanisms in factory environments, essential for designing robust industrial wireless networks at upper mid-band frequencies.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "6 pages, 4 figures"
    },
    {
        "paper id": "2411.03568",
        "abstract url": "https://arxiv.org/abs/2411.03568",
        "title": "The American Sign Language Knowledge Graph: Infusing ASL Models with Linguistic Knowledge",
        "rating": "-2",
        "keywords": [
            [
                "Graph"
            ],
            [
                "Sign Language"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Language models for American Sign Language (ASL) could make language technologies substantially more accessible to those who sign. To train models on tasks such as isolated sign recognition (ISR) and ASL-to-English translation, datasets provide annotated video examples of ASL signs. To facilitate the generalizability and explainability of these models, we introduce the American Sign Language Knowledge Graph (ASLKG), compiled from twelve sources of expert linguistic knowledge. We use the ASLKG to train neuro-symbolic models for 3 ASL understanding tasks, achieving accuracies of 91% on ISR, 14% for predicting the semantic features of unseen signs, and 36% for classifying the topic of Youtube-ASL videos.",
        "subjects": [
            "cs.CL",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03576",
        "abstract url": "https://arxiv.org/abs/2411.03576",
        "title": "Hybrid Attention for Robust RGB-T Pedestrian Detection in Real-World Conditions",
        "rating": "-2",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "thermal"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Multispectral pedestrian detection has gained significant attention in recent years, particularly in autonomous driving applications. To address the challenges posed by adversarial illumination conditions, the combination of thermal and visible images has demonstrated its advantages. However, existing fusion methods rely on the critical assumption that the RGB-Thermal (RGB-T) image pairs are fully overlapping. These assumptions often do not hold in real-world applications, where only partial overlap between images can occur due to sensors configuration. Moreover, sensor failure can cause loss of information in one modality. In this paper, we propose a novel module called the Hybrid Attention (HA) mechanism as our main contribution to mitigate performance degradation caused by partial overlap and sensor failure, i.e. when at least part of the scene is acquired by only one sensor. We propose an improved RGB-T fusion algorithm, robust against partial overlap and sensor failure encountered during inference in real-world applications. We also leverage a mobile-friendly backbone to cope with resource constraints in embedded systems. We conducted experiments by simulating various partial overlap and sensor failure scenarios to evaluate the performance of our proposed method. The results demonstrate that our approach outperforms state-of-the-art methods, showcasing its superiority in handling real-world challenges.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Accepted for publication in IEEE Robotics and Automation Letters, October 2024"
    },
    {
        "paper id": "2411.03596",
        "abstract url": "https://arxiv.org/abs/2411.03596",
        "title": "Enhancing the Expressivity of Temporal Graph Networks through Source-Target Identification",
        "rating": "-2",
        "keywords": [
            [
                "Graph"
            ],
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Despite the successful application of Temporal Graph Networks (TGNs) for tasks such as dynamic node classification and link prediction, they still perform poorly on the task of dynamic node affinity prediction -- where the goal is to predict `how much' two nodes will interact in the future. In fact, simple heuristic approaches such as persistent forecasts and moving averages over \\emph{ground-truth labels} significantly and consistently outperform TGNs. Building on this observation, we find that computing heuristics \\textit{over messages} is an equally competitive approach, outperforming TGN and all current temporal graph (TG) models on dynamic node affinity prediction. In this paper, we prove that no formulation of TGN can represent persistent forecasting or moving averages over messages, and propose to enhance the expressivity of TGNs by adding source-target identification to each interaction event message. We show that this modification is required to represent persistent forecasting, moving averages, and the broader class of autoregressive models over messages. Our proposed method, TGNv2, significantly outperforms TGN and all current TG models on all Temporal Graph Benchmark (TGB) dynamic node affinity prediction datasets.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted to NeurIPS Symmetry and Geometry in Neural Representations Workshop 2024"
    },
    {
        "paper id": "2411.03606",
        "abstract url": "https://arxiv.org/abs/2411.03606",
        "title": "Beam Tracking for Full-Duplex User Terminals in Low Earth Orbit Satellite Communication Systems",
        "rating": "-2",
        "keywords": [
            [
                "Satellite"
            ]
        ],
        "abstract": "This paper introduces a novel beam tracking scheme for full-duplex ground user terminals aiming to transmit uplink and receive downlink from two low Earth orbit (LEO) satellites at the same time and same frequency. Our proposed technique leverages observed phenomena from a recent measurement campaign to strategically select transmit and receive beams which couple low self-interference across the satellites' trajectories, thereby enabling in-band full-duplex operation. Our scheme takes a measurement-driven approach, meaning it does not rely on explicit knowledge of the self-interference channel and can inherently account for hardware impairments or other nonidealities. We show that our proposed scheme reliably selects beams which spatially cancel self-interference to below the noise floor, circumventing the need for digital/analog cancellation. Simulation results using satellite and orbital parameters published in 3GPP and FCC filings show that this substantial reduction in self-interference does not prohibitively compromise beamforming gain, allowing the user terminal to attain near-maximal SINRs, thus unlocking full-duplex operation.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03632",
        "abstract url": "https://arxiv.org/abs/2411.03632",
        "title": "Quantum fault tolerance with constant-space and logarithmic-time overheads",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "In a model of fault-tolerant quantum computation with quick and noiseless polyloglog-time auxiliary classical computation, we construct a fault tolerance protocol with constant-space and $\\widetilde{O}(\\log N)$-time overhead, where $\\widetilde{O}(\\cdot)$ hides sub-polylog factors. Our construction utilizes constant-rate quantum locally testable codes (qLTC), new fault-tolerant gadgets on qLTCs and qLDPC codes, and a new analysis framework. In particular, 1) we develop a new simple and self-contained construction of magic state distillation for qubits using qudit quantum Reed-Solomon codes with $(\\log \\frac{1}{\\varepsilon})^\u03b3$ spacetime overhead, where $\u03b3\\rightarrow 0$. 2) We prove that the recent family of almost-good qLTCs of Dinur-Lin-Vidick admit parallel single-shot decoders against adversarial errors of weight scaling with the code distance. 3) We develop logical state preparation and logical gate procedures with $\\widetilde{O}(1)$-spacetime overhead on qLTCs. 4) To combine these ingredients, we introduce a new framework of fault tolerance analysis called the weight enumerator formalism. The framework permits easy formal composition of fault-tolerant gadgets, so we expect it to be of independent interest. Our work gives the lowest spacetime overhead to date, which, for the first time, matches that of classical fault tolerance up to sub-polylog factors. We conjecture this is optimal up to sub-polylog factors.",
        "subjects": [
            "quant-ph",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03635",
        "abstract url": "https://arxiv.org/abs/2411.03635",
        "title": "Digital Twin-Assisted Robust and Adaptive Resource Slicing in LEO Satellite Networks",
        "rating": "-2",
        "keywords": [
            [
                "Satellite"
            ]
        ],
        "abstract": "Resource slicing in low Earth orbit satellite networks (LSN) is essential to support diversified services. In this paper, we investigate a resource slicing problem in LSN to reserve resources in satellites to achieve efficient resource provisioning. To address the challenges of non-stationary service demands, inaccurate prediction, and satellite mobility, we propose an adaptive digital twin (DT)-assisted resource slicing scheme for robust and adaptive resource management in LSN. Specifically, a slice DT, being able to capture the service demand prediction uncertainty through collected service demand data, is constructed to enhance the robustness of resource slicing decisions for dynamic service demands. In addition, the constructed DT can emulate resource slicing decisions for evaluating their performance, enabling adaptive slicing decision updates to efficiently reserve resources in LSN. Simulation results demonstrate that the proposed scheme outperforms benchmark methods, achieving low service demand violations with efficient resource consumption.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "Accepted by IEEE GLOBECOM 2024"
    },
    {
        "paper id": "2411.03638",
        "abstract url": "https://arxiv.org/abs/2411.03638",
        "title": "Adaptive Stereo Depth Estimation with Multi-Spectral Images Across All Lighting Conditions",
        "rating": "-2",
        "keywords": [
            [
                "Depth"
            ],
            [
                "thermal"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Depth estimation under adverse conditions remains a significant challenge. Recently, multi-spectral depth estimation, which integrates both visible light and thermal images, has shown promise in addressing this issue. However, existing algorithms struggle with precise pixel-level feature matching, limiting their ability to fully exploit geometric constraints across different spectra. To address this, we propose a novel framework incorporating stereo depth estimation to enforce accurate geometric constraints. In particular, we treat the visible light and thermal images as a stereo pair and utilize a Cross-modal Feature Matching (CFM) Module to construct a cost volume for pixel-level matching. To mitigate the effects of poor lighting on stereo matching, we introduce Degradation Masking, which leverages robust monocular thermal depth estimation in degraded regions. Our method achieves state-of-the-art (SOTA) performance on the Multi-Spectral Stereo (MS2) dataset, with qualitative evaluations demonstrating high-quality depth maps under varying lighting conditions.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03646",
        "abstract url": "https://arxiv.org/abs/2411.03646",
        "title": "Quantum LDPC Codes of Almost Linear Distance via Homological Products",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "We present new constructions of quantum codes of linear or close-to-linear distance and dimension with low-weight stabilizers. Only a few constructions of such codes were previously known, and were primarily based on a specific operation from homological algebra, namely the balanced product. In contrast, our constructions are based on a more basic and widely used product, namely the homological product (i.e. the tensor product of chain complexes). Our results help address the natural question: When do homological products preserve good code distance? Our first main result constructs asymptotically good $[[N,\u0398(N),\u0398(N)]]$ quantum codes with small polynomial stabilizer weight from homological products of codes with a property called product-expansion. This notion was recently introduced and used to bound the distance of balanced product quantum codes; we apply it instead to homological products. For every $\u03b5>0$, our second main result constructs close-to-linear distance $[[N,N^{1-\u03b5},N^{1-\u03b5}]]$ (subsystem) quantum LDPC codes with constant stabilizer weight from iterated homological products of a constant-sized quantum locally testable code. The key insight here is that by using subsystem codes (but still with constant-weight stabilizers), we can circumvent a particular obstruction that limited the distance of many prior product code constructions to at most $\\tilde{O}(\\sqrt{N})$.",
        "subjects": [
            "quant-ph",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02866",
        "abstract url": "https://arxiv.org/abs/2411.02866",
        "title": "Double Whammy: Stealthy Data Manipulation aided Reconstruction Attack on Graph Federated Learning",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "GNN",
                "Graph"
            ],
            [
                "Attack"
            ],
            [
                "cs.SI",
                "cs.CY"
            ]
        ],
        "abstract": "Recent research has constructed successful graph reconstruction attack (GRA) on GFL. But these attacks are still challenged in aspects of effectiveness and stealth. To address the issues, we propose the first Data Manipulation aided Reconstruction attack on GFL, dubbed as DMan4Rec. The malicious client is born to manipulate its locally collected data to enhance graph stealing privacy from benign ones, so as to construct double whammy on GFL. It differs from previous work in three terms: (1) effectiveness - to fully utilize the sparsity and feature smoothness of the graph, novel penalty terms are designed adaptive to diverse similarity functions for connected and unconnected node pairs, as well as incorporation label smoothing on top of the original cross-entropy loss. (2) scalability - DMan4Rec is capable of both white-box and black-box attacks via training a supervised model to infer the posterior probabilities obtained from limited queries (3) stealthiness - by manipulating the malicious client's node features, it can maintain the overall graph structure's invariance and conceal the attack. Comprehensive experiments on four real datasets and three GNN models demonstrate that DMan4Rec achieves the state-of-the-art (SOTA) attack performance, e.g., the attack AUC and precision improved by 9.2% and 10.5% respectively compared with the SOTA baselines. Particularly, DMan4Rec achieves an AUC score and a precision score of up to 99.59% and 99.56%, respectively in black-box setting. Nevertheless, the complete overlap of the distribution graphs supports the stealthiness of the attack. Besides, DMan4Rec still beats the defensive GFL, which alarms a new threat to GFL.",
        "subjects": [
            "cs.SI",
            "cs.CY"
        ],
        "comment": "The paper is currently being submitted for publication (The submitted journal is TNSE)"
    },
    {
        "paper id": "2411.02975",
        "abstract url": "https://arxiv.org/abs/2411.02975",
        "title": "Transformer-Based Fault-Tolerant Control for Fixed-Wing UAVs Using Knowledge Distillation and In-Context Adaptation",
        "rating": "-2.5",
        "keywords": [
            [
                "Flight"
            ],
            [
                "UAV"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This study presents a transformer-based approach for fault-tolerant control in fixed-wing Unmanned Aerial Vehicles (UAVs), designed to adapt in real time to dynamic changes caused by structural damage or actuator failures. Unlike traditional Flight Control Systems (FCSs) that rely on classical control theory and struggle under severe alterations in dynamics, our method directly maps outer-loop reference values -- altitude, heading, and airspeed -- into control commands using the in-context learning and attention mechanisms of transformers, thus bypassing inner-loop controllers and fault-detection layers. Employing a teacher-student knowledge distillation framework, the proposed approach trains a student agent with partial observations by transferring knowledge from a privileged expert agent with full observability, enabling robust performance across diverse failure scenarios. Experimental results demonstrate that our transformer-based controller outperforms industry-standard FCS and state-of-the-art reinforcement learning (RL) methods, maintaining high tracking accuracy and stability in nominal conditions and extreme failure cases, highlighting its potential for enhancing UAV operational safety and reliability.",
        "subjects": [
            "cs.RO",
            "cs.LG",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03156",
        "abstract url": "https://arxiv.org/abs/2411.03156",
        "title": "Unleashing the power of novel conditional generative approaches for new materials discovery",
        "rating": "-2.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "chemical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "For a very long time, computational approaches to the design of new materials have relied on an iterative process of finding a candidate material and modeling its properties. AI has played a crucial role in this regard, helping to accelerate the discovery and optimization of crystal properties and structures through advanced computational methodologies and data-driven approaches. To address the problem of new materials design and fasten the process of new materials search, we have applied latest generative approaches to the problem of crystal structure design, trying to solve the inverse problem: by given properties generate a structure that satisfies them without utilizing supercomputer powers. In our work we propose two approaches: 1) conditional structure modification: optimization of the stability of an arbitrary atomic configuration, using the energy difference between the most energetically favorable structure and all its less stable polymorphs and 2) conditional structure generation. We used a representation for materials that includes the following information: lattice, atom coordinates, atom types, chemical features, space group and formation energy of the structure. The loss function was optimized to take into account the periodic boundary conditions of crystal structures. We have applied Diffusion models approach, Flow matching, usual Autoencoder (AE) and compared the results of the models and approaches. As a metric for the study, physical PyMatGen matcher was employed: we compare target structure with generated one using default tolerances. So far, our modifier and generator produce structures with needed properties with accuracy 41% and 82% respectively. To prove the offered methodology efficiency, inference have been carried out, resulting in several potentially new structures with formation energy below the AFLOW-derived convex hulls.",
        "subjects": [
            "cond-mat.mtrl-sci",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03163",
        "abstract url": "https://arxiv.org/abs/2411.03163",
        "title": "Efficient Hamiltonian, structure and trace distance learning of Gaussian states",
        "rating": "-2.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this work, we initiate the study of Hamiltonian learning for positive temperature bosonic Gaussian states, the quantum generalization of the widely studied problem of learning Gaussian graphical models. We obtain efficient protocols, both in sample and computational complexity, for the task of inferring the parameters of their underlying quadratic Hamiltonian under the assumption of bounded temperature, squeezing, displacement and maximal degree of the interaction graph. Our protocol only requires heterodyne measurements, which are often experimentally feasible, and has a sample complexity that scales logarithmically with the number of modes. Furthermore, we show that it is possible to learn the underlying interaction graph in a similar setting and sample complexity. Taken together, our results put the status of the quantum Hamiltonian learning problem for continuous variable systems in a much more advanced state when compared to spins, where state-of-the-art results are either unavailable or quantitatively inferior to ours. In addition, we use our techniques to obtain the first results on learning Gaussian states in trace distance with a quadratic scaling in precision and polynomial in the number of modes, albeit imposing certain restrictions on the Gaussian states. Our main technical innovations are several continuity bounds for the covariance and Hamiltonian matrix of a Gaussian state, which are of independent interest, combined with what we call the local inversion technique. In essence, the local inversion technique allows us to reliably infer the Hamiltonian of a Gaussian state by only estimating in parallel submatrices of the covariance matrix whose size scales with the desired precision, but not the number of modes. This way we bypass the need to obtain precise global estimates of the covariance matrix, controlling the sample complexity.",
        "subjects": [
            "quant-ph",
            "cs.LG"
        ],
        "comment": "43 pages, 1 figure"
    },
    {
        "paper id": "2411.03287",
        "abstract url": "https://arxiv.org/abs/2411.03287",
        "title": "The Future of Intelligent Healthcare: A Systematic Analysis and Discussion on the Integration and Impact of Robots Using Large Language Models for Healthcare",
        "rating": "-2.5",
        "keywords": [
            [
                "robotics",
                "robot"
            ],
            [
                "health",
                "Healthcare",
                "clinical"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The potential use of large language models (LLMs) in healthcare robotics can help address the significant demand put on healthcare systems around the world with respect to an aging demographic and a shortage of healthcare professionals. Even though LLMs have already been integrated into medicine to assist both clinicians and patients, the integration of LLMs within healthcare robots has not yet been explored for clinical settings. In this perspective paper, we investigate the groundbreaking developments in robotics and LLMs to uniquely identify the needed system requirements for designing health specific LLM based robots in terms of multi modal communication through human robot interactions (HRIs), semantic reasoning, and task planning. Furthermore, we discuss the ethical issues, open challenges, and potential future research directions for this emerging innovative field.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.ET",
            "cs.HC",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03656",
        "abstract url": "https://arxiv.org/abs/2411.03656",
        "title": "Requirements Engineering for Older Adult Digital Health Software: A Systematic Literature Review",
        "rating": "-2.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "Health"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Growth of the older adult population has led to an increasing interest in technology-supported aged care. However, the area has some challenges such as a lack of caregivers and limitations in understanding the emotional, social, physical, and mental well-being needs of seniors. Furthermore, there is a gap in the understanding between developers and ageing people of their requirements. Digital health can be important in supporting older adults wellbeing, emotional requirements, and social needs. Requirements Engineering (RE) is a major software engineering field, which can help to identify, elicit and prioritize the requirements of stakeholders and ensure that the systems meet standards for performance, reliability, and usability. We carried out a systematic review of the literature on RE for older adult digital health software. This was necessary to show the representatives of the current stage of understanding the needs of older adults in aged care digital health. Using established guidelines outlined by the Kitchenham method, the PRISMA and the PICO guideline, we developed a protocol, followed by the systematic exploration of eight databases. This resulted in 69 primary studies of high relevance, which were subsequently subjected to data extraction, synthesis, and reporting. We highlight key RE processes in digital health software for ageing people. It explored the utilization of technology for older user well-being and care, and the evaluations of such solutions. The review also identified key limitations found in existing primary studies that inspire future research opportunities. The results indicate that requirement gathering and understanding have a significant variation between different studies. The differences are in the quality, depth, and techniques adopted for requirement gathering and these differences are largely due to uneven adoption of RE methods.",
        "subjects": [
            "cs.SE",
            "cs.AI",
            "cs.HC",
            "cs.LG"
        ],
        "comment": "arxiv version of SLR on RE for Older Adult Digital Health Software"
    },
    {
        "paper id": "2411.04150",
        "abstract url": "https://arxiv.org/abs/2411.04150",
        "title": "BAPULM: Binding Affinity Prediction using Language Models",
        "rating": "-2.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "chemical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Identifying drug-target interactions is essential for developing effective therapeutics. Binding affinity quantifies these interactions, and traditional approaches rely on computationally intensive 3D structural data. In contrast, language models can efficiently process sequential data, offering an alternative approach to molecular representation. In the current study, we introduce BAPULM, an innovative sequence-based framework that leverages the chemical latent representations of proteins via ProtT5-XL-U50 and ligands through MolFormer, eliminating reliance on complex 3D configurations. Our approach was validated extensively on benchmark datasets, achieving scoring power (R) values of 0.925 $\\pm$ 0.043, 0.914 $\\pm$ 0.004, and 0.8132 $\\pm$ 0.001 on benchmark1k2101, Test2016_290, and CSAR-HiQ_36, respectively. These findings indicate the robustness and accuracy of BAPULM across diverse datasets and underscore the potential of sequence-based models in-silico drug discovery, offering a scalable alternative to 3D-centric methods for screening potential ligands.",
        "subjects": [
            "q-bio.QM",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02868",
        "abstract url": "https://arxiv.org/abs/2411.02868",
        "title": "iAnomaly: A Toolkit for Generating Performance Anomaly Datasets in Edge-Cloud Integrated Computing Environments",
        "rating": "-3",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "Microservice architectures are increasingly used to modularize IoT applications and deploy them in distributed and heterogeneous edge computing environments. Over time, these microservice-based IoT applications are susceptible to performance anomalies caused by resource hogging (e.g., CPU or memory), resource contention, etc., which can negatively impact their Quality of Service and violate their Service Level Agreements. Existing research on performance anomaly detection in edge computing environments is limited primarily due to the absence of publicly available edge performance anomaly datasets or due to the lack of accessibility of real edge setups to generate necessary data. To address this gap, we propose iAnomaly: a full-system emulator equipped with open-source tools and fully automated dataset generation capabilities to generate labeled normal and anomaly data based on user-defined configurations. We also release a performance anomaly dataset generated using iAnomaly, which captures performance data for several microservice-based IoT applications with heterogeneous QoS and resource requirements while introducing a variety of anomalies. This dataset effectively represents the characteristics found in real edge environments, and the anomalous data in the dataset adheres to the required standards of a high-quality performance anomaly dataset.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "Accepted for publication at the 17th IEEE/ACM International Conference on Utility and Cloud Computing (UCC 2024)"
    },
    {
        "paper id": "2411.02891",
        "abstract url": "https://arxiv.org/abs/2411.02891",
        "title": "Nature's All-in-One: Multitasking Robots Inspired by Dung Beetles",
        "rating": "-3",
        "keywords": [
            [
                "robot"
            ],
            [
                "biomechanical"
            ]
        ],
        "abstract": "Dung beetles impressively coordinate their six legs simultaneously to effectively roll large dung balls. They are also capable of rolling dung balls varying in the weight on different terrains. The mechanisms underlying how their motor commands are adapted to walk and simultaneously roll balls (multitasking behavior) under different conditions remain unknown. Therefore, this study unravels the mechanisms of how dung beetles roll dung balls and adapt their leg movements to stably roll balls over different terrains for multitasking robots. We synthesize a modular neural-based loco-manipulation control inspired by and based on ethological observations of the ball-rolling behavior of dung beetles. The proposed neural-based control contains various neural modules, including a central pattern generator (CPG) module, a pattern formation network (PFN) module, and a robot orientation control (ROC) module. The integrated neural control mechanisms can successfully control a dung beetle-like robot (ALPHA) with biomechanical feet to perform adaptive robust (multitasking) loco-manipulation (walking and ball-rolling) on various terrains (flat and uneven). It can also deal with different ball weights (2.0 and 4.6 kg) and ball types (soft and rigid). The control mechanisms can serve as guiding principles for solving complex sensory-motor coordination for multitasking robots. Furthermore, this study contributes to biological research by enhancing our scientific understanding of sensory-motor coordination for complex adaptive (multitasking) loco-manipulation behavior in animals.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02974",
        "abstract url": "https://arxiv.org/abs/2411.02974",
        "title": "Region-Guided Attack on the Segment Anything Model (SAM)",
        "rating": "-3",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "Attack"
            ],
            [
                "medical"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The Segment Anything Model (SAM) is a cornerstone of image segmentation, demonstrating exceptional performance across various applications, particularly in autonomous driving and medical imaging, where precise segmentation is crucial. However, SAM is vulnerable to adversarial attacks that can significantly impair its functionality through minor input perturbations. Traditional techniques, such as FGSM and PGD, are often ineffective in segmentation tasks due to their reliance on global perturbations that overlook spatial nuances. Recent methods like Attack-SAM-K and UAD have begun to address these challenges, but they frequently depend on external cues and do not fully leverage the structural interdependencies within segmentation processes. This limitation underscores the need for a novel adversarial strategy that exploits the unique characteristics of segmentation tasks. In response, we introduce the Region-Guided Attack (RGA), designed specifically for SAM. RGA utilizes a Region-Guided Map (RGM) to manipulate segmented regions, enabling targeted perturbations that fragment large segments and expand smaller ones, resulting in erroneous outputs from SAM. Our experiments demonstrate that RGA achieves high success rates in both white-box and black-box scenarios, emphasizing the need for robust defenses against such sophisticated attacks. RGA not only reveals SAM's vulnerabilities but also lays the groundwork for developing more resilient defenses against adversarial threats in image segmentation.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03011",
        "abstract url": "https://arxiv.org/abs/2411.03011",
        "title": "Set-Membership Estimation for Fault Diagnosis of Nonlinear Systems",
        "rating": "-3",
        "keywords": [
            [
                "Vehicle"
            ],
            [
                "Diagnosis"
            ]
        ],
        "abstract": "This paper introduces a Fault Diagnosis (Detection, Isolation, and Estimation) method using Set-Membership Estimation (SME) designed for a class of nonlinear systems that are linear to the fault parameters. The methodology advances fault diagnosis by continuously evaluating an estimate of the fault parameter and a feasible parameter set where the true fault parameter belongs. Unlike previous SME approaches, in this work, we address nonlinear systems subjected to both input and output uncertainties by utilizing inclusion functions and interval arithmetic. Additionally, we present an approach to outer-approximate the polytopic description of the feasible parameter set by effectively balancing approximation accuracy with computational efficiency resulting in improved fault detectability. Lastly, we introduce adaptive regularization of the parameter estimates to enhance the estimation process when the input-output data are sparse or non-informative, enhancing fault identifiability. We demonstrate the effectiveness of this method in simulations involving an Autonomous Surface Vehicle in both a path-following and a realistic collision avoidance scenario, underscoring its potential to enhance safety and reliability in critical applications.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03082",
        "abstract url": "https://arxiv.org/abs/2411.03082",
        "title": "Self-supervised cross-modality learning for uncertainty-aware object detection and recognition in applications which lack pre-labelled training data",
        "rating": "-3",
        "keywords": [
            [
                "3D",
                "RGB-D"
            ],
            [
                "robotics"
            ],
            [
                "industrial"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "This paper shows how an uncertainty-aware, deep neural network can be trained to detect, recognise and localise objects in 2D RGB images, in applications lacking annotated train-ng datasets. We propose a self-supervising teacher-student pipeline, in which a relatively simple teacher classifier, trained with only a few labelled 2D thumbnails, automatically processes a larger body of unlabelled RGB-D data to teach a student network based on a modified YOLOv3 architecture. Firstly, 3D object detection with back projection is used to automatically extract and teach 2D detection and localisation information to the student network. Secondly, a weakly supervised 2D thumbnail classifier, with minimal training on a small number of hand-labelled images, is used to teach object category recognition. Thirdly, we use a Gaussian Process GP to encode and teach a robust uncertainty estimation functionality, so that the student can output confidence scores with each categorization. The resulting student significantly outperforms the same YOLO architecture trained directly on the same amount of labelled data. Our GP-based approach yields robust and meaningful uncertainty estimations for complex industrial object classifications. The end-to-end network is also capable of real-time processing, needed for robotics applications. Our method can be applied to many important industrial tasks, where labelled datasets are typically unavailable. In this paper, we demonstrate an example of detection, localisation, and object category recognition of nuclear mixed-waste materials in highly cluttered and unstructured scenes. This is critical for robotic sorting and handling of legacy nuclear waste, which poses complex environmental remediation challenges in many nuclearised nations.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.NE"
        ],
        "comment": "16 pages"
    },
    {
        "paper id": "2411.03114",
        "abstract url": "https://arxiv.org/abs/2411.03114",
        "title": "Investigating the Applicability of a Snapshot Computed Tomography Imaging Spectrometer for the Prediction of Brix and pH of Grapes",
        "rating": "-3",
        "keywords": [
            [
                "hyperspectral imaging"
            ],
            [
                "quality assessment"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, a recently developed snapshot hyperspectral imaging (HSI) system based on Computed Tomography Imaging Spectroscopy (CTIS) is utilized to determine Brix and pH values in Sheegene 20 table grapes through Partial Least Squares Regression (PLSR) modeling. The performance of the CTIS system is compared with that of a state-of-the-art line scan HSI system by imaging 100 grapes across both platforms. Reference measurements of Brix and pH values are obtained directly using a refractometer and a pH meter, as these parameters are essential for assessing the quality of table and wine grapes. The findings indicate that the spectra captured by the CTIS camera correlate well with the reference measurements, despite the system's narrower spectral range. The CTIS camera's advantages, including its lower cost, portability, and reduced susceptibility to motion errors, highlight its potential for promising in-field applications in grape quality assessment.",
        "subjects": [
            "physics.app-ph",
            "cs.CV",
            "physics.optics"
        ],
        "comment": "15 pages, 10 figures"
    },
    {
        "paper id": "2411.03189",
        "abstract url": "https://arxiv.org/abs/2411.03189",
        "title": "Energy-Aware Predictive Motion Planning for Autonomous Vehicles Using a Hybrid Zonotope Constraint Representation",
        "rating": "-3",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "drone"
            ]
        ],
        "abstract": "Uncrewed aerial systems have tightly coupled energy and motion dynamics which must be accounted for by onboard planning algorithms. This work proposes a strategy for coupled motion and energy planning using model predictive control (MPC). A reduced-order linear time-invariant model of coupled energy and motion dynamics is presented. Constrained zonotopes are used to represent state and input constraints, and hybrid zonotopes are used to represent non-convex constraints tied to a map of the environment. The structures of these constraint representations are exploited within a mixed-integer quadratic program solver tailored to MPC motion planning problems. Results apply the proposed methodology to coupled motion and energy utilization planning problems for 1) a hybrid-electric vehicle that must restrict engine usage when flying over regions with noise restrictions, and 2) an electric package delivery drone that must track waysets with both position and battery state of charge requirements. By leveraging the structure-exploiting solver, the proposed mixed-integer MPC formulations can be implemented in real time.",
        "subjects": [
            "eess.SY",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03240",
        "abstract url": "https://arxiv.org/abs/2411.03240",
        "title": "Distributed Quantum Advantage for Local Problems",
        "rating": "-3",
        "keywords": [
            [
                "graph"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "We present the first local problem that shows a super-constant separation between the classical randomized LOCAL model of distributed computing and its quantum counterpart. By prior work, such a separation was known only for an artificial graph problem with an inherently global definition [Le Gall et al. 2019]. We present a problem that we call iterated GHZ, which is defined using only local constraints. Formally, it is a family of locally checkable labeling problems [Naor and Stockmeyer 1995]; in particular, solutions can be verified with a constant-round distributed algorithm. We show that in graphs of maximum degree $\u0394$, any classical (deterministic or randomized) LOCAL model algorithm will require $\u03a9(\u0394)$ rounds to solve the iterated GHZ problem, while the problem can be solved in $1$ round in quantum-LOCAL. We use the round elimination technique to prove that the iterated GHZ problem requires $\u03a9(\u0394)$ rounds for classical algorithms. This is the first work that shows that round elimination is indeed able to separate the two models, and this also demonstrates that round elimination cannot be used to prove lower bounds for quantum-LOCAL. To apply round elimination, we introduce a new technique that allows us to discover appropriate problem relaxations in a mechanical way; it turns out that this new technique extends beyond the scope of the iterated GHZ problem and can be used to e.g. reproduce prior results on maximal matchings [FOCS 2019, PODC 2020] in a systematic manner.",
        "subjects": [
            "cs.DC",
            "cs.CC",
            "quant-ph"
        ],
        "comment": "64 pages, 9 figures"
    },
    {
        "paper id": "2411.03291",
        "abstract url": "https://arxiv.org/abs/2411.03291",
        "title": "Using Assurance Cases to Guide Verification and Validation of Research Software",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "medical"
            ]
        ],
        "abstract": "Research software engineers can use Assurance Cases (ACs) to guide Verification and Validation (VnV) efforts. An AC is a structured argument that a property like correctness holds. We illustrate how ACs can guide VnV activities via a case study of software for automatically extracting the 3D segmentation of the aorta from medical images of the chest. The AC argument suggests that the following evidence is required: comparison to a pseudo-oracle; traceability between requirements, design, code and tests; review of all artifacts by a domain expert with proper credentials; documentation of input assumptions; and a warning that only qualified people should use the software. The case study highlights that code is not the only artifact of interest for building confidence and that making an explicit distinction between software and user responsibilities is useful.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "7 pages, 5 figures"
    },
    {
        "paper id": "2411.03371",
        "abstract url": "https://arxiv.org/abs/2411.03371",
        "title": "Blockchain-Based Multi-Path Mobile Access Point Selection for Secure 5G VANETs",
        "rating": "-3",
        "keywords": [
            [
                "attack"
            ],
            [
                "5G"
            ]
        ],
        "abstract": "This letter presents a blockchain-based multi-path mobile access point (MAP) selection strategy for secure 5G vehicular ad-hoc networks (VANETs). The proposed method leverages blockchain technology for decentralized, transparent, and secure MAP selection, while the multi-path transmission strategy enhances network reliability and reduces communication delays. A trust-based attack detection mechanism is integrated to ensure network security. Simulation results demonstrate that the proposed algorithm reduces both handover frequency and average communication delay by over 80%, and successfully identifies and excludes more than 95% of Sybil nodes, ensuring reliable and secure communication in highly dynamic vehicular environments.",
        "subjects": [
            "cs.CR",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03483",
        "abstract url": "https://arxiv.org/abs/2411.03483",
        "title": "Augmented-Reality Enabled Crop Monitoring with Robot Assistance",
        "rating": "-3",
        "keywords": [
            [
                "robotics",
                "Robot",
                "navigation"
            ],
            [
                "agricultural"
            ]
        ],
        "abstract": "The integration of augmented reality (AR), extended reality (XR), and virtual reality (VR) technologies in agriculture has shown significant promise in enhancing various agricultural practices. Mobile robots have also been adopted as assessment tools in precision agriculture, improving economic efficiency and productivity, and minimizing undesired effects such as weeds and pests. Despite considerable work on both fronts, the combination of a versatile User Interface (UI) provided by an AR headset with the integration and direct interaction and control of a mobile field robot has not yet been fully explored or standardized. This work aims to address this gap by providing real-time data input and control output of a mobile robot for precision agriculture through a virtual environment enabled by an AR headset interface. The system leverages open-source computational tools and off-the-shelf hardware for effective integration. Distinctive case studies are presented where growers or technicians can interact with a legged robot via an AR headset and a UI. Users can teleoperate the robot to gather information in an area of interest, request real-time graphed status of an area, or have the robot autonomously navigate to selected areas for measurement updates. The proposed system utilizes a custom local navigation method with a fixed holographic coordinate system in combination with QR codes. This step toward fusing AR and robotics in agriculture aims to provide practical solutions for real-time data management and control enabled by human-robot interaction. The implementation can be extended to various robot applications in agriculture and beyond, promoting a unified framework for on-demand and autonomous robot operation in the field.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03542",
        "abstract url": "https://arxiv.org/abs/2411.03542",
        "title": "Exploring the Benefits of Domain-Pretraining of Generative Large Language Models for Chemistry",
        "rating": "-3",
        "keywords": [
            [
                "Chemistry"
            ],
            [
                "named entity recognition"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "A proliferation of Large Language Models (the GPT series, BLOOM, LLaMA, and more) are driving forward novel development of multipurpose AI for a variety of tasks, particularly natural language processing (NLP) tasks. These models demonstrate strong performance on a range of tasks; however, there has been evidence of brittleness when applied to more niche or narrow domains where hallucinations or fluent but incorrect responses reduce performance. Given the complex nature of scientific domains, it is prudent to investigate the trade-offs of leveraging off-the-shelf versus more targeted foundation models for scientific domains. In this work, we examine the benefits of in-domain pre-training for a given scientific domain, chemistry, and compare these to open-source, off-the-shelf models with zero-shot and few-shot prompting. Our results show that not only do in-domain base models perform reasonably well on in-domain tasks in a zero-shot setting but that further adaptation using instruction fine-tuning yields impressive performance on chemistry-specific tasks such as named entity recognition and molecular formula generation.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03556",
        "abstract url": "https://arxiv.org/abs/2411.03556",
        "title": "VQ-ACE: Efficient Policy Search for Dexterous Robotic Manipulation via Action Chunking Embedding",
        "rating": "-3",
        "keywords": [
            [
                "Robotic Manipulation"
            ],
            [
                "biomimetic"
            ]
        ],
        "abstract": "Dexterous robotic manipulation remains a significant challenge due to the high dimensionality and complexity of hand movements required for tasks like in-hand manipulation and object grasping. This paper addresses this issue by introducing Vector Quantized Action Chunking Embedding (VQ-ACE), a novel framework that compresses human hand motion into a quantized latent space, significantly reducing the action space's dimensionality while preserving key motion characteristics. By integrating VQ-ACE with both Model Predictive Control (MPC) and Reinforcement Learning (RL), we enable more efficient exploration and policy learning in dexterous manipulation tasks using a biomimetic robotic hand. Our results show that latent space sampling with MPC produces more human-like behavior in tasks such as Ball Rolling and Object Picking, leading to higher task success rates and reduced control costs. For RL, action chunking accelerates learning and improves exploration, demonstrated through faster convergence in tasks like cube stacking and in-hand cube reorientation. These findings suggest that VQ-ACE offers a scalable and effective solution for robotic manipulation tasks involving complex, high-dimensional state spaces, contributing to more natural and adaptable robotic systems.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03636",
        "abstract url": "https://arxiv.org/abs/2411.03636",
        "title": "Domain Generalization for Cross-Receiver Radio Frequency Fingerprint Identification",
        "rating": "-3",
        "keywords": [
            [
                "federated learning"
            ],
            [
                "IoT"
            ]
        ],
        "abstract": "Radio Frequency Fingerprint Identification (RFFI) technology uniquely identifies emitters by analyzing unique distortions in the transmitted signal caused by non-ideal hardware. Recently, RFFI based on deep learning methods has gained popularity and is seen as a promising way to address the device authentication problem for Internet of Things (IoT) systems. However, in cross-receiver scenarios, where the RFFI model is trained over RF signals from some receivers but deployed at a new receiver, the alteration of receivers' characteristics would lead to data distribution shift and cause significant performance degradation at the new receiver. To address this problem, we first perform a theoretical analysis of the cross-receiver generalization error bound and propose a sufficient condition, named Separable Condition (SC), to minimize the classification error probability on the new receiver. Guided by the SC, a Receiver-Independent Emitter Identification (RIEI)model is devised to decouple the received signals into emitter-related features and receiver-related features and only the emitter-related features are used for identification. Furthermore, by leveraging federated learning, we also develop a FedRIEI model to eliminate the need for centralized collection of raw data from multiple receivers. Experiments on two real-world datasets demonstrate the superiority of our proposed methods over some baseline methods.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Accepted by IEEE Internet of Things Journal"
    },
    {
        "paper id": "2411.03013",
        "abstract url": "https://arxiv.org/abs/2411.03013",
        "title": "CRT-Fusion: Camera, Radar, Temporal Fusion Using Motion Information for 3D Object Detection",
        "rating": "-3.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "Radar"
            ],
            [
                "robotics"
            ],
            [
                "bird's-eye view",
                "BEV"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Accurate and robust 3D object detection is a critical component in autonomous vehicles and robotics. While recent radar-camera fusion methods have made significant progress by fusing information in the bird's-eye view (BEV) representation, they often struggle to effectively capture the motion of dynamic objects, leading to limited performance in real-world scenarios. In this paper, we introduce CRT-Fusion, a novel framework that integrates temporal information into radar-camera fusion to address this challenge. Our approach comprises three key modules: Multi-View Fusion (MVF), Motion Feature Estimator (MFE), and Motion Guided Temporal Fusion (MGTF). The MVF module fuses radar and image features within both the camera view and bird's-eye view, thereby generating a more precise unified BEV representation. The MFE module conducts two simultaneous tasks: estimation of pixel-wise velocity information and BEV segmentation. Based on the velocity and the occupancy score map obtained from the MFE module, the MGTF module aligns and fuses feature maps across multiple timestamps in a recurrent manner. By considering the motion of dynamic objects, CRT-Fusion can produce robust BEV feature maps, thereby improving detection accuracy and robustness. Extensive evaluations on the challenging nuScenes dataset demonstrate that CRT-Fusion achieves state-of-the-art performance for radar-camera-based 3D object detection. Our approach outperforms the previous best method in terms of NDS by +1.7%, while also surpassing the leading approach in mAP by +1.4%. These significant improvements in both metrics showcase the effectiveness of our proposed fusion strategy in enhancing the reliability and accuracy of 3D object detection.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at NeurIPS2024"
    },
    {
        "paper id": "2411.03363",
        "abstract url": "https://arxiv.org/abs/2411.03363",
        "title": "TDDBench: A Benchmark for Training data detection",
        "rating": "-3.5",
        "keywords": [
            [
                "unlearning"
            ],
            [
                "Attack"
            ],
            [
                "tabular"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Training Data Detection (TDD) is a task aimed at determining whether a specific data instance is used to train a machine learning model. In the computer security literature, TDD is also referred to as Membership Inference Attack (MIA). Given its potential to assess the risks of training data breaches, ensure copyright authentication, and verify model unlearning, TDD has garnered significant attention in recent years, leading to the development of numerous methods. Despite these advancements, there is no comprehensive benchmark to thoroughly evaluate the effectiveness of TDD methods. In this work, we introduce TDDBench, which consists of 13 datasets spanning three data modalities: image, tabular, and text. We benchmark 21 different TDD methods across four detection paradigms and evaluate their performance from five perspectives: average detection performance, best detection performance, memory consumption, and computational efficiency in both time and memory. With TDDBench, researchers can identify bottlenecks and areas for improvement in TDD algorithms, while practitioners can make informed trade-offs between effectiveness and efficiency when selecting TDD algorithms for specific use cases. Our large-scale benchmarking also reveals the generally unsatisfactory performance of TDD algorithms across different datasets. To enhance accessibility and reproducibility, we open-source TDDBench for the research community.",
        "subjects": [
            "cs.CR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03365",
        "abstract url": "https://arxiv.org/abs/2411.03365",
        "title": "Enhanced Real-Time Threat Detection in 5G Networks: A Self-Attention RNN Autoencoder Approach for Spectral Intrusion Analysis",
        "rating": "-3.5",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "attacks"
            ],
            [
                "5G"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In the rapidly evolving landscape of 5G technology, safeguarding Radio Frequency (RF) environments against sophisticated intrusions is paramount, especially in dynamic spectrum access and management. This paper presents an enhanced experimental model that integrates a self-attention mechanism with a Recurrent Neural Network (RNN)-based autoencoder for the detection of anomalous spectral activities in 5G networks at the waveform level. Our approach, grounded in time-series analysis, processes in-phase and quadrature (I/Q) samples to identify irregularities that could indicate potential jamming attacks. The model's architecture, augmented with a self-attention layer, extends the capabilities of RNN autoencoders, enabling a more nuanced understanding of temporal dependencies and contextual relationships within the RF spectrum. Utilizing a simulated 5G Radio Access Network (RAN) test-bed constructed with srsRAN 5G and Software Defined Radios (SDRs), we generated a comprehensive stream of data that reflects real-world RF spectrum conditions and attack scenarios. The model is trained to reconstruct standard signal behavior, establishing a normative baseline against which deviations, indicative of security threats, are identified. The proposed architecture is designed to balance between detection precision and computational efficiency, so the LSTM network, enriched with self-attention, continues to optimize for minimal execution latency and power consumption. Conducted on a real-world SDR-based testbed, our results demonstrate the model's improved performance and accuracy in threat detection. Keywords: self-attention, real-time intrusion detection, RNN autoencoder, Transformer architecture, LSTM, time series anomaly detection, 5G Security, spectrum access security.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.LG",
            "cs.NI"
        ],
        "comment": "This article has been accepted for publication in WiOpt 2024"
    },
    {
        "paper id": "2411.03654",
        "abstract url": "https://arxiv.org/abs/2411.03654",
        "title": "PyroGuardian: An IoT-Enabled System for Health and Location Monitoring in High-Risk Firefighting Environments",
        "rating": "-3.5",
        "keywords": [
            [
                "Health"
            ],
            [
                "IoT"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "First responders risk their lives to reduce property damage and prevent injuries during disasters. Among first responders, firefighters work with fires in residential properties, forests, or other locations where fire occurs. We built the PyroGuardian system that uses wearable modules to transmit unit information over Long Range (LoRa) to an Android tablet. The tablet runs our application, PyroPortal, to assign each firefighter's stats, such as body temperature, heart rate, and GPS location. PyroPortal displays this information on unit dashboards, and markers on Google Maps represent the firefighter's location and the direction they are facing. These dashboards can help the incident commander (IC) make more informed decisions on mission control operations and remove specific units whose health stats, such as oximeter and pulse, passed certain thresholds. PyroGuardian completes all these tasks at an affordable cost and in an impressive maximum range between the units and IC. In addition, PyroGuardian has various application scenarios, such as law enforcement and military operations, besides firefighting. We also conducted a sample mission inside a burning building while real firefighters watched. After the demonstration, they completed a survey on system usability and PyroGuardian's potential to meet their requirements.",
        "subjects": [
            "cs.NI",
            "cs.CY",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03143",
        "abstract url": "https://arxiv.org/abs/2411.03143",
        "title": "Self-supervised Hierarchical Representation for Medication Recommendation",
        "rating": "-4",
        "keywords": [
            [
                "medical",
                "health",
                "clinical"
            ],
            [
                "Recommendation"
            ]
        ],
        "abstract": "Medication recommender is to suggest appropriate medication combinations based on a patient's health history, e.g., diagnoses and procedures. Existing works represent different diagnoses/procedures well separated by one-hot encodings. However, they ignore the latent hierarchical structures of these medical terms, undermining the generalization performance of the model. For example, \"Respiratory Diseases\", \"Chronic Respiratory Diseases\" and \"Chronic Bronchiti\" have a hierarchical relationship, progressing from general to specific. To address this issue, we propose a novel hierarchical encoder named HIER to hierarchically represent diagnoses and procedures, which is based on standard medical codes and compatible with any existing methods. Specifically, the proposed method learns relation embedding with a self-supervised objective for incorporating the neighbor hierarchical structure. Additionally, we develop the position encoding to explicitly introduce global hierarchical position. Extensive experiments demonstrate significant and consistent improvements in recommendation accuracy across four baselines and two real-world clinical datasets.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03194",
        "abstract url": "https://arxiv.org/abs/2411.03194",
        "title": "Energy Consumption in Robotics: A Simplified Modeling Approach",
        "rating": "-4",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "Robotics",
                "robot"
            ],
            [
                "industrial"
            ]
        ],
        "abstract": "The energy use of a robot is trajectory-dependent, and thus can be reduced by optimization of the trajectory. Current methods for robot trajectory optimization can reduce energy up to 15\\% for fixed start and end points, however their use in industrial robot planning is still restricted due to model complexity and lack of integration with planning tools which address other concerns (e.g. collision avoidance). We propose an approach that uses differentiable inertial and kinematic models from standard open-source tools, integrating with standard ROS planning methods. An inverse dynamics-based energy model is optionally extended with a single-parameter electrical model, simplifying the model identification process. We compare the inertial and electrical models on a collaborative robot, showing that simplified models provide competitive accuracy and are easier to deploy in practice.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "5 pages"
    },
    {
        "paper id": "2411.03398",
        "abstract url": "https://arxiv.org/abs/2411.03398",
        "title": "DP-HLS: A High-Level Synthesis Framework for Accelerating Dynamic Programming Algorithms in Bioinformatics",
        "rating": "-4",
        "keywords": [
            [
                "Bioinformatics"
            ],
            [
                "FPGAs"
            ]
        ],
        "abstract": "Dynamic programming (DP) based algorithms are essential yet compute-intensive parts of numerous bioinformatics pipelines, which typically involve populating a 2-D scoring matrix based on a recursive formula, optionally followed by a traceback step to get the optimal alignment path. DP algorithms are used in a wide spectrum of bioinformatics tasks, including read assembly, homology search, gene annotation, basecalling, and phylogenetic inference. So far, specialized hardware like ASICs and FPGAs have provided massive speedup for these algorithms. However, these solutions usually represent a single design point in the DP algorithmic space and typically require months of manual effort to implement using low-level hardware description languages (HDLs). This paper introduces DP-HLS, a novel framework based on High-Level Synthesis (HLS) that simplifies and accelerates the development of a broad set of bioinformatically relevant DP algorithms in hardware. DP-HLS features an easy-to-use template with integrated HLS directives, enabling efficient hardware solutions without requiring hardware design knowledge. In our experience, DP-HLS significantly reduced the development time of new kernels (months to days) and produced designs with comparable resource utilization to open-source hand-coded HDL-based implementations and performance within 7.7-16.8% margin. DP-HLS is compatible with AWS EC2 F1 FPGA instances. To demonstrate the versatility of the DP-HLS framework, we implemented 15 diverse DP kernels, achieving 1.3-32x improved throughput over state-of-the-art GPU and CPU baselines and providing the first open-source FPGA implementation for several of them. The DP-HLS codebase is available freely under the MIT license and its detailed wiki is available to assist new users.",
        "subjects": [
            "cs.AR",
            "cs.DC"
        ],
        "comment": "* Both authors contributed equally to this research. Detailed Wiki: https://turakhia.ucsd.edu/DP-HLS/. DP-HLS codebase: https://github.com/TurakhiaLab/DP-HLS"
    },
    {
        "paper id": "2411.03582",
        "abstract url": "https://arxiv.org/abs/2411.03582",
        "title": "Privacy Preserving Mechanisms for Coordinating Airspace Usage in Advanced Air Mobility",
        "rating": "-4",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "graph"
            ],
            [
                "drone"
            ]
        ],
        "abstract": "Advanced Air Mobility (AAM) operations are expected to transform air transportation while challenging current air traffic management practices. By introducing a novel market-based mechanism, we address the problem of on-demand allocation of capacity-constrained airspace to AAM vehicles with heterogeneous and private valuations. We model airspace and air infrastructure as a collection of contiguous regions with constraints on the number of vehicles that simultaneously enter, stay, or exit each region. Vehicles request access to the airspace with trajectories spanning multiple regions at different times. We use the graph structure of our airspace model to formulate the allocation problem as a path allocation problem on a time-extended graph. To ensure the cost information of AAM vehicles remains private, we introduce a novel mechanism that allocates each vehicle a budget of \"air-credits\" and anonymously charges prices for traversing the edges of the time-extended graph. We seek to compute a competitive equilibrium that ensures that: (i) capacity constraints are satisfied, (ii) a strictly positive resource price implies that the sector capacity is fully utilized, and (iii) the allocation is integral and optimal for each AAM vehicle given current prices, without requiring access to individual vehicle utilities. However, a competitive equilibrium with integral allocations may not always exist. We provide sufficient conditions for the existence and computation of a fractional-competitive equilibrium, where allocations can be fractional. Building on these theoretical insights, we propose a distributed, iterative, two-step algorithm that: 1) computes a fractional competitive equilibrium, and 2) derives an integral allocation from this equilibrium. We validate the effectiveness of our approach in allocating trajectories for two emerging urban air mobility services: drone delivery and air taxis.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "31 pages, 7 figures, 3 tables"
    },
    {
        "paper id": "2411.03364",
        "abstract url": "https://arxiv.org/abs/2411.03364",
        "title": "DM4Steal: Diffusion Model For Link Stealing Attack On Graph Neural Networks",
        "rating": "-4.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "GNN",
                "Graph"
            ],
            [
                "Attack"
            ],
            [
                "recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph has become increasingly integral to the advancement of recommendation systems, particularly with the fast development of graph neural network(GNN). By exploring the virtue of rich node features and link information, GNN is designed to provide personalized and accurate suggestions. Meanwhile, the privacy leakage of GNN in such contexts has also captured special attention. Prior work has revealed that a malicious user can utilize auxiliary knowledge to extract sensitive link data of the target graph, integral to recommendation systems, via the decision made by the target GNN model. This poses a significant risk to the integrity and confidentiality of data used in recommendation system. Though important, previous works on GNN's privacy leakage are still challenged in three aspects, i.e., limited stealing attack scenarios, sub-optimal attack performance, and adaptation against defense. To address these issues, we propose a diffusion model based link stealing attack, named DM4Steal. It differs previous work from three critical aspects. (i) Generality: aiming at six attack scenarios with limited auxiliary knowledge, we propose a novel training strategy for diffusion models so that DM4Steal is transferable to diverse attack scenarios. (ii) Effectiveness: benefiting from the retention of semantic structure in the diffusion model during the training process, DM4Steal is capable to learn the precise topology of the target graph through the GNN decision process. (iii) Adaptation: when GNN is defensive (e.g., DP, Dropout), DM4Steal relies on the stability that comes from sampling the score model multiple times to keep performance degradation to a minimum, thus DM4Steal implements successful adaptive attack on defensive GNN.",
        "subjects": [
            "cs.CR",
            "cs.IR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02814",
        "abstract url": "https://arxiv.org/abs/2411.02814",
        "title": "The Hitchhiker's Guide to Programming and Optimizing CXL-Based Heterogeneous Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present a thorough analysis of the use of CXL-based heterogeneous systems. We built a cluster of server systems that combines different vendor's CPUs and various types of CXL devices. We further developed a heterogeneous memory benchmark suite, Heimdall, to profile the performance of such heterogeneous systems. By leveraging Heimdall, we unveiled the detailed architecture design in these systems, drew observations on optimizing performance for workloads, and pointed out directions for future development of CXL-based heterogeneous systems.",
        "subjects": [
            "cs.PF",
            "cs.AR",
            "cs.DC",
            "cs.OS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02819",
        "abstract url": "https://arxiv.org/abs/2411.02819",
        "title": "Coboundary expansion of coset complexes",
        "rating": "-10",
        "keywords": [],
        "abstract": "Coboundary expansion is a high dimensional generalization of the Cheeger constant to simplicial complexes. Originally, this notion was motivated by the fact that it implies topological expansion, but nowadays a significant part of the motivation stems from its deep connection to problems in theoretical computer science such as agreement expansion in the low soundness regime. In this paper, we prove coboundary expansion with non-Abelian coefficients for the coset complex construction of Kaufman and Oppenheim. Our proof uses a novel global argument, as opposed to the local-to-global arguments that are used to prove cosystolic expansion.",
        "subjects": [
            "math.CO",
            "cs.CC",
            "math.AT",
            "math.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02827",
        "abstract url": "https://arxiv.org/abs/2411.02827",
        "title": "Hybrid Beamforming for Integrated Sensing and Communications With Low Resolution DACs",
        "rating": "-10",
        "keywords": [],
        "abstract": "Integrated sensing and communications (ISAC) has emerged as a means to efficiently utilize spectrum and thereby save cost and power. At the higher end of the spectrum, ISAC systems operate at wideband using large antenna arrays to meet the stringent demands for high-resolution sensing and enhanced communications capacity. On the other hand, the overall design should satisfy energy-efficiency and hardware constraints such as operating on low resolution components for a practical scenario. Therefore, this paper presents the design of Hybrid ANalog and Digital BeAmformers with Low resoLution (HANDBALL) digital-to-analog converters (DACs). We introduce a greedy-search-based approach to design the analog beamformers for multi-user multi-target ISAC scenario. Then, the quantization distortion is taken into account in order to design the baseband beamformer with low resolution DACs. We evaluated performance of the proposed HANDBALL technique in terms of both spectral efficiency and sensing beampattern, providing a satisfactory sensing and communication performance for both one-bit and few-bit designs.",
        "subjects": [
            "eess.SP",
            "cs.IT"
        ],
        "comment": "Accepted Paper in IEEE Wireless Communications Letters"
    },
    {
        "paper id": "2411.02831",
        "abstract url": "https://arxiv.org/abs/2411.02831",
        "title": "Enhancing EmoBot: An In-Depth Analysis of User Satisfaction and Faults in an Emotion-Aware Chatbot",
        "rating": "-10",
        "keywords": [],
        "abstract": "The research community has traditionally shown a keen interest in emotion modeling, with a notable emphasis on the detection aspect. In contrast, the exploration of emotion generation has received less attention.This study delves into an existing state-of-the-art emotional chatbot, EmoBot, designed for generating emotions in general-purpose conversations. This research involves a comprehensive examination, including a survey to evaluate EmoBot's proficiency in key dimensions like usability, accuracy, and overall user satisfaction, with a specific focus on fault tolerance. By closely examining the chatbot's operations, we identified some noteworthy shortcomings in the existing model. We propose some solutions designed to address and overcome the identified issues.",
        "subjects": [
            "cs.HC",
            "cs.IR"
        ],
        "comment": "3 pages, extended abstract"
    },
    {
        "paper id": "2411.02842",
        "abstract url": "https://arxiv.org/abs/2411.02842",
        "title": "Metaheuristics for the Template Design Problem: Encoding, Symmetry and Hybridisation",
        "rating": "-10",
        "keywords": [],
        "abstract": "The template design problem (TDP) is a hard combinatorial problem with a high number of symmetries which makes solving it more complicated. A number of techniques have been proposed in the literature to optimise its resolution, ranging from complete methods to stochastic ones. However, although metaheuristics are considered efficient methods that can find enough-quality solutions at a reasonable computational cost, these techniques have not proven to be truly efficient enough to deal with this problem. This paper explores and analyses a wide range of metaheuristics to tackle the problem with the aim of assessing their suitability for finding template designs. We tackle the problem using a wide set of metaheuristics whose implementation is guided by a number of issues such as problem formulation, solution encoding, the symmetrical nature of the problem, and distinct forms of hybridisation. For the TDP, we also propose a slot-based alternative problem formulation (distinct to other slot-based proposals), which represents another option other than the classical variation-based formulation of the problem. An empirical analysis, assessing the performance of all the metaheuristics (i.e., basic, integrative and collaborative algorithms working on different search spaces and with/without symmetry breaking) shows that some of our proposals can be considered the state-of-the-art when they are applied to specific problem instances.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "30 pages, 4 figures"
    },
    {
        "paper id": "2411.02845",
        "abstract url": "https://arxiv.org/abs/2411.02845",
        "title": "Max-Distance Sparsification for Diversification and Clustering",
        "rating": "-10",
        "keywords": [],
        "abstract": "Let $\\mathcal{D}$ be a set family that is the solution domain of some combinatorial problem. The \\emph{max-min diversification problem on $\\mathcal{D}$} is the problem to select $k$ sets from $\\mathcal{D}$ such that the Hamming distance between any two selected sets is at least $d$. FPT algorithms parameterized by $k,l:=\\max_{D\\in \\mathcal{D}}|D|$ and $k,d$ have been actively studied recently for several specific domains. This paper provides unified algorithmic frameworks to solve this problem. Specifically, for each parameterization $k,l$ and $k,d$, we provide an FPT oracle algorithm for the max-min diversification problem using oracles related to $\\mathcal{D}$. We then demonstrate that our frameworks generalize most of the existing domain-specific tractability results and provide the first FPT algorithms for several domains. Our main technical breakthrough is introducing the notion of \\emph{max-distance sparsifier} of $\\mathcal{D}$, a domain on which the max-min diversification problem is equivalent to the same problem on the original domain $\\mathcal{D}$. The core of our framework is to design FPT oracle algorithms that construct a constant-size max-distance sparsifier of $\\mathcal{D}$. Using max-distance sparsifiers, we provide FPT algorithms for the max-min and max-sum diversification problems on $\\mathcal{D}$, as well as $k$-center and $k$-sum-of-radii clustering problems on $\\mathcal{D}$, which are also natural problems in the context of diversification and have their own interests.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "28 pages"
    },
    {
        "paper id": "2411.02857",
        "abstract url": "https://arxiv.org/abs/2411.02857",
        "title": "Multi-Scale Temporal Analysis for Failure Prediction in Energy Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Many existing models struggle to predict nonlinear behavior during extreme weather conditions. This study proposes a multi-scale temporal analysis for failure prediction in energy systems using PMU data. The model integrates multi-scale analysis with machine learning to capture both short-term and long-term behavior. PMU data lacks labeled states despite logged failure records, making it difficult to distinguish between normal and disturbance conditions. We address this through: (1) Extracting domain features from PMU time series data; (2) Applying multi-scale windows (30s, 60s, 180s) for pattern detection; (3) Using Recursive Feature Elimination to identify key features; (4) Training multiple machine learning models. Key contributions: Identifying significant features across multi-scale windows; Demonstrating LightGBM's superior performance (0.896 precision); Showing multi-scale analysis outperforms single-window models (0.841). Our work focuses on weather-related failures, with plans to extend to equipment failure and lightning events.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "6 pages, 3 figures, RAMS 2025"
    },
    {
        "paper id": "2411.02899",
        "abstract url": "https://arxiv.org/abs/2411.02899",
        "title": "Codes with restricted overlaps: expandability, constructions, and bounds",
        "rating": "-10",
        "keywords": [],
        "abstract": "Consider a $q$-ary block code satisfying the property that no $l$-letters long codeword's prefix occurs as a suffix of any codeword for $l$ inside some interval. We determine a general upper bound on the maximum size of these codes and a tighter bound for codes where overlaps with lengths not exceeding $k$ are prohibited. We then provide constructions for codes with various restrictions on overlap lengths and use them to determine lower bounds on the maximum sizes. In particular, we construct $(1,k)$-overlap-free codes where $k \\geq n/2$ and $n$ denotes the block size, expand a known construction of $(k,n-1)$-overlap-free codes, and combine the ideas behind both constructions to obtain $(t_1,t_2)$-overlap-free codes and codes that are simultaneously $(1,k)$- and $(n-k,n-1)$-overlap-free for some $k < n/2$. In the case when overlaps of lengths between 1 and $k$ are prohibited, we complete the characterisation of non-expandable codes started by Cai, Wang, and Feng (2023).",
        "subjects": [
            "cs.IT",
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02903",
        "abstract url": "https://arxiv.org/abs/2411.02903",
        "title": "Efficient assembly of bolted joints using numerical FEM",
        "rating": "-10",
        "keywords": [],
        "abstract": "Several factors can cause leakages in a bolted flange connection, external loads being the most important one. External loads such as those produced by misalignment introduce an external axial load combined with a bending moment which could lead to either an excessive compression of the gasket and its subsequent crushing, and/or a separation of the flanges causing leakage failure. Several studies have been carried out to study these prejudicial effects, but no practical solutions are proposed to compensate for them. In this sense, this work presents a numerical methodology that iteratively calculates the non-uniform bolt tightening load distribution to achieve a uniform gasket stress distribution that improves the leakage performance of bolted joints subjected to external loads.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02905",
        "abstract url": "https://arxiv.org/abs/2411.02905",
        "title": "Load distribution and friction torque in four-point contact slewing bearings considering manufacturing errors and ring flexibility",
        "rating": "-10",
        "keywords": [],
        "abstract": "This work introduces a methodology for the calculation of the load distribution in four-point contact slewing bearings considering ball preload, manufacturing errors and ring flexibility. The model is built by the formulation and minimization of the potential energy of the bearing. Comparing with the rigid rings assumption, the results show that ring deformations involve lower interferences in idling conditions, and have a great effect in the load distribution, but not under external loads. Additionally, a new approach has been proposed for the calculation of the friction torque, which has lower computational cost in comparison to a previous approach by the authors, so more accurate results can be obtained due to refined calculations with no significant cost increase.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02906",
        "abstract url": "https://arxiv.org/abs/2411.02906",
        "title": "Structural modeling of crossed roller wire race bearings: analytical submodel for the roller-wire-ring set",
        "rating": "-10",
        "keywords": [],
        "abstract": "Since wire bearings were patented in 1936, they have been used in applications where weight saving is a key aspect. Nevertheless, little work can be found in literature regarding their structural behaviour. In order to predict how a wire bearing reacts under load, the most feasible way is to model and analyse it via Finite Elements (FE). However, the great amount of elements needed to properly model the complex contact configuration among rollers, wires and raceways, make this approach unaffordable. In this sense, this manuscript develops an analytical submodel that represents the structural behaviour of the roller-wire-ring set, in order to include it later in a global FE model to account for ring flexibility and boundary conditions. This way, the computationally intensive FE modelling of these complex zones can be substituted by the analytical model developed in this work, and a much manageable global FE model can be used to predict the structural response of the bearing. This analytical model has demonstrated a good agreement with local FE models of the roller-wire-ring set.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02911",
        "abstract url": "https://arxiv.org/abs/2411.02911",
        "title": "Synergizing Hyper-accelerated Power Optimization and Wavelength-Dependent QoT-Aware Cross-Layer Design in Next-Generation Multi-Band EONs",
        "rating": "-10",
        "keywords": [],
        "abstract": "The extension of elastic optical networks (EON) to multi-band transmission (MB-EON) shows promise in enhancing spectral efficiency, throughput, and long-term cost-effectiveness for telecom operators. However, designing MB-EON networks introduces complex challenges, notably the optimization of physical parameters like optical power and quality of transmission (QoT). Frequency-dependent characteristics of fiber, such as loss, dispersion, and nonlinear effects, alongside inter-channel stimulated Raman scattering, pose significant hurdles when extending beyond the L+C (LC) band to a continuous spectrum over 100 nm. In this study, we propose a span-by-span methodology for optimal power allocation, introducing two hyper-accelerated power optimization (HPO) strategies: flat launch power (FLP) and flat received power (FRP). These approaches significantly expedite network power optimization while preserving the stability of running services. Our comparative analysis of FLP and FRP models reveals that while FRP has a minimal effect on capacity (increasing less than 10 Tbps for an L+C+S (LCS) system over 100 km), it improves flatness and GSNR/OSNR metrics in the S-band by approximately 2/0 dB and 2.5/6 dB, respectively. A network-wide analysis across various topologies shows that the FRP technique enhances minimum GSNR, contributing to a throughput increase of 12% to 75%, depending on network scale, at a 1% bandwidth blocking rate. Lastly, our application of HPO in MB-EON for both local and global power optimization demonstrates that while both approaches offer comparable performance, global optimization is simpler and more cost-effective for large-scale networks.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02936",
        "abstract url": "https://arxiv.org/abs/2411.02936",
        "title": "Deriving Nonuniform Lower Bounds from Uniform Nondeterministic Lower Bounds",
        "rating": "-10",
        "keywords": [],
        "abstract": "Proving complexity lower bounds remains a challenging task: currently, we only know how to prove conditional uniform (algorithm) lower bounds and nonuniform (circuit) lower bounds in restricted circuit models. About a decade ago, Williams (STOC 2010) showed how to derive nonuniform lower bounds from uniform \\emph{upper} bounds. Since then, a number of results of this kind have been proved. For example, Jahanjou et al. (ICALP 2015) and Carmosino et al. (ITCS 2016) proved that if NSETH fails, then $\\text{E}^{\\text{NP}}$ has circuit size $\u03c9(n)$. Just recently, it was shown how uniform \\emph{lower} bounds can be used to derive nonuniform lower bounds: Belova et al. (SODA 2024) showed that if MAX-3-SAT cannot be solved in co-nondeterministic time $O(2^{(1 - \\varepsilon)n})$, then there exists an explicit polynomial family of high arithmetic circuit size; Williams (FOCS 2024) showed that if NSETH is true, then Boolean Inner Product requires large $\\text{ETHR} \\circ \\text{ETHR}$ circuits. In this paper, we continue developing this line of research and show that nondeterministic uniform lower bounds imply nonuniform lower bounds for various types of objects that are notoriously hard to analyze: circuits, matrix rigidity, and tensor rank. Specifically, we prove that if NSETH is true, then there exists a monotone Boolean function family in coNP of monotone circuit size $2^{\u03a9(n / \\log n)}$. Combining this with the result above, we get win-win circuit lower bounds: either $\\text{E}^{\\text{NP}}$ requires circuits of size $w(n)$ or coNP requires monotone circuits of size $2^{\u03a9(n / \\log n)}$. We also prove that MAX-3-SAT cannot be solved in co-nondeterministic time $O(2^{(1 - \\varepsilon)n})$, then there exist small explicit families of high rigidity matrices and high rank tensors.",
        "subjects": [
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02942",
        "abstract url": "https://arxiv.org/abs/2411.02942",
        "title": "Constant Approximation for Weighted Nash Social Welfare with Submodular Valuations",
        "rating": "-10",
        "keywords": [],
        "abstract": "We study the problem of assigning items to agents so as to maximize the \\emph{weighted} Nash Social Welfare (NSW) under submodular valuations. The best-known result for the problem is an $O(nw_{\\max})$-approximation due to Garg, Husic, Li, Vega, and Vondrak~\\cite{GHL23}, where $w_{\\max}$ is the maximum weight over all agents. Obtaining a constant approximation algorithm is an open problem in the field that has recently attracted considerable attention. We give the first such algorithm for the problem, thus solving the open problem in the affirmative. Our algorithm is based on the natural Configuration LP for the problem, which was introduced recently by Feng and Li~\\cite{FL24} for the additive valuation case. Our rounding algorithm is similar to that of Li \\cite{Li25} developed for the unrelated machine scheduling problem to minimize weighted completion time. Roughly speaking, we designate the largest item in each configuration as a large item and the remaining items as small items. So, every agent gets precisely 1 fractional large item in the configuration LP solution. With the rounding algorithm in \\cite{Li25}, we can ensure that in the obtained solution, every agent gets precisely 1 large item, and the assignments of small items are negatively correlated.",
        "subjects": [
            "cs.GT",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02945",
        "abstract url": "https://arxiv.org/abs/2411.02945",
        "title": "Instant Resonance: Dual Strategy Enhances the Data Consensus Success Rate of Blockchain Threshold Signature Oracles",
        "rating": "-10",
        "keywords": [],
        "abstract": "With the rapid development of Decentralized Finance (DeFi) and Real-World Assets (RWA), the importance of blockchain oracles in real-time data acquisition has become increasingly prominent. Using cryptographic techniques, threshold signature oracles can achieve consensus on data from multiple nodes and provide corresponding proofs to ensure the credibility and security of the information. However, in real-time data acquisition, threshold signature methods face challenges such as data inconsistency and low success rates in heterogeneous environments, which limit their practical application potential. To address these issues, this paper proposes an innovative dual-strategy approach to enhance the success rate of data consensus in blockchain threshold signature oracles. Firstly, we introduce a Representative Enhanced Aggregation Strategy (REP-AG) that improves the representativeness of data submitted by nodes, ensuring consistency with data from other nodes, and thereby enhancing the usability of threshold signatures. Additionally, we present a Timing Optimization Strategy (TIM-OPT) that dynamically adjusts the timing of nodes' access to data sources to maximize consensus success rates. Experimental results indicate that REP-AG improves the aggregation success rate by approximately 56.6\\% compared to the optimal baseline, while the implementation of TIM-OPT leads to an average increase of approximately 32.9\\% in consensus success rates across all scenarios.",
        "subjects": [
            "cs.DC",
            "cs.ET"
        ],
        "comment": "Submitted to FGCS"
    },
    {
        "paper id": "2411.02959",
        "abstract url": "https://arxiv.org/abs/2411.02959",
        "title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Retrieval-Augmented Generation (RAG) has been shown to improve knowledge capabilities and alleviate the hallucination problem of LLMs. The Web is a major source of external knowledge used in RAG systems, and many commercial systems such as ChatGPT and Perplexity have used Web search engines as their major retrieval systems. Typically, such RAG systems retrieve search results, download HTML sources of the results, and then extract plain texts from the HTML sources. Plain text documents or chunks are fed into the LLMs to augment the generation. However, much of the structural and semantic information inherent in HTML, such as headings and table structures, is lost during this plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG, which uses HTML instead of plain text as the format of retrieved knowledge in RAG. We believe HTML is better than plain text in modeling knowledge in external documents, and most LLMs possess robust capacities to understand HTML. However, utilizing HTML presents new challenges. HTML contains additional content such as tags, JavaScript, and CSS specifications, which bring extra input tokens and noise to the RAG system. To address this issue, we propose HTML cleaning, compression, and pruning strategies, to shorten the HTML while minimizing the loss of information. Specifically, we design a two-step block-tree-based pruning method that prunes useless HTML blocks and keeps only the relevant part of the HTML. Experiments on six QA datasets confirm the superiority of using HTML in RAG systems.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02977",
        "abstract url": "https://arxiv.org/abs/2411.02977",
        "title": "Relating Apartness and Branching Bisimulation Games",
        "rating": "-10",
        "keywords": [],
        "abstract": "Geuvers and Jacobs (LMCS 2021) formulated the notion of apartness relation on state-based systems modelled as coalgebras. In this context apartness is formally dual to bisimilarity, and gives an explicit proof system for showing that certain states are not bisimilar. In the current paper, we relate apartness to another classical element of the theory of behavioural equivalences: that of turn-based two-player games. Studying both strong and branching bisimilarity, we show that winning configurations for the Spoiler player correspond to apartness proofs, for transition systems that are image-finite (in the case of strong bisimilarity) and finite (in the case of branching bisimilarity).",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.02980",
        "abstract url": "https://arxiv.org/abs/2411.02980",
        "title": "Counting random $k$-SAT near the satisfiability threshold",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present efficient counting and sampling algorithms for random $k$-SAT when the clause density satisfies $\u03b1\\le \\frac{2^k}{\\mathrm{poly}(k)}.$ In particular, the exponential term $2^k$ matches the satisfiability threshold $\u0398(2^k)$ for the existence of a solution and the (conjectured) algorithmic threshold $2^k (\\ln k) / k$ for efficiently finding a solution. Previously, the best-known counting and sampling algorithms required far more restricted densities $\u03b1\\lesssim 2^{k/3}$ [He, Wu, Yang, SODA '23]. Notably, our result goes beyond the lower bound $d\\gtrsim 2^{k/2}$ for worst-case $k$-SAT with bounded-degree $d$ [Bez\u00e1kov\u00e1 et al, SICOMP '19], showing that for counting and sampling, the average-case random $k$-SAT model is computationally much easier than the worst-case model. At the heart of our approach is a new refined analysis of the recent novel coupling procedure by [Wang, Yin, FOCS '24], utilizing the structural properties of random constraint satisfaction problems (CSPs). Crucially, our analysis avoids reliance on the $2$-tree structure used in prior works, which cannot extend beyond the worst-case threshold $2^{k/2}$. Instead, we employ a witness tree similar to that used in the analysis of the Moser-Tardos algorithm [Moser, Tardos, JACM '10] for the Lov\u00e1sz Local lemma, which may be of independent interest. Our new analysis provides a universal framework for efficient counting and sampling for random atomic CSPs, including, for example, random hypergraph colorings. At the same time, it immediately implies as corollaries several structural and probabilistic properties of random CSPs that have been widely studied but rarely justified, including replica symmetry and non-reconstruction.",
        "subjects": [
            "cs.DS",
            "cs.DM"
        ],
        "comment": "39 pages"
    },
    {
        "paper id": "2411.03037",
        "abstract url": "https://arxiv.org/abs/2411.03037",
        "title": "Top-k Stabbing Interval Queries",
        "rating": "-10",
        "keywords": [],
        "abstract": "We investigate a weighted variant of the interval stabbing problem, where the goal is to design an efficient data structure for a given set $\\mathcal{I}$ of weighted intervals such that, for a query point $q$ and an integer $k>0$, we can report the $k$ intervals with largest weights among those stabbed by $q$. In this paper, we present a linear space solution with $O(\\log n + k)$ query time. Moreover, we also present another trade-off for the problem.",
        "subjects": [
            "cs.DS",
            "cs.CG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03054",
        "abstract url": "https://arxiv.org/abs/2411.03054",
        "title": "Alternate Learning and Compression Approaching R(D)",
        "rating": "-10",
        "keywords": [],
        "abstract": "The inherent trade-off in on-line learning is between exploration and exploitation. A good balance between these two (conflicting) goals can achieve a better long-term performance. Can we define an optimal balance? We propose to study this question through a backward-adaptive lossy compression system, which exhibits a \"natural\" trade-off between exploration and exploitation.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "This paper was presented as a poster in the workshop `Learn 2 Compress', in ISIT 2024, Athens, Greece, July 2024. It was processed and reviewed in the Open Review system"
    },
    {
        "paper id": "2411.03066",
        "abstract url": "https://arxiv.org/abs/2411.03066",
        "title": "Equivalence of Deterministic Weighted Real-time One-Counter Automata",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper introduces deterministic weighted real-time one-counter automaton (DWROCA). A DWROCA is a deterministic real-time one-counter automaton whose transitions are assigned a weight from a field. Two DWROCAs are equivalent if every word accepted by one is accepted by the other with the same weight. DWROCA is a sub-class of weighted one-counter automata with counter-determinacy. It is known that the equivalence problem for this model is in P. This paper gives a simpler proof and a better polynomial-time algorithm for checking the equivalence of two DWROCAs.",
        "subjects": [
            "cs.FL"
        ],
        "comment": "14 pages, 4 figures"
    },
    {
        "paper id": "2411.03069",
        "abstract url": "https://arxiv.org/abs/2411.03069",
        "title": "Conformance Games for Graded Semantics",
        "rating": "-10",
        "keywords": [],
        "abstract": "Game-theoretic characterizations of process equivalences traditionally form a central topic in concurrency; for example, most equivalences on the classical linear-time / branching-time spectrum come with such characterizations. Recent work on so-called graded semantics has led to a generic behavioural equivalence game that covers the mentioned games on the linear-time~/ branching-time spectrum and moreover applies in coalgebraic generality, and thus instantiates also to equivalence games on systems with non-relational branching type (probabilistic, weighted, game-based etc.). In the present work, we generalize this approach to cover other types of process comparison beyond equivalence, such as behavioural preorders or pseudometrics. At the most general level, we abstract such notions of behavoiural conformance in terms of topological categories, and later specialize to conformances presented as relational structures to obtain a concrete syntax. We obtain a sound and complete generic game for behavioural conformances in this sense. We present a number of instantiations, obtaining game characterizations of, e.g., trace inclusion, probabilistic trace distance, bisimulation topologies, and simulation distances on metric labelled transition systems.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03070",
        "abstract url": "https://arxiv.org/abs/2411.03070",
        "title": "Extensions of the Cylindrical Algebraic Covering Method for Quantifiers",
        "rating": "-10",
        "keywords": [],
        "abstract": "The cylindrical algebraic covering method was originally proposed to decide the satisfiability of a set of non-linear real arithmetic constraints. We reformulate and extend the cylindrical algebraic covering method to allow for checking the truth of arbitrary non-linear arithmetic formulas, adding support for both quantifiers and Boolean structure. Furthermore, we also propose a variant to perform quantifier elimination on such formulas. After introducing the algorithm, we elaborate on various extensions, optimizations and heuristics. Finally, we present an experimental evaluation of our implementation and provide a comparison with state-of-the-art SMT solvers and quantifier elimination tools.",
        "subjects": [
            "cs.SC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03071",
        "abstract url": "https://arxiv.org/abs/2411.03071",
        "title": "Multi-dimensional Approximate Counting",
        "rating": "-10",
        "keywords": [],
        "abstract": "The celebrated Morris counter uses $\\log_2\\log_2 n + O(\\log_2 \u03c3^{-1})$ bits to count up to $n$ with a relative error $\u03c3$, where if $\\hat\u03bb$ is the estimate of the current count $\u03bb$, then $\\mathbb{E}|\\hat\u03bb-\u03bb|^2 <\u03c3^2\u03bb^2$. A natural generalization is \\emph{multi-dimensional} approximate counting. Let $d\\geq 1$ be the dimension. The count vector $x\\in \\mathbb{N}^d$ is incremented entry-wisely over a stream of coordinates $(w_1,\\ldots,w_n)\\in [d]^n$, where upon receiving $w_k\\in[d]$, $x_{w_k}\\gets x_{w_k}+1$. A \\emph{$d$-dimensional approximate counter} is required to count $d$ coordinates simultaneously and return an estimate $\\hat{x}$ of the count vector $x$. Aden-Ali, Han, Nelson, and Yu \\cite{aden2022amortized} showed that the trivial solution of using $d$ Morris counters that track $d$ coordinates separately is already optimal in space, \\emph{if each entry only allows error relative to itself}, i.e., $\\mathbb{E}|\\hat{x}_j-x_j|^2<\u03c3^2|x_j|^2$ for each $j\\in [d]$. However, for another natural error metric -- the \\emph{Euclidean mean squared error} $\\mathbb{E} |\\hat{x}-x|^2$ -- we show that using $d$ separate Morris counters is sub-optimal. In this work, we present a simple and optimal $d$-dimensional counter with Euclidean relative error $\u03c3$, i.e., $\\mathbb{E} |\\hat{x}-x|^2 <\u03c3^2|x|^2$ where $|x|=\\sqrt{\\sum_{j=1}^d x_j^2}$, with a matching lower bound. The upper and lower bounds are proved with ideas that are strikingly simple. The upper bound is constructed with a certain variable-length integer encoding and the lower bound is derived from a straightforward volumetric estimation of sphere covering.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03084",
        "abstract url": "https://arxiv.org/abs/2411.03084",
        "title": "Sum Rate Maximization in the Constant Envelope MIMO Downlink with the RZF Precoder",
        "rating": "-10",
        "keywords": [],
        "abstract": "Feeding power amplifiers (PAs) with constant envelope (CE) signals is an effective way to reduce the power consumption in massive multiple-input-multiple-output (MIMO) systems. The nonlinear distortion caused by CE signaling must be mitigated by means of signal processing to improve the achievable sum rates. To this purpose, many linear and nonlinear precoding techniques have been developed for the CE MIMO downlink. The vast majority of these CE precoding techniques do not include a power allocation scheme, which is indispensable to achieve adequate performances in the downlink with channel gain imbalances between users. In this paper, we present two algorithms to produce a power allocation scheme for regularized zero-forcing (RZF) precoding in CE MIMO downlink. Both techniques are based on transforming the CE quantized MIMO downlink to an approximately equivalent system of parallel single-input-single-output (SISO) channels. The first technique is proven to solve the sum rate maximization problem in the approximate system optimally, whereas the second technique obtains the local maximum with lower complexity. We also extend another state-of-the-art quantization aware sum rate maximization algorithm with linear precoding to the CE downlink. Numerical results illustrate significant gains for the performance of the RZF precoder when the CE quantization is taken into account in a power allocation. Another key numerical result is that the proposed RZF techniques achieve almost the identical performance so that the one with lower computational complexity is chosen as the main method. Results also show that the proposed RZF precoding schemes perform at least as good as the state-of-the-art method with an advantage that the main RZF method has significantly lower computational complexity than the state-of-the-art.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "This paper has been submitted to IEEE Transactions on Vehicular Technology for review and was presented in part at 2023 IEEE 98th Vehicular Technology Conference (VTC2023-Fall)"
    },
    {
        "paper id": "2411.03096",
        "abstract url": "https://arxiv.org/abs/2411.03096",
        "title": "On the Complexity of Pure-State Consistency of Local Density Matrices",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this work we investigate the computational complexity of the pure consistency of local density matrices ($\\mathsf{PureCLDM}$) and pure $N$-representability ($\\mathsf{Pure}$-$N$-$\\mathsf{Representability}$) problems. In these problems the input is a set of reduced density matrices and the task is to determine whether there exists a global \\emph{pure} state consistent with these reduced density matrices. While mixed $\\mathsf{CLDM}$, i.e. where the global state can be mixed, was proven to be $\\mathsf{QMA}$-complete by Broadbent and Grilo [JoC 2022], almost nothing was known about the complexity of the pure version. Before our work the best upper and lower bounds were $\\mathsf{QMA}(2)$ and $\\mathsf{QMA}$. Our contribution to the understanding of these problems is twofold. Firstly, we define a pure state analogue of the complexity class $\\mathsf{QMA}^+$ of Aharanov and Regev [FOCS 2003], which we call $\\mathsf{PureSuperQMA}$. We prove that both $\\mathsf{Pure}$-$N$-$\\mathsf{Representability}$ and $\\mathsf{PureCLDM}$ are complete for this new class. Along the way we supplement Broadbent and Grilo by proving hardness for 2-qubit reduced density matrices and showing that mixed $N$-$\\mathsf{Representability}$ is $\\mathsf{QMA}$ complete. Secondly, we improve the upper bound on $\\mathsf{PureCLDM}$. Using methods from algebraic geometry, we prove that $\\mathsf{PureSuperQMA} \\subseteq \\mathsf{PSPACE}$. Our methods, and the $\\mathsf{PSPACE}$ upper bound, are also valid for $\\mathsf{PureCLDM}$ with exponential or even perfect precision, hence $\\mathsf{precisePureCLDM}$ is not $\\mathsf{preciseQMA}(2) = \\mathsf{NEXP}$-complete, unless $\\mathsf{PSPACE} = \\mathsf{NEXP}$. We view this as evidence for a negative answer to the longstanding open question whether $\\mathsf{PureCLDM}$ is $\\mathsf{QMA}(2)$-complete.",
        "subjects": [
            "quant-ph",
            "cs.CC"
        ],
        "comment": "42 pages, 1 figure"
    },
    {
        "paper id": "2411.03099",
        "abstract url": "https://arxiv.org/abs/2411.03099",
        "title": "Optimized Cryo-CMOS Technology with VTH<0.2V and Ion>1.2mA/um for High-Peformance Computing",
        "rating": "-10",
        "keywords": [],
        "abstract": "We report the design-technology co-optimization (DTCO) scheme to develop a 28-nm cryogenic CMOS (Cryo-CMOS) technology for high-performance computing (HPC). The precise adjustment of halo implants manages to compensate the threshold voltage (VTH) shift at low temperatures. The optimized NMOS and PMOS transistors, featured by VTH<0.2V, sub-threshold swing (SS)<30 mV/dec, and on-state current (Ion)>1.2mA/um at 77K, warrant a reliable sub-0.6V operation. Moreover, the enhanced driving strength of Cryo-CMOS inherited from a higher transconductance leads to marked improvements in elevating the ring oscillator frequency by 20%, while reducing the power consumption of the compute-intensive cryogenic IC system by 37% at 77K.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03103",
        "abstract url": "https://arxiv.org/abs/2411.03103",
        "title": "Benign landscape for Burer-Monteiro factorizations of MaxCut-type semidefinite programs",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider MaxCut-type semidefinite programs (SDP) which admit a low rank solution. To numerically leverage the low rank hypothesis, a standard algorithmic approach is the Burer-Monteiro factorization, which allows to significantly reduce the dimensionality of the problem at the cost of its convexity. We give a sharp condition on the conditioning of the Laplacian matrix associated with the SDP under which any second-order critical point of the non-convex problem is a global minimizer. By applying our theorem, we improve on recent results about the correctness of the Burer-Monteiro approach on $\\mathbb{Z}_2$-synchronization problems.",
        "subjects": [
            "math.OC",
            "cs.CC",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03108",
        "abstract url": "https://arxiv.org/abs/2411.03108",
        "title": "\"Create a Fear of Missing Out\" -- ChatGPT Implements Unsolicited Deceptive Designs in Generated Websites Without Warning",
        "rating": "-10",
        "keywords": [],
        "abstract": "With the recent advancements in Large Language Models (LLMs), web developers increasingly apply their code-generation capabilities to website design. However, since these models are trained on existing designerly knowledge, they may inadvertently replicate bad or even illegal practices, especially deceptive designs (DD). This paper examines whether users can accidentally create DD for a fictitious webshop using GPT-4. We recruited 20 participants, asking them to use ChatGPT to generate functionalities (product overview or checkout) and then modify these using neutral prompts to meet a business goal (e.g., \"increase the likelihood of us selling our product\"). We found that all 20 generated websites contained at least one DD pattern (mean: 5, max: 9), with GPT-4 providing no warnings. When reflecting on the designs, only 4 participants expressed concerns, while most considered the outcomes satisfactory and not morally problematic, despite the potential ethical and legal implications for end-users and those adopting ChatGPT's recommendations",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03121",
        "abstract url": "https://arxiv.org/abs/2411.03121",
        "title": "Fully Dynamic $k$-Median with Near-Optimal Update Time and Recourse",
        "rating": "-10",
        "keywords": [],
        "abstract": "In metric $k$-clustering, we are given as input a set of $n$ points in a general metric space, and we have to pick $k$ centers and cluster the input points around these chosen centers, so as to minimize an appropriate objective function. In recent years, significant effort has been devoted to the study of metric $k$-clustering problems in a dynamic setting, where the input keeps changing via updates (point insertions/deletions), and we have to maintain a good clustering throughout these updates. The performance of such a dynamic algorithm is measured in terms of three parameters: (i) Approximation ratio, which signifies the quality of the maintained solution, (ii) Recourse, which signifies how stable the maintained solution is, and (iii) Update time, which signifies the efficiency of the algorithm. We consider the metric $k$-median problem, where the objective is the sum of the distances of the points to their nearest centers. We design the first dynamic algorithm for this problem with near-optimal guarantees across all three performance measures (up to a constant factor in approximation ratio, and polylogarithmic factors in recourse and update time). Specifically, we obtain a $O(1)$-approximation algorithm for dynamic metric $k$-median with $\\tilde{O}(1)$ recourse and $\\tilde{O}(k)$ update time. Prior to our work, the state-of-the-art here was the recent result of [Bhattacharya et al., FOCS'24], who obtained $O(\u03b5^{-1})$-approximation ratio with $\\tilde{O}(k^\u03b5)$ recourse and $\\tilde{O}(k^{1+\u03b5})$ update time. We achieve our results by carefully synthesizing the concept of robust centers introduced in [Fichtenberger et al., SODA'21] along with the randomized local search subroutine from [Bhattacharya et al., FOCS'24], in addition to several key technical insights of our own.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03137",
        "abstract url": "https://arxiv.org/abs/2411.03137",
        "title": "From Pen to Prompt: How Creative Writers Integrate AI into their Writing Practice",
        "rating": "-10",
        "keywords": [],
        "abstract": "Creative writers have a love for their craft, yet AI systems using large language models (LLMs) offer the automation of significant parts of the writing process. So why do some creative writers choose to integrate AI into their workflows? To explore this, we interview and observe a writing session with 18 creative writers who already use AI regularly in their writing practice. Our findings reveal that creative writers are intentional about how they incorporate AI, making many deliberate decisions about when and how to engage AI based on the core values they hold about writing. These values, such as authenticity and craftsmanship, alongside writers' relationships with and use of AI influence the parts of writing over which they wish to maintain control. Through our analysis, we contribute a taxonomy of writer values, writer relationships with AI, and integration strategies, and discuss how these three elements interrelate.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03160",
        "abstract url": "https://arxiv.org/abs/2411.03160",
        "title": "Hybrid Rebeca Revisited",
        "rating": "-10",
        "keywords": [],
        "abstract": "Hybrid Rebeca is introduced for modeling asynchronous event-based Cyber-Physical Systems (CPSs). In this work, we extend Hybrid Rebeca to allow the modeling of non-deterministic time behavior. We provide a set of rules to define the semantic model of Hybrid Rebeca models in terms of Time Transition Systems which represents an over-approximation of the reachable states of a Hybrid Rebeca model. Then, we adapt the reachability analysis algorithm of Flow$^*$ for Hybrid Rebeca models leveraging our semantic rules. This improves the analysis significantly because the previous technique relied on the reachability analysis of hybrid automata by deriving a monolithic hybrid automaton from a given Hybrid Rebeca model, leading to a huge hybrid automaton. We illustrate the applicability of our approach through a case study.",
        "subjects": [
            "cs.FL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03174",
        "abstract url": "https://arxiv.org/abs/2411.03174",
        "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
        "rating": "-10",
        "keywords": [],
        "abstract": "As a core component in modern data centers, key-value cache provides high-throughput and low-latency services for high-speed data processing. The effectiveness of a key-value cache relies on its ability of accommodating the needed data. However, expanding the cache capacity is often more difficult than commonly expected because of many practical constraints, such as server costs, cooling issues, rack space, and even human resource expenses. A potential solution is compression, which virtually extends the cache capacity by condensing data in cache. In practice, this seemingly simple idea has not gained much traction in key-value cache system design, due to several critical issues: the compression-unfriendly index structure, severe read/write amplification, wasteful decompression operations, and heavy computing cost. This paper presents a hybrid DRAM-SSD cache design to realize a systematic integration of data compression in key-value cache. By treating compression as an essential component, we have redesigned the indexing structure, data management, and leveraged the emerging computational SSD hardware for collaborative optimizations. We have developed a prototype, called ZipCache. Our experimental results show that ZipCache can achieve up to 72.4% higher throughput and 42.4% lower latency, while reducing the write amplification by up to 26.2 times.",
        "subjects": [
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03176",
        "abstract url": "https://arxiv.org/abs/2411.03176",
        "title": "Developing Simulation Models for Soft Robotic Grippers in Webots",
        "rating": "-10",
        "keywords": [],
        "abstract": "Robotic simulators provide cost-effective and risk-free virtual environments for studying robotic designs, control algorithms, and sensor integrations. They typically host extensive libraries of sensors and actuators that facilitate rapid prototyping and design evaluations in simulation. The use of the most prominent existing robotic simulators is however limited to simulation of rigid-link robots. On the other hand, there exist dedicated specialized environments for simulating soft robots. This separation limits the study of soft robotic systems, particularly in hybrid scenarios where soft and rigid sub-systems co-exist. In this work, we develop a lightweight open-source digital twin of a commercially available soft gripper, directly integrated within the robotic simulator Webots. We use a Rigid-Link-Discretization (RLD) model to simulate the soft gripper. Using a Particle Swarm Optimization (PSO) approach, we identify the parameters of the RLD model based on the kinematics and dynamics of the physical system and show the efficacy of our modeling approach in validation experiments. All software and experimental details are available on github: https://github.com/anonymousgituser1/Robosoft2025",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages, 9 figures, 1 table"
    },
    {
        "paper id": "2411.03219",
        "abstract url": "https://arxiv.org/abs/2411.03219",
        "title": "Exploring the Cybersecurity-Resilience Gap: An Analysis of Student Attitudes and Behaviors in Higher Education",
        "rating": "-10",
        "keywords": [],
        "abstract": "Cyberattacks frequently target higher educational institutions, making cybersecurity awareness and resilience critical for students. However, limited research exists on cybersecurity awareness, attitudes, and resilience among students in higher education. This study addresses this gap using the Theory of Planned Behavior as a theoretical framework. A modified Human Aspects of Information Security Questionnaire was employed to gather 266 valid responses from undergraduate and postgraduate students at a South African higher education institution. Key dimensions of cybersecurity awareness and behavior, including password management, email usage, social media practices, and mobile device security, were assessed. A significant disparity in cybersecurity awareness and practices, with postgraduate students demonstrating superior performance across several dimensions was noted. This research postulates the existence of a Cybersecurity-Education Inflection Point during the transition to postgraduate studies, coined as the Cybersecurity-Resilience Gap. These concepts provide a foundation for developing targeted cybersecurity education initiatives in higher education, particularly highlighting the need for earlier intervention at the undergraduate level.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03220",
        "abstract url": "https://arxiv.org/abs/2411.03220",
        "title": "A Unified Fault Ride Through Technique for Virtual Oscillator based Grid Forming Controllers",
        "rating": "-10",
        "keywords": [],
        "abstract": "Grid-forming technology has a crucial role in achieving the future all renewable power grid. Among different types of grid-forming controllers, Virtual Oscillator (VO) based Controllers (VOCs) are the most advanced. VOCs outperform the conventional droop-based grid-forming controllers in terms of dynamic performance and synchronization stability by adapting time-domain-based implementation. However, because of the time-domain-based implementation, the same Fault Ride-through (FRT) techniques for droop-based controllers are incompatible with VOCs. Existing literature has successfully incorporated current limiting techniques in VOCs to protect the converters during severe transient conditions. Nevertheless, some very important aspects of FRT requirements are not attended to by the existing literature on VOCs, such as maintaining synchronization with the network during a fault, minimizing power oscillation during a fault, and at the fault clearance. First, this article introduces a unique analytical approach for quantifying the underlying dynamics of VOCs during faults. Next, using the mentioned analysis and in-depth reasoning, the systematic development of a unique FRT control architecture for VOCs is presented. The proposed FRT technique has unified both current and voltage synchronization in the same architecture to work successfully under three-phase and unbalanced faults. The performance of the proposed FRT technique is thoroughly validated using simulation studies.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03227",
        "abstract url": "https://arxiv.org/abs/2411.03227",
        "title": "Tight Sampling Bounds for Eigenvalue Approximation",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider the problem of estimating the spectrum of a symmetric bounded entry (not necessarily PSD) matrix via entrywise sampling. This problem was introduced by [Bhattacharjee, Dexter, Drineas, Musco, Ray '22], where it was shown that one can obtain an $\u03b5n$ additive approximation to all eigenvalues of $A$ by sampling a principal submatrix of dimension $\\frac{\\text{poly}(\\log n)}{\u03b5^3}$. We improve their analysis by showing that it suffices to sample a principal submatrix of dimension $\\tilde{O}(\\frac{1}{\u03b5^2})$ (with no dependence on $n$). This matches known lower bounds and therefore resolves the sample complexity of this problem up to $\\log\\frac{1}\u03b5$ factors. Using similar techniques, we give a tight $\\tilde{O}(\\frac{1}{\u03b5^2})$ bound for obtaining an additive $\u03b5\\|A\\|_F$ approximation to the spectrum of $A$ via squared row-norm sampling, improving on the previous best $\\tilde{O}(\\frac{1}{\u03b5^{8}})$ bound. We also address the problem of approximating the top eigenvector for a bounded entry, PSD matrix $A.$ In particular, we show that sampling $O(\\frac{1}\u03b5)$ columns of $A$ suffices to produce a unit vector $u$ with $u^T A u \\geq \u03bb_1(A) - \u03b5n$. This matches what one could achieve via the sampling bound of [Musco, Musco'17] for the special case of approximating the top eigenvector, but does not require adaptivity. As additional applications, we observe that our sampling results can be used to design a faster eigenvalue estimation sketch for dense matrices resolving a question of [Swartworth, Woodruff'23], and can also be combined with [Musco, Musco'17] to achieve $O(1/\u03b5^3)$ (adaptive) sample complexity for approximating the spectrum of a bounded entry PSD matrix to $\u03b5n$ additive error.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03243",
        "abstract url": "https://arxiv.org/abs/2411.03243",
        "title": "Guidelines para Desenvolvimento de Jogos Mobile Inclusivos",
        "rating": "-10",
        "keywords": [],
        "abstract": "Games represent a significant part of modern culture, which demonstrates the importance of ensuring that everyone can participate and play in order to feel included in our society. However, most digital games end up being inaccessible to people with disabilities. Part of the problem when thinking about inclusive game design is that there is no single solution for accessibility, and what works well for one group may not work for another. This work proposes a set of guidelines for the development of inclusive mobile games, considering the widespread use of smartphones by the population and the need to include people with disabilities in the gaming culture.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "in Portuguese language"
    },
    {
        "paper id": "2411.03247",
        "abstract url": "https://arxiv.org/abs/2411.03247",
        "title": "Exploring Multi-Fidelity Aeroelastic Tailoring: Prospect and Model Assessment",
        "rating": "-10",
        "keywords": [],
        "abstract": "The design and optimisation of aircraft wings are critical tasks in aerospace engineering, requiring a balance between structural integrity, aerostructural performance, and manufacturability. This multifaceted challenge involves the interplay of various disciplines, each with distinct parameters and constraints. Traditional design approaches often fall short, necessitating advanced methodologies like Multidisciplinary Design Optimisation (MDO). MDO integrates aerodynamic, structural, and manufacturability analyses to explore a vast design space and identify optimal solutions that meet performance, safety, and cost criteria. The work highlights the challenge of optimising aircraft designs using multiple models of varying fidelity. Traditional sequential optimisation approaches, which progressively integrate disciplines, may miss potential superior designs due to limited initial information. Instead, concurrent optimisation schemes are explored, utilising both low-fidelity (beam-based) and high-fidelity (shell-based) models. This approach promises structural feasibility, reduces computational costs, and incorporates high-fidelity information early in the design process. The envisioned methodology bridges different design stages, enabling better overall aircraft performance. By aligning and comparing a beam-based and shell-based model, the study explores their use in multi-fidelity optimisation. The results demonstrate the feasibility and benefits of this approach, offering a robust framework for future aircraft design projects.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03248",
        "abstract url": "https://arxiv.org/abs/2411.03248",
        "title": "On the Role of Constraints in the Complexity of Min-Max Optimization",
        "rating": "-10",
        "keywords": [],
        "abstract": "We investigate the role of constraints in the computational complexity of min-max optimization. The work of Daskalakis, Skoulakis, and Zampetakis [2021] was the first to study min-max optimization through the lens of computational complexity, showing that min-max problems with nonconvex-nonconcave objectives are PPAD-hard. However, their proof hinges on the presence of joint constraints between the maximizing and minimizing players. The main goal of this paper is to understand the role of these constraints in min-max optimization. The first contribution of this paper is a fundamentally new proof of their main result, which improves it in multiple directions: it holds for degree 2 polynomials, it is essentially tight in the parameters, and it is much simpler than previous approaches, clearly highlighting the role of constraints in the hardness of the problem. Second, we show that with general constraints (i.e., the min player and max player have different constraints), even convex-concave min-max optimization becomes PPAD-hard. Along the way, we also provide PPAD-membership of a general problem related to quasi-variational inequalities, which has applications beyond our problem.",
        "subjects": [
            "cs.GT",
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03265",
        "abstract url": "https://arxiv.org/abs/2411.03265",
        "title": "Information geometry of diffeomorphism groups",
        "rating": "-10",
        "keywords": [],
        "abstract": "The study of diffeomorphism groups and their applications to problems in analysis and geometry has a long history. In geometric hydrodynamics, pioneered by V.~Arnold in the 1960s, one considers an ideal fluid flow as the geodesic motion on the infinite-dimensional group of volume-preserving diffeomorphisms of the fluid domain with respect to the metric defined by the kinetic energy. Similar considerations on the space of densities lead to a geometric description of optimal mass transport and the Kantorovich-Wasserstein metric. Likewise, information geometry associated with the Fisher-Rao metric and the Hellinger distance has an equally beautiful infinite-dimensional geometric description and can be regarded as a higher-order Sobolev analogue of optimal transportation. In this work we review various metrics on diffeomorphism groups relevant to this approach and introduce appropriate topology, smooth structures and dynamics on the corresponding infinite-dimensional manifolds. Our main goal is to demonstrate how, alongside topological hydrodynamics, Hamiltonian dynamics and optimal mass transport, information geometry with its elaborate toolbox has become yet another exciting field for applications of geometric analysis on diffeomorphism groups.",
        "subjects": [
            "math.DG",
            "cs.IT",
            "math-ph"
        ],
        "comment": "89 pages, 10 figures"
    },
    {
        "paper id": "2411.03277",
        "abstract url": "https://arxiv.org/abs/2411.03277",
        "title": "Asymptotic stability equals exponential stability -- while you twist your eyes",
        "rating": "-10",
        "keywords": [],
        "abstract": "Suppose that two vector fields on a smooth manifold render some equilibrium point globally asymptotically stable (GAS). We show that there exists a homotopy between the corresponding semiflows such that this point remains GAS along this homotopy.",
        "subjects": [
            "math.DS",
            "eess.SY",
            "math.OC"
        ],
        "comment": "Preprint under review, 27 pages, 2 figures, comments are welcome"
    },
    {
        "paper id": "2411.03288",
        "abstract url": "https://arxiv.org/abs/2411.03288",
        "title": "Model Predictive Control of Collinear Coulomb Spacecraft Formations",
        "rating": "-10",
        "keywords": [],
        "abstract": "A model predictive control scheme to stabilize desired configurations of collinear Coulomb spacecraft formations is derived in this paper. The nonlinearities of the dynamics with respect to the input make this problem difficult to solve, computationally. It is shown that the nonlinearities in the input lead to a finite horizon optimization problem which is a nonconvex quadratically-constrained quadratic program (QCQP). A convex relaxation of the nonconvex QCQP is therefore derived which can be solved quickly using a convex optimization solver. A simulation of a four spacecraft formation is provided which demonstrates why optimizing over a prediction horizon is a prudent approach to Coulomb spacecraft formation control.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "6 pages, 5 figures"
    },
    {
        "paper id": "2411.03295",
        "abstract url": "https://arxiv.org/abs/2411.03295",
        "title": "Examining Human-AI Collaboration for Co-Writing Constructive Comments Online",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper examines how large language models (LLMs) can help people write constructive comments in online debates on divisive social issues and whether the notions of constructiveness vary across cultures. Through controlled experiments with 600 participants from India and the US, who reviewed and wrote constructive comments on online threads on Islamophobia and homophobia, we found potential misalignment in how LLMs and humans perceive constructiveness in online comments. While the LLM was more likely to view dialectical comments as more constructive, participants favored comments that emphasized logic and facts more than the LLM did. Despite these differences, participants rated LLM-generated and human-AI co-written comments as significantly more constructive than those written independently by humans. Our analysis also revealed that LLM-generated and human-AI co-written comments exhibited more linguistic features associated with constructiveness compared to human-written comments on divisive topics. When participants used LLMs to refine their comments, the resulting comments were longer, more polite, positive, less toxic, and more readable, with added argumentative features that retained the original intent but occasionally lost nuances. Based on these findings, we discuss ethical and design considerations in using LLMs to facilitate constructive discourse online.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03299",
        "abstract url": "https://arxiv.org/abs/2411.03299",
        "title": "Concurrent Composition for Continual Mechanisms",
        "rating": "-10",
        "keywords": [],
        "abstract": "A series of recent works by Lyu, Wang, Vadhan, and Zhang (TCC `21, NeurIPS `22, STOC `23) showed that composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of interactive differentially private mechanism, when differential privacy is measured using $f$-DP and the adversary is adaptive. We extend their work to the $\\textit{continual observation setting,}$ where the data is arriving online in a potentially adaptive manner. More specifically, we show that all composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of continual differentially private mechanism, where the adversary is adaptive. We show this result for $f$-DP, which also implies the result for pure DP and $(\u03b5, \u03b4)$-DP.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03369",
        "abstract url": "https://arxiv.org/abs/2411.03369",
        "title": "Towards Interoperability Testing of Smart Energy Systems -- An Overview and Discussion of Possibilities",
        "rating": "-10",
        "keywords": [],
        "abstract": "Interoperability is the key to implementing a wide range of energy systems applications. It involves the seamless cooperation of different methods and components. With smart energy systems, interoperability faces challenges due to integrating differ-ent approaches and technologies. This includes dealing with heterogeneous approaches with various communication proto-cols and data formats. However, it is essential for smart energy systems to carry out thorough interoperability tests. They are usually diverse, and challenging, thus requiring careful consideration of compatibility issues and complex integration scenari-os. Overcoming these challenges requires a systematic approach that includes thorough test planning, rigorous testing, and continuous test monitoring. Although numerous testing approaches exist, most are more developed at the component/device level than at the system level. Consequently, there are few approaches and related facilities to test the interoperability of smart energy approaches and solutions at the system level. This work analyses existing interoperability test concepts, identi-fies enablers and the potential for harmonisation of procedures, and proposes further developments of these approaches.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "14th Mediterranean Conference on Power Generation Transmission, Distribution and Energy Conversion (MED POWER 2024)"
    },
    {
        "paper id": "2411.03370",
        "abstract url": "https://arxiv.org/abs/2411.03370",
        "title": "Balancing Profit and Traveller Acceptance in Ride-Pooling Personalised Fares",
        "rating": "-10",
        "keywords": [],
        "abstract": "In a ride-pooling system, travellers experience discomfort associated with a detour and a longer travel time, which is compensated with a sharing discount. Most studies assume travellers receive either a flat discount or, in rare cases, a proportional to the inconvenience. We show the system benefits from individually tailored fares. We argue that fares that optimise an expected profit of an operator also improve system-wide performance if they include travellers' acceptance. Our pricing method is set in a heterogeneous population, where travellers have varying levels of value-of-time and willingness-to-share, unknown to the operator. A high fare discourages clients from the service, while a low fare reduces the profit margin. Notably, a shared ride is only realised if accepted by all co-travellers (decision is driven by the latent behavioural factors). Our method reveals intriguing properties of the shareability topology. Not only identifies rides efficient for the system and supports them with reduced fares (to increase their realisation probability), but also identifies travellers unattractive for the system (e.g. due to incompatibility with other travellers) and effectively shifts them to private rides via high fares. Unlike in previous methods, such approach naturally balances the travellers satisfaction and the profit maximisation. With an experiment set in NYC, we show that this leads to significant improvements over the flat discount baseline: the mileage (proxy for environmental externalities) is reduced by 4.5% and the operator generates more profit per mile (over 20% improvement). We argue that ride pooling systems with fares that maximise profitability are more sustainable and efficient if they include travellers' satisfaction. Keywords: ride-pooling, personalised pricing, individual discounts",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03380",
        "abstract url": "https://arxiv.org/abs/2411.03380",
        "title": "A networked small-gain theorem based on discrete-time diagonal stability",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present a new sufficient condition for finite-gain $L_2$ input-to-output stability of a networked system. The condition requires a matrix, that combines information on the $L_2$ gains of the sub-systems and their interconnections, to be discrete-time diagonally stable (DTDS). We show that the new result generalizes the standard small gain theorem for the negative feedback connection of two sub-systems. An important advantage of the new result is that known sufficient conditions for DTDS can be applied to derive sufficient conditions for networked input-to-output stability. We demonstrate this using several examples. We also derive a new necessary and sufficient condition for a matrix that is a rank one perturbation of a Schur diagonal matrix to be DTDS.",
        "subjects": [
            "eess.SY",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03390",
        "abstract url": "https://arxiv.org/abs/2411.03390",
        "title": "Six Candidates Suffice to Win a Voter Majority",
        "rating": "-10",
        "keywords": [],
        "abstract": "A cornerstone of social choice theory is Condorcet's paradox which says that in an election where $n$ voters rank $m$ candidates it is possible that, no matter which candidate is declared the winner, a majority of voters would have preferred an alternative candidate. Instead, can we always choose a small committee of winning candidates that is preferred to any alternative candidate by a majority of voters? Elkind, Lang, and Saffidine raised this question and called such a committee a Condorcet winning set. They showed that winning sets of size $2$ may not exist, but sets of size logarithmic in the number of candidates always do. In this work, we show that Condorcet winning sets of size $6$ always exist, regardless of the number of candidates or the number of voters. More generally, we show that if $\\frac\u03b1{1 - \\ln \u03b1} \\geq \\frac{2}{k + 1}$, then there always exists a committee of size $k$ such that less than an $\u03b1$ fraction of the voters prefer an alternate candidate. These are the first nontrivial positive results that apply for all $k \\geq 2$. Our proof uses the probabilistic method and the minimax theorem, inspired by recent work on approximately stable committee selection. We construct a distribution over committees that performs sufficiently well (when compared against any candidate on any small subset of the voters) so that this distribution must contain a committee with the desired property in its support.",
        "subjects": [
            "cs.GT",
            "cs.DM",
            "cs.DS",
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03404",
        "abstract url": "https://arxiv.org/abs/2411.03404",
        "title": "EVA-S3PC: Efficient, Verifiable, Accurate Secure Matrix Multiplication Protocol Assembly and Its Application in Regression",
        "rating": "-10",
        "keywords": [],
        "abstract": "Efficient multi-party secure matrix multiplication is crucial for privacy-preserving machine learning, but existing mixed-protocol frameworks often face challenges in balancing security, efficiency, and accuracy. This paper presents an efficient, verifiable and accurate secure three-party computing (EVA-S3PC) framework that addresses these challenges with elementary 2-party and 3-party matrix operations based on data obfuscation techniques. We propose basic protocols for secure matrix multiplication, inversion, and hybrid multiplication, ensuring privacy and result verifiability. Experimental results demonstrate that EVA-S3PC achieves up to 14 significant decimal digits of precision in Float64 calculations, while reducing communication overhead by up to $54.8\\%$ compared to state of art methods. Furthermore, 3-party regression models trained using EVA-S3PC on vertically partitioned data achieve accuracy nearly identical to plaintext training, which illustrates its potential in scalable, efficient, and accurate solution for secure collaborative modeling across domains.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "18 pages,22 figures"
    },
    {
        "paper id": "2411.03407",
        "abstract url": "https://arxiv.org/abs/2411.03407",
        "title": "Chorded Cycle Facets of Clique Partitioning Polytopes",
        "rating": "-10",
        "keywords": [],
        "abstract": "The $q$-chorded $k$-cycle inequalities are a class of valid inequalities for the clique partitioning polytope. It is known that for $q = 2$ or $q = \\tfrac{k-1}{2}$, these inequalities induce facets of the clique partitioning polytope if and only if $k$ is odd. We solve the open problem of characterizing such facets for arbitrary $k$ and $q$. More specifically, we prove that the $q$-chorded $k$-cycle inequalities induce facets of the clique partitioning polytope if and only if two conditions are satisfied: $k = 1$ mod $q$, and if $k=3q+1$ then $q=3$ or $q$ is even. This establishes the existence of many facets induced by $q$-chorded $k$-cycle inequalities beyond those previously known.",
        "subjects": [
            "cs.DM",
            "math.OC"
        ],
        "comment": "17 pages"
    },
    {
        "paper id": "2411.03444",
        "abstract url": "https://arxiv.org/abs/2411.03444",
        "title": "Algebraic metacomplexity and representation theory",
        "rating": "-10",
        "keywords": [],
        "abstract": "We prove that in the algebraic metacomplexity framework, the decomposition of metapolynomials into their isotypic components can be implemented efficiently, namely with only a quasipolynomial blowup in the circuit size. This means that many existing algebraic complexity lower bound proofs can be efficiently converted into isotypic lower bound proofs via highest weight metapolynomials, a notion studied in geometric complexity theory. In the context of algebraic natural proofs, our result means that without loss of generality algebraic natural proofs can be assumed to be isotypic. Our proof is built on the Poincar\u00e9--Birkhoff--Witt theorem for Lie algebras and on Gelfand--Tsetlin theory, for which we give the necessary comprehensive background.",
        "subjects": [
            "cs.CC",
            "math.AG",
            "math.RT"
        ],
        "comment": "29 pages. Comments are welcome!"
    },
    {
        "paper id": "2411.03474",
        "abstract url": "https://arxiv.org/abs/2411.03474",
        "title": "Computational Tools for Real-time Analysis of High-throughput High-resolution TEM (HRTEM) Images of Conjugated Polymers",
        "rating": "-10",
        "keywords": [],
        "abstract": "Automated analysis of high-resolution transmission electron microscopy (HRTEM) images is increasingly essential for advancing research in organic electronics, where precise characterization of nanoscale crystal structures is crucial for optimizing material properties. This paper introduces an open-source computational framework designed for real-time analysis of HRTEM data, with a focus on characterizing complex microstructures in conjugated polymers, and illustrated using Poly[N-9$'$-heptadecanyl-2,7-carbazole-alt-5,5-(4$'$,7$'$-di-2-thienyl-2$'$,1$'$,3$'$-benzothiadiazole)] (PCDTBT), a key material in organic photovoltaics. The framework employs fast, automated image processing algorithms, enabling rapid extraction of structural features like \\textit{d}-spacing, orientation, and shape metrics. Gaussian process optimization rapidly identifies the user-defined parameters in the approach, reducing the need for manual parameter tuning and thus enhancing reproducibility and usability. Additionally, the framework is compatible with high-performance computing (HPC) environments, allowing for efficient, large-scale data processing at near real-time speeds. A unique feature of the framework is a Wasserstein distance-based stopping criterion, which optimizes data collection by determining when further sampling no longer adds statistically significant information. This capability optimizes the amount of time the TEM facility is used while ensuring data adequacy for in-depth analysis. Open-source and tested on a substantial PCDTBT dataset, this tool offers a powerful, robust, and accessible solution for high-throughput material characterization in organic electronics.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "15 pages, 9 figures, 3 tables"
    },
    {
        "paper id": "2411.03503",
        "abstract url": "https://arxiv.org/abs/2411.03503",
        "title": "TwiNet: Connecting Real World Networks to their Digital Twins Through a Live Bidirectional Link",
        "rating": "-10",
        "keywords": [],
        "abstract": "Only the chairs can edit The wireless spectrum's increasing complexity poses challenges and opportunities, highlighting the necessity for real-time solutions and robust data processing capabilities. Digital Twin (DT), virtual replicas of physical systems, integrate real-time data to mirror their real-world counterparts, enabling precise monitoring and optimization. Incorporating DTs into wireless communication enhances predictive maintenance, resource allocation, and troubleshooting, thus bolstering network reliability. Our paper introduces TwiNet, enabling bidirectional, near-realtime links between real-world wireless spectrum scenarios and DT replicas. Utilizing the protocol, MQTT, we can achieve data transfer times with an average latency of 14 ms, suitable for real-time communication. This is confirmed by monitoring real-world traffic and mirroring it in real-time within the DT's wireless environment. We evaluate TwiNet's performance in two use cases: (i) assessing risky traffic configurations of UEs in a Safe Adaptive Data Rate (SADR) system, improving network performance by approximately 15% compared to original network selections; and (ii) deploying new CNNs in response to jammed pilots, achieving up to 97% accuracy training on artificial data and deploying a new model in as low as 2 minutes to counter persistent adversaries. TwiNet enables swift deployment and adaptation of DTs, addressing crucial challenges in modern wireless communication systems.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "6 pages, 7 figures, conference paper"
    },
    {
        "paper id": "2411.03507",
        "abstract url": "https://arxiv.org/abs/2411.03507",
        "title": "Model-based Deep Learning for QoS-Aware Rate-Splitting Multiple Access Wireless Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Next generation communications demand for better spectrum management, lower latency, and guaranteed quality-of-service (QoS). Recently, Artificial intelligence (AI) has been widely introduced to advance these aspects in next generation wireless systems. However, such AI applications suffer from limited training data, low robustness, and poor generalization capabilities. To address these issues, a model-driven deep unfolding (DU) algorithm is introduced in this paper to bridge the gap between traditional model-driven communication algorithms and data-driven deep learning. Focusing on the QoS-aware rate-splitting multiple access (RSMA) resource allocation problem in multi-user communications, a conventional fractional programming (FP) algorithm is first applied as a benchmark. The solution is then refined by the application of projection gradient descent (PGD). DU is employed to further speed up convergence procedure, hence improving the efficiency of PGD. Moreover, the feasibility of results is guaranteed by designing a low-complexity projection based on scale factors, plus adding violation control mechanisms into the loss function that minimizes error rates. Finally, we provide a detailed analysis of the computational complexity and analysis design of the proposed DU algorithm. Extensive simulations are conducted and the results demonstrate that the proposed DU algorithm can reach the optimal communication efficiency with a mere $0.024\\%$ violation rate for 4 layers DU. The DU algorithm also exhibits robustness in out-of-distribution tests and can be effectively trained with as few as 50 samples.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication"
    },
    {
        "paper id": "2411.03514",
        "abstract url": "https://arxiv.org/abs/2411.03514",
        "title": "Deriving Analytical Solutions Using Symbolic Matrix Structural Analysis: Part 1 -- Continuous Beams",
        "rating": "-10",
        "keywords": [],
        "abstract": "This study investigates the use of symbolic computation in Matrix Structural Analysis (MSA) for continuous beams, leveraging the MATLAB Symbolic Math Toolbox. By employing symbolic MSA, analytical expressions for displacements, support reactions, and internal forces are derived, offering deeper insights into structural behavior. This approach facilitates efficient and scalable sensitivity analysis, where partial derivatives of outputs concerning input parameters can be directly computed, enhancing design exploration. The development includes an open-source MATLAB program, hosted on GitHub, enabling symbolic analysis of continuous beams subjected to point and uniform loads. This approach is invaluable for both engineering practice and pedagogy, enriching the understanding of structural mechanics and aiding in education by illustrating clear parameter relationships. The program supports deriving influence lines and identifying maximum response values.",
        "subjects": [
            "cs.CE",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03530",
        "abstract url": "https://arxiv.org/abs/2411.03530",
        "title": "Improving precision of A/B experiments using trigger intensity",
        "rating": "-10",
        "keywords": [],
        "abstract": "In industry, online randomized controlled experiment (a.k.a A/B experiment) is a standard approach to measure the impact of a causal change. These experiments have small treatment effect to reduce the potential blast radius. As a result, these experiments often lack statistical significance due to low signal-to-noise ratio. To improve the precision (or reduce standard error), we introduce the idea of trigger observations where the output of the treatment and the control model are different. We show that the evaluation with full information about trigger observations (full knowledge) improves the precision in comparison to a baseline method. However, detecting all such trigger observations is a costly affair, hence we propose a sampling based evaluation method (partial knowledge) to reduce the cost. The randomness of sampling introduces bias in the estimated outcome. We theoretically analyze this bias and show that the bias is inversely proportional to the number of observations used for sampling. We also compare the proposed evaluation methods using simulation and empirical data. In simulation, evaluation with full knowledge reduces the standard error as much as 85%. In empirical setup, evaluation with partial knowledge reduces the standard error by 36.48%.",
        "subjects": [
            "econ.EM",
            "cs.CE"
        ],
        "comment": "11 pages, 3 page appendix, 6 figures"
    },
    {
        "paper id": "2411.03583",
        "abstract url": "https://arxiv.org/abs/2411.03583",
        "title": "Beyond Regularity: Simple versus Optimal Mechanisms, Revisited",
        "rating": "-10",
        "keywords": [],
        "abstract": "A large proportion of the Bayesian mechanism design literature is restricted to the family of regular distributions $\\mathbb{F}_{\\tt reg}$ [Mye81] or the family of monotone hazard rate (MHR) distributions $\\mathbb{F}_{\\tt MHR}$ [BMP63], which overshadows this beautiful and well-developed theory. We (re-)introduce two generalizations, the family of quasi-regular distributions $\\mathbb{F}_{\\tt Q-reg}$ and the family of quasi-MHR distributions $\\mathbb{F}_{\\tt Q-MHR}$. All four families together form the following hierarchy: $\\mathbb{F}_{\\tt MHR} \\subsetneq (\\mathbb{F}_{\\tt reg} \\cap \\mathbb{F}_{\\tt Q-MHR}) \\subsetneq \\mathbb{F}_{\\tt Q-reg}$ and $\\mathbb{F}_{\\tt Q-MHR} \\subsetneq (\\mathbb{F}_{\\tt reg} \\cup \\mathbb{F}_{\\tt Q-MHR}) \\subsetneq \\mathbb{F}_{\\tt Q-reg}$. The significance of our new families is manifold. First, their defining conditions are immediate relaxations of the regularity/MHR conditions (i.e., monotonicity of the virtual value functions and/or the hazard rate functions), which reflect economic intuition. Second, they satisfy natural mathematical properties (about order statistics) that are violated by both original families $\\mathbb{F}_{\\tt reg}$ and $\\mathbb{F}_{\\tt MHR}$. Third but foremost, numerous results [BK96, HR09a, CD15, DRY15, HR14, AHN+19, JLTX20, JLQ+19b, FLR19, GHZ19b, JLX23, LM24] established before for regular/MHR distributions now can be generalized, with or even without quantitative losses.",
        "subjects": [
            "cs.GT",
            "econ.TH",
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03601",
        "abstract url": "https://arxiv.org/abs/2411.03601",
        "title": "One-dimensional cellular automata with a unique active transition",
        "rating": "-10",
        "keywords": [],
        "abstract": "A one-dimensional cellular automaton $\u03c4: A^\\mathbb{Z} \\to A^\\mathbb{Z}$ is a transformation of the full shift defined via a finite neighborhood $S \\subset \\mathbb{Z}$ and a local function $\u03bc: A^S \\to A$. We study the family of cellular automata whose finite neighborhood $S$ is an interval containing $0$, and there exists a pattern $p \\in A^S$ satisfying that $\u03bc(z) = z(0)$ if and only if $z \\neq p$; this means that these cellular automata have a unique \\emph{active transition}. Despite its simplicity, this family presents interesting and subtle problems, as the behavior of the cellular automaton completely depends on the structure of $p$. We show that every cellular automaton $\u03c4$ with a unique active transition $p \\in A^S$ is either idempotent or strictly almost equicontinuous, and we completely characterize each one of these situations in terms of $p$. In essence, the idempotence of $\u03c4$ depends on the existence of a certain subpattern of $p$ with a translational symmetry.",
        "subjects": [
            "nlin.CG",
            "cs.FL",
            "math.DS"
        ],
        "comment": "13 pages"
    },
    {
        "paper id": "2411.03603",
        "abstract url": "https://arxiv.org/abs/2411.03603",
        "title": "CPEG: Leveraging Consistency Policy with Consensus Guidance for Multi-agent Exploration",
        "rating": "-10",
        "keywords": [],
        "abstract": "Efficient exploration is crucial in cooperative multi-agent reinforcement learning (MARL), especially in sparse-reward settings. However, due to the reliance on the unimodal policy, existing methods are prone to falling into the local optima, hindering the effective exploration of better policies. Furthermore, tackling multi-agent tasks in complex environments requires cooperation during exploration, posing substantial challenges for MARL methods. To address these issues, we propose a Consistency Policy with consEnsus Guidance (CPEG), with two primary components: (a) introducing a multimodal policy to enhance exploration capabilities, and (b) sharing the consensus among agents to foster agent cooperation. For component (a), CPEG incorporates a Consistency model as the policy, leveraging its multimodal nature and stochastic characteristics to facilitate exploration. Regarding component (b), CPEG introduces a Consensus Learner to deduce the consensus on the global state from local observations. This consensus then serves as a guidance for the Consistency Policy, promoting cooperation among agents. The proposed method is evaluated in multi-agent particle environments (MPE) and multi-agent MuJoCo (MAMuJoCo), and empirical results indicate that CPEG not only achieves improvements in sparse-reward settings but also matches the performance of baselines in dense-reward environments.",
        "subjects": [
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03612",
        "abstract url": "https://arxiv.org/abs/2411.03612",
        "title": "Multi-bit Distributed Detection of Sparse Stochastic Signals over Error-Prone Reporting Channels",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider a distributed detection problem within a wireless sensor network (WSN), where a substantial number of sensors cooperate to detect the existence of sparse stochastic signals. To achieve a trade-off between detection performance and system constraints, multi-bit quantizers are employed at local sensors. Then, two quantization strategies, namely raw quantization (RQ) and likelihood ratio quantization (LQ), are examined. The multi-bit quantized signals undergo encoding into binary codewords and are subsequently transmitted to the fusion center via error-prone reporting channels. Upon exploiting the locally most powerful test (LMPT) strategy, we devise two multi-bit LMPT detectors in which quantized raw observations and local likelihood ratios are fused respectively. Moreover, the asymptotic detection performance of the proposed quantized detectors is analyzed, and closed-form expressions for the detection and false alarm probabilities are derived. Furthermore, the multi-bit quantizer design criterion, considering both RQ and LQ, is then proposed to achieve near-optimal asymptotic performance for our proposed detectors. The normalized Fisher information and asymptotic relative efficiency are derived, serving as tools to analyze and compensate for the loss of information introduced by the quantization. Simulation results validate the effectiveness of the proposed detectors, especially in scenarios with low signal-to-noise ratios and poor channel conditions.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Accepted by IEEE Transactions on Signal and Information Processing over Networks"
    },
    {
        "paper id": "2411.03633",
        "abstract url": "https://arxiv.org/abs/2411.03633",
        "title": "Privacy-Preserving Resilient Vector Consensus",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper studies privacy-preserving resilient vector consensus in multi-agent systems against faulty agents, where normal agents can achieve consensus within the convex hull of their initial states while protecting state vectors from being disclosed. Specifically, we consider a modification of an existing algorithm known as Approximate Distributed Robust Convergence Using Centerpoints (ADRC), i.e., Privacy-Preserving ADRC (PP-ADRC). Under PP-ADRC, each normal agent introduces multivariate Gaussian noise to its state during each iteration. We first provide sufficient conditions to ensure that all normal agents' states can achieve mean square convergence under PP-ADRC. Then, we analyze convergence accuracy from two perspectives, i.e., the Mahalanobis distance of the final value from its expectation and the Hausdorff distance based alteration of the convex hull caused by noise when only partial dimensions are added with noise. Then, we employ concentrated geo-privacy to characterize privacy preservation and conduct a thorough comparison with differential privacy. Finally, numerical simulations demonstrate the theoretical results.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03643",
        "abstract url": "https://arxiv.org/abs/2411.03643",
        "title": "Hierarchical Self-Organization in Fixed-Magnetization Particle Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Hierarchical sorting is a fundamental task for programmable matter, inspired by the spontaneous formation of interfaces and membranes in nature. The task entails particles of different types, present in fixed densities, sorting into corresponding regions of a space that are themselves organized. By analyzing the Gibbs distribution of a general fixed-magnetization model of equilibrium statistical mechanics, we prove that particles moving stochastically according to local affinities solve the hierarchical sorting task. The analysis of fixed-magnetization models is notoriously difficult, and approaches that have led to recent breakthroughs in sampling the low-temperature regime only work in the variable-magnetization setting by default. To overcome this barrier, we introduce a new approach for comparing the partition functions of fixed- and variable-magnetization models. The core technique identifies a special class of configurations that contribute comparably to the two partition functions, which then serves as a bridge between the fixed- and variable-magnetization settings. Our main result is an estimate of the Gibbs distribution that unifies existing and new results for models at fixed magnetization, including the Ising, Potts, and Blume--Capel models, and leads to stochastic distributed algorithms for hierarchical sorting and other self-organizing tasks, like compression and separation.",
        "subjects": [
            "math-ph",
            "cond-mat.stat-mech",
            "cs.DC",
            "math.PR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.03645",
        "abstract url": "https://arxiv.org/abs/2411.03645",
        "title": "Exploiting Stragglers in Distributed Computing Systems with Task Grouping",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider the problem of stragglers in distributed computing systems. Stragglers, which are compute nodes that unpredictably slow down, often increase the completion times of tasks. One common approach to mitigating stragglers is work replication, where only the first completion among replicated tasks is accepted, discarding the others. However, discarding work leads to resource wastage. In this paper, we propose a method for exploiting the work completed by stragglers rather than discarding it. The idea is to increase the granularity of the assigned work, and to increase the frequency of worker updates. We show that the proposed method reduces the completion time of tasks via experiments performed on a simulated cluster as well as on Amazon EC2 with Apache Hadoop.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "This paper has been accepted for publication in IEEE Transactions on Services Computing. The initial results presented in this paper appeared in the proceedings of the Allerton Conference on Communication, Control, and Computing in 2023"
    },
    {
        "paper id": "2411.03647",
        "abstract url": "https://arxiv.org/abs/2411.03647",
        "title": "On the Error-correcting Capability of Twisted Centralizer Codes Obtained from a Fixed Rank-1 Matrix",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we give a generalization on the error correcting capability of twisted centralizer codes obtained from a fixed rank 1 matrix. In particular, we fix the combinatorial matrix which is obtained by getting the linear combination of the matrix whose all entries are 1 and the identity matrix of order n. Results reveal that such codes have a dimension 1 for any fixed combinatorial matrix and constant a hence having a relatively low information rate due to the way its codewords are constructed, but are found to be maximum distance separable codes.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "4 pages"
    },
    {
        "paper id": "2411.03657",
        "abstract url": "https://arxiv.org/abs/2411.03657",
        "title": "How do practitioners gain confidence in assurance cases?",
        "rating": "-10",
        "keywords": [],
        "abstract": "CONTEXT: Assurance Cases (ACs) are prepared to argue that the system's desired quality attributes (e.g., safety or security) are satisfied. While there is strong adoption of ACs, practitioners are often left asking an important question: are we confident that the claims made by the case are true? While many confidence assessment methods (CAMs) exist, little is known about the use of these methods in practice OBJECTIVE: Develop an understanding of the current state of practice for AC confidence assessment: what methods are used in practice and what barriers exist for their use? METHOD: Structured interviews were performed with practitioners with experience contributing to real-world ACs. Open-coding was performed on transcripts. A description of the current state of AC practice and future considerations for researchers was synthesized from the results. RESULTS: A total of n = 19 practitioners were interviewed. The most common CAMs were (peer-)review of ACs, dialectic reasoning (\"defeaters\"), and comparing against checklists. Participants preferred qualitative methods and expressed concerns about quantitative CAMs. Barriers to using CAMs included additional work, inadequate guidance, subjectivity and interpretation of results, and trustworthiness of methods. CONCLUSION: While many CAMs are described in the literature there is a gap between the proposed methods and needs of practitioners. Researchers working in this area should consider the need to: connect CAMs to established practices, use CAMs to communicate with interest holders, crystallize the details of CAM application, curate accessible guidance, and confirm that methods are trustworthy.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    }
]