[
    {
        "paper id": "2411.12785",
        "abstract url": "https://arxiv.org/abs/2411.12785",
        "title": "Joint Vision-Language Social Bias Removal for CLIP",
        "rating": "3",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "Social Bias"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Vision-Language (V-L) pre-trained models such as CLIP show prominent capabilities in various downstream tasks. Despite this promise, V-L models are notoriously limited by their inherent social biases. A typical demonstration is that V-L models often produce biased predictions against specific groups of people, significantly undermining their real-world applicability. Existing approaches endeavor to mitigate the social bias problem in V-L models by removing biased attribute information from model embeddings. However, after our revisiting of these methods, we find that their bias removal is frequently accompanied by greatly compromised V-L alignment capabilities. We then reveal that this performance degradation stems from the unbalanced debiasing in image and text embeddings. To address this issue, we propose a novel V-L debiasing framework to align image and text biases followed by removing them from both modalities. By doing so, our method achieves multi-modal bias mitigation while maintaining the V-L alignment in the debiased embeddings. Additionally, we advocate a new evaluation protocol that can 1) holistically quantify the model debiasing and V-L alignment ability, and 2) evaluate the generalization of social bias removal models. We believe this work will offer new insights and guidance for future studies addressing the social bias problem in CLIP.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12355",
        "abstract url": "https://arxiv.org/abs/2411.12355",
        "title": "DynFocus: Dynamic Cooperative Network Empowers LLMs with Video Understanding",
        "rating": "2",
        "keywords": [
            [
                "memory-efficient"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The challenge in LLM-based video understanding lies in preserving visual and semantic information in long videos while maintaining a memory-affordable token count. However, redundancy and correspondence in videos have hindered the performance potential of existing methods. Through statistical learning on current datasets, we observe that redundancy occurs in both repeated and answer-irrelevant frames, and the corresponding frames vary with different questions. This suggests the possibility of adopting dynamic encoding to balance detailed video information preservation with token budget reduction. To this end, we propose a dynamic cooperative network, DynFocus, for memory-efficient video encoding in this paper. Specifically, i) a Dynamic Event Prototype Estimation (DPE) module to dynamically select meaningful frames for question answering; (ii) a Compact Cooperative Encoding (CCE) module that encodes meaningful frames with detailed visual appearance and the remaining frames with sketchy perception separately. We evaluate our method on five publicly available benchmarks, and experimental results consistently demonstrate that our method achieves competitive performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages, 6 figures"
    },
    {
        "paper id": "2411.12537",
        "abstract url": "https://arxiv.org/abs/2411.12537",
        "title": "Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues",
        "rating": "2",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers in large language modeling, offering linear scaling with sequence length and improved training efficiency. However, LRNNs struggle to perform state-tracking which may impair performance in tasks such as code evaluation or tracking a chess game. Even parity, the simplest state-tracking task, which non-linear RNNs like LSTM handle effectively, cannot be solved by current LRNNs. Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity stems from restricting the value range of their diagonal state-transition matrices to $[0, 1]$ and that incorporating negative values can resolve this issue. We extend this result to non-diagonal LRNNs, which have recently shown promise in models such as DeltaNet. We prove that finite precision LRNNs with state-transition matrices having only positive eigenvalues cannot solve parity, while complex eigenvalues are needed to count modulo $3$. Notably, we also prove that LRNNs can learn any regular language when their state-transition matrices are products of identity minus vector outer product matrices, each with eigenvalues in the range $[-1, 1]$. Our empirical results confirm that extending the eigenvalue range of models like Mamba and DeltaNet to include negative values not only enables them to solve parity but consistently improves their performance on state-tracking tasks. Furthermore, pre-training LRNNs with an extended eigenvalue range for language modeling achieves comparable performance and stability while showing promise on code and math data. Our work enhances the expressivity of modern LRNNs, broadening their applicability without changing the cost of training or inference.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "cs.FL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12593",
        "abstract url": "https://arxiv.org/abs/2411.12593",
        "title": "AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction",
        "rating": "2",
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The advancements in large language models (LLMs) have propelled the improvement of video understanding tasks by incorporating LLMs with visual models. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat) are constrained to processing short-duration videos. Recent attempts to understand long-term videos by extracting and compressing visual features into a fixed memory size. Nevertheless, those methods leverage only visual modality to merge video tokens and overlook the correlation between visual and textual queries, leading to difficulties in effectively handling complex question-answering tasks. To address the challenges of long videos and complex prompts, we propose AdaCM$^2$, which, for the first time, introduces an adaptive cross-modality memory reduction approach to video-text alignment in an auto-regressive manner on video streams. Our extensive experiments on various video understanding tasks, such as video captioning, video question answering, and video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art performance across multiple datasets while significantly reducing memory usage. Notably, it achieves a 4.5% improvement across multiple tasks in the LVU dataset with a GPU memory consumption reduction of up to 65%.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12780",
        "abstract url": "https://arxiv.org/abs/2411.12780",
        "title": "Faster Multi-GPU Training with PPLL: A Pipeline Parallelism Framework Leveraging Local Learning",
        "rating": "2",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Currently, training large-scale deep learning models is typically achieved through parallel training across multiple GPUs. However, due to the inherent communication overhead and synchronization delays in traditional model parallelism methods, seamless parallel training cannot be achieved, which, to some extent, affects overall training efficiency. To address this issue, we present PPLL (Pipeline Parallelism based on Local Learning), a novel framework that leverages local learning algorithms to enable effective parallel training across multiple GPUs. PPLL divides the model into several distinct blocks, each allocated to a separate GPU. By utilizing queues to manage data transfers between GPUs, PPLL ensures seamless cross-GPU communication, allowing multiple blocks to execute forward and backward passes in a pipelined manner. This design minimizes idle times and prevents bottlenecks typically caused by sequential gradient updates, thereby accelerating the overall training process. We validate PPLL through extensive experiments using ResNet and Vision Transformer (ViT) architectures on CIFAR-10, SVHN, and STL-10 datasets. Our results demonstrate that PPLL significantly enhances the training speed of the local learning method while achieving comparable or even superior training speed to traditional pipeline parallelism (PP) without sacrificing model performance. In a 4-GPU training setup, PPLL accelerated local learning training on ViT and ResNet by 162% and 33%, respectively, achieving 1.25x and 0.85x the speed of traditional pipeline parallelism.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12787",
        "abstract url": "https://arxiv.org/abs/2411.12787",
        "title": "Visual Cue Enhancement and Dual Low-Rank Adaptation for Efficient Visual Instruction Fine-Tuning",
        "rating": "2",
        "keywords": [
            [
                "efficient fine-tuning"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Fine-tuning multimodal large language models (MLLMs) presents significant challenges, including a reliance on high-level visual features that limits fine-grained detail comprehension, and data conflicts that arise from task complexity. To address these issues, we propose an efficient fine-tuning framework with two novel approaches: Vision Cue Enhancement (VCE) and Dual Low-Rank Adaptation (Dual-LoRA). VCE enhances the vision projector by integrating multi-level visual cues, improving the model's ability to capture fine-grained visual features. Dual-LoRA introduces a dual low-rank structure for instruction tuning, decoupling learning into skill and task spaces to enable precise control and efficient adaptation across diverse tasks. Our method simplifies implementation, enhances visual comprehension, and improves adaptability. Experiments on both downstream tasks and general benchmarks demonstrate the effectiveness of our proposed approach.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.15201",
        "abstract url": "https://arxiv.org/abs/2411.15201",
        "title": "Beyond Visual Understanding: Introducing PARROT-360V for Vision Language Model Benchmarking",
        "rating": "2",
        "keywords": [
            [
                "Vision Language",
                "VLMs"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Current benchmarks for evaluating Vision Language Models (VLMs) often fall short in thoroughly assessing model abilities to understand and process complex visual and textual content. They typically focus on simple tasks that do not require deep reasoning or the integration of multiple data modalities to solve an original problem. To address this gap, we introduce the PARROT-360V Benchmark, a novel and comprehensive benchmark featuring 2487 challenging visual puzzles designed to test VLMs on complex visual reasoning tasks. We evaluated leading models: GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Pro, using PARROT-360V to assess their capabilities in combining visual clues with language skills to solve tasks in a manner akin to human problem-solving. Our findings reveal a notable performance gap: state-of-the-art models scored between 28 to 56 percentage on our benchmark, significantly lower than their performance on popular benchmarks. This underscores the limitations of current VLMs in handling complex, multi-step reasoning tasks and highlights the need for more robust evaluation frameworks to advance the field.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "7 pages, 4 figures, Accepted at COLING 2025"
    },
    {
        "paper id": "2411.12235",
        "abstract url": "https://arxiv.org/abs/2411.12235",
        "title": "BoolQuestions: Does Dense Retrieval Understand Boolean Logic in Language?",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "EMNLP"
            ]
        ],
        "abstract": "Dense retrieval, which aims to encode the semantic information of arbitrary text into dense vector representations or embeddings, has emerged as an effective and efficient paradigm for text retrieval, consequently becoming an essential component in various natural language processing systems. These systems typically focus on optimizing the embedding space by attending to the relevance of text pairs, while overlooking the Boolean logic inherent in language, which may not be captured by current training objectives. In this work, we first investigate whether current retrieval systems can comprehend the Boolean logic implied in language. To answer this question, we formulate the task of Boolean Dense Retrieval and collect a benchmark dataset, BoolQuestions, which covers complex queries containing basic Boolean logic and corresponding annotated passages. Through extensive experimental results on the proposed task and benchmark dataset, we draw the conclusion that current dense retrieval systems do not fully understand Boolean logic in language, and there is a long way to go to improve our dense retrieval systems. Furthermore, to promote further research on enhancing the understanding of Boolean logic for language models, we explore Boolean operation on decomposed query and propose a contrastive continual training method that serves as a strong baseline for the research community.",
        "subjects": [
            "cs.IR",
            "cs.CL"
        ],
        "comment": "Findings of the Association for Computational Linguistics: EMNLP 2024"
    },
    {
        "paper id": "2411.12372",
        "abstract url": "https://arxiv.org/abs/2411.12372",
        "title": "RedPajama: an Open Dataset for Training Large Language Models",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top-performing models lack transparency in their dataset curation and model development processes, posing an obstacle to the development of fully open language models. In this paper, we identify three core data-related challenges that must be addressed to advance open-source language models. These include (1) transparency in model development, including the data curation process, (2) access to large quantities of high-quality data, and (3) availability of artifacts and metadata for dataset curation and analysis. To address these challenges, we release RedPajama-V1, an open reproduction of the LLaMA training dataset. In addition, we release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata. Together, the RedPajama datasets comprise over 100 trillion tokens spanning multiple domains and with their quality signals facilitate the filtering of data, aiming to inspire the development of numerous new datasets. To date, these datasets have already been used in the training of strong language models used in production, such as Snowflake Arctic, Salesforce's XGen and AI2's OLMo. To provide insight into the quality of RedPajama, we present a series of analyses and ablation studies with decoder-only language models with up to 1.6B parameters. Our findings demonstrate how quality signals for web data can be effectively leveraged to curate high-quality subsets of the dataset, underscoring the potential of RedPajama to advance the development of transparent and high-performing language models at scale.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks"
    },
    {
        "paper id": "2411.12992",
        "abstract url": "https://arxiv.org/abs/2411.12992",
        "title": "MemoryFormer: Minimize Transformer Computation by Removing Fully-Connected Layers",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In order to reduce the computational complexity of large language models, great efforts have been made to to improve the efficiency of transformer models such as linear attention and flash-attention. However, the model size and corresponding computational complexity are constantly scaled up in pursuit of higher performance. In this work, we present MemoryFormer, a novel transformer architecture which significantly reduces the computational complexity (FLOPs) from a new perspective. We eliminate nearly all the computations of the transformer model except for the necessary computation required by the multi-head attention operation. This is made possible by utilizing an alternative method for feature transformation to replace the linear projection of fully-connected layers. Specifically, we first construct a group of in-memory lookup tables that store a large amount of discrete vectors to replace the weight matrix used in linear projection. We then use a hash algorithm to retrieve a correlated subset of vectors dynamically based on the input embedding. The retrieved vectors combined together will form the output embedding, which provides an estimation of the result of matrix multiplication operation in a fully-connected layer. Compared to conducting matrix multiplication, retrieving data blocks from memory is a much cheaper operation which requires little computations. We train MemoryFormer from scratch and conduct extensive experiments on various benchmarks to demonstrate the effectiveness of the proposed model.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "NeurIPS2024"
    },
    {
        "paper id": "2411.12240",
        "abstract url": "https://arxiv.org/abs/2411.12240",
        "title": "Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) based on transformer architectures have revolutionized a variety of domains, with tokenization playing a pivotal role in their pre-processing and fine-tuning stages. In multilingual models, particularly those tailored for Indic languages, effective tokenization is crucial for optimizing performance. This paper presents a comprehensive evaluation of tokenizers used by 12 LLMs across all 22 official languages of India, with a focus on comparing the efficiency of their tokenization processes. We employed the Normalized Sequence Length (NSL) as a key metric in our analysis. Our findings reveal that the SUTRA tokenizer outperforms all other models, including several Indic-specific models, excelling in 14 languages. Notable insights include the SUTRA tokenizer's superior handling of Indic languages, GPT-4o's advancement over its predecessor GPT-4 in processing Indian languages, and the limited performance of Project Indus in certain languages. This study underscores the critical importance of developing targeted tokenization strategies for multilingual and Indic-centric models, laying the groundwork for future improvements in tokenizer design to enhance linguistic coverage and model efficiency.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12259",
        "abstract url": "https://arxiv.org/abs/2411.12259",
        "title": "Prototype Optimization with Neural ODE for Few-Shot Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Few-Shot Learning (FSL) is a challenging task, which aims to recognize novel classes with few examples. Pre-training based methods effectively tackle the problem by pre-training a feature extractor and then performing class prediction via a cosine classifier with mean-based prototypes. Nevertheless, due to the data scarcity, the mean-based prototypes are usually biased. In this paper, we attempt to diminish the prototype bias by regarding it as a prototype optimization problem. To this end, we propose a novel prototype optimization framework to rectify prototypes, i.e., introducing a meta-optimizer to optimize prototypes. Although the existing meta-optimizers can also be adapted to our framework, they all overlook a crucial gradient bias issue, i.e., the mean-based gradient estimation is also biased on sparse data. To address this issue, in this paper, we regard the gradient and its flow as meta-knowledge and then propose a novel Neural Ordinary Differential Equation (ODE)-based meta-optimizer to optimize prototypes, called MetaNODE. Although MetaNODE has shown superior performance, it suffers from a huge computational burden. To further improve its computation efficiency, we conduct a detailed analysis on MetaNODE and then design an effective and efficient MetaNODE extension version (called E2MetaNODE). It consists of two novel modules: E2GradNet and E2Solver, which aim to estimate accurate gradient flows and solve optimal prototypes in an effective and efficient manner, respectively. Extensive experiments show that 1) our methods achieve superior performance over previous FSL methods and 2) our E2MetaNODE significantly improves computation efficiency meanwhile without performance degradation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "An extended version of metanode: prototype optimization as a neural ode for few-shot learning. arXiv admin note: text overlap with arXiv:2103.14341"
    },
    {
        "paper id": "2411.12270",
        "abstract url": "https://arxiv.org/abs/2411.12270",
        "title": "KDC-MAE: Knowledge Distilled Contrastive Mask Auto-Encoder",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this work, we attempted to extend the thought and showcase a way forward for the Self-supervised Learning (SSL) learning paradigm by combining contrastive learning, self-distillation (knowledge distillation) and masked data modelling, the three major SSL frameworks, to learn a joint and coordinated representation. The proposed technique of SSL learns by the collaborative power of different learning objectives of SSL. Hence to jointly learn the different SSL objectives we proposed a new SSL architecture KDC-MAE, a complementary masking strategy to learn the modular correspondence, and a weighted way to combine them coordinately. Experimental results conclude that the contrastive masking correspondence along with the KD learning objective has lent a hand to performing better learning for multiple modalities over multiple tasks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12275",
        "abstract url": "https://arxiv.org/abs/2411.12275",
        "title": "Building Trust: Foundations of Security, Safety and Transparency in AI",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "This paper explores the rapidly evolving ecosystem of publicly available AI models, and their potential implications on the security and safety landscape. As AI models become increasingly prevalent, understanding their potential risks and vulnerabilities is crucial. We review the current security and safety scenarios while highlighting challenges such as tracking issues, remediation, and the apparent absence of AI model lifecycle and ownership processes. Comprehensive strategies to enhance security and safety for both model developers and end-users are proposed. This paper aims to provide some of the foundational pieces for more standardized security, safety, and transparency in the development and operation of AI models and the larger open ecosystems and communities forming around them.",
        "subjects": [
            "cs.CY",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12276",
        "abstract url": "https://arxiv.org/abs/2411.12276",
        "title": "libcll: an Extendable Python Toolkit for Complementary-Label Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Complementary-label learning (CLL) is a weakly supervised learning paradigm for multiclass classification, where only complementary labels -- indicating classes an instance does not belong to -- are provided to the learning algorithm. Despite CLL's increasing popularity, previous studies highlight two main challenges: (1) inconsistent results arising from varied assumptions on complementary label generation, and (2) high barriers to entry due to the lack of a standardized evaluation platform across datasets and algorithms. To address these challenges, we introduce \\texttt{libcll}, an extensible Python toolkit for CLL research. \\texttt{libcll} provides a universal interface that supports a wide range of generation assumptions, both synthetic and real-world datasets, and key CLL algorithms. The toolkit is designed to mitigate inconsistencies and streamline the research process, with easy installation, comprehensive usage guides, and quickstart tutorials that facilitate efficient adoption and implementation of CLL techniques. Extensive ablation studies conducted with \\texttt{libcll} demonstrate its utility in generating valuable insights to advance future CLL research.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "10 pages, 3 figures"
    },
    {
        "paper id": "2411.12286",
        "abstract url": "https://arxiv.org/abs/2411.12286",
        "title": "GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for Task-Oriented Grasping",
        "rating": "1",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Inferring affordable (i.e., graspable) parts of arbitrary objects based on human specifications is essential for robots advancing toward open-vocabulary manipulation. Current grasp planners, however, are hindered by limited vision-language comprehension and time-consuming 3D radiance modeling, restricting real-time, open-vocabulary interactions with objects. To address these limitations, we propose GLOVER, a unified Generalizable Open-Vocabulary Affordance Reasoning framework, which fine-tunes the Large Language Models (LLMs) to predict visual affordance of graspable object parts within RGB feature space. We compile a dataset of over 10,000 images from human-object interactions, annotated with unified visual and linguistic affordance labels, to enable multi-modal fine-tuning. GLOVER inherits world knowledge and common-sense reasoning from LLMs, facilitating more fine-grained object understanding and sophisticated tool-use reasoning. To enable effective real-world deployment, we present Affordance-Aware Grasping Estimation (AGE), a non-parametric grasp planner that aligns the gripper pose with a superquadric surface derived from affordance data. In evaluations across 30 real-world scenes, GLOVER achieves success rates of 86.0% in part identification and 76.3% in grasping, with speeds approximately 330 times faster in affordance reasoning and 40 times faster in grasping pose estimation than the previous state-of-the-art.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12287",
        "abstract url": "https://arxiv.org/abs/2411.12287",
        "title": "CUE-M: Contextual Understanding and Enhanced Search with Multimodal Large Language Model",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The integration of Retrieval-Augmented Generation (RAG) with Multimodal Large Language Models (MLLMs) has expanded the scope of multimodal query resolution. However, current systems struggle with intent understanding, information retrieval, and safety filtering, limiting their effectiveness. This paper introduces Contextual Understanding and Enhanced Search with MLLM (CUE-M), a novel multimodal search pipeline that addresses these challenges through a multi-stage framework comprising image context enrichment, intent refinement, contextual query generation, external API integration, and relevance-based filtering. CUE-M incorporates a robust safety framework combining image-based, text-based, and multimodal classifiers, dynamically adapting to instance- and category-specific risks. Evaluations on a multimodal Q&A dataset and a public safety benchmark demonstrate that CUE-M outperforms baselines in accuracy, knowledge integration, and safety, advancing the capabilities of multimodal retrieval systems.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Preprint. Under review"
    },
    {
        "paper id": "2411.12293",
        "abstract url": "https://arxiv.org/abs/2411.12293",
        "title": "Generative Timelines for Instructed Visual Assembly",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "The objective of this work is to manipulate visual timelines (e.g. a video) through natural language instructions, making complex timeline editing tasks accessible to non-expert or potentially even disabled users. We call this task Instructed visual assembly. This task is challenging as it requires (i) identifying relevant visual content in the input timeline as well as retrieving relevant visual content in a given input (video) collection, (ii) understanding the input natural language instruction, and (iii) performing the desired edits of the input visual timeline to produce an output timeline. To address these challenges, we propose the Timeline Assembler, a generative model trained to perform instructed visual assembly tasks. The contributions of this work are three-fold. First, we develop a large multimodal language model, which is designed to process visual content, compactly represent timelines and accurately interpret timeline editing instructions. Second, we introduce a novel method for automatically generating datasets for visual assembly tasks, enabling efficient training of our model without the need for human-labeled data. Third, we validate our approach by creating two novel datasets for image and video assembly, demonstrating that the Timeline Assembler substantially outperforms established baseline models, including the recent GPT-4o, in accurately executing complex assembly instructions across various real-world inspired scenarios.",
        "subjects": [
            "cs.CV",
            "cs.HC",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12331",
        "abstract url": "https://arxiv.org/abs/2411.12331",
        "title": "Accelerating UMAP for Large-Scale Datasets Through Spectral Coarsening",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper introduces an innovative approach to dramatically accelerate UMAP using spectral data compression.The proposed method significantly reduces the size of the dataset, preserving its essential manifold structure through an advanced spectral compression technique. This allows UMAP to perform much faster while maintaining the quality of its embeddings. Experiments on real-world datasets, such as USPS, demonstrate the method's ability to achieve substantial data reduction without compromising embedding fidelity.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12357",
        "abstract url": "https://arxiv.org/abs/2411.12357",
        "title": "A Layered Architecture for Developing and Enhancing Capabilities in Large Language Model-based Software Systems",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Significant efforts has been made to expand the use of Large Language Models (LLMs) beyond basic language tasks. While the generalizability and versatility of LLMs have enabled widespread adoption, evolving demands in application development often exceed their native capabilities. Meeting these demands may involve a diverse set of methods, such as enhancing creativity through either inference temperature adjustments or creativity-provoking prompts. Selecting the right approach is critical, as different methods lead to trade-offs in engineering complexity, scalability, and operational costs. This paper introduces a layered architecture that organizes LLM software system development into distinct layers, each characterized by specific attributes. By aligning capabilities with these layers, the framework encourages the systematic implementation of capabilities in effective and efficient ways that ultimately supports desired functionalities and qualities. Through practical case studies, we illustrate the utility of the framework. This work offers developers actionable insights for selecting suitable technologies in LLM-based software system development, promoting robustness and scalability.",
        "subjects": [
            "cs.SE",
            "cs.AI",
            "cs.CL",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12395",
        "abstract url": "https://arxiv.org/abs/2411.12395",
        "title": "Do LLMs Understand Ambiguity in Text? A Case Study in Open-world Question Answering",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Ambiguity in natural language poses significant challenges to Large Language Models (LLMs) used for open-domain question answering. LLMs often struggle with the inherent uncertainties of human communication, leading to misinterpretations, miscommunications, hallucinations, and biased responses. This significantly weakens their ability to be used for tasks like fact-checking, question answering, feature extraction, and sentiment analysis. Using open-domain question answering as a test case, we compare off-the-shelf and few-shot LLM performance, focusing on measuring the impact of explicit disambiguation strategies. We demonstrate how simple, training-free, token-level disambiguation methods may be effectively used to improve LLM performance for ambiguous question answering tasks. We empirically show our findings and discuss best practices and broader impacts regarding ambiguity in LLMs.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Accepted at the REU Symposium at IEEE BigData 2024"
    },
    {
        "paper id": "2411.12405",
        "abstract url": "https://arxiv.org/abs/2411.12405",
        "title": "Evaluating the Prompt Steerability of Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Building pluralistic AI requires designing models that are able to be shaped to represent a wide range of value systems and cultures. Achieving this requires first being able to evaluate the degree to which a given model is capable of reflecting various personas. To this end, we propose a benchmark for evaluating the steerability of model personas as a function of prompting. Our design is based on a formal definition of prompt steerability, which analyzes the degree to which a model's joint behavioral distribution can be shifted from its baseline behavior. By defining steerability indices and inspecting how these indices change as a function of steering effort, we can estimate the steerability of a model across various persona dimensions and directions. Our benchmark reveals that the steerability of many current models is limited -- due to both a skew in their baseline behavior and an asymmetry in their steerability across many persona dimensions. We release an implementation of our benchmark at https://github.com/IBM/prompt-steering.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12448",
        "abstract url": "https://arxiv.org/abs/2411.12448",
        "title": "Large Language Models for Lossless Image Compression: Next-Pixel Prediction in Language Space is All You Need",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "We have recently witnessed that ``Intelligence\" and `` Compression\" are the two sides of the same coin, where the language large model (LLM) with unprecedented intelligence is a general-purpose lossless compressor for various data modalities. This attribute particularly appeals to the lossless image compression community, given the increasing need to compress high-resolution images in the current streaming media era. Consequently, a spontaneous envision emerges: Can the compression performance of the LLM elevate lossless image compression to new heights? However, our findings indicate that the naive application of LLM-based lossless image compressors suffers from a considerable performance gap compared with existing state-of-the-art (SOTA) codecs on common benchmark datasets. In light of this, we are dedicated to fulfilling the unprecedented intelligence (compression) capacity of the LLM for lossless image compression tasks, thereby bridging the gap between theoretical and practical compression performance. Specifically, we propose P$^{2}$-LLM, a next-pixel prediction-based LLM, which integrates various elaborated insights and methodologies, \\textit{e.g.,} pixel-level priors, the in-context ability of LLM, and a pixel-level semantic preservation strategy, to enhance the understanding capacity of pixel sequences for better next-pixel predictions. Extensive experiments on benchmark datasets demonstrate that P$^{2}$-LLM can beat SOTA classical and learned codecs.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12460",
        "abstract url": "https://arxiv.org/abs/2411.12460",
        "title": "Guide-to-Explain for Controllable Summarization",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Recently, large language models (LLMs) have demonstrated remarkable performance in abstractive summarization tasks. However, controllable summarization with LLMs remains underexplored, limiting their ability to generate summaries that align with specific user preferences. In this paper, we first investigate the capability of LLMs to control diverse attributes, revealing that they encounter greater challenges with numerical attributes, such as length and extractiveness, compared to linguistic attributes. To address this challenge, we propose a guide-to-explain framework (GTE) for controllable summarization. Our GTE framework enables the model to identify misaligned attributes in the initial draft and guides it in explaining errors in the previous output. Based on this reflection, the model generates a well-adjusted summary. As a result, by allowing the model to reflect on its misalignment, we generate summaries that satisfy the desired attributes in surprisingly fewer iterations than other iterative methods solely using LLMs.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12483",
        "abstract url": "https://arxiv.org/abs/2411.12483",
        "title": "Analysing Explanation-Related Interactions in Collaborative Perception-Cognition-Communication-Action",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Effective communication is essential in collaborative tasks, so AI-equipped robots working alongside humans need to be able to explain their behaviour in order to cooperate effectively and earn trust. We analyse and classify communications among human participants collaborating to complete a simulated emergency response task. The analysis identifies messages that relate to various kinds of interactive explanations identified in the explainable AI literature. This allows us to understand what type of explanations humans expect from their teammates in such settings, and thus where AI-equipped robots most need explanation capabilities. We find that most explanation-related messages seek clarification in the decisions or actions taken. We also confirm that messages have an impact on the performance of our simulated task.",
        "subjects": [
            "cs.HC",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "4 pages, 3 figures, published as a Late Breaking Report in RO-MAN 2024"
    },
    {
        "paper id": "2411.12484",
        "abstract url": "https://arxiv.org/abs/2411.12484",
        "title": "Regular-pattern-sensitive CRFs for Distant Label Interactions",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Linear-chain conditional random fields (CRFs) are a common model component for sequence labeling tasks when modeling the interactions between different labels is important. However, the Markov assumption limits linear-chain CRFs to only directly modeling interactions between adjacent labels. Weighted finite-state transducers (FSTs) are a related approach which can be made to model distant label-label interactions, but exact label inference is intractable for these models in the general case, and the task of selecting an appropriate automaton structure for the desired interaction types poses a practical challenge. In this work, we present regular-pattern-sensitive CRFs (RPCRFs), a method of enriching standard linear-chain CRFs with the ability to learn long-distance label interactions which occur in user-specified patterns. This approach allows users to write regular-expression label patterns concisely specifying which types of interactions the model should take into account, allowing the model to learn from data whether and in which contexts these patterns occur. The result can be interpreted alternatively as a CRF augmented with additional, non-local potentials, or as a finite-state transducer whose structure is defined by a set of easily-interpretable patterns. Critically, unlike the general case for FSTs (and for non-chain CRFs), exact training and inference are tractable for many pattern sets. In this work, we detail how a RPCRF can be automatically constructed from a set of user-specified patterns, and demonstrate the model's effectiveness on synthetic data, showing how different types of patterns can capture different nonlocal dependency structures in label sequences.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12498",
        "abstract url": "https://arxiv.org/abs/2411.12498",
        "title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Large language models (LLMs) are capable of solving a wide range of tasks, yet they have struggled with reasoning. To address this, we propose $\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs' reasoning capabilities by program-generated logical reasoning samples. We first establish principles for designing high-quality samples by integrating symbolic logic theory and previous empirical insights. Then, based on these principles, we construct a synthetic corpus named $\\textbf{Formal Logic Deduction Diverse}$ ($\\textbf{FLD}$$^{\\times 2}$), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors. Finally, we empirically show that ALT on FLD$^{\\times2}$ substantially enhances the reasoning capabilities of state-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of up to 30 points on logical reasoning benchmarks, up to 10 points on math and coding benchmarks, and 5 points on the benchmark suite BBH.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.LO"
        ],
        "comment": "NeurIPS 2024"
    },
    {
        "paper id": "2411.12520",
        "abstract url": "https://arxiv.org/abs/2411.12520",
        "title": "VMGNet: A Low Computational Complexity Robotic Grasping Network Based on VMamba with Multi-Scale Feature Fusion",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "While deep learning-based robotic grasping technology has demonstrated strong adaptability, its computational complexity has also significantly increased, making it unsuitable for scenarios with high real-time requirements. Therefore, we propose a low computational complexity and high accuracy model named VMGNet for robotic grasping. For the first time, we introduce the Visual State Space into the robotic grasping field to achieve linear computational complexity, thereby greatly reducing the model's computational cost. Meanwhile, to improve the accuracy of the model, we propose an efficient and lightweight multi-scale feature fusion module, named Fusion Bridge Module, to extract and fuse information at different scales. We also present a new loss function calculation method to enhance the importance differences between subtasks, improving the model's fitting ability. Experiments show that VMGNet has only 8.7G Floating Point Operations and an inference time of 8.1 ms on our devices. VMGNet also achieved state-of-the-art performance on the Cornell and Jacquard public datasets. To validate VMGNet's effectiveness in practical applications, we conducted real grasping experiments in multi-object scenarios, and VMGNet achieved an excellent performance with a 94.4% success rate in real-world grasping tasks. The video for the real-world robotic grasping experiments is available at https://youtu.be/S-QHBtbmLc4.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12525",
        "abstract url": "https://arxiv.org/abs/2411.12525",
        "title": "Rethinking Top Probability from Multi-view for Distracted Driver Behaviour Localization",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Naturalistic driving action localization task aims to recognize and comprehend human behaviors and actions from video data captured during real-world driving scenarios. Previous studies have shown great action localization performance by applying a recognition model followed by probability-based post-processing. Nevertheless, the probabilities provided by the recognition model frequently contain confused information causing challenge for post-processing. In this work, we adopt an action recognition model based on self-supervise learning to detect distracted activities and give potential action probabilities. Subsequently, a constraint ensemble strategy takes advantages of multi-camera views to provide robust predictions. Finally, we introduce a conditional post-processing operation to locate distracted behaviours and action temporal boundaries precisely. Experimenting on test set A2, our method obtains the sixth position on the public leaderboard of track 3 of the 2024 AI City Challenge.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Computer Vision and Pattern Recognition Workshop 2024"
    },
    {
        "paper id": "2411.12539",
        "abstract url": "https://arxiv.org/abs/2411.12539",
        "title": "Predicting Customer Satisfaction by Replicating the Survey Response Distribution",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "For many call centers, customer satisfaction (CSAT) is a key performance indicator (KPI). However, only a fraction of customers take the CSAT survey after the call, leading to a biased and inaccurate average CSAT value, and missed opportunities for coaching, follow-up, and rectification. Therefore, call centers can benefit from a model predicting customer satisfaction on calls where the customer did not complete the survey. Given that CSAT is a closely monitored KPI, it is critical to minimize any bias in the average predicted CSAT (pCSAT). In this paper, we introduce a method such that predicted CSAT (pCSAT) scores accurately replicate the distribution of survey CSAT responses for every call center with sufficient data in a live production environment. The method can be applied to many multiclass classification problems to improve the class balance and minimize its changes upon model updates.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12558",
        "abstract url": "https://arxiv.org/abs/2411.12558",
        "title": "Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Open-set Domain Adaptation (OSDA) aims to adapt a model from a labeled source domain to an unlabeled target domain, where novel classes - also referred to as target-private unknown classes - are present. Source-free Open-set Domain Adaptation (SF-OSDA) methods address OSDA without accessing labeled source data, making them particularly relevant under privacy constraints. However, SF-OSDA presents significant challenges due to distribution shifts and the introduction of novel classes. Existing SF-OSDA methods typically rely on thresholding the prediction entropy of a sample to identify it as either a known or unknown class but fail to explicitly learn discriminative features for the target-private unknown classes. We propose Recall and Refine (RRDA), a novel SF-OSDA framework designed to address these limitations by explicitly learning features for target-private unknown classes. RRDA employs a two-step process. First, we enhance the model's capacity to recognize unknown classes by training a target classifier with an additional decision boundary, guided by synthetic samples generated from target domain features. This enables the classifier to effectively separate known and unknown classes. In the second step, we adapt the entire model to the target domain, addressing both domain shifts and improving generalization to unknown classes. Any off-the-shelf source-free domain adaptation method (e.g., SHOT, AaD) can be seamlessly integrated into our framework at this stage. Extensive experiments on three benchmark datasets demonstrate that RRDA significantly outperforms existing SF-OSDA and OSDA methods.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12571",
        "abstract url": "https://arxiv.org/abs/2411.12571",
        "title": "Large Language Models for Combinatorial Optimization of Design Structure Matrix",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Combinatorial optimization (CO) is essential for improving efficiency and performance in engineering applications. As complexity increases with larger problem sizes and more intricate dependencies, identifying the optimal solution become challenging. When it comes to real-world engineering problems, algorithms based on pure mathematical reasoning are limited and incapable to capture the contextual nuances necessary for optimization. This study explores the potential of Large Language Models (LLMs) in solving engineering CO problems by leveraging their reasoning power and contextual knowledge. We propose a novel LLM-based framework that integrates network topology and domain knowledge to optimize the sequencing of Design Structure Matrix (DSM)-a common CO problem. Our experiments on various DSM cases demonstrate that the proposed method achieves faster convergence and higher solution quality than benchmark methods. Moreover, results show that incorporating contextual domain knowledge significantly improves performance despite the choice of LLMs. These findings highlight the potential of LLMs in tackling complex real-world CO problems by combining semantic and mathematical reasoning. This approach paves the way for a new paradigm in in real-world combinatorial optimization.",
        "subjects": [
            "cs.CE",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12580",
        "abstract url": "https://arxiv.org/abs/2411.12580",
        "title": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The capabilities and limitations of Large Language Models have been sketched out in great detail in recent years, providing an intriguing yet conflicting picture. On the one hand, LLMs demonstrate a general ability to solve problems. On the other hand, they show surprising reasoning gaps when compared to humans, casting doubt on the robustness of their generalisation strategies. The sheer volume of data used in the design of LLMs has precluded us from applying the method traditionally used to measure generalisation: train-test set separation. To overcome this, we study what kind of generalisation strategies LLMs employ when performing reasoning tasks by investigating the pretraining data they rely on. For two models of different sizes (7B and 35B) and 2.5B of their pretraining tokens, we identify what documents influence the model outputs for three simple mathematical reasoning tasks and contrast this to the data that are influential for answering factual questions. We find that, while the models rely on mostly distinct sets of data for each factual question, a document often has a similar influence across different reasoning questions within the same task, indicating the presence of procedural knowledge. We further find that the answers to factual questions often show up in the most influential data. However, for reasoning questions the answers usually do not show up as highly influential, nor do the answers to the intermediate reasoning steps. When we characterise the top ranked documents for the reasoning questions qualitatively, we confirm that the influential documents often contain procedural knowledge, like demonstrating how to obtain a solution using formulae or code. Our findings indicate that the approach to reasoning the models use is unlike retrieval, and more like a generalisable strategy that synthesises procedural knowledge from documents doing a similar form of reasoning.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12587",
        "abstract url": "https://arxiv.org/abs/2411.12587",
        "title": "Whisper Finetuning on Nepali Language",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Despite the growing advancements in Automatic Speech Recognition (ASR) models, the development of robust models for underrepresented languages, such as Nepali, remains a challenge. This research focuses on making an exhaustive and generalized dataset followed by fine-tuning OpenAI's Whisper models of different sizes to improve transcription (speech-to-text) accuracy for the Nepali language. We leverage publicly available ASR datasets and self-recorded custom datasets with a diverse range of accents, dialects, and speaking styles further enriched through augmentation. Our experimental results demonstrate that fine-tuning Whisper models on our curated custom dataset substantially reduces the Word Error Rate (WER) across all model sizes attributed to larger data variations in terms of speaker's age, gender, and sentiment, acoustic environment, dialect, denser audio segments (15-30 seconds) that are more compatible with Whisper's input, and manual curation of audios and transcriptions. Notably, our approach outperforms Whisper's baseline models trained on Fleur's dataset, achieving WER reductions of up to 36.2% on the small and 23.8% on medium models. Furthermore, we show that data augmentation plays a significant role in enhancing model robustness. Our approach underlines the importance of dataset quality, variation, and augmentation in the adaptation of state-of-the-art models to underrepresented languages for developing accurate ASR systems.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12650",
        "abstract url": "https://arxiv.org/abs/2411.12650",
        "title": "Optimizing Airline Reservation Systems with Edge-Enabled Microservices: A Framework for Real-Time Data Processing and Enhanced User Responsiveness",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The growing complexity of the operations of airline reservations requires a smart solution for the adoption of novel approaches to the development of quick, efficient, and adaptive reservation systems. This paper outlines in detail a conceptual framework for the implementation of edge computing microservices in order to address the shortcomings of traditional centralized architectures. Specifically, as edge computing allows for certain activities such as seat inventory checks, booking processes and even confirmation to be done nearer to the user, thus lessening the overall response time and improving the performance of the system. In addition, the framework value should include achieving the high performance of the system such as low latency, high throughput and higher user experience. The major design components include deployed distributed computing microservices orchestrated by Kubernetes, real-time message processing system with Kafka and its elastic scaling. Other operational components include Prometheus and Grafana, which are used to monitor and manage resources, ensuring that all operational processes are optimized. Although this research focuses on a design and theoretical scheming of the framework, its use is foreseen to be more advantageous in facilitating a transform in the provision of services in the airline industry by improving customers' satisfaction, providing infrastructure which is cheap to install and efficiently supporting technology changes such as artificial intelligence and internet of things embedded systems. This research addresses the increasing demand for new technologies with modern well-distributed and real-time-centric systems and also provides a basis for future case implementation and testing. As such, the proposed architecture offers a market-ready, extensible solution to the problems posed by existing airline reservation systems .",
        "subjects": [
            "cs.SE",
            "cs.AI",
            "cs.CE",
            "cs.CL",
            "cs.DC"
        ],
        "comment": "22 pages, 11 figures"
    },
    {
        "paper id": "2411.12720",
        "abstract url": "https://arxiv.org/abs/2411.12720",
        "title": "Scaling laws for nonlinear dynamical models of speech",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The addition of a nonlinear restoring force to dynamical models of the speech gesture significantly improves the empirical accuracy of model predictions, but nonlinearity introduces challenges in selecting appropriate parameters and numerical stability, especially when modelling variation in empirical data. We address this issue by introducing simple numerical methods for parameterization of nonlinear task dynamic models. We first illustrate the problem and then outline solutions in the form of power laws that scale nonlinear stiffness terms. We apply the scaling laws to a cubic model and show how they facilitate interpretable simulations of the nonlinear gestural dynamics underpinning speech production.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12724",
        "abstract url": "https://arxiv.org/abs/2411.12724",
        "title": "Heuristic-Free Multi-Teacher Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We introduce Teacher2Task, a novel framework for multi-teacher learning that eliminates the need for manual aggregation heuristics. Existing multi-teacher methods typically rely on such heuristics to combine predictions from multiple teachers, often resulting in sub-optimal aggregated labels and the propagation of aggregation errors. Teacher2Task addresses these limitations by introducing teacher-specific input tokens and reformulating the training process. Instead of relying on aggregated labels, the framework transforms the training data, consisting of ground truth labels and annotations from N teachers, into N+1 distinct tasks: N auxiliary tasks that predict the labeling styles of the N individual teachers, and one primary task that focuses on the ground truth labels. This approach, drawing upon principles from multiple learning paradigms, demonstrates strong empirical results across a range of architectures, modalities, and tasks.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12728",
        "abstract url": "https://arxiv.org/abs/2411.12728",
        "title": "Information Theory of Meaningful Communication",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "In Shannon's seminal paper, entropy of printed English, treated as a stationary stochastic process, was estimated to be roughly 1 bit per character. However, considered as a means of communication, language differs considerably from its printed form: (i) the units of information are not characters or even words but clauses, i.e. shortest meaningful parts of speech; and (ii) what is transmitted is principally the meaning of what is being said or written, while the precise phrasing that was used to communicate the meaning is typically ignored. In this study, we show that one can leverage recently developed large language models to quantify information communicated in meaningful narratives in terms of bits of meaning per clause.",
        "subjects": [
            "cs.CL",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12736",
        "abstract url": "https://arxiv.org/abs/2411.12736",
        "title": "ACING: Actor-Critic for Instruction Learning in Black-Box Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The effectiveness of Large Language Models (LLMs) in solving tasks vastly depends on the quality of the instructions, which often require fine-tuning through extensive human effort. This highlights the need for automated instruction optimization; however, this optimization is particularly challenging when dealing with black-box LLMs, where model parameters and gradients remain inaccessible. We propose ACING, a task-specific prompt optimization approach framed as a stateless continuous-action Reinforcement Learning (RL) problem, known as the continuum bandit setting. ACING leverages an actor-critic-based method to optimize prompts, learning from non-differentiable reward signals. We validate ACING by optimizing prompts for ChatGPT on 30 instruction-based tasks. ACING consistently outperforms baseline methods, achieving a median score improvement of 10 percentage points. Furthermore, ACING not only recovers but also surpasses human-crafted expert instructions, achieving up to a 39 percentage point improvement against human benchmarks.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "eess.SY",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12776",
        "abstract url": "https://arxiv.org/abs/2411.12776",
        "title": "Cross-Layer Encrypted Semantic Communication Framework for Panoramic Video Transmission",
        "rating": "1",
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "In this paper, we propose a cross-layer encrypted semantic communication (CLESC) framework for panoramic video transmission, incorporating feature extraction, encoding, encryption, cyclic redundancy check (CRC), and retransmission processes to achieve compatibility between semantic communication and traditional communication systems. Additionally, we propose an adaptive cross-layer transmission mechanism that dynamically adjusts CRC, channel coding, and retransmission schemes based on the importance of semantic information. This ensures that important information is prioritized under poor transmission conditions. To verify the aforementioned framework, we also design an end-to-end adaptive panoramic video semantic transmission (APVST) network that leverages a deep joint source-channel coding (Deep JSCC) structure and attention mechanism, integrated with a latitude adaptive module that facilitates adaptive semantic feature extraction and variable-length encoding of panoramic videos. The proposed CLESC is also applicable to the transmission of other modal data. Simulation results demonstrate that the proposed CLESC effectively achieves compatibility and adaptation between semantic communication and traditional communication systems, improving both transmission efficiency and channel adaptability. Compared to traditional cross-layer transmission schemes, the CLESC framework can reduce bandwidth consumption by 85% while showing significant advantages under low signal-to-noise ratio (SNR) conditions.",
        "subjects": [
            "eess.IV",
            "cs.CR",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12781",
        "abstract url": "https://arxiv.org/abs/2411.12781",
        "title": "FGP: Feature-Gradient-Prune for Efficient Convolutional Layer Pruning",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "To reduce computational overhead while maintaining model performance, model pruning techniques have been proposed. Among these, structured pruning, which removes entire convolutional channels or layers, significantly enhances computational efficiency and is compatible with hardware acceleration. However, existing pruning methods that rely solely on image features or gradients often result in the retention of redundant channels, negatively impacting inference efficiency. To address this issue, this paper introduces a novel pruning method called Feature-Gradient Pruning (FGP). This approach integrates both feature-based and gradient-based information to more effectively evaluate the importance of channels across various target classes, enabling a more accurate identification of channels that are critical to model performance. Experimental results demonstrate that the proposed method improves both model compactness and practicality while maintaining stable performance. Experiments conducted across multiple tasks and datasets show that FGP significantly reduces computational costs and minimizes accuracy loss compared to existing methods, highlighting its effectiveness in optimizing pruning outcomes. The source code is available at: https://github.com/FGP-code/FGP.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12792",
        "abstract url": "https://arxiv.org/abs/2411.12792",
        "title": "CLIC: Contrastive Learning Framework for Unsupervised Image Complexity Representation",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "As an essential visual attribute, image complexity affects human image comprehension and directly influences the performance of computer vision tasks. However, accurately assessing and quantifying image complexity faces significant challenges. Previous works needed more generalization capabilities and well-labeled datasets to learn image complexity features. However, creating such datasets requires expensive manual labeling costs, and the models inevitably learn about human subjective biases. To address the above problems, we propose CLIC, an unsupervised framework based on contrastive learning, for learning image complexity representations. The method learns image complexity features on unlabeled data, avoiding the high labeling cost. Specifically, we propose a unique positive and negative sample selection strategy to reinforce the differences in complexity features. At the same time, we introduce an image prior-based Complexity-Aware Loss to constrain the learning process of the model. We conducted extensive experiments for verification, and the results show that CLIC can effectively learn the image complexity representation. CLIC obtained competitive results with supervised methods by fine-tuning on IC9600. In addition, CLIC applied to downstream tasks shows significant performance improvements, demonstrating the potential for application in various real-world scenarios. \\href{https://github.com/xauat-liushipeng/CLIC}{code}",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12817",
        "abstract url": "https://arxiv.org/abs/2411.12817",
        "title": "What Makes a Good Dataset for Knowledge Distillation?",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Knowledge distillation (KD) has been a popular and effective method for model compression. One important assumption of KD is that the teacher's original dataset will also be available when training the student. However, in situations such as continual learning and distilling large models trained on company-withheld datasets, having access to the original data may not always be possible. This leads practitioners towards utilizing other sources of supplemental data, which could yield mixed results. One must then ask: \"what makes a good dataset for transferring knowledge from teacher to student?\" Many would assume that only real in-domain imagery is viable, but is that the only option? In this work, we explore multiple possible surrogate distillation datasets and demonstrate that many different datasets, even unnatural synthetic imagery, can serve as a suitable alternative in KD. From examining these alternative datasets, we identify and present various criteria describing what makes a good dataset for distillation. Source code will be available in the future.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12828",
        "abstract url": "https://arxiv.org/abs/2411.12828",
        "title": "Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large language model (LLM) agents show promise in an increasing number of domains. In many proposed applications, it is expected that the agent reasons over accumulated experience presented in an input prompt. We propose the OEDD (Operationalize Experience Despite Distraction) corpus, a human-annotator-validated body of scenarios with pre-scripted agent histories where the agent must make a decision based on disparate experiential information in the presence of a distractor. We evaluate three state-of-the-art LLMs (GPT-3.5 Turbo, GPT-4o, and Gemini 1.5 Pro) using a minimal chain-of-thought prompting strategy and observe that when (1) the input context contains over 1,615 tokens of historical interactions, (2) a crucially decision-informing premise is the rightful conclusion over two disparate environment premises, and (3) a trivial, but distracting red herring fact follows, all LLMs perform worse than random choice at selecting the better of two actions. Our code and test corpus are publicly available at: https://github.com/sonnygeorge/OEDD .",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12830",
        "abstract url": "https://arxiv.org/abs/2411.12830",
        "title": "Class-Incremental Learning for Sound Event Localization and Detection",
        "rating": "1",
        "keywords": [
            [
                "eess.AS"
            ]
        ],
        "abstract": "This paper investigates the feasibility of class-incremental learning (CIL) for Sound Event Localization and Detection (SELD) tasks. The method features an incremental learner that can learn new sound classes independently while preserving knowledge of old classes. The continual learning is achieved through a mean square error-based distillation loss to minimize output discrepancies between subsequent learners. The experiments are conducted on the TAU-NIGENS Spatial Sound Events 2021 dataset, which includes 12 different sound classes and demonstrate the efficacy of proposed method. We begin by learning 8 classes and introduce the 4 new classes at next stage. After the incremental phase, the system is evaluated on the full set of learned classes. Results show that, for this realistic dataset, our proposed method successfully maintains baseline performance across all metrics.",
        "subjects": [
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12832",
        "abstract url": "https://arxiv.org/abs/2411.12832",
        "title": "HyperGAN-CLIP: A Unified Framework for Domain Adaptation, Image Synthesis and Manipulation",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generative Adversarial Networks (GANs), particularly StyleGAN and its variants, have demonstrated remarkable capabilities in generating highly realistic images. Despite their success, adapting these models to diverse tasks such as domain adaptation, reference-guided synthesis, and text-guided manipulation with limited training data remains challenging. Towards this end, in this study, we present a novel framework that significantly extends the capabilities of a pre-trained StyleGAN by integrating CLIP space via hypernetworks. This integration allows dynamic adaptation of StyleGAN to new domains defined by reference images or textual descriptions. Additionally, we introduce a CLIP-guided discriminator that enhances the alignment between generated images and target domains, ensuring superior image quality. Our approach demonstrates unprecedented flexibility, enabling text-guided image manipulation without the need for text-specific training data and facilitating seamless style transfer. Comprehensive qualitative and quantitative evaluations confirm the robustness and superior performance of our framework compared to existing methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted for publication in SIGGRAPH Asia 2024. Project Website: https://cyberiada.github.io/HyperGAN-CLIP/"
    },
    {
        "paper id": "2411.12843",
        "abstract url": "https://arxiv.org/abs/2411.12843",
        "title": "Reward Modeling with Ordinal Feedback: Wisdom of the Crowd",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Learning a reward model (RM) from human preferences has been an important component in aligning large language models (LLMs). The canonical setup of learning RMs from pairwise preference data is rooted in the classic Bradley-Terry (BT) model that accepts binary feedback, i.e., the label being either Response 1 is better than Response 2, or the opposite. Such a setup inevitably discards potentially useful samples (such as \"tied\" between the two responses) and loses more fine-grained information (such as \"slightly better\"). In this paper, we propose a framework for learning RMs under ordinal feedback which generalizes the case of binary preference feedback to any arbitrary granularity. Specifically, we first identify a marginal unbiasedness condition, which generalizes the assumption of the BT model in the existing binary feedback setting. The condition validates itself via the sociological concept of the wisdom of the crowd. Under the condition, we develop a natural probability model for pairwise preference data under ordinal feedback and analyze its properties. We prove the statistical benefits of ordinal feedback in terms of reducing the Rademacher complexity compared to the case of binary feedback. The proposed learning objective and the theory also extend to hinge loss and direct policy optimization (DPO). In particular, the theoretical analysis may be of independent interest when applying to a seemingly unrelated problem of knowledge distillation to interpret the bias-variance trade-off therein. The framework also sheds light on writing guidance for human annotators. Our numerical experiments validate that fine-grained feedback leads to better reward learning for both in-distribution and out-of-distribution settings. Further experiments show that incorporating a certain proportion of samples with tied preference boosts RM learning.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12878",
        "abstract url": "https://arxiv.org/abs/2411.12878",
        "title": "Local Anti-Concentration Class: Logarithmic Regret for Greedy Linear Contextual Bandit",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We study the performance guarantees of exploration-free greedy algorithms for the linear contextual bandit problem. We introduce a novel condition, named the \\textit{Local Anti-Concentration} (LAC) condition, which enables a greedy bandit algorithm to achieve provable efficiency. We show that the LAC condition is satisfied by a broad class of distributions, including Gaussian, exponential, uniform, Cauchy, and Student's~$t$ distributions, along with other exponential family distributions and their truncated variants. This significantly expands the class of distributions under which greedy algorithms can perform efficiently. Under our proposed LAC condition, we prove that the cumulative expected regret of the greedy algorithm for the linear contextual bandit is bounded by $O(\\operatorname{poly} \\log T)$. Our results establish the widest range of distributions known to date that allow a sublinear regret bound for greedy algorithms, further achieving a sharp poly-logarithmic regret.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "NeurIPS2024"
    },
    {
        "paper id": "2411.12882",
        "abstract url": "https://arxiv.org/abs/2411.12882",
        "title": "ProSec: Fortifying Code LLMs with Proactive Security Alignment",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recent advances in code-specific large language models (LLMs) have greatly enhanced code generation and refinement capabilities. However, the safety of code LLMs remains under-explored, posing potential risks as insecure code generated by these models may introduce vulnerabilities into real-world systems. Previous work proposes to collect security-focused instruction-tuning dataset from real-world vulnerabilities. It is constrained by the data sparsity of vulnerable code, and has limited applicability in the iterative post-training workflows of modern LLMs. In this paper, we propose ProSec, a novel proactive security alignment approach designed to align code LLMs with secure coding practices. ProSec systematically exposes the vulnerabilities in a code LLM by synthesizing error-inducing coding scenarios from Common Weakness Enumerations (CWEs), and generates fixes to vulnerable code snippets, allowing the model to learn secure practices through advanced preference learning objectives. The scenarios synthesized by ProSec triggers 25 times more vulnerable code than a normal instruction-tuning dataset, resulting in a security-focused alignment dataset 7 times larger than the previous work. Experiments show that models trained with ProSec is 29.2% to 35.5% more secure compared to previous work, with a marginal negative effect of less than 2 percentage points on model's utility.",
        "subjects": [
            "cs.CR",
            "cs.CL",
            "cs.SE"
        ],
        "comment": "The first two authors contributed equally to this work"
    },
    {
        "paper id": "2411.12892",
        "abstract url": "https://arxiv.org/abs/2411.12892",
        "title": "Selective Attention: Enhancing Transformer through Principled Context Control",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The attention mechanism within the transformer architecture enables the model to weigh and combine tokens based on their relevance to the query. While self-attention has enjoyed major success, it notably treats all queries $q$ in the same way by applying the mapping $V^\\top\\text{softmax}(Kq)$, where $V,K$ are the value and key embeddings respectively. In this work, we argue that this uniform treatment hinders the ability to control contextual sparsity and relevance. As a solution, we introduce the $\\textit{Selective Self-Attention}$ (SSA) layer that augments the softmax nonlinearity with a principled temperature scaling strategy. By controlling temperature, SSA adapts the contextual sparsity of the attention map to the query embedding and its position in the context window. Through theory and experiments, we demonstrate that this alleviates attention dilution, aids the optimization process, and enhances the model's ability to control softmax spikiness of individual queries. We also incorporate temperature scaling for value embeddings and show that it boosts the model's ability to suppress irrelevant/noisy tokens. Notably, SSA is a lightweight method which introduces less than 0.5% new parameters through a weight-sharing strategy and can be fine-tuned on existing LLMs. Extensive empirical evaluations demonstrate that SSA-equipped models achieve a noticeable and consistent accuracy improvement on language modeling benchmarks.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12925",
        "abstract url": "https://arxiv.org/abs/2411.12925",
        "title": "Loss-to-Loss Prediction: Scaling Laws for All Datasets",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "While scaling laws provide a reliable methodology for predicting train loss across compute scales for a single data distribution, less is known about how these predictions should change as we change the distribution. In this paper, we derive a strategy for predicting one loss from another and apply it to predict across different pre-training datasets and from pre-training data to downstream task data. Our predictions extrapolate well even at 20x the largest FLOP budget used to fit the curves. More precisely, we find that there are simple shifted power law relationships between (1) the train losses of two models trained on two separate datasets when the models are paired by training compute (train-to-train), (2) the train loss and the test loss on any downstream distribution for a single model (train-to-test), and (3) the test losses of two models trained on two separate train datasets (test-to-test). The results hold up for pre-training datasets that differ substantially (some are entirely code and others have no code at all) and across a variety of downstream tasks. Finally, we find that in some settings these shifted power law relationships can yield more accurate predictions than extrapolating single-dataset scaling laws.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12946",
        "abstract url": "https://arxiv.org/abs/2411.12946",
        "title": "A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models are prone to off-topic misuse, where users may prompt these models to perform tasks beyond their intended scope. Current guardrails, which often rely on curated examples or custom classifiers, suffer from high false-positive rates, limited adaptability, and the impracticality of requiring real-world data that is not available in pre-production. In this paper, we introduce a flexible, data-free guardrail development methodology that addresses these challenges. By thoroughly defining the problem space qualitatively and passing this to an LLM to generate diverse prompts, we construct a synthetic dataset to benchmark and train off-topic guardrails that outperform heuristic approaches. Additionally, by framing the task as classifying whether the user prompt is relevant with respect to the system prompt, our guardrails effectively generalize to other misuse categories, including jailbreak and harmful prompts. Lastly, we further contribute to the field by open-sourcing both the synthetic dataset and the off-topic guardrail models, providing valuable resources for developing guardrails in pre-production environments and supporting future research and development in LLM safety.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "8 pages, 5 figures"
    },
    {
        "paper id": "2411.12951",
        "abstract url": "https://arxiv.org/abs/2411.12951",
        "title": "On the Consistency of Video Large Language Models in Temporal Comprehension",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Video large language models (Video-LLMs) can temporally ground language queries and retrieve video moments. Yet, such temporal comprehension capabilities are neither well-studied nor understood. So we conduct a study on prediction consistency -- a key indicator for robustness and trustworthiness of temporal grounding. After the model identifies an initial moment within the video content, we apply a series of probes to check if the model's responses align with this initial grounding as an indicator of reliable comprehension. Our results reveal that current Video-LLMs are sensitive to variations in video contents, language queries, and task settings, unveiling severe deficiencies in maintaining consistency. We further explore common prompting and instruction-tuning methods as potential solutions, but find that their improvements are often unstable. To that end, we propose event temporal verification tuning that explicitly accounts for consistency, and demonstrate significant improvements for both grounding and consistency. Our data and code will be available at https://github.com/minjoong507/Consistency-of-Video-LLM.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12977",
        "abstract url": "https://arxiv.org/abs/2411.12977",
        "title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Collaborative Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Contemporary embodied agents, such as Voyager in Minecraft, have demonstrated promising capabilities in open-ended individual learning. However, when powered with open large language models (LLMs), these agents often struggle with rudimentary tasks, even when fine-tuned on domain-specific knowledge. Inspired by human cultural learning, we present \\collabvoyager, a novel framework that enhances Voyager with lifelong collaborative learning through explicit perspective-taking. \\collabvoyager introduces three key innovations: (1) theory of mind representations linking percepts, beliefs, desires, and actions; (2) natural language communication between agents; and (3) semantic memory of task and environment knowledge and episodic memory of collaboration episodes. These advancements enable agents to reason about their and others' mental states, empirically addressing two prevalent failure modes: false beliefs and faulty task executions. In mixed-expertise Minecraft experiments, \\collabvoyager agents outperform Voyager counterparts, significantly improving task completion rate by $66.6\\% (+39.4\\%)$ for collecting one block of dirt and $70.8\\% (+20.8\\%)$ for collecting one wood block. They exhibit emergent behaviors like knowledge transfer from expert to novice agents and collaborative code correction. \\collabvoyager agents also demonstrate the ability to adapt to out-of-distribution tasks by using their previous experiences and beliefs obtained through collaboration. In this open-ended social learning paradigm, \\collabvoyager paves the way for the democratic development of embodied AI, where agents learn in deployment from both peer and environmental feedback.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12980",
        "abstract url": "https://arxiv.org/abs/2411.12980",
        "title": "LaVida Drive: Vision-Text Interaction VLM for Autonomous Driving with Token Selection, Recovery and Enhancement",
        "rating": "1",
        "keywords": [
            [
                "Visual Language",
                "VLM"
            ],
            [
                "Autonomous Driving",
                "vehicle"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in Visual Language Models (VLMs) have made them crucial for visual question answering (VQA) in autonomous driving, enabling natural human-vehicle interactions. However, existing methods often struggle in dynamic driving environments, as they usually focus on static images or videos and rely on downsampling to manage computational costs. This results in the loss of critical details and the difficulty in effectively integrating spatial and temporal information, undermining fine-grained perception and temporal coherence essential for effective decision-making. To tackle these challenges, we introduce LaVida Drive, a novel and efficient VQA framework for autonomous driving. LaVida Drive seamlessly integrates temporal data while maintaining high-resolution inputs for detailed visual perception. It optimizes spatial processing by retaining high-resolution data for intricate details and using lower-resolution inputs for temporal analysis to focus on motion-related features, thereby boosting computational efficiency. The core of LaVida Drive consists of two modules: the \\textit{Query-aware Token Selection} module and the \\textit{Spatial-Temporal Token Recovery and Enhancement} module. The former dynamically selects the most relevant visual tokens based on semantic alignment with the input query, reducing the token count from high-resolution spatial input. The latter ensures smooth and coherent interactions between spatial and temporal information, preserving contextual continuity across frames. Extensive experiments on various autonomous driving question-answering benchmarks show that LaVida Drive significantly reduces visual tokens, enhances efficiency, and improves overall performance.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12986",
        "abstract url": "https://arxiv.org/abs/2411.12986",
        "title": "Training Bilingual LMs with Data Constraints in the Targeted Language",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models are trained on massive scrapes of the web, as required by current scaling laws. Most progress is made for English, given its abundance of high-quality pretraining data. For most other languages, however, such high quality pretraining data is unavailable. In this work, we study how to boost pretrained model performance in a data constrained target language by enlisting data from an auxiliary language for which high quality data is available. We study this by quantifying the performance gap between training with data in a data-rich auxiliary language compared with training in the target language, exploring the benefits of translation systems, studying the limitations of model scaling for data constrained languages, and proposing new methods for upsampling data from the auxiliary language. Our results show that stronger auxiliary datasets result in performance gains without modification to the model or training objective for close languages, and, in particular, that performance gains due to the development of more information-rich English pretraining datasets can extend to targeted language settings with limited data.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "22 pages, 14 figures, 15 tables"
    },
    {
        "paper id": "2411.12990",
        "abstract url": "https://arxiv.org/abs/2411.12990",
        "title": "BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "AI models are increasingly prevalent in high-stakes environments, necessitating thorough assessment of their capabilities and risks. Benchmarks are popular for measuring these attributes and for comparing model performance, tracking progress, and identifying weaknesses in foundation and non-foundation models. They can inform model selection for downstream tasks and influence policy initiatives. However, not all benchmarks are the same: their quality depends on their design and usability. In this paper, we develop an assessment framework considering 46 best practices across an AI benchmark's lifecycle and evaluate 24 AI benchmarks against it. We find that there exist large quality differences and that commonly used benchmarks suffer from significant issues. We further find that most benchmarks do not report statistical significance of their results nor allow for their results to be easily replicated. To support benchmark developers in aligning with best practices, we provide a checklist for minimum quality assurance based on our assessment. We also develop a living repository of benchmark assessments to support benchmark comparability, accessible at betterbench.stanford.edu.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted as a Spotlight Poster to NeurIPS 2024"
    },
    {
        "paper id": "2411.13001",
        "abstract url": "https://arxiv.org/abs/2411.13001",
        "title": "Collaborative Feature-Logits Contrastive Learning for Open-Set Semi-Supervised Object Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Current Semi-Supervised Object Detection (SSOD) methods enhance detector performance by leveraging large amounts of unlabeled data, assuming that both labeled and unlabeled data share the same label space. However, in open-set scenarios, the unlabeled dataset contains both in-distribution (ID) classes and out-of-distribution (OOD) classes. Applying semi-supervised detectors in such settings can lead to misclassifying OOD class as ID classes. To alleviate this issue, we propose a simple yet effective method, termed Collaborative Feature-Logits Detector (CFL-Detector). Specifically, we introduce a feature-level clustering method using contrastive loss to clarify vector boundaries in the feature space and highlight class differences. Additionally, by optimizing the logits-level uncertainty classification loss, the model enhances its ability to effectively distinguish between ID and OOD classes. Extensive experiments demonstrate that our method achieves state-of-the-art performance compared to existing methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13005",
        "abstract url": "https://arxiv.org/abs/2411.13005",
        "title": "DT-LSD: Deformable Transformer-based Line Segment Detection",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Line segment detection is a fundamental low-level task in computer vision, and improvements in this task can impact more advanced methods that depend on it. Most new methods developed for line segment detection are based on Convolutional Neural Networks (CNNs). Our paper seeks to address challenges that prevent the wider adoption of transformer-based methods for line segment detection. More specifically, we introduce a new model called Deformable Transformer-based Line Segment Detection (DT-LSD) that supports cross-scale interactions and can be trained quickly. This work proposes a novel Deformable Transformer-based Line Segment Detector (DT-LSD) that addresses LETR's drawbacks. For faster training, we introduce Line Contrastive DeNoising (LCDN), a technique that stabilizes the one-to-one matching process and speeds up training by 34$\\times$. We show that DT-LSD is faster and more accurate than its predecessor transformer-based model (LETR) and outperforms all CNN-based models in terms of accuracy. In the Wireframe dataset, DT-LSD achieves 71.7 for $sAP^{10}$ and 73.9 for $sAP^{15}$; while 33.2 for $sAP^{10}$ and 35.1 for $sAP^{15}$ in the YorkUrban dataset.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13009",
        "abstract url": "https://arxiv.org/abs/2411.13009",
        "title": "LLMSteer: Improving Long-Context LLM Inference by Steering Attention on Reused Contexts",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "As large language models (LLMs) show impressive performance on complex tasks, they still struggle with longer contextual understanding and high computational costs. To balance efficiency and quality, we introduce LLMSteer, a fine-tuning-free framework that enhances LLMs through query-independent attention steering. Tested on popular LLMs and datasets, LLMSteer narrows the performance gap with baselines by 65.9% and reduces the runtime delay by up to 4.8x compared to recent attention steering methods.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13017",
        "abstract url": "https://arxiv.org/abs/2411.13017",
        "title": "Breaking the Cycle of Recurring Failures: Applying Generative AI to Root Cause Analysis in Legacy Banking Systems",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Traditional banks face significant challenges in digital transformation, primarily due to legacy system constraints and fragmented ownership. Recent incidents show that such fragmentation often results in superficial incident resolutions, leaving root causes unaddressed and causing recurring failures. We introduce a novel approach to post-incident analysis, integrating knowledge-based GenAI agents with the \"Five Whys\" technique to examine problem descriptions and change request data. This method uncovered that approximately 70% of the incidents previously attributed to management or vendor failures were due to underlying internal code issues. We present a case study to show the impact of our method. By scanning over 5,000 projects, we identified over 400 files with a similar root cause. Overall, we leverage the knowledge-based agents to automate and elevate root cause analysis, transforming it into a more proactive process. These agents can be applied across other phases of the software development lifecycle, further improving development processes.",
        "subjects": [
            "cs.SE",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13021",
        "abstract url": "https://arxiv.org/abs/2411.13021",
        "title": "Chanel-Orderer: A Channel-Ordering Predictor for Tri-Channel Natural Images",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper shows a proof-of-concept that, given a typical 3-channel images but in a randomly permuted channel order, a model (termed as Chanel-Orderer) with ad-hoc inductive biases in terms of both architecture and loss functions can accurately predict the channel ordering and knows how to make it right. Specifically, Chanel-Orderer learns to score each of the three channels with the priors of object semantics and uses the resulting scores to predict the channel ordering. This brings up benefits into a typical scenario where an \\texttt{RGB} image is often mis-displayed in the \\texttt{BGR} format and needs to be corrected into the right order. Furthermore, as a byproduct, the resulting model Chanel-Orderer is able to tell whether a given image is a near-gray-scale image (near-monochromatic) or not (polychromatic). Our research suggests that Chanel-Orderer mimics human visual coloring of our physical natural world.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13033",
        "abstract url": "https://arxiv.org/abs/2411.13033",
        "title": "LMM-driven Semantic Image-Text Coding for Ultra Low-bitrate Learned Image Compression",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Supported by powerful generative models, low-bitrate learned image compression (LIC) models utilizing perceptual metrics have become feasible. Some of the most advanced models achieve high compression rates and superior perceptual quality by using image captions as sub-information. This paper demonstrates that using a large multi-modal model (LMM), it is possible to generate captions and compress them within a single model. We also propose a novel semantic-perceptual-oriented fine-tuning method applicable to any LIC network, resulting in a 41.58\\% improvement in LPIPS BD-rate compared to existing methods. Our implementation and pre-trained weights are available at https://github.com/tokkiwa/ImageTextCoding.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "IEEE VCIP 2024 poster"
    },
    {
        "paper id": "2411.13612",
        "abstract url": "https://arxiv.org/abs/2411.13612",
        "title": "Efficient Streaming Voice Steganalysis in Challenging Detection Scenarios",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "In recent years, there has been an increasing number of information hiding techniques based on network streaming media, focusing on how to covertly and efficiently embed secret information into real-time transmitted network media signals to achieve concealed communication. The misuse of these techniques can lead to significant security risks, such as the spread of malicious code, commands, and viruses. Current steganalysis methods for network voice streams face two major challenges: efficient detection under low embedding rates and short duration conditions. These challenges arise because, with low embedding rates (e.g., as low as 10%) and short transmission durations (e.g., only 0.1 second), detection models struggle to acquire sufficiently rich sample features, making effective steganalysis difficult. To address these challenges, this paper introduces a Dual-View VoIP Steganalysis Framework (DVSF). The framework first randomly obfuscates parts of the native steganographic descriptors in VoIP stream segments, making the steganographic features of hard-to-detect samples more pronounced and easier to learn. It then captures fine-grained local features related to steganography, building on the global features of VoIP. Specially constructed VoIP segment triplets further adjust the feature distances within the model. Ultimately, this method effectively address the detection difficulty in VoIP. Extensive experiments demonstrate that our method significantly improves the accuracy of streaming voice steganalysis in these challenging detection scenarios, surpassing existing state-of-the-art methods and offering superior near-real-time performance.",
        "subjects": [
            "cs.CR",
            "cs.LG",
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.14483",
        "abstract url": "https://arxiv.org/abs/2411.14483",
        "title": "Ranking Unraveled: Recipes for LLM Rankings in Head-to-Head AI Combat",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Deciding which large language model (LLM) to use is a complex challenge. Pairwise ranking has emerged as a new method for evaluating human preferences for LLMs. This approach entails humans evaluating pairs of model outputs based on a predefined criterion. By collecting these comparisons, a ranking can be constructed using methods such as Elo. However, applying these algorithms as constructed in the context of LLM evaluation introduces several challenges. In this paper, we explore the effectiveness of ranking systems for head-to-head comparisons of LLMs. We formally define a set of fundamental principles for effective ranking and conduct a series of extensive evaluations on the robustness of several ranking algorithms in the context of LLMs. Our analysis uncovers key insights into the factors that affect ranking accuracy and efficiency, offering guidelines for selecting the most appropriate methods based on specific evaluation contexts and resource constraints.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.14484",
        "abstract url": "https://arxiv.org/abs/2411.14484",
        "title": "Robust Planning with Compound LLM Architectures: An LLM-Modulo Approach",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Previous work has attempted to boost Large Language Model (LLM) performance on planning and scheduling tasks through a variety of prompt engineering techniques. While these methods can work within the distributions tested, they are neither robust nor predictable. This limitation can be addressed through compound LLM architectures where LLMs work in conjunction with other components to ensure reliability. In this paper, we present a technical evaluation of a compound LLM architecture--the LLM-Modulo framework. In this framework, an LLM is paired with a complete set of sound verifiers that validate its output, re-prompting it if it fails. This approach ensures that the system can never output any fallacious output, and therefore that every output generated is guaranteed correct--something previous techniques have not been able to claim. Our results, evaluated across four scheduling domains, demonstrate significant performance gains with the LLM-Modulo framework using various models. Additionally, we explore modifications to the base configuration of the framework and assess their impact on overall system performance.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.14485",
        "abstract url": "https://arxiv.org/abs/2411.14485",
        "title": "Mediating Modes of Thought: LLM's for design scripting",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Here is an updated version of your abstract, cleaned for submission to arXiv with potential \"bad characters\" corrected to conform to ASCII standards: Architects adopt visual scripting and parametric design tools to explore more expansive design spaces (Coates, 2010), refine their thinking about the geometric logic of their design (Woodbury, 2010), and overcome conventional software limitations (Burry, 2011). Despite two decades of effort to make design scripting more accessible, a disconnect between a designer's free ways of thinking and the rigidity of algorithms remains (Burry, 2011). Recent developments in Large Language Models (LLMs) suggest this might soon change, as LLMs encode a general understanding of human context and exhibit the capacity to produce geometric logic. This project speculates that if LLMs can effectively mediate between user intent and algorithms, they become a powerful tool to make scripting in design more widespread and fun. We explore if such systems can interpret natural language prompts to assemble geometric operations relevant to computational design scripting. In the system, multiple layers of LLM agents are configured with specific context to infer the user intent and construct a sequential logic. Given a user's high-level text prompt, a geometric description is created, distilled into a sequence of logic operations, and mapped to software-specific commands. The completed script is constructed in the user's visual programming interface. The system succeeds in generating complete visual scripts up to a certain complexity but fails beyond this complexity threshold. It shows how LLMs can make design scripting much more aligned with human creativity and thought. Future research should explore conversational interactions, expand to multimodal inputs and outputs, and assess the performance of these tools.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.HC"
        ],
        "comment": "Published at ACADIA 2024"
    },
    {
        "paper id": "2411.17716",
        "abstract url": "https://arxiv.org/abs/2411.17716",
        "title": "Generating CKM Using Others' Data: Cross-AP CKM Inference with Deep Learning",
        "rating": "1",
        "keywords": [
            [
                "eess.IV"
            ]
        ],
        "abstract": "Channel knowledge map (CKM) is a promising paradigm shift towards environment-aware communication and sensing by providing location-specific prior channel knowledge before real-time communication. Although CKM is particularly appealing for dense networks such as cell-free networks, it remains a challenge to efficiently generate CKMs in dense networks. For a dense network with CKMs of existing access points (APs), it will be useful to efficiently generate CKMs of potentially new APs with only AP location information. The generation of inferred CKMs across APs can help dense networks achieve convenient initial CKM generation, environment-aware AP deployment, and cost-effective CKM updates. Considering that different APs in the same region share the same physical environment, there exists a natural correlation between the channel knowledge of different APs. Therefore, by mining the implicit correlation between location-specific channel knowledge, cross-AP CKM inference can be realized using data from other APs. This paper proposes a cross-AP inference method to generate CKMs of potentially new APs with deep learning. The location of the target AP is fed into the UNet model in combination with the channel knowledge of other existing APs, and supervised learning is performed based on the channel knowledge of the target AP. Based on the trained UNet and the channel knowledge of the existing APs, the CKM inference of the potentially new AP can be generated across APs. The generation results of the inferred CKM validate the feasibility and effectiveness of cross-AP CKM inference with other APs' channel knowledge.",
        "subjects": [
            "eess.SP",
            "eess.IV",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.17835",
        "abstract url": "https://arxiv.org/abs/2411.17835",
        "title": "Arabic-Nougat: Fine-Tuning Vision Transformers for Arabic OCR and Markdown Extraction",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "We present Arabic-Nougat, a suite of OCR models for converting Arabic book pages into structured Markdown text. Based on Meta's Nougat architecture, Arabic-Nougat includes three specialized models: arabic-small-nougat, arabic-base-nougat, and arabic-large-nougat. These models are fine-tuned on a synthetic dataset, arabic-img2md, comprising 13.7k pairs of Arabic book pages and their Markdown representations. Key contributions include the Aranizer-PBE-86k tokenizer, designed for efficient tokenization, and the use of torch.bfloat16 precision with Flash Attention 2 for optimized training and inference. Our models achieve state-of-the-art performance, with arabic-large-nougat delivering the highest Markdown Structure Accuracy and the lowest Character Error Rate. Additionally, we release a large-scale dataset containing 1.1 billion Arabic tokens extracted from over 8,500 books using our best-performing model, providing a valuable resource for Arabic OCR research. All models, datasets, and code are open-sourced and available at https://github.com/MohamedAliRashad/arabic-nougat.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "7 pages, 1 figure"
    },
    {
        "paper id": "2411.12246",
        "abstract url": "https://arxiv.org/abs/2411.12246",
        "title": "Efficient Training in Multi-Agent Reinforcement Learning: A Communication-Free Framework for the Box-Pushing Problem",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Self-organizing systems consist of autonomous agents that can perform complex tasks and adapt to dynamic environments without a central controller. Prior research often relies on reinforcement learning to enable agents to gain the skills needed for task completion, such as in the box-pushing environment. However, when agents push from opposing directions during exploration, they tend to exert equal and opposite forces on the box, resulting in minimal displacement and inefficient training. This paper proposes a model called Shared Pool of Information (SPI), which enables information to be accessible to all agents and facilitates coordination, reducing force conflicts among agents and enhancing exploration efficiency. Through computer simulations, we demonstrate that SPI not only expedites the training process but also requires fewer steps per episode, significantly improving the agents' collaborative effectiveness.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "17 pages, 16 figures"
    },
    {
        "paper id": "2411.12255",
        "abstract url": "https://arxiv.org/abs/2411.12255",
        "title": "Error-Feedback Model for Output Correction in Bilateral Control-Based Imitation Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In recent years, imitation learning using neural networks has enabled robots to perform flexible tasks. However, since neural networks operate in a feedforward structure, they do not possess a mechanism to compensate for output errors. To address this limitation, we developed a feedback mechanism to correct these errors. By employing a hierarchical structure for neural networks comprising lower and upper layers, the lower layer was controlled to follow the upper layer. Additionally, using a multi-layer perceptron in the lower layer, which lacks an internal state, enhanced the error feedback. In the character-writing task, this model demonstrated improved accuracy in writing previously untrained characters. In the character-writing task, this model demonstrated improved accuracy in writing previously untrained characters. Through autonomous control with error feedback, we confirmed that the lower layer could effectively track the output of the upper layer. This study represents a promising step toward integrating neural networks with control theories.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12265",
        "abstract url": "https://arxiv.org/abs/2411.12265",
        "title": "On the Accuracy and Precision of Moving Averages to Estimate Wi-Fi Link Quality",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The radio spectrum is characterized by a noticeable variability, which impairs performance and determinism of every wireless communication technology. To counteract this aspect, mechanisms like Minstrel are customarily employed in real Wi-Fi devices, and the adoption of machine learning for optimization is envisaged in next-generation Wi-Fi 8. All these approaches require communication quality to be monitored at runtime. In this paper, the effectiveness of simple techniques based on moving averages to estimate wireless link quality is analyzed, to assess their advantages and weaknesses. Results can be used, e.g., as a baseline when studying how artificial intelligence can be employed to mitigate unpredictability of wireless networks by providing reliable estimates about current spectrum conditions.",
        "subjects": [
            "cs.NI",
            "cs.LG"
        ],
        "comment": "preprint, 8 pages, 2024"
    },
    {
        "paper id": "2411.12334",
        "abstract url": "https://arxiv.org/abs/2411.12334",
        "title": "Learning from Label Proportions and Covariate-shifted Instances",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In many applications, especially due to lack of supervision or privacy concerns, the training data is grouped into bags of instances (feature-vectors) and for each bag we have only an aggregate label derived from the instance-labels in the bag. In learning from label proportions (LLP) the aggregate label is the average of the instance-labels in a bag, and a significant body of work has focused on training models in the LLP setting to predict instance-labels. In practice however, the training data may have fully supervised albeit covariate-shifted source data, along with the usual target data with bag-labels, and we wish to train a good instance-level predictor on the target domain. We call this the covariate-shifted hybrid LLP problem. Fully supervised covariate shifted data often has useful training signals and the goal is to leverage them for better predictive performance in the hybrid LLP setting. To achieve this, we develop methods for hybrid LLP which naturally incorporate the target bag-labels along with the source instance-labels, in the domain adaptation framework. Apart from proving theoretical guarantees bounding the target generalization error, we also conduct experiments on several publicly available datasets showing that our methods outperform LLP and domain adaptation baselines as well techniques from previous related work.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12364",
        "abstract url": "https://arxiv.org/abs/2411.12364",
        "title": "Ultra-Sparse Memory Network",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "It is widely acknowledged that the performance of Transformer models is exponentially related to their number of parameters and computational complexity. While approaches like Mixture of Experts (MoE) decouple parameter count from computational complexity, they still face challenges in inference due to high memory access costs. This work introduces UltraMem, incorporating large-scale, ultra-sparse memory layer to address these limitations. Our approach significantly reduces inference latency while maintaining model performance. We also investigate the scaling laws of this new architecture, demonstrating that it not only exhibits favorable scaling properties but outperforms traditional models. In our experiments, we train networks with up to 20 million memory slots. The results show that our method achieves state-of-the-art inference speed and model performance within a given computational budget.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "10 pages, 6 figures"
    },
    {
        "paper id": "2411.12435",
        "abstract url": "https://arxiv.org/abs/2411.12435",
        "title": "STRisk: A Socio-Technical Approach to Assess Hacking Breaches Risk",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Data breaches have begun to take on new dimensions and their prediction is becoming of great importance to organizations. Prior work has addressed this issue mainly from a technical perspective and neglected other interfering aspects such as the social media dimension. To fill this gap, we propose STRisk which is a predictive system where we expand the scope of the prediction task by bringing into play the social media dimension. We study over 3800 US organizations including both victim and non-victim organizations. For each organization, we design a profile composed of a variety of externally measured technical indicators and social factors. In addition, to account for unreported incidents, we consider the non-victim sample to be noisy and propose a noise correction approach to correct mislabeled organizations. We then build several machine learning models to predict whether an organization is exposed to experience a hacking breach. By exploiting both technical and social features, we achieve a Area Under Curve (AUC) score exceeding 98%, which is 12% higher than the AUC achieved using only technical features. Furthermore, our feature importance analysis reveals that open ports and expired certificates are the best technical predictors, while spreadability and agreeability are the best social predictors.",
        "subjects": [
            "cs.CR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12438",
        "abstract url": "https://arxiv.org/abs/2411.12438",
        "title": "Dimension Reduction via Sum-of-Squares and Improved Clustering Algorithms for Non-Spherical Mixtures",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We develop a new approach for clustering non-spherical (i.e., arbitrary component covariances) Gaussian mixture models via a subroutine, based on the sum-of-squares method, that finds a low-dimensional separation-preserving projection of the input data. Our method gives a non-spherical analog of the classical dimension reduction, based on singular value decomposition, that forms a key component of the celebrated spherical clustering algorithm of Vempala and Wang [VW04] (in addition to several other applications). As applications, we obtain an algorithm to (1) cluster an arbitrary total-variation separated mixture of $k$ centered (i.e., zero-mean) Gaussians with $n\\geq \\operatorname{poly}(d) f(w_{\\min}^{-1})$ samples and $\\operatorname{poly}(n)$ time, and (2) cluster an arbitrary total-variation separated mixture of $k$ Gaussians with identical but arbitrary unknown covariance with $n \\geq d^{O(\\log w_{\\min}^{-1})} f(w_{\\min}^{-1})$ samples and $n^{O(\\log w_{\\min}^{-1})}$ time. Here, $w_{\\min}$ is the minimum mixing weight of the input mixture, and $f$ does not depend on the dimension $d$. Our algorithms naturally extend to tolerating a dimension-independent fraction of arbitrary outliers. Before this work, the techniques in the state-of-the-art non-spherical clustering algorithms needed $d^{O(k)} f(w_{\\min}^{-1})$ time and samples for clustering such mixtures. Our results may come as a surprise in the context of the $d^{\u03a9(k)}$ statistical query lower bound [DKS17] for clustering non-spherical Gaussian mixtures. While this result is usually thought to rule out $d^{o(k)}$ cost algorithms for the problem, our results show that the lower bounds can in fact be circumvented for a remarkably general class of Gaussian mixtures.",
        "subjects": [
            "cs.DS",
            "cs.LG",
            "stat.ML"
        ],
        "comment": "64 pages"
    },
    {
        "paper id": "2411.12469",
        "abstract url": "https://arxiv.org/abs/2411.12469",
        "title": "AI Flow at the Network Edge",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Recent advancements in large language models (LLMs) and their multimodal variants have led to remarkable progress across various domains, demonstrating impressive capabilities and unprecedented potential. In the era of ubiquitous connectivity, leveraging communication networks to distribute intelligence is a transformative concept, envisioning AI-powered services accessible at the network edge. However, pushing large models from the cloud to resource-constrained environments faces critical challenges. Model inference on low-end devices leads to excessive latency and performance bottlenecks, while raw data transmission over limited bandwidth networks causes high communication overhead. This article presents AI Flow, a framework that streamlines the inference process by jointly leveraging the heterogeneous resources available across devices, edge nodes, and cloud servers, making intelligence flow across networks. To facilitate cooperation among multiple computational nodes, the proposed framework explores a paradigm shift in the design of communication network systems from transmitting information flow to intelligence flow, where the goal of communications is task-oriented and folded into the inference process. Experimental results demonstrate the effectiveness of the proposed framework through an image captioning use case, showcasing the ability to reduce response latency while maintaining high-quality captions. This article serves as a position paper for identifying the motivation, challenges, and principles of AI Flow.",
        "subjects": [
            "eess.SP",
            "cs.AI",
            "cs.LG",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12476",
        "abstract url": "https://arxiv.org/abs/2411.12476",
        "title": "Comparing Prior and Learned Time Representations in Transformer Models of Timeseries",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "What sets timeseries analysis apart from other machine learning exercises is that time representation becomes a primary aspect of the experiment setup, as it must adequately represent the temporal relations that are relevant for the application at hand. In the work described here we study wo different variations of the Transformer architecture: one where we use the fixed time representation proposed in the literature and one where the time representation is learned from the data. Our experiments use data from predicting the energy output of solar panels, a task that exhibits known periodicities (daily and seasonal) that is straight-forward to encode in the fixed time representation. Our results indicate that even in an experiment where the phenomenon is well-understood, it is difficult to encode prior knowledge due to side-effects that are difficult to mitigate. We conclude that research work is needed to work the human into the learning loop in ways that improve the robustness and trust-worthiness of the network.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Presented at the AI in Natural Sciences and Technology (AINST) track of the 13th Conference on Artificial Intelligence (SETN 2024), 11-13 September 2024, Piraeus, Greece"
    },
    {
        "paper id": "2411.12517",
        "abstract url": "https://arxiv.org/abs/2411.12517",
        "title": "The Hermeneutic Turn of AI: Is the Machine Capable of Interpreting?",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "This article aims to demonstrate how the approach to computing is being disrupted by deep learning (artificial neural networks), not only in terms of techniques but also in our interactions with machines. It also addresses the philosophical tradition of hermeneutics (Don Ihde, Wilhelm Dilthey) to highlight a parallel with this movement and to demystify the idea of human-like AI.",
        "subjects": [
            "cs.CY",
            "cs.AI",
            "cs.HC"
        ],
        "comment": "4 pages."
    },
    {
        "paper id": "2411.12570",
        "abstract url": "https://arxiv.org/abs/2411.12570",
        "title": "A data driven approach to classify descriptors based on their efficiency in translating noisy trajectories into physically-relevant information",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Reconstructing the physical complexity of many-body dynamical systems can be challenging. Starting from the trajectories of their constitutive units (raw data), typical approaches require selecting appropriate descriptors to convert them into time-series, which are then analyzed to extract interpretable information. However, identifying the most effective descriptor is often non-trivial. Here, we report a data-driven approach to compare the efficiency of various descriptors in extracting information from noisy trajectories and translating it into physically relevant insights. As a prototypical system with non-trivial internal complexity, we analyze molecular dynamics trajectories of an atomistic system where ice and water coexist in equilibrium near the solid/liquid transition temperature. We compare general and specific descriptors often used in aqueous systems: number of neighbors, molecular velocities, Smooth Overlap of Atomic Positions (SOAP), Local Environments and Neighbors Shuffling (LENS), Orientational Tetrahedral Order, and distance from the fifth neighbor ($d_5$). Using Onion Clustering -- an efficient unsupervised method for single-point time-series analysis -- we assess the maximum extractable information for each descriptor and rank them via a high-dimensional metric. Our results show that advanced descriptors like SOAP and LENS outperform classical ones due to higher signal-to-noise ratios. Nonetheless, even simple descriptors can rival or exceed advanced ones after local signal denoising. For example, $d_5$, initially among the weakest, becomes the most effective at resolving the system's non-local dynamical complexity after denoising. This work highlights the critical role of noise in information extraction from molecular trajectories and offers a data-driven approach to identify optimal descriptors for systems with characteristic internal complexity.",
        "subjects": [
            "cond-mat.mtrl-sci",
            "cs.LG"
        ],
        "comment": "19 pages, 5 figures + 3 in supporting information (at the bottom of the manuscript)"
    },
    {
        "paper id": "2411.12601",
        "abstract url": "https://arxiv.org/abs/2411.12601",
        "title": "Hypergraph $p$-Laplacian equations for data interpolation and semi-supervised learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Hypergraph learning with $p$-Laplacian regularization has attracted a lot of attention due to its flexibility in modeling higher-order relationships in data. This paper focuses on its fast numerical implementation, which is challenging due to the non-differentiability of the objective function and the non-uniqueness of the minimizer. We derive a hypergraph $p$-Laplacian equation from the subdifferential of the $p$-Laplacian regularization. A simplified equation that is mathematically well-posed and computationally efficient is proposed as an alternative. Numerical experiments verify that the simplified $p$-Laplacian equation suppresses spiky solutions in data interpolation and improves classification accuracy in semi-supervised learning. The remarkably low computational cost enables further applications.",
        "subjects": [
            "math.NA",
            "cs.LG"
        ],
        "comment": "16 pages"
    },
    {
        "paper id": "2411.12636",
        "abstract url": "https://arxiv.org/abs/2411.12636",
        "title": "PyAWD: A Library for Generating Large Synthetic Datasets of Acoustic Wave Propagation with Devito",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Seismic data is often sparse and unevenly distributed due to the high costs and logistical challenges associated with deploying physical seismometers, limiting the application of Machine Learning (ML) in earthquake analysis. To address this gap, we introduce PyAWD, a Python library designed to generate high-resolution synthetic datasets simulating spatio-temporal acoustic wave propagation in both two-dimensional and three-dimensional heterogeneous media. By allowing fine control over parameters such as wave speed, external forces, spatial and temporal discretization, and media composition, PyAWD enables the creation of ML-scale datasets that capture the complexity of seismic wave behavior. We illustrate the library's potential with an epicenter retrieval task, showcasing its suitability for designing complex, accurate seismic problems that support advanced ML approaches in the absence or lack of dense real-world data.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12644",
        "abstract url": "https://arxiv.org/abs/2411.12644",
        "title": "CodeXEmbed: A Generalist Embedding Model Family for Multiligual and Multi-task Code Retrieval",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Despite the success of text retrieval in many NLP tasks, code retrieval remains a largely underexplored area. Most text retrieval systems are tailored for natural language queries, often neglecting the specific challenges of retrieving code. This gap leaves existing models unable to effectively capture the diversity of programming languages and tasks across different domains, highlighting the need for more focused research in code retrieval. To address this, we introduce CodeXEmbed, a family of large-scale code embedding models ranging from 400M to 7B parameters. Our novel training pipeline unifies multiple programming languages and transforms various code-related tasks into a common retrieval framework, enhancing model generalizability and retrieval performance. Our 7B model sets a new state-of-the-art (SOTA) in code retrieval, outperforming the previous leading model, Voyage-Code, by over 20% on CoIR benchmark. In addition to excelling in code retrieval, our models demonstrate competitive performance on the widely adopted BeIR text retrieval benchmark, offering versatility across domains. Experimental results demonstrate that improving retrieval performance significantly enhances end-to-end Retrieval-Augmented Generation (RAG) performance for code-related tasks.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12665",
        "abstract url": "https://arxiv.org/abs/2411.12665",
        "title": "Auto-Evaluation with Few Labels through Post-hoc Regression",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Continually evaluating large generative models provides a unique challenge. Often, human annotations are necessary to evaluate high-level properties of these models (e.g. in text or images). However, collecting human annotations of samples can be resource intensive, and using other machine learning systems to provide the annotations, or automatic evaluation, can introduce systematic errors into the evaluation. The Prediction Powered Inference (PPI) framework provides a way of leveraging both the statistical power of automatic evaluation and a small pool of labelled data to produce a low-variance, unbiased estimate of the quantity being evaluated for. However, most work on PPI considers a relatively sizable set of labelled samples, which is not always practical to obtain. To this end, we present two new PPI-based techniques that leverage robust regressors to produce even lower variance estimators in the few-label regime.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12700",
        "abstract url": "https://arxiv.org/abs/2411.12700",
        "title": "Learning multivariate Gaussians with imperfect advice",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We revisit the problem of distribution learning within the framework of learning-augmented algorithms. In this setting, we explore the scenario where a probability distribution is provided as potentially inaccurate advice on the true, unknown distribution. Our objective is to develop learning algorithms whose sample complexity decreases as the quality of the advice improves, thereby surpassing standard learning lower bounds when the advice is sufficiently accurate. Specifically, we demonstrate that this outcome is achievable for the problem of learning a multivariate Gaussian distribution $N(\\boldsymbol\u03bc, \\boldsymbol\u03a3)$ in the PAC learning setting. Classically, in the advice-free setting, $\\tilde\u0398(d^2/\\varepsilon^2)$ samples are sufficient and worst case necessary to learn $d$-dimensional Gaussians up to TV distance $\\varepsilon$ with constant probability. When we are additionally given a parameter $\\tilde{\\boldsymbol\u03a3}$ as advice, we show that $\\tilde{O}(d^{2-\u03b2}/\\varepsilon^2)$ samples suffices whenever $\\| \\tilde{\\boldsymbol\u03a3}^{-1/2} \\boldsymbol\u03a3 \\tilde{\\boldsymbol\u03a3}^{-1/2} - \\boldsymbol{I_d} \\|_1 \\leq \\varepsilon d^{1-\u03b2}$ (where $\\|\\cdot\\|_1$ denotes the entrywise $\\ell_1$ norm) for any $\u03b2> 0$, yielding a polynomial improvement over the advice-free setting.",
        "subjects": [
            "cs.LG",
            "cs.DS",
            "cs.IT",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12726",
        "abstract url": "https://arxiv.org/abs/2411.12726",
        "title": "LazyDINO: Fast, scalable, and efficiently amortized Bayesian inversion via structure-exploiting and surrogate-driven measure transport",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present LazyDINO, a transport map variational inference method for fast, scalable, and efficiently amortized solutions of high-dimensional nonlinear Bayesian inverse problems with expensive parameter-to-observable (PtO) maps. Our method consists of an offline phase in which we construct a derivative-informed neural surrogate of the PtO map using joint samples of the PtO map and its Jacobian. During the online phase, when given observational data, we seek rapid posterior approximation using surrogate-driven training of a lazy map [Brennan et al., NeurIPS, (2020)], i.e., a structure-exploiting transport map with low-dimensional nonlinearity. The trained lazy map then produces approximate posterior samples or density evaluations. Our surrogate construction is optimized for amortized Bayesian inversion using lazy map variational inference. We show that (i) the derivative-based reduced basis architecture [O'Leary-Roseberry et al., Comput. Methods Appl. Mech. Eng., 388 (2022)] minimizes the upper bound on the expected error in surrogate posterior approximation, and (ii) the derivative-informed training formulation [O'Leary-Roseberry et al., J. Comput. Phys., 496 (2024)] minimizes the expected error due to surrogate-driven transport map optimization. Our numerical results demonstrate that LazyDINO is highly efficient in cost amortization for Bayesian inversion. We observe one to two orders of magnitude reduction of offline cost for accurate posterior approximation, compared to simulation-based amortized inference via conditional transport and conventional surrogate-driven transport. In particular, LazyDINO outperforms Laplace approximation consistently using fewer than 1000 offline samples, while other amortized inference methods struggle and sometimes fail at 16,000 offline samples.",
        "subjects": [
            "math.NA",
            "cs.LG",
            "stat.CO",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12778",
        "abstract url": "https://arxiv.org/abs/2411.12778",
        "title": "Lucia: A Temporal Computing Platform for Contextual Intelligence",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "The rapid evolution of artificial intelligence, especially through multi-modal large language models, has redefined user interactions, enabling responses that are contextually rich and human-like. As AI becomes an integral part of daily life, a new frontier has emerged: developing systems that not only understand spatial and sensory data but also interpret temporal contexts to build long-term, personalized memories. This report introduces Lucia, an open-source Temporal Computing Platform designed to enhance human cognition by capturing and utilizing continuous contextual memory. Lucia introduces a lightweight, wearable device that excels in both comfort and real-time data accessibility, distinguishing itself from existing devices that typically prioritize either wearability or perceptual capabilities alone. By recording and interpreting daily activities over time, Lucia enables users to access a robust temporal memory, enhancing cognitive processes such as decision-making and memory recall.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12831",
        "abstract url": "https://arxiv.org/abs/2411.12831",
        "title": "Towards motion from video diffusion models",
        "rating": "0.5",
        "keywords": [
            [
                "diffusion",
                "text-to-video"
            ],
            [
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Text-conditioned video diffusion models have emerged as a powerful tool in the realm of video generation and editing. But their ability to capture the nuances of human movement remains under-explored. Indeed the ability of these models to faithfully model an array of text prompts can lead to a wide host of applications in human and character animation. In this work, we take initial steps to investigate whether these models can effectively guide the synthesis of realistic human body animations. Specifically we propose to synthesize human motion by deforming an SMPL-X body representation guided by Score distillation sampling (SDS) calculated using a video diffusion model. By analyzing the fidelity of the resulting animations, we gain insights into the extent to which we can obtain motion using publicly available text-to-video diffusion models using SDS. Our findings shed light on the potential and limitations of these models for generating diverse and plausible human motions, paving the way for further research in this exciting area.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at ECCV 2024 Workshop :Foundation Models for 3D Humans"
    },
    {
        "paper id": "2411.12841",
        "abstract url": "https://arxiv.org/abs/2411.12841",
        "title": "Data-to-Model Distillation: Data-Efficient Learning Framework",
        "rating": "0.5",
        "keywords": [
            [
                "architecture search"
            ],
            [
                "cs.LG",
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Dataset distillation aims to distill the knowledge of a large-scale real dataset into small yet informative synthetic data such that a model trained on it performs as well as a model trained on the full dataset. Despite recent progress, existing dataset distillation methods often struggle with computational efficiency, scalability to complex high-resolution datasets, and generalizability to deep architectures. These approaches typically require retraining when the distillation ratio changes, as knowledge is embedded in raw pixels. In this paper, we propose a novel framework called Data-to-Model Distillation (D2M) to distill the real dataset's knowledge into the learnable parameters of a pre-trained generative model by aligning rich representations extracted from real and generated images. The learned generative model can then produce informative training images for different distillation ratios and deep architectures. Extensive experiments on 15 datasets of varying resolutions show D2M's superior performance, re-distillation efficiency, and cross-architecture generalizability. Our method effectively scales up to high-resolution 128x128 ImageNet-1K. Furthermore, we verify D2M's practical benefits for downstream applications in neural architecture search.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Accepted in the 18th European Conference on Computer Vision (ECCV 2024), Milan, Italy, September 29 October 4, 2024"
    },
    {
        "paper id": "2411.12847",
        "abstract url": "https://arxiv.org/abs/2411.12847",
        "title": "mDAE : modified Denoising AutoEncoder for missing data imputation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "This paper introduces a methodology based on Denoising AutoEncoder (DAE) for missing data imputation. The proposed methodology, called mDAE hereafter, results from a modification of the loss function and a straightforward procedure for choosing the hyper-parameters. An ablation study shows on several UCI Machine Learning Repository datasets, the benefit of using this modified loss function and an overcomplete structure, in terms of Root Mean Squared Error (RMSE) of reconstruction. This numerical study is completed by comparing the mDAE methodology with eight other methods (four standard and four more recent). A criterion called Mean Distance to Best (MDB) is proposed to measure how a method performs globally well on all datasets. This criterion is defined as the mean (over the datasets) of the distances between the RMSE of the considered method and the RMSE of the best method. According to this criterion, the mDAE methodology was consistently ranked among the top methods (along with SoftImput and missForest), while the four more recent methods were systematically ranked last. The Python code of the numerical study will be available on GitHub so that results can be reproduced or generalized with other datasets and methods.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12854",
        "abstract url": "https://arxiv.org/abs/2411.12854",
        "title": "A new Input Convex Neural Network with application to options pricing",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We introduce a new class of neural networks designed to be convex functions of their inputs, leveraging the principle that any convex function can be represented as the supremum of the affine functions it dominates. These neural networks, inherently convex with respect to their inputs, are particularly well-suited for approximating the prices of options with convex payoffs. We detail the architecture of this, and establish theoretical convergence bounds that validate its approximation capabilities. We also introduce a \\emph{scrambling} phase to improve the training of these networks. Finally, we demonstrate numerically the effectiveness of these networks in estimating prices for three types of options with convex payoffs: Basket, Bermudan, and Swing options.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "29 pages"
    },
    {
        "paper id": "2411.12859",
        "abstract url": "https://arxiv.org/abs/2411.12859",
        "title": "The Game-Theoretic Symbiosis of Trust and AI in Networked Systems",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "This chapter explores the symbiotic relationship between Artificial Intelligence (AI) and trust in networked systems, focusing on how these two elements reinforce each other in strategic cybersecurity contexts. AI's capabilities in data processing, learning, and real-time response offer unprecedented support for managing trust in dynamic, complex networks. However, the successful integration of AI also hinges on the trustworthiness of AI systems themselves. Using a game-theoretic framework, this chapter presents approaches to trust evaluation, the strategic role of AI in cybersecurity, and governance frameworks that ensure responsible AI deployment. We investigate how trust, when dynamically managed through AI, can form a resilient security ecosystem. By examining trust as both an AI output and an AI requirement, this chapter sets the foundation for a positive feedback loop where AI enhances network security and the trust placed in AI systems fosters their adoption.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12873",
        "abstract url": "https://arxiv.org/abs/2411.12873",
        "title": "Tensor-Based Foundations of Ordinary Least Squares and Neural Network Regression Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This article introduces a novel approach to the mathematical development of Ordinary Least Squares and Neural Network regression models, diverging from traditional methods in current Machine Learning literature. By leveraging Tensor Analysis and fundamental matrix computations, the theoretical foundations of both models are meticulously detailed and extended to their complete algorithmic forms. The study culminates in the presentation of three algorithms, including a streamlined version of the Backpropagation Algorithm for Neural Networks, illustrating the benefits of this new mathematical approach.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "16 pages, 3 algorithms"
    },
    {
        "paper id": "2411.12877",
        "abstract url": "https://arxiv.org/abs/2411.12877",
        "title": "The Illusion of Empathy: How AI Chatbots Shape Conversation Perception",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "As AI chatbots become more human-like by incorporating empathy, understanding user-centered perceptions of chatbot empathy and its impact on conversation quality remains essential yet under-explored. This study examines how chatbot identity and perceived empathy influence users' overall conversation experience. Analyzing 155 conversations from two datasets, we found that while GPT-based chatbots were rated significantly higher in conversational quality, they were consistently perceived as less empathetic than human conversational partners. Empathy ratings from GPT-4o annotations aligned with users' ratings, reinforcing the perception of lower empathy in chatbots. In contrast, 3 out of 5 empathy models trained on human-human conversations detected no significant differences in empathy language between chatbots and humans. Our findings underscore the critical role of perceived empathy in shaping conversation quality, revealing that achieving high-quality human-AI interactions requires more than simply embedding empathetic language; it necessitates addressing the nuanced ways users interpret and experience empathy in conversations with chatbots.",
        "subjects": [
            "cs.HC",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12898",
        "abstract url": "https://arxiv.org/abs/2411.12898",
        "title": "Problem-dependent convergence bounds for randomized linear gradient compression",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In distributed optimization, the communication of model updates can be a performance bottleneck. Consequently, gradient compression has been proposed as a means of increasing optimization throughput. In general, due to information loss, compression introduces a penalty on the number of iterations needed to reach a solution. In this work, we investigate how the iteration penalty depends on the interaction between compression and problem structure, in the context of non-convex stochastic optimization. We focus on linear compression schemes, where compression and decompression can be modeled as multiplication with a random matrix. We consider several distributions of matrices, among them random orthogonal matrices and matrices with random Gaussian entries. We find that in each case, the impact of compression on convergence can be quantified in terms of the norm of the Hessian of the objective, using a norm defined by the compression scheme. The analysis reveals that in certain cases, compression performance is related to low-rank structure or other spectral properties of the problem. In these cases, our bounds predict that the penalty introduced by compression is significantly reduced compared to worst-case bounds that only consider the compression level, ignoring problem data. We verify the theoretical findings on several optimization problems, including fine-tuning an image classification model.",
        "subjects": [
            "math.OC",
            "cs.LG"
        ],
        "comment": "15 pages, 3 figures"
    },
    {
        "paper id": "2411.12907",
        "abstract url": "https://arxiv.org/abs/2411.12907",
        "title": "Narrative Information Theory",
        "rating": "0.5",
        "keywords": [
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We propose an information-theoretic framework to measure narratives, providing a formalism to understand pivotal moments, cliffhangers, and plot twists. This approach offers creatives and AI researchers tools to analyse and benchmark human- and AI-created stories. We illustrate our method in TV shows, showing its ability to quantify narrative complexity and emotional dynamics across genres. We discuss applications in media and in human-in-the-loop generative AI storytelling.",
        "subjects": [
            "cs.MM",
            "cs.IT"
        ],
        "comment": "To be published in NeurIPS 2024 Workshop on Creativity & Generative AI. 7 pages, 3 figures"
    },
    {
        "paper id": "2411.12921",
        "abstract url": "https://arxiv.org/abs/2411.12921",
        "title": "A Comparative Study of Text Retrieval Models on DaReCzech",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "This article presents a comprehensive evaluation of 7 off-the-shelf document retrieval models: Splade, Plaid, Plaid-X, SimCSE, Contriever, OpenAI ADA and Gemma2 chosen to determine their performance on the Czech retrieval dataset DaReCzech. The primary objective of our experiments is to estimate the quality of modern retrieval approaches in the Czech language. Our analyses include retrieval quality, speed, and memory footprint. Secondly, we analyze whether it is better to use the model directly in Czech text, or to use machine translation into English, followed by retrieval in English. Our experiments identify the most effective option for Czech information retrieval. The findings revealed notable performance differences among the models, with Gemma22 achieving the highest precision and recall, while Contriever performing poorly. Conclusively, SPLADE and PLAID models offered a balance of efficiency and performance.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12924",
        "abstract url": "https://arxiv.org/abs/2411.12924",
        "title": "Human-In-the-Loop Software Development Agents",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Recently, Large Language Models (LLMs)-based multi-agent paradigms for software engineering are introduced to automatically resolve software development tasks (e.g., from a given issue to source code). However, existing work is evaluated based on historical benchmark datasets, does not consider human feedback at each stage of the automated software development process, and has not been deployed in practice. In this paper, we introduce a Human-in-the-loop LLM-based Agents framework (HULA) for software development that allows software engineers to refine and guide LLMs when generating coding plans and source code for a given task. We design, implement, and deploy the HULA framework into Atlassian JIRA for internal uses. Through a multi-stage evaluation of the HULA framework, Atlassian software engineers perceive that HULA can minimize the overall development time and effort, especially in initiating a coding plan and writing code for straightforward tasks. On the other hand, challenges around code quality are raised to be solved in some cases. We draw lessons learned and discuss opportunities for future work, which will pave the way for the advancement of LLM-based agents in software development.",
        "subjects": [
            "cs.SE",
            "cs.AI",
            "cs.HC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12930",
        "abstract url": "https://arxiv.org/abs/2411.12930",
        "title": "LEDRO: LLM-Enhanced Design Space Reduction and Optimization for Analog Circuits",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Traditional approaches for designing analog circuits are time-consuming and require significant human expertise. Existing automation efforts using methods like Bayesian Optimization (BO) and Reinforcement Learning (RL) are sub-optimal and costly to generalize across different topologies and technology nodes. In our work, we introduce a novel approach, LEDRO, utilizing Large Language Models (LLMs) in conjunction with optimization techniques to iteratively refine the design space for analog circuit sizing. LEDRO is highly generalizable compared to other RL and BO baselines, eliminating the need for design annotation or model training for different topologies or technology nodes. We conduct a comprehensive evaluation of our proposed framework and baseline on 22 different Op-Amp topologies across four FinFET technology nodes. Results demonstrate the superior performance of LEDRO as it outperforms our best baseline by an average of 13% FoM improvement with 2.15x speed-up on low complexity Op-Amps and 48% FoM improvement with 1.7x speed-up on high complexity Op-Amps. This highlights LEDRO's effective performance, efficiency, and generalizability.",
        "subjects": [
            "cs.LG",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12995",
        "abstract url": "https://arxiv.org/abs/2411.12995",
        "title": "Eliminating Ratio Bias for Gradient-based Simulated Parameter Estimation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This article addresses the challenge of parameter calibration in stochastic models where the likelihood function is not analytically available. We propose a gradient-based simulated parameter estimation framework, leveraging a multi-time scale algorithm that tackles the issue of ratio bias in both maximum likelihood estimation and posterior density estimation problems. Additionally, we introduce a nested simulation optimization structure, providing theoretical analyses including strong convergence, asymptotic normality, convergence rate, and budget allocation strategies for the proposed algorithm. The framework is further extended to neural network training, offering a novel perspective on stochastic approximation in machine learning. Numerical experiments show that our algorithm can improve the estimation accuracy and save computational costs.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13004",
        "abstract url": "https://arxiv.org/abs/2411.13004",
        "title": "MERLOT: A Distilled LLM-based Mixture-of-Experts Framework for Scalable Encrypted Traffic Classification",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We present MERLOT, a scalable mixture-of-expert (MoE) based refinement of distilled large language model optimized for encrypted traffic classification. By applying model distillation techniques in a teacher-student paradigm, compact models derived from GPT-2-base retain high classification accuracy while minimizing computational costs. These models function as specialized experts in an MoE architecture, dynamically assigned via a gating network. Unlike generation-based methods, our approach directly classifies encrypted traffic using the final decoder token with contextual feature embedding as input. Experiments on 10 datasets show superior or competitive performance over the state-of-the-art models while significantly reducing resource demands, underscoring its effectiveness and robustness.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13008",
        "abstract url": "https://arxiv.org/abs/2411.13008",
        "title": "Evaluating LLMs Capabilities Towards Understanding Social Dynamics",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Social media discourse involves people from different backgrounds, beliefs, and motives. Thus, often such discourse can devolve into toxic interactions. Generative Models, such as Llama and ChatGPT, have recently exploded in popularity due to their capabilities in zero-shot question-answering. Because these models are increasingly being used to ask questions of social significance, a crucial research question is whether they can understand social media dynamics. This work provides a critical analysis regarding generative LLM's ability to understand language and dynamics in social contexts, particularly considering cyberbullying and anti-cyberbullying (posts aimed at reducing cyberbullying) interactions. Specifically, we compare and contrast the capabilities of different large language models (LLMs) to understand three key aspects of social dynamics: language, directionality, and the occurrence of bullying/anti-bullying messages. We found that while fine-tuned LLMs exhibit promising results in some social media understanding tasks (understanding directionality), they presented mixed results in others (proper paraphrasing and bullying/anti-bullying detection). We also found that fine-tuning and prompt engineering mechanisms can have positive effects in some tasks. We believe that a understanding of LLM's capabilities is crucial to design future models that can be effectively used in social applications.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "To appear in ASONAM 24 proceedings"
    },
    {
        "paper id": "2411.13010",
        "abstract url": "https://arxiv.org/abs/2411.13010",
        "title": "Deriving Activation Functions via Integration",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Activation functions play a crucial role in introducing non-linearities to deep neural networks. We propose a novel approach to designing activation functions by focusing on their gradients and deriving the corresponding functions through integration. Our work introduces the Expanded Integral of the Exponential Linear Unit (xIELU), a trainable piecewise activation function derived by integrating trainable affine transformations applied on the ELU activation function. xIELU combines two key gradient properties: a trainable and linearly increasing gradient for positive inputs, similar to ReLU$^2$, and a trainable negative gradient flow for negative inputs, akin to xSiLU. Conceptually, xIELU can be viewed as extending ReLU$^2$ to effectively handle negative inputs. In experiments with 1.1B parameter Llama models trained on 126B tokens of FineWeb Edu, xIELU achieves lower perplexity compared to both ReLU$^2$ and SwiGLU when matched for the same compute cost and parameter count.",
        "subjects": [
            "cs.LG",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13032",
        "abstract url": "https://arxiv.org/abs/2411.13032",
        "title": "\"It was 80% me, 20% AI\": Seeking Authenticity in Co-Writing with Large Language Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "Given the rising proliferation and diversity of AI writing assistance tools, especially those powered by large language models (LLMs), both writers and readers may have concerns about the impact of these tools on the authenticity of writing work. We examine whether and how writers want to preserve their authentic voice when co-writing with AI tools and whether personalization of AI writing support could help achieve this goal. We conducted semi-structured interviews with 19 professional writers, during which they co-wrote with both personalized and non-personalized AI writing-support tools. We supplemented writers' perspectives with opinions from 30 avid readers about the written work co-produced with AI collected through an online survey. Our findings illuminate conceptions of authenticity in human-AI co-creation, which focus more on the process and experience of constructing creators' authentic selves. While writers reacted positively to personalized AI writing tools, they believed the form of personalization needs to target writers' growth and go beyond the phase of text production. Overall, readers' responses showed less concern about human-AI co-writing. Readers could not distinguish AI-assisted work, personalized or not, from writers' solo-written work and showed positive attitudes toward writers experimenting with new technology for creative writing.",
        "subjects": [
            "cs.HC",
            "cs.AI",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13608",
        "abstract url": "https://arxiv.org/abs/2411.13608",
        "title": "Integrating Dynamic Correlation Shifts and Weighted Benchmarking in Extreme Value Analysis",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper presents an innovative approach to Extreme Value Analysis (EVA) by introducing the Extreme Value Dynamic Benchmarking Method (EVDBM). EVDBM integrates extreme value theory to detect extreme events and is coupled with the novel Dynamic Identification of Significant Correlation (DISC)-Thresholding algorithm, which enhances the analysis of key variables under extreme conditions. By integrating return values predicted through EVA into the benchmarking scores, we are able to transform these scores to reflect anticipated conditions more accurately. This provides a more precise picture of how each case is projected to unfold under extreme conditions. As a result, the adjusted scores offer a forward-looking perspective, highlighting potential vulnerabilities and resilience factors for each case in a way that static historical data alone cannot capture. By incorporating both historical and probabilistic elements, the EVDBM algorithm provides a comprehensive benchmarking framework that is adaptable to a range of scenarios and contexts. The methodology is applied to real PV data, revealing critical low - production scenarios and significant correlations between variables, which aid in risk management, infrastructure design, and long-term planning, while also allowing for the comparison of different production plants. The flexibility of EVDBM suggests its potential for broader applications in other sectors where decision-making sensitivity is crucial, offering valuable insights to improve outcomes.",
        "subjects": [
            "stat.AP",
            "cs.AI"
        ],
        "comment": "33 pages, 8 figures"
    },
    {
        "paper id": "2411.13611",
        "abstract url": "https://arxiv.org/abs/2411.13611",
        "title": "DSTC: Direct Preference Learning with Only Self-Generated Tests and Code to Improve Code LMs",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Direct preference learning offers a promising and computation-efficient beyond supervised fine-tuning (SFT) for improving code generation in coding large language models (LMs). However, the scarcity of reliable preference data is a bottleneck for the performance of direct preference learning to improve the coding accuracy of code LMs. In this paper, we introduce \\underline{\\textbf{D}}irect Preference Learning with Only \\underline{\\textbf{S}}elf-Generated \\underline{\\textbf{T}}ests and \\underline{\\textbf{C}}ode (DSTC), a framework that leverages only self-generated code snippets and tests to construct reliable preference pairs such that direct preference learning can improve LM coding accuracy without external annotations. DSTC combines a minimax selection process and test-code concatenation to improve preference pair quality, reducing the influence of incorrect self-generated tests and enhancing model performance without the need for costly reward models. When applied with direct preference learning methods such as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization (KTO), DSTC yields stable improvements in coding accuracy (pass@1 score) across diverse coding benchmarks, including HumanEval, MBPP, and BigCodeBench, demonstrating both its effectiveness and scalability for models of various sizes. This approach autonomously enhances code generation accuracy across LLMs of varying sizes, reducing reliance on expensive annotated coding datasets.",
        "subjects": [
            "cs.SE",
            "cs.AI"
        ],
        "comment": "We temporarily modified the author list because of the pending verification from the company"
    },
    {
        "paper id": "2411.14478",
        "abstract url": "https://arxiv.org/abs/2411.14478",
        "title": "Why you don't overfit, and don't need Bayes if you only train for one epoch",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Here, we show that in the data-rich setting where you only train on each datapoint once (or equivalently, you only train for one epoch), standard \"maximum likelihood\" training optimizes the true data generating process (DGP) loss, which is equivalent to the test loss. Further, we show that the Bayesian model average optimizes the same objective, albeit while taking the expectation over uncertainty induced by finite data. As standard maximum likelihood training in the single-epoch setting optimizes the same objective as Bayesian inference, we argue that we do not expect Bayesian inference to offer any advantages in terms of overfitting or calibration in these settings. This explains the diminishing importance of Bayes in areas such as LLMs, which are often trained with one (or very few) epochs.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.15189",
        "abstract url": "https://arxiv.org/abs/2411.15189",
        "title": "Order Is All You Need for Categorical Data Clustering",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Categorical data composed of nominal valued attributes are ubiquitous in knowledge discovery and data mining tasks. Due to the lack of well-defined metric space, categorical data distributions are difficult to intuitively understand. Clustering is a popular technique suitable for data analysis. However, the success of clustering often relies on reasonable distance metrics, which happens to be what categorical data naturally lack. Therefore, the cluster analysis of categorical data is considered a critical but challenging problem. This paper introduces the new finding that the order relation among attribute values is the decisive factor in clustering accuracy, and is also the key to understanding the categorical data clusters. To automatically obtain the orders, we propose a new learning paradigm that allows joint learning of clusters and the orders. It turns out that clustering with order learning achieves superior clustering accuracy, and the learned orders provide intuition for understanding the cluster distribution of categorical data. Extensive experiments with statistical evidence and case studies have verified the effectiveness of the new ``order is all you need'' insight and the proposed method.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.15190",
        "abstract url": "https://arxiv.org/abs/2411.15190",
        "title": "Transforming Triple-Entry Accounting with Machine Learning: A Path to Enhanced Transparency Through Analytics",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Triple Entry (TE) is an accounting method that utilizes three accounts or 'entries' to record each transaction, rather than the conventional double-entry bookkeeping system. Existing studies have found that TE accounting, with its additional layer of verification and disclosure of inter-organizational relationships, could help improve transparency in complex financial and supply chain transactions such as blockchain. Machine learning (ML) presents a promising avenue to augment the transparency advantages of TE accounting. By automating some of the data collection and analysis needed for TE bookkeeping, ML techniques have the potential to make this more transparent accounting method scalable for large organizations with complex international supply chains, further enhancing the visibility and trustworthiness of financial reporting. By leveraging ML algorithms, anomalies within distributed ledger data can be swiftly identified, flagging potential instances of fraud or errors. Furthermore, by delving into transaction relationships over time, ML can untangle intricate webs of transactions, shedding light on obscured dealings and adding an investigative dimension. This paper aims to demonstrate the interaction between TE and ML and how they can leverage transparency levels.",
        "subjects": [
            "cs.CR",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.15191",
        "abstract url": "https://arxiv.org/abs/2411.15191",
        "title": "Tailoring the Hyperparameters of a Wide-Kernel Convolutional Neural Network to Fit Different Bearing Fault Vibration Datasets",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "State-of-the-art algorithms are reported to be almost perfect at distinguishing the vibrations arising from healthy and damaged machine bearings, according to benchmark datasets at least. However, what about their application to new data? In this paper, we are able to confirm that neural networks for bearing fault detection can be crippled by incorrect hyperparameterisation, and also that the correct hyperparameter settings can actually change when transitioning to new data. The paper weaves together multiple methods to explain the behaviour of the hyperparameters of a wide-kernel convolutional neural network and how to set them. Since guidance already exists for generic hyperparameters like minibatch size, we focus on how to set architecture-specific hyperparameters such as the width of the convolutional kernels, a topic which might otherwise be obscure. We reflect different data properties by fusing information from seven different benchmark datasets, and our results show that the kernel size in the first layer in particular is sensitive to changes in the data. Looking deeper, we use manipulated copies of one dataset in an attempt to spot why the kernel size sometimes needs to change. The relevance of sampling rate is studied by using different levels of resampling, and spectral content is studied by increasingly filtering out high frequencies. At the end of our paper we conclude by stating clear guidance on how to set the hyperparameters of our neural network architecture.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "eess.SP"
        ],
        "comment": "71 pages, 14 figures, 7 tables"
    },
    {
        "paper id": "2411.15197",
        "abstract url": "https://arxiv.org/abs/2411.15197",
        "title": "K-means Derived Unsupervised Feature Selection using Improved ADMM",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Feature selection is important for high-dimensional data analysis and is non-trivial in unsupervised learning problems such as dimensionality reduction and clustering. The goal of unsupervised feature selection is finding a subset of features such that the data points from different clusters are well separated. This paper presents a novel method called K-means Derived Unsupervised Feature Selection (K-means UFS). Unlike most existing spectral analysis based unsupervised feature selection methods, we select features using the objective of K-means. We develop an alternating direction method of multipliers (ADMM) to solve the NP-hard optimization problem of our K-means UFS model. Extensive experiments on real datasets show that our K-means UFS is more effective than the baselines in selecting features for clustering.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.15202",
        "abstract url": "https://arxiv.org/abs/2411.15202",
        "title": "A Comparison of Machine Learning Algorithms for Predicting Sea Surface Temperature in the Great Barrier Reef Region",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Predicting Sea Surface Temperature (SST) in the Great Barrier Reef (GBR) region is crucial for the effective management of its fragile ecosystems. This study provides a rigorous comparative analysis of several machine learning techniques to identify the most effective method for SST prediction in this area. We evaluate the performance of ridge regression, Least Absolute Shrinkage and Selection Operator (LASSO), Random Forest, and Extreme Gradient Boosting (XGBoost) algorithms. Our results reveal that while LASSO and ridge regression perform well, Random Forest and XGBoost significantly outperform them in terms of predictive accuracy, as evidenced by lower Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Prediction Error (RMSPE). Additionally, XGBoost demonstrated superior performance in minimizing Kullback- Leibler Divergence (KLD), indicating a closer alignment of predicted probability distributions with actual observations. These findings highlight the efficacy of using ensemble methods, particularly XGBoost, for predicting sea surface temperatures, making them valuable tools for climatological and environmental modeling.",
        "subjects": [
            "physics.ao-ph",
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12250",
        "abstract url": "https://arxiv.org/abs/2411.12250",
        "title": "ADV2E: Bridging the Gap Between Analogue Circuit and Discrete Frames in the Video-to-Events Simulator",
        "rating": "0",
        "keywords": [
            [
                "Event cameras"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Event cameras operate fundamentally differently from traditional Active Pixel Sensor (APS) cameras, offering significant advantages. Recent research has developed simulators to convert video frames into events, addressing the shortage of real event datasets. Current simulators primarily focus on the logical behavior of event cameras. However, the fundamental analogue properties of pixel circuits are seldom considered in simulator design. The gap between analogue pixel circuit and discrete video frames causes the degeneration of synthetic events, particularly in high-contrast scenes. In this paper, we propose a novel method of generating reliable event data based on a detailed analysis of the pixel circuitry in event cameras. We incorporate the analogue properties of event camera pixel circuits into the simulator design: (1) analogue filtering of signals from light intensity to events, and (2) a cutoff frequency that is independent of video frame rate. Experimental results on two relevant tasks, including semantic segmentation and image reconstruction, validate the reliability of simulated event data, even in high-contrast scenes. This demonstrates that deep neural networks exhibit strong generalization from simulated to real event data, confirming that the synthetic events generated by the proposed method are both realistic and well-suited for effective training.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "10 pages, 6 figures"
    },
    {
        "paper id": "2411.12268",
        "abstract url": "https://arxiv.org/abs/2411.12268",
        "title": "A Neural Denoising Vocoder for Clean Waveform Generation from Noisy Mel-Spectrogram based on Amplitude and Phase Predictions",
        "rating": "0",
        "keywords": [
            [
                "speech enhancement"
            ],
            [
                "eess.AS"
            ]
        ],
        "abstract": "This paper proposes a novel neural denoising vocoder that can generate clean speech waveforms from noisy mel-spectrograms. The proposed neural denoising vocoder consists of two components, i.e., a spectrum predictor and a enhancement module. The spectrum predictor first predicts the noisy amplitude and phase spectra from the input noisy mel-spectrogram, and subsequently the enhancement module recovers the clean amplitude and phase spectrum from noisy ones. Finally, clean speech waveforms are reconstructed through inverse short-time Fourier transform (iSTFT). All operations are performed at the frame-level spectral domain, with the APNet vocoder and MP-SENet speech enhancement model used as the backbones for the two components, respectively. Experimental results demonstrate that our proposed neural denoising vocoder achieves state-of-the-art performance compared to existing neural vocoders on the VoiceBank+DEMAND dataset. Additionally, despite the lack of phase information and partial amplitude information in the input mel-spectrogram, the proposed neural denoising vocoder still achieves comparable performance with the serveral advanced speech enhancement methods.",
        "subjects": [
            "eess.AS",
            "eess.SP"
        ],
        "comment": "Accepted by NCMMSC2024"
    },
    {
        "paper id": "2411.12279",
        "abstract url": "https://arxiv.org/abs/2411.12279",
        "title": "HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper proposes a two-phase text-to-floorplan generation method, which guides a Large Language Model (LLM) to generate an initial layout (Layout-LLM) and refines them into the final floorplans through conditional diffusion model. We incorporate a Chain-of-Thought approach to prompt the LLM based on user text specifications, enabling a more user-friendly and intuitive house layout design. This method allows users to describe their needs in natural language, enhancing accessibility and providing clearer geometric constraints. The final floorplans generated by Layout-LLM through conditional diffusion refinement are more accurate and better meet user requirements. Experimental results demonstrate that our approach achieves state-of-the-art performance across all metrics, validating its effectiveness in practical home design applications. We plan to release our code for public use.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12306",
        "abstract url": "https://arxiv.org/abs/2411.12306",
        "title": "Diffusion Product Quantization",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this work, we explore the quantization of diffusion models in extreme compression regimes to reduce model size while maintaining performance. We begin by investigating classical vector quantization but find that diffusion models are particularly susceptible to quantization error, with the codebook size limiting generation quality. To address this, we introduce product quantization, which offers improved reconstruction precision and larger capacity -- crucial for preserving the generative capabilities of diffusion models. Furthermore, we propose a method to compress the codebook by evaluating the importance of each vector and removing redundancy, ensuring the model size remaining within the desired range. We also introduce an end-to-end calibration approach that adjusts assignments during the forward pass and optimizes the codebook using the DDPM loss. By compressing the model to as low as 1 bit (resulting in over 24 times reduction in model size), we achieve a balance between compression and quality. We apply our compression method to the DiT model on ImageNet and consistently outperform other quantization approaches, demonstrating competitive generative performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12309",
        "abstract url": "https://arxiv.org/abs/2411.12309",
        "title": "DGTR: Distributed Gaussian Turbo-Reconstruction for Sparse-View Vast Scenes",
        "rating": "0",
        "keywords": [
            [
                "3d",
                "depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Novel-view synthesis (NVS) approaches play a critical role in vast scene reconstruction. However, these methods rely heavily on dense image inputs and prolonged training times, making them unsuitable where computational resources are limited. Additionally, few-shot methods often struggle with poor reconstruction quality in vast environments. This paper presents DGTR, a novel distributed framework for efficient Gaussian reconstruction for sparse-view vast scenes. Our approach divides the scene into regions, processed independently by drones with sparse image inputs. Using a feed-forward Gaussian model, we predict high-quality Gaussian primitives, followed by a global alignment algorithm to ensure geometric consistency. Synthetic views and depth priors are incorporated to further enhance training, while a distillation-based model aggregation mechanism enables efficient reconstruction. Our method achieves high-quality large-scale scene reconstruction and novel-view synthesis in significantly reduced training times, outperforming existing approaches in both speed and scalability. We demonstrate the effectiveness of our framework on vast aerial scenes, achieving high-quality results within minutes. Code will released on our [https://3d-aigc.github.io/DGTR].",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Code will released on our [https://3d-aigc.github.io/DGTR]"
    },
    {
        "paper id": "2411.12313",
        "abstract url": "https://arxiv.org/abs/2411.12313",
        "title": "C$^{2}$INet: Realizing Incremental Trajectory Prediction with Prior-Aware Continual Causal Intervention",
        "rating": "0",
        "keywords": [
            [
                "autonomous driving",
                "Trajectory"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Trajectory prediction for multi-agents in complex scenarios is crucial for applications like autonomous driving. However, existing methods often overlook environmental biases, which leads to poor generalization. Additionally, hardware constraints limit the use of large-scale data across environments, and continual learning settings exacerbate the challenge of catastrophic forgetting. To address these issues, we propose the Continual Causal Intervention (C$^{2}$INet) method for generalizable multi-agent trajectory prediction within a continual learning framework. Using variational inference, we align environment-related prior with posterior estimator of confounding factors in the latent space, thereby intervening in causal correlations that affect trajectory representation. Furthermore, we store optimal variational priors across various scenarios using a memory queue, ensuring continuous debiasing during incremental task training. The proposed C$^{2}$INet enhances adaptability to diverse tasks while preserving previous task information to prevent catastrophic forgetting. It also incorporates pruning strategies to mitigate overfitting. Comparative evaluations on three real and synthetic complex datasets against state-of-the-art methods demonstrate that our proposed method consistently achieves reliable prediction performance, effectively mitigating confounding factors unique to different scenarios. This highlights the practical value of our method for real-world applications.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12319",
        "abstract url": "https://arxiv.org/abs/2411.12319",
        "title": "CLIP Unreasonable Potential in Single-Shot Face Recognition",
        "rating": "0",
        "keywords": [
            [
                "vision language"
            ],
            [
                "facial"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Face recognition is a core task in computer vision designed to identify and authenticate individuals by analyzing facial patterns and features. This field intersects with artificial intelligence image processing and machine learning with applications in security authentication and personalization. Traditional approaches in facial recognition focus on capturing facial features like the eyes, nose and mouth and matching these against a database to verify identities. However challenges such as high false positive rates have persisted often due to the similarity among individuals facial features. Recently Contrastive Language Image Pretraining (CLIP) a model developed by OpenAI has shown promising advancements by linking natural language processing with vision tasks allowing it to generalize across modalities. Using CLIP's vision language correspondence and single-shot finetuning the model can achieve lower false positive rates upon deployment without the need of mass facial features extraction. This integration demonstrating CLIP's potential to address persistent issues in face recognition model performance without complicating our training paradigm.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12338",
        "abstract url": "https://arxiv.org/abs/2411.12338",
        "title": "Target Height Estimation Using a Single Acoustic Camera for Compensation in 2D Seabed Mosaicking",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This letter proposes a novel approach for compensating target height data in 2D seabed mosaicking for low-visibility underwater perception. Acoustic cameras are effective sensors for sensing the marine environments due to their high-resolution imaging capabilities and robustness to darkness and turbidity. However, the loss of elevation angle during the imaging process results in a lack of target height information in the original acoustic camera images, leading to a simplistic 2D representation of the seabed mosaicking. In perceiving cluttered and unexplored marine environments, target height data is crucial for avoiding collisions with marine robots. This study proposes a novel approach for estimating seabed target height using a single acoustic camera and integrates height data into 2D seabed mosaicking to compensate for the missing 3D dimension of seabed targets. Unlike classic methods that model the loss of elevation angle to achieve seabed 3D reconstruction, this study focuses on utilizing available acoustic cast shadow clues and simple sensor motion to quickly estimate target height. The feasibility of our proposal is verified through a water tank experiment and a simulation experiment.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": "8 pages,conference"
    },
    {
        "paper id": "2411.12440",
        "abstract url": "https://arxiv.org/abs/2411.12440",
        "title": "Beyond Gaussians: Fast and High-Fidelity 3D Splatting with Linear Kernels",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have substantially improved novel view synthesis, enabling high-quality reconstruction and real-time rendering. However, blurring artifacts, such as floating primitives and over-reconstruction, remain challenging. Current methods address these issues by refining scene structure, enhancing geometric representations, addressing blur in training images, improving rendering consistency, and optimizing density control, yet the role of kernel design remains underexplored. We identify the soft boundaries of Gaussian ellipsoids as one of the causes of these artifacts, limiting detail capture in high-frequency regions. To bridge this gap, we introduce 3D Linear Splatting (3DLS), which replaces Gaussian kernels with linear kernels to achieve sharper and more precise results, particularly in high-frequency regions. Through evaluations on three datasets, 3DLS demonstrates state-of-the-art fidelity and accuracy, along with a 30% FPS improvement over baseline 3DGS. The implementation will be made publicly available upon acceptance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12449",
        "abstract url": "https://arxiv.org/abs/2411.12449",
        "title": "Neon: News Entity-Interaction Extraction for Enhanced Question Answering",
        "rating": "0",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Capturing fresh information in near real-time and using it to augment existing large language models (LLMs) is essential to generate up-to-date, grounded, and reliable output. This problem becomes particularly challenging when LLMs are used for informational tasks in rapidly evolving fields, such as Web search related to recent or unfolding events involving entities, where generating temporally relevant responses requires access to up-to-the-hour news sources. However, the information modeled by the parametric memory of LLMs is often outdated, and Web results from prototypical retrieval systems may fail to capture the latest relevant information and struggle to handle conflicting reports in evolving news. To address this challenge, we present the NEON framework, designed to extract emerging entity interactions -- such as events or activities -- as described in news articles. NEON constructs an entity-centric timestamped knowledge graph that captures such interactions, thereby facilitating enhanced QA capabilities related to news events. Our framework innovates by integrating open Information Extraction (openIE) style tuples into LLMs to enable in-context retrieval-augmented generation. This integration demonstrates substantial improvements in QA performance when tackling temporal, entity-centric search queries. Through NEON, LLMs can deliver more accurate, reliable, and up-to-date responses.",
        "subjects": [
            "cs.CL",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12452",
        "abstract url": "https://arxiv.org/abs/2411.12452",
        "title": "GaussianPretrain: A Simple Unified 3D Gaussian Representation for Visual Pre-training in Autonomous Driving",
        "rating": "0",
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "3D",
                "NeRF"
            ],
            [
                "Autonomous Driving",
                "LiDAR"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Self-supervised learning has made substantial strides in image processing, while visual pre-training for autonomous driving is still in its infancy. Existing methods often focus on learning geometric scene information while neglecting texture or treating both aspects separately, hindering comprehensive scene understanding. In this context, we are excited to introduce GaussianPretrain, a novel pre-training paradigm that achieves a holistic understanding of the scene by uniformly integrating geometric and texture representations. Conceptualizing 3D Gaussian anchors as volumetric LiDAR points, our method learns a deepened understanding of scenes to enhance pre-training performance with detailed spatial structure and texture, achieving that 40.6% faster than NeRF-based method UniPAD with 70% GPU memory only. We demonstrate the effectiveness of GaussianPretrain across multiple 3D perception tasks, showing significant performance improvements, such as a 7.05% increase in NDS for 3D object detection, boosts mAP by 1.9% in HD map construction and 0.8% improvement on Occupancy prediction. These significant gains highlight GaussianPretrain's theoretical innovation and strong practical potential, promoting visual pre-training development for autonomous driving. Source code will be available at https://github.com/Public-BOTs/GaussianPretrain",
        "subjects": [
            "cs.CV"
        ],
        "comment": "10 pages, 5 figures"
    },
    {
        "paper id": "2411.12471",
        "abstract url": "https://arxiv.org/abs/2411.12471",
        "title": "SCIGS: 3D Gaussians Splatting from a Snapshot Compressive Image",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "NeRF"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Snapshot Compressive Imaging (SCI) offers a possibility for capturing information in high-speed dynamic scenes, requiring efficient reconstruction method to recover scene information. Despite promising results, current deep learning-based and NeRF-based reconstruction methods face challenges: 1) deep learning-based reconstruction methods struggle to maintain 3D structural consistency within scenes, and 2) NeRF-based reconstruction methods still face limitations in handling dynamic scenes. To address these challenges, we propose SCIGS, a variant of 3DGS, and develop a primitive-level transformation network that utilizes camera pose stamps and Gaussian primitive coordinates as embedding vectors. This approach resolves the necessity of camera pose in vanilla 3DGS and enhances multi-view 3D structural consistency in dynamic scenes by utilizing transformed primitives. Additionally, a high-frequency filter is introduced to eliminate the artifacts generated during the transformation. The proposed SCIGS is the first to reconstruct a 3D explicit scene from a single compressed image, extending its application to dynamic 3D scenes. Experiments on both static and dynamic scenes demonstrate that SCIGS not only enhances SCI decoding but also outperforms current state-of-the-art methods in reconstructing dynamic 3D scenes from a single compressed image. The code will be made available upon publication.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12473",
        "abstract url": "https://arxiv.org/abs/2411.12473",
        "title": "NMT-Obfuscator Attack: Ignore a sentence in translation with only one word",
        "rating": "0",
        "keywords": [
            [
                "Attack"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Neural Machine Translation systems are used in diverse applications due to their impressive performance. However, recent studies have shown that these systems are vulnerable to carefully crafted small perturbations to their inputs, known as adversarial attacks. In this paper, we propose a new type of adversarial attack against NMT models. In this attack, we find a word to be added between two sentences such that the second sentence is ignored and not translated by the NMT model. The word added between the two sentences is such that the whole adversarial text is natural in the source language. This type of attack can be harmful in practical scenarios since the attacker can hide malicious information in the automatic translation made by the target NMT model. Our experiments show that different NMT models and translation tasks are vulnerable to this type of attack. Our attack can successfully force the NMT models to ignore the second part of the input in the translation for more than 50% of all cases while being able to maintain low perplexity for the whole input.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12493",
        "abstract url": "https://arxiv.org/abs/2411.12493",
        "title": "Bias-Free Sentiment Analysis through Semantic Blinding and Graph Neural Networks",
        "rating": "0",
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces the Semantic Propagation Graph Neural Network (SProp GNN), a machine learning sentiment analysis (SA) architecture that relies exclusively on syntactic structures and word-level emotional cues to predict emotions in text. By semantically blinding the model to information about specific words, it is robust to biases such as political or gender bias that have been plaguing previous machine learning-based SA systems. The SProp GNN shows performance superior to lexicon-based alternatives such as VADER and EmoAtlas on two different prediction tasks, and across two languages. Additionally, it approaches the accuracy of transformer-based models while significantly reducing bias in emotion prediction tasks. By offering improved explainability and reducing bias, the SProp GNN bridges the methodological gap between interpretable lexicon approaches and powerful, yet often opaque, deep learning models, offering a robust tool for fair and effective emotion analysis in understanding human behavior through text.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12523",
        "abstract url": "https://arxiv.org/abs/2411.12523",
        "title": "Data Pruning in Generative Diffusion Models",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Data pruning is the problem of identifying a core subset that is most beneficial to training and discarding the remainder. While pruning strategies are well studied for discriminative models like those used in classification, little research has gone into their application to generative models. Generative models aim to estimate the underlying distribution of the data, so presumably they should benefit from larger datasets. In this work we aim to shed light on the accuracy of this statement, specifically answer the question of whether data pruning for generative diffusion models could have a positive impact. Contrary to intuition, we show that eliminating redundant or noisy data in large datasets is beneficial particularly when done strategically. We experiment with several pruning methods including recent-state-of-art methods, and evaluate over CelebA-HQ and ImageNet datasets. We demonstrate that a simple clustering method outperforms other sophisticated and computationally demanding methods. We further exhibit how we can leverage clustering to balance skewed datasets in an unsupervised manner to allow fair sampling for underrepresented populations in the data distribution, which is a crucial problem in generative models.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12603",
        "abstract url": "https://arxiv.org/abs/2411.12603",
        "title": "STREAM: A Universal State-Space Model for Sparse Geometric Data",
        "rating": "0",
        "keywords": [
            [
                "point cloud"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Handling sparse and unstructured geometric data, such as point clouds or event-based vision, is a pressing challenge in the field of machine vision. Recently, sequence models such as Transformers and state-space models entered the domain of geometric data. These methods require specialized preprocessing to create a sequential view of a set of points. Furthermore, prior works involving sequence models iterate geometric data with either uniform or learned step sizes, implicitly relying on the model to infer the underlying geometric structure. In this work, we propose to encode geometric structure explicitly into the parameterization of a state-space model. State-space models are based on linear dynamics governed by a one-dimensional variable such as time or a spatial coordinate. We exploit this dynamic variable to inject relative differences of coordinates into the step size of the state-space model. The resulting geometric operation computes interactions between all pairs of N points in O(N) steps. Our model deploys the Mamba selective state-space model with a modified CUDA kernel to efficiently map sparse geometric data to modern hardware. The resulting sequence model, which we call STREAM, achieves competitive results on a range of benchmarks from point-cloud classification to event-based vision and audio classification. STREAM demonstrates a powerful inductive bias for sparse geometric data by improving the PointMamba baseline when trained from scratch on the ModelNet40 and ScanObjectNN point cloud analysis datasets. It further achieves, for the first time, 100% test accuracy on all 11 classes of the DVS128 Gestures dataset.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12620",
        "abstract url": "https://arxiv.org/abs/2411.12620",
        "title": "Maps from Motion (MfM): Generating 2D Semantic Maps from Sparse Multi-view Images",
        "rating": "0",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "World-wide detailed 2D maps require enormous collective efforts. OpenStreetMap is the result of 11 million registered users manually annotating the GPS location of over 1.75 billion entries, including distinctive landmarks and common urban objects. At the same time, manual annotations can include errors and are slow to update, limiting the map's accuracy. Maps from Motion (MfM) is a step forward to automatize such time-consuming map making procedure by computing 2D maps of semantic objects directly from a collection of uncalibrated multi-view images. From each image, we extract a set of object detections, and estimate their spatial arrangement in a top-down local map centered in the reference frame of the camera that captured the image. Aligning these local maps is not a trivial problem, since they provide incomplete, noisy fragments of the scene, and matching detections across them is unreliable because of the presence of repeated pattern and the limited appearance variability of urban objects. We address this with a novel graph-based framework, that encodes the spatial and semantic distribution of the objects detected in each image, and learns how to combine them to predict the objects' poses in a global reference system, while taking into account all possible detection matches and preserving the topology observed in each image. Despite the complexity of the problem, our best model achieves global 2D registration with an average accuracy within 4 meters (i.e., below GPS accuracy) even on sparse sequences with strong viewpoint change, on which COLMAP has an 80% failure rate. We provide extensive evaluation on synthetic and real-world data, showing how the method obtains a solution even in scenarios where standard optimization techniques fail.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12629",
        "abstract url": "https://arxiv.org/abs/2411.12629",
        "title": "Estimating Dark Matter Halo Masses in Simulated Galaxy Clusters with Graph Neural Networks",
        "rating": "0",
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "cs.AI"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Galaxies grow and evolve in dark matter halos. Because dark matter is not visible, galaxies' halo masses ($\\rm{M}_{\\rm{halo}}$) must be inferred indirectly. We present a graph neural network (GNN) model for predicting $\\rm{M}_{\\rm{halo}}$ from stellar mass ($\\rm{M}_{*}$) in simulated galaxy clusters using data from the IllustrisTNG simulation suite. Unlike traditional machine learning models like random forests, our GNN captures the information-rich substructure of galaxy clusters by using spatial and kinematic relationships between galaxy neighbour. A GNN model trained on the TNG-Cluster dataset and independently tested on the TNG300 simulation achieves superior predictive performance compared to other baseline models we tested. Future work will extend this approach to different simulations and real observational datasets to further validate the GNN model's ability to generalise.",
        "subjects": [
            "astro-ph.GA",
            "astro-ph.CO",
            "astro-ph.IM",
            "cs.AI"
        ],
        "comment": "9 pages, 4 figures, accepted at the NeurIPS ML4PS 2024 workshop"
    },
    {
        "paper id": "2411.12663",
        "abstract url": "https://arxiv.org/abs/2411.12663",
        "title": "PoM: Efficient Image and Video Generation with the Polynomial Mixer",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion models based on Multi-Head Attention (MHA) have become ubiquitous to generate high quality images and videos. However, encoding an image or a video as a sequence of patches results in costly attention patterns, as the requirements both in terms of memory and compute grow quadratically. To alleviate this problem, we propose a drop-in replacement for MHA called the Polynomial Mixer (PoM) that has the benefit of encoding the entire sequence into an explicit state. PoM has a linear complexity with respect to the number of tokens. This explicit state also allows us to generate frames in a sequential fashion, minimizing memory and compute requirement, while still being able to train in parallel. We show the Polynomial Mixer is a universal sequence-to-sequence approximator, just like regular MHA. We adapt several Diffusion Transformers (DiT) for generating images and videos with PoM replacing MHA, and we obtain high quality samples while using less computational resources. The code is available at https://github.com/davidpicard/HoMM.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12671",
        "abstract url": "https://arxiv.org/abs/2411.12671",
        "title": "Neurosymbolic Graph Enrichment for Grounded World Models",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The development of artificial intelligence systems capable of understanding and reasoning about complex real-world scenarios is a significant challenge. In this work we present a novel approach to enhance and exploit LLM reactive capability to address complex problems and interpret deeply contextual real-world meaning. We introduce a method and a tool for creating a multimodal, knowledge-augmented formal representation of meaning that combines the strengths of large language models with structured semantic representations. Our method begins with an image input, utilizing state-of-the-art large language models to generate a natural language description. This description is then transformed into an Abstract Meaning Representation (AMR) graph, which is formalized and enriched with logical design patterns, and layered semantics derived from linguistic and factual knowledge bases. The resulting graph is then fed back into the LLM to be extended with implicit knowledge activated by complex heuristic learning, including semantic implicatures, moral values, embodied cognition, and metaphorical representations. By bridging the gap between unstructured language models and formal semantic structures, our method opens new avenues for tackling intricate problems in natural language understanding and reasoning.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.ET"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12703",
        "abstract url": "https://arxiv.org/abs/2411.12703",
        "title": "Strengthening Fake News Detection: Leveraging SVM and Sophisticated Text Vectorization Techniques. Defying BERT?",
        "rating": "0",
        "keywords": [
            [
                "SVM"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The rapid spread of misinformation, particularly through online platforms, underscores the urgent need for reliable detection systems. This study explores the utilization of machine learning and natural language processing, specifically Support Vector Machines (SVM) and BERT, to detect news that are fake. We employ three distinct text vectorization methods for SVM: Term Frequency Inverse Document Frequency (TF-IDF), Word2Vec, and Bag of Words (BoW) evaluating their effectiveness in distinguishing between genuine and fake news. Additionally, we compare these methods against the transformer large language model, BERT. Our comprehensive approach includes detailed preprocessing steps, rigorous model implementation, and thorough evaluation to determine the most effective techniques. The results demonstrate that while BERT achieves superior accuracy with 99.98% and an F1-score of 0.9998, the SVM model with a linear kernel and BoW vectorization also performs exceptionally well, achieving 99.81% accuracy and an F1-score of 0.9980. These findings highlight that, despite BERT's superior performance, SVM models with BoW and TF-IDF vectorization methods come remarkably close, offering highly competitive performance with the advantage of lower computational requirements.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "6 pages, 3 tables and 6 Figures. Submitted to a conference"
    },
    {
        "paper id": "2411.12713",
        "abstract url": "https://arxiv.org/abs/2411.12713",
        "title": "CATCH: Complementary Adaptive Token-level Contrastive Decoding to Mitigate Hallucinations in LVLMs",
        "rating": "0",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Large Vision-Language Model (LVLM) systems have demonstrated impressive vision-language reasoning capabilities but suffer from pervasive and severe hallucination issues, posing significant risks in critical domains such as healthcare and autonomous systems. Despite previous efforts to mitigate hallucinations, a persistent issue remains: visual defect from vision-language misalignment, creating a bottleneck in visual processing capacity. To address this challenge, we develop Complementary Adaptive Token-level Contrastive Decoding to Mitigate Hallucinations in LVLMs (CATCH), based on the Information Bottleneck theory. CATCH introduces Complementary Visual Decoupling (CVD) for visual information separation, Non-Visual Screening (NVS) for hallucination detection, and Adaptive Token-level Contrastive Decoding (ATCD) for hallucination mitigation. CATCH addresses issues related to visual defects that cause diminished fine-grained feature perception and cumulative hallucinations in open-ended scenarios. It is applicable to various visual question-answering tasks without requiring any specific data or prior knowledge, and generalizes robustly to new tasks without additional training, opening new possibilities for advancing LVLM in various challenging applications.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12775",
        "abstract url": "https://arxiv.org/abs/2411.12775",
        "title": "Revisiting Fake News Detection: Towards Temporality-aware Evaluation by Leveraging Engagement Earliness",
        "rating": "0",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.AI",
                "cs.SI",
                "cs.CL"
            ]
        ],
        "abstract": "Social graph-based fake news detection aims to identify news articles containing false information by utilizing social contexts, e.g., user information, tweets and comments. However, conventional methods are evaluated under less realistic scenarios, where the model has access to future knowledge on article-related and context-related data during training. In this work, we newly formalize a more realistic evaluation scheme that mimics real-world scenarios, where the data is temporality-aware and the detection model can only be trained on data collected up to a certain point in time. We show that the discriminative capabilities of conventional methods decrease sharply under this new setting, and further propose DAWN, a method more applicable to such scenarios. Our empirical findings indicate that later engagements (e.g., consuming or reposting news) contribute more to noisy edges that link real news-fake news pairs in the social graph. Motivated by this, we utilize feature representations of engagement earliness to guide an edge weight estimator to suppress the weights of such noisy edges, thereby enhancing the detection performance of DAWN. Through extensive experiments, we demonstrate that DAWN outperforms existing fake news detection methods under real-world environments. The source code is available at https://github.com/LeeJunmo/DAWN.",
        "subjects": [
            "cs.SI",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "WSDM 2025"
    },
    {
        "paper id": "2411.12788",
        "abstract url": "https://arxiv.org/abs/2411.12788",
        "title": "Mini-Splatting2: Building 360 Scenes within Minutes via Aggressive Gaussian Densification",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this study, we explore the essential challenge of fast scene optimization for Gaussian Splatting. Through a thorough analysis of the geometry modeling process, we reveal that dense point clouds can be effectively reconstructed early in optimization through Gaussian representations. This insight leads to our approach of aggressive Gaussian densification, which provides a more efficient alternative to conventional progressive densification methods. By significantly increasing the number of critical Gaussians, we enhance the model capacity to capture dense scene geometry at the early stage of optimization. This strategy is seamlessly integrated into the Mini-Splatting densification and simplification framework, enabling rapid convergence without compromising quality. Additionally, we introduce visibility culling within Gaussian Splatting, leveraging per-view Gaussian importance as precomputed visibility to accelerate the optimization process. Our Mini-Splatting2 achieves a balanced trade-off among optimization time, the number of Gaussians, and rendering quality, establishing a strong baseline for future Gaussian-Splatting-based works. Our work sets the stage for more efficient, high-quality 3D scene modeling in real-world applications, and the code will be made available no matter acceptance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12790",
        "abstract url": "https://arxiv.org/abs/2411.12790",
        "title": "Visual-Oriented Fine-Grained Knowledge Editing for MultiModal Large Language Models",
        "rating": "0",
        "keywords": [
            [
                "Knowledge Editing"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Knowledge editing aims to efficiently and cost-effectively correct inaccuracies and update outdated information. Recently, there has been growing interest in extending knowledge editing from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs), which integrate both textual and visual information, introducing additional editing complexities. Existing multimodal knowledge editing works primarily focus on text-oriented, coarse-grained scenarios, failing to address the unique challenges posed by multimodal contexts. In this paper, we propose a visual-oriented, fine-grained multimodal knowledge editing task that targets precise editing in images with multiple interacting entities. We introduce the Fine-Grained Visual Knowledge Editing (FGVEdit) benchmark to evaluate this task. Moreover, we propose a Multimodal Scope Classifier-based Knowledge Editor (MSCKE) framework. MSCKE leverages a multimodal scope classifier that integrates both visual and textual information to accurately identify and update knowledge related to specific entities within images. This approach ensures precise editing while preserving irrelevant information, overcoming the limitations of traditional text-only editing methods. Extensive experiments on the FGVEdit benchmark demonstrate that MSCKE outperforms existing methods, showcasing its effectiveness in solving the complex challenges of multimodal knowledge editing.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12811",
        "abstract url": "https://arxiv.org/abs/2411.12811",
        "title": "Stylecodes: Encoding Stylistic Information For Image Generation",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Diffusion models excel in image generation, but controlling them remains a challenge. We focus on the problem of style-conditioned image generation. Although example images work, they are cumbersome: srefs (style-reference codes) from MidJourney solve this issue by expressing a specific image style in a short numeric code. These have seen widespread adoption throughout social media due to both their ease of sharing and the fact they allow using an image for style control, without having to post the source images themselves. However, users are not able to generate srefs from their own images, nor is the underlying training procedure public. We propose StyleCodes: an open-source and open-research style encoder architecture and training procedure to express image style as a 20-symbol base64 code. Our experiments show that our encoding results in minimal loss in quality compared to traditional image-to-style techniques.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "code: https://github.com/CiaraStrawberry/stylecodes project page: https://ciarastrawberry.github.io/stylecodes.github.io/. arXiv admin note: substantial text overlap with arXiv:2408.03209"
    },
    {
        "paper id": "2411.12829",
        "abstract url": "https://arxiv.org/abs/2411.12829",
        "title": "Human-Robot Dialogue Annotation for Multi-Modal Common Ground",
        "rating": "0",
        "keywords": [
            [
                "Robot",
                "navigation"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In this paper, we describe the development of symbolic representations annotated on human-robot dialogue data to make dimensions of meaning accessible to autonomous systems participating in collaborative, natural language dialogue, and to enable common ground with human partners. A particular challenge for establishing common ground arises in remote dialogue (occurring in disaster relief or search-and-rescue tasks), where a human and robot are engaged in a joint navigation and exploration task of an unfamiliar environment, but where the robot cannot immediately share high quality visual information due to limited communication constraints. Engaging in a dialogue provides an effective way to communicate, while on-demand or lower-quality visual information can be supplemented for establishing common ground. Within this paradigm, we capture propositional semantics and the illocutionary force of a single utterance within the dialogue through our Dialogue-AMR annotation, an augmentation of Abstract Meaning Representation. We then capture patterns in how different utterances within and across speaker floors relate to one another in our development of a multi-floor Dialogue Structure annotation schema. Finally, we begin to annotate and analyze the ways in which the visual modalities provide contextual information to the dialogue for overcoming disparities in the collaborators' understanding of the environment. We conclude by discussing the use-cases, architectures, and systems we have implemented from our annotations that enable physical robots to autonomously engage with humans in bi-directional dialogue and navigation.",
        "subjects": [
            "cs.HC",
            "cs.CL",
            "cs.RO"
        ],
        "comment": "52 pages, 14 figures"
    },
    {
        "paper id": "2411.12844",
        "abstract url": "https://arxiv.org/abs/2411.12844",
        "title": "SCOUT: A Situated and Multi-Modal Human-Robot Dialogue Corpus",
        "rating": "0",
        "keywords": [
            [
                "Robot",
                "navigation"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "We introduce the Situated Corpus Of Understanding Transactions (SCOUT), a multi-modal collection of human-robot dialogue in the task domain of collaborative exploration. The corpus was constructed from multiple Wizard-of-Oz experiments where human participants gave verbal instructions to a remotely-located robot to move and gather information about its surroundings. SCOUT contains 89,056 utterances and 310,095 words from 278 dialogues averaging 320 utterances per dialogue. The dialogues are aligned with the multi-modal data streams available during the experiments: 5,785 images and 30 maps. The corpus has been annotated with Abstract Meaning Representation and Dialogue-AMR to identify the speaker's intent and meaning within an utterance, and with Transactional Units and Relations to track relationships between utterances to reveal patterns of the Dialogue Structure. We describe how the corpus and its annotations have been used to develop autonomous human-robot systems and enable research in open questions of how humans speak to robots. We release this corpus to accelerate progress in autonomous, situated, human-robot dialogue, especially in the context of navigation tasks where details about the environment need to be discovered.",
        "subjects": [
            "cs.HC",
            "cs.CL",
            "cs.RO"
        ],
        "comment": "14 pages, 7 figures"
    },
    {
        "paper id": "2411.12915",
        "abstract url": "https://arxiv.org/abs/2411.12915",
        "title": "VILA-M3: Enhancing Vision-Language Models with Medical Expert Knowledge",
        "rating": "0",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "Medical",
                "healthcare",
                "clinical",
                "radiology"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generalist vision language models (VLMs) have made significant strides in computer vision, but they fall short in specialized fields like healthcare, where expert knowledge is essential. In traditional computer vision tasks, creative or approximate answers may be acceptable, but in healthcare, precision is paramount.Current large multimodal models like Gemini and GPT-4o are insufficient for medical tasks due to their reliance on memorized internet knowledge rather than the nuanced expertise required in healthcare. VLMs are usually trained in three stages: vision pre-training, vision-language pre-training, and instruction fine-tuning (IFT). IFT has been typically applied using a mixture of generic and healthcare data. In contrast, we propose that for medical VLMs, a fourth stage of specialized IFT is necessary, which focuses on medical data and includes information from domain expert models. Domain expert models developed for medical use are crucial because they are specifically trained for certain clinical tasks, e.g. to detect tumors and classify abnormalities through segmentation and classification, which learn fine-grained features of medical data$-$features that are often too intricate for a VLM to capture effectively especially in radiology. This paper introduces a new framework, VILA-M3, for medical VLMs that utilizes domain knowledge via expert models. Through our experiments, we show an improved state-of-the-art (SOTA) performance with an average improvement of ~9% over the prior SOTA model Med-Gemini and ~6% over models trained on the specific tasks. Our approach emphasizes the importance of domain expertise in creating precise, reliable VLMs for medical applications.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12962",
        "abstract url": "https://arxiv.org/abs/2411.12962",
        "title": "Bring the Heat: Rapid Trajectory Optimization with Pseudospectral Techniques and the Affine Geometric Heat Flow Equation",
        "rating": "0",
        "keywords": [
            [
                "time-efficient"
            ],
            [
                "Trajectory"
            ]
        ],
        "abstract": "Generating optimal trajectories for high-dimensional robotic systems in a time-efficient manner while adhering to constraints is a challenging task. This paper introduces PHLAME, which applies pseudospectral collocation and spatial vector algebra to efficiently solve the Affine Geometric Heat Flow (AGHF) Partial Differential Equation (PDE) for trajectory optimization. Unlike traditional PDE approaches like the Hamilton-Jacobi-Bellman (HJB) PDE, which solve for a function over the entire state space, computing a solution to the AGHF PDE scales more efficiently because its solution is defined over a two-dimensional domain, thereby avoiding the intractability of state-space scaling. To solve the AGHF one usually applies the Method of Lines (MOL), which discretizes one variable of the AGHF PDE, and converts the PDE into a system of ordinary differential equations (ODEs) that are solved using standard time-integration methods. Though powerful, this method requires a fine discretization to generate accurate solutions and requires evaluating the AGHF PDE which is computationally expensive for high-dimensional systems. PHLAME overcomes this deficiency by using a pseudospectral method, which reduces the number of function evaluations required to yield a high accuracy solution thereby allowing it to scale efficiently to high-dimensional robotic systems. To further increase computational speed, this paper presents analytical expressions for the AGHF and its Jacobian, both of which can be computed efficiently using rigid body dynamics algorithms. PHLAME is tested across various dynamical systems, with and without obstacles and compared to a number of state-of-the-art techniques. PHLAME generates trajectories for a 44-dimensional state-space system in $\\sim5$ seconds, much faster than current state-of-the-art techniques. A project page is available at https://roahmlab.github.io/PHLAME/",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": "26 pages, 8 figures, A project page can be found at https://roahmlab.github.io/PHLAME/"
    },
    {
        "paper id": "2411.13026",
        "abstract url": "https://arxiv.org/abs/2411.13026",
        "title": "X as Supervision: Contending with Depth Ambiguity in Unsupervised Monocular 3D Pose Estimation",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent unsupervised methods for monocular 3D pose estimation have endeavored to reduce dependence on limited annotated 3D data, but most are solely formulated in 2D space, overlooking the inherent depth ambiguity issue. Due to the information loss in 3D-to-2D projection, multiple potential depths may exist, yet only some of them are plausible in human structure. To tackle depth ambiguity, we propose a novel unsupervised framework featuring a multi-hypothesis detector and multiple tailored pretext tasks. The detector extracts multiple hypotheses from a heatmap within a local window, effectively managing the multi-solution problem. Furthermore, the pretext tasks harness 3D human priors from the SMPL model to regularize the solution space of pose estimation, aligning it with the empirical distribution of 3D human structures. This regularization is partially achieved through a GCN-based discriminator within the discriminative learning, and is further complemented with synthetic images through rendering, ensuring plausible estimations. Consequently, our approach demonstrates state-of-the-art unsupervised 3D pose estimation performance on various human datasets. Further evaluations on data scale-up and one animal dataset highlight its generalization capabilities. Code will be available at https://github.com/Charrrrrlie/X-as-Supervision.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.14479",
        "abstract url": "https://arxiv.org/abs/2411.14479",
        "title": "GRL-Prompt: Towards Knowledge Graph based Prompt Optimization via Reinforcement Learning",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have demonstrated impressive success in a wide range of natural language processing (NLP) tasks due to their extensive general knowledge of the world. Recent works discovered that the performance of LLMs is heavily dependent on the input prompt. However, prompt engineering is usually done manually in a trial-and-error fashion, which can be labor-intensive and challenging in order to find the optimal prompts. To address these problems and unleash the utmost potential of LLMs, we propose a novel LLMs-agnostic framework for prompt optimization, namely GRL-Prompt, which aims to automatically construct optimal prompts via reinforcement learning (RL) in an end-to-end manner. To provide structured action/state representation for optimizing prompts, we construct a knowledge graph (KG) that better encodes the correlation between the user query and candidate in-context examples. Furthermore, a policy network is formulated to generate the optimal action by selecting a set of in-context examples in a rewardable order to construct the prompt. Additionally, the embedding-based reward shaping is utilized to stabilize the RL training process. The experimental results show that GRL-Prompt outperforms recent state-of-the-art methods, achieving an average increase of 0.10 in ROUGE-1, 0.07 in ROUGE-2, 0.07 in ROUGE-L, and 0.05 in BLEU.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.15193",
        "abstract url": "https://arxiv.org/abs/2411.15193",
        "title": "Gradient-Weighted Feature Back-Projection: A Fast Alternative to Feature Distillation in 3D Gaussian Splatting",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "We introduce a training-free method for feature field rendering in Gaussian splatting. Our approach back-projects 2D features into pre-trained 3D Gaussians, using a weighted sum based on each Gaussian's influence in the final rendering. While most training-based feature field rendering methods excel at 2D segmentation but perform poorly at 3D segmentation without post-processing, our method achieves high-quality results in both 2D and 3D segmentation. Experimental results demonstrate that our approach is fast, scalable, and offers performance comparable to training-based methods.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.15194",
        "abstract url": "https://arxiv.org/abs/2411.15194",
        "title": "Guiding Word Equation Solving using Graph Neural Networks (Extended Technical Report)",
        "rating": "0",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "This paper proposes a Graph Neural Network-guided algorithm for solving word equations, based on the well-known Nielsen transformation for splitting equations. The algorithm iteratively rewrites the first terms of each side of an equation, giving rise to a tree-like search space. The choice of path at each split point of the tree significantly impacts solving time, motivating the use of Graph Neural Networks (GNNs) for efficient split decision-making. Split decisions are encoded as multi-classification tasks, and five graph representations of word equations are introduced to encode their structural information for GNNs. The algorithm is implemented as a solver named DragonLi. Experiments are conducted on artificial and real-world benchmarks. The algorithm performs particularly well on satisfiable problems. For single word \\mbox{equations}, DragonLi can solve significantly more problems than well-established string solvers. For the conjunction of multiple word equations, DragonLi is competitive with state-of-the-art string solvers.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.15195",
        "abstract url": "https://arxiv.org/abs/2411.15195",
        "title": "Graph Neural Network-Based Entity Extraction and Relationship Reasoning in Complex Knowledge Graphs",
        "rating": "0",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "This study proposed a knowledge graph entity extraction and relationship reasoning algorithm based on a graph neural network, using a graph convolutional network and graph attention network to model the complex structure in the knowledge graph. By building an end-to-end joint model, this paper achieves efficient recognition and reasoning of entities and relationships. In the experiment, this paper compared the model with a variety of deep learning algorithms and verified its superiority through indicators such as AUC, recall rate, precision rate, and F1 value. The experimental results show that the model proposed in this paper performs well in all indicators, especially in complex knowledge graphs, it has stronger generalization ability and stability. This provides strong support for further research on knowledge graphs and also demonstrates the application potential of graph neural networks in entity extraction and relationship reasoning.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.15199",
        "abstract url": "https://arxiv.org/abs/2411.15199",
        "title": "Adaptively Controllable Diffusion Model for Efficient Conditional Image Generation",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "With the development of artificial intelligence, more and more attention has been put onto generative models, which represent the creativity, a very important aspect of intelligence. In recent years, diffusion models have been studied and proven to be more reasonable and effective than previous methods. However, common diffusion frameworks suffer from controllability problems. Although extra conditions have been considered by some work to guide the diffusion process for a specific target generation, it only controls the generation result but not its process. In this work, we propose a new adaptive framework, $\\textit{Adaptively Controllable Diffusion (AC-Diff) Model}$, to automatically and fully control the generation process, including not only the type of generation result but also the length and parameters of the generation process. Both inputs and conditions will be first fed into a $\\textit{Conditional Time-Step (CTS) Module}$ to determine the number of steps needed for a generation. Then according to the length of the process, the diffusion rate parameters will be estimated through our $\\textit{Adaptive Hybrid Noise Schedule (AHNS) Module}$. We further train the network with the corresponding adaptive sampling mechanism to learn how to adjust itself according to the conditions for the overall performance improvement. To enable its practical applications, AC-Diff is expected to largely reduce the average number of generation steps and execution time while maintaining the same performance as done in the literature diffusion models.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12256",
        "abstract url": "https://arxiv.org/abs/2411.12256",
        "title": "Restructuring Tractable Probabilistic Circuits",
        "rating": "-0.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Probabilistic circuits (PCs) is a unifying representation for probabilistic models that support tractable inference. Numerous applications of PCs like controllable text generation depend on the ability to efficiently multiply two circuits. Existing multiplication algorithms require that the circuits respect the same structure, i.e. variable scopes decomposes according to the same vtree. In this work, we propose and study the task of restructuring structured(-decomposable) PCs, that is, transforming a structured PC such that it conforms to a target vtree. We propose a generic approach for this problem and show that it leads to novel polynomial-time algorithms for multiplying circuits respecting different vtrees, as well as a practical depth-reduction algorithm that preserves structured decomposibility. Our work opens up new avenues for tractable PC inference, suggesting the possibility of training with less restrictive PC structures while enabling efficient inference by changing their structures at inference time.",
        "subjects": [
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12329",
        "abstract url": "https://arxiv.org/abs/2411.12329",
        "title": "Attributed Graph Clustering in Collaborative Settings",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG",
                "cs.SI"
            ]
        ],
        "abstract": "Graph clustering is an unsupervised machine learning method that partitions the nodes in a graph into different groups. Despite achieving significant progress in exploiting both attributed and structured data information, graph clustering methods often face practical challenges related to data isolation. Moreover, the absence of collaborative methods for graph clustering limits their effectiveness. In this paper, we propose a collaborative graph clustering framework for attributed graphs, supporting attributed graph clustering over vertically partitioned data with different participants holding distinct features of the same data. Our method leverages a novel technique that reduces the sample space, improving the efficiency of the attributed graph clustering method. Furthermore, we compare our method to its centralized counterpart under a proximity condition, demonstrating that the successful local results of each participant contribute to the overall success of the collaboration. We fully implement our approach and evaluate its utility and efficiency by conducting experiments on four public datasets. The results demonstrate that our method achieves comparable accuracy levels to centralized attributed graph clustering methods. Our collaborative graph clustering framework provides an efficient and effective solution for graph clustering challenges related to data isolation.",
        "subjects": [
            "cs.LG",
            "cs.SI"
        ],
        "comment": "16 pages, 3 figures"
    },
    {
        "paper id": "2411.12330",
        "abstract url": "https://arxiv.org/abs/2411.12330",
        "title": "Graph as a feature: improving node classification with non-neural graph-aware logistic regression",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph Neural Networks (GNNs) and their message passing framework that leverages both structural and feature information, have become a standard method for solving graph-based machine learning problems. However, these approaches still struggle to generalise well beyond datasets that exhibit strong homophily, where nodes of the same class tend to connect. This limitation has led to the development of complex neural architectures that pose challenges in terms of efficiency and scalability. In response to these limitations, we focus on simpler and more scalable approaches and introduce Graph-aware Logistic Regression (GLR), a non-neural model designed for node classification tasks. Unlike traditional graph algorithms that use only a fraction of the information accessible to GNNs, our proposed model simultaneously leverages both node features and the relationships between entities. However instead of relying on message passing, our approach encodes each node's relationships as an additional feature vector, which is then combined with the node's self attributes. Extensive experimental results, conducted within a rigorous evaluation framework, show that our proposed GLR approach outperforms both foundational and sophisticated state-of-the-art GNN models in node classification tasks. Going beyond the traditional limited benchmarks, our experiments indicate that GLR increases generalisation ability while reaching performance gains in computation time up to two orders of magnitude compared to it best neural competitor.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12377",
        "abstract url": "https://arxiv.org/abs/2411.12377",
        "title": "Non-IID data in Federated Learning: A Systematic Review with Taxonomy, Metrics, Methods, Frameworks and Future Directions",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent advances in machine learning have highlighted Federated Learning (FL) as a promising approach that enables multiple distributed users (so-called clients) to collectively train ML models without sharing their private data. While this privacy-preserving method shows potential, it struggles when data across clients is not independent and identically distributed (non-IID) data. The latter remains an unsolved challenge that can result in poorer model performance and slower training times. Despite the significance of non-IID data in FL, there is a lack of consensus among researchers about its classification and quantification. This systematic review aims to fill that gap by providing a detailed taxonomy for non-IID data, partition protocols, and metrics to quantify data heterogeneity. Additionally, we describe popular solutions to address non-IID data and standardized frameworks employed in FL with heterogeneous data. Based on our state-of-the-art review, we present key lessons learned and suggest promising future research directions.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12433",
        "abstract url": "https://arxiv.org/abs/2411.12433",
        "title": "Preference-Conditioned Gradient Variations for Multi-Objective Quality-Diversity",
        "rating": "-0.5",
        "keywords": [
            [
                "robotics"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "In a variety of domains, from robotics to finance, Quality-Diversity algorithms have been used to generate collections of both diverse and high-performing solutions. Multi-Objective Quality-Diversity algorithms have emerged as a promising approach for applying these methods to complex, multi-objective problems. However, existing methods are limited by their search capabilities. For example, Multi-Objective Map-Elites depends on random genetic variations which struggle in high-dimensional search spaces. Despite efforts to enhance search efficiency with gradient-based mutation operators, existing approaches consider updating solutions to improve on each objective separately rather than achieving desired trade-offs. In this work, we address this limitation by introducing Multi-Objective Map-Elites with Preference-Conditioned Policy-Gradient and Crowding Mechanisms: a new Multi-Objective Quality-Diversity algorithm that uses preference-conditioned policy-gradient mutations to efficiently discover promising regions of the objective space and crowding mechanisms to promote a uniform distribution of solutions on the Pareto front. We evaluate our approach on six robotics locomotion tasks and show that our method outperforms or matches all state-of-the-art Multi-Objective Quality-Diversity methods in all six, including two newly proposed tri-objective tasks. Importantly, our method also achieves a smoother set of trade-offs, as measured by newly-proposed sparsity-based metrics. This performance comes at a lower computational storage cost compared to previous methods.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12588",
        "abstract url": "https://arxiv.org/abs/2411.12588",
        "title": "Learning To Sample the Meta-Paths for Social Event Detection",
        "rating": "-0.5",
        "keywords": [
            [
                "graphs"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Social media data is inherently rich, as it includes not only text content, but also users, geolocation, entities, temporal information, and their relationships. This data richness can be effectively modeled using heterogeneous information networks (HINs) as it can handle multiple types of nodes and relationships, allowing for a comprehensive representation of complex interactions within social data. Meta-path-based methods use the sequences of relationships between different types of nodes in an HIN to capture the diverse and rich relationships within the social networks. However, the performance of social event detection methods is highly sensitive to the selection of meta-paths and existing meta-path based detectors either rely on human efforts or struggle to determining the effective meta-path set for model training and evaluation. In order to automatically discover the most important meta-paths, we propose a simple, yet effective, end-to-end Learning To Sample (LTS) framework for meta-path searching. Specifically, we build graphs that contain not only user profiles, textual content, and details about entities, but also the intricate relationships among them. The prioritized meta-paths, based on their importance, are sampled from the maintained distribution and their features are constructed before feeding into the social event detector. After picking up the top-ranked meta-paths, we streamline the exponential increment of meta-path combinations into a finite set of highly influential ones. The chosen meta-paths, along with their respective weights, are then used to train our social event detection model. As an alternative to social event detector training, we further propose an extra non-parametric evaluation process in order to determine the importance of each meta-path, which can further guide the paths sampling during model training.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12597",
        "abstract url": "https://arxiv.org/abs/2411.12597",
        "title": "GNNAS-Dock: Budget Aware Algorithm Selection with Graph Neural Networks for Molecular Docking",
        "rating": "-0.5",
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Molecular docking is a major element in drug discovery and design. It enables the prediction of ligand-protein interactions by simulating the binding of small molecules to proteins. Despite the availability of numerous docking algorithms, there is no single algorithm consistently outperforms the others across a diverse set of docking scenarios. This paper introduces GNNAS-Dock, a novel Graph Neural Network (GNN)-based automated algorithm selection system for molecular docking in blind docking situations. GNNs are accommodated to process the complex structural data of both ligands and proteins. They benefit from the inherent graph-like properties to predict the performance of various docking algorithms under different conditions. The present study pursues two main objectives: 1) predict the performance of each candidate docking algorithm, in terms of Root Mean Square Deviation (RMSD), thereby identifying the most accurate method for specific scenarios; and 2) choose the best computationally efficient docking algorithm for each docking case, aiming to reduce the time required for docking while maintaining high accuracy. We validate our approach on PDBBind 2020 refined set, which contains about 5,300 pairs of protein-ligand complexes.",
        "subjects": [
            "q-bio.BM",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12600",
        "abstract url": "https://arxiv.org/abs/2411.12600",
        "title": "Provable unlearning in topic modeling and downstream tasks",
        "rating": "-0.5",
        "keywords": [
            [
                "unlearning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Machine unlearning algorithms are increasingly important as legal concerns arise around the provenance of training data, but verifying the success of unlearning is often difficult. Provable guarantees for unlearning are often limited to supervised learning settings. In this paper, we provide the first theoretical guarantees for unlearning in the pre-training and fine-tuning paradigm by studying topic models, simple bag-of-words language models that can be adapted to solve downstream tasks like retrieval and classification. First, we design a provably effective unlearning algorithm for topic models that incurs a computational overhead independent of the size of the original dataset. Our analysis additionally quantifies the deletion capacity of the model -- i.e., the number of examples that can be unlearned without incurring a significant cost in model performance. Finally, we formally extend our analyses to account for adaptation to a given downstream task. In particular, we design an efficient algorithm to perform unlearning after fine-tuning the topic model via a linear head. Notably, we show that it is easier to unlearn pre-training data from models that have been fine-tuned to a particular task, and one can unlearn this data without modifying the base model.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12701",
        "abstract url": "https://arxiv.org/abs/2411.12701",
        "title": "When Backdoors Speak: Understanding LLM Backdoor Attacks Through Model-Generated Explanations",
        "rating": "-0.5",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large Language Models (LLMs) are vulnerable to backdoor attacks, where hidden triggers can maliciously manipulate model behavior. While several backdoor attack methods have been proposed, the mechanisms by which backdoor functions operate in LLMs remain underexplored. In this paper, we move beyond attacking LLMs and investigate backdoor functionality through the novel lens of natural language explanations. Specifically, we leverage LLMs' generative capabilities to produce human-understandable explanations for their decisions, allowing us to compare explanations for clean and poisoned samples. We explore various backdoor attacks and embed the backdoor into LLaMA models for multiple tasks. Our experiments show that backdoored models produce higher-quality explanations for clean data compared to poisoned data, while generating significantly more consistent explanations for poisoned data than for clean data. We further analyze the explanation generation process, revealing that at the token level, the explanation token of poisoned samples only appears in the final few transformer layers of the LLM. At the sentence level, attention dynamics indicate that poisoned inputs shift attention from the input context when generating the explanation. These findings deepen our understanding of backdoor attack mechanisms in LLMs and offer a framework for detecting such vulnerabilities through explainability techniques, contributing to the development of more secure LLMs.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12730",
        "abstract url": "https://arxiv.org/abs/2411.12730",
        "title": "Testing classical properties from quantum data",
        "rating": "-0.5",
        "keywords": [
            [
                "time-efficient"
            ],
            [
                "quantum"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Many properties of Boolean functions can be tested far more efficiently than the function can be learned. However, this advantage often disappears when testers are limited to random samples--a natural setting for data science--rather than queries. In this work we investigate the quantum version of this scenario: quantum algorithms that test properties of a function $f$ solely from quantum data in the form of copies of the function state for $f$. For three well-established properties, we show that the speedup lost when restricting classical testers to samples can be recovered by testers that use quantum data. For monotonicity testing, we give a quantum algorithm that uses $\\tilde{\\mathcal{O}}(n^2)$ function state copies as compared to the $2^{\u03a9(\\sqrt{n})}$ samples required classically. We also present $\\mathcal{O}(1)$-copy testers for symmetry and triangle-freeness, comparing favorably to classical lower bounds of $\u03a9(n^{1/4})$ and $\u03a9(n)$ samples respectively. These algorithms are time-efficient and necessarily include techniques beyond the Fourier sampling approaches applied to earlier testing problems. These results make the case for a general study of the advantages afforded by quantum data for testing. We contribute to this project by complementing our upper bounds with a lower bound of $\u03a9(1/\\varepsilon)$ for monotonicity testing from quantum data in the proximity regime $\\varepsilon\\leq\\mathcal{O}(n^{-3/2})$. This implies a strict separation between testing monotonicity from quantum data and from quantum queries--where $\\tilde{\\mathcal{O}}(n)$ queries suffice when $\\varepsilon=\u0398(n^{-3/2})$. We also exhibit a testing problem that can be solved from $\\mathcal{O}(1)$ classical queries but requires $\u03a9(2^{n/2})$ function state copies, complementing a separation of the same magnitude in the opposite direction derived from the Forrelation problem.",
        "subjects": [
            "quant-ph",
            "cs.CC",
            "cs.DS",
            "cs.LG"
        ],
        "comment": "38 + 14 pages, 2 tables, 2 figures"
    },
    {
        "paper id": "2411.12732",
        "abstract url": "https://arxiv.org/abs/2411.12732",
        "title": "Benchmarking Positional Encodings for GNNs and Graph Transformers",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Recent advances in Graph Neural Networks (GNNs) and Graph Transformers (GTs) have been driven by innovations in architectures and Positional Encodings (PEs), which are critical for augmenting node features and capturing graph topology. PEs are essential for GTs, where topological information would otherwise be lost without message-passing. However, PEs are often tested alongside novel architectures, making it difficult to isolate their effect on established models. To address this, we present a comprehensive benchmark of PEs in a unified framework that includes both message-passing GNNs and GTs. We also establish theoretical connections between MPNNs and GTs and introduce a sparsified GRIT attention mechanism to examine the influence of global connectivity. Our findings demonstrate that previously untested combinations of GNN architectures and PEs can outperform existing methods and offer a more comprehensive picture of the state-of-the-art. To support future research and experimentation in our framework, we make the code publicly available.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12872",
        "abstract url": "https://arxiv.org/abs/2411.12872",
        "title": "From Text to Pose to Image: Improving Diffusion Model Control and Quality",
        "rating": "-0.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "Diffusion",
                "text-to-image"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In the last two years, text-to-image diffusion models have become extremely popular. As their quality and usage increase, a major concern has been the need for better output control. In addition to prompt engineering, one effective method to improve the controllability of diffusion models has been to condition them on additional modalities such as image style, depth map, or keypoints. This forms the basis of ControlNets or Adapters. When attempting to apply these methods to control human poses in outputs of text-to-image diffusion models, two main challenges have arisen. The first challenge is generating poses following a wide range of semantic text descriptions, for which previous methods involved searching for a pose within a dataset of (caption, pose) pairs. The second challenge is conditioning image generation on a specified pose while keeping both high aesthetic and high pose fidelity. In this article, we fix these two main issues by introducing a text-to-pose (T2P) generative model alongside a new sampling algorithm, and a new pose adapter that incorporates more pose keypoints for higher pose fidelity. Together, these two new state-of-the-art models enable, for the first time, a generative text-to-pose-to-image framework for higher pose control in diffusion models. We release all models and the code used for the experiments at https://github.com/clement-bonnet/text-to-pose.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Published at the NeurIPS 2024 Workshop on Compositional Learning: Perspectives, Methods, and Paths Forward"
    },
    {
        "paper id": "2411.12876",
        "abstract url": "https://arxiv.org/abs/2411.12876",
        "title": "Puppet-CNN: Input-Adaptive Convolutional Neural Networks with Model Compression using Ordinary Differential Equation",
        "rating": "-0.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Convolutional Neural Network (CNN) has been applied to more and more scenarios due to its excellent performance in many machine learning tasks, especially with deep and complex structures. However, as the network goes deeper, more parameters need to be stored and optimized. Besides, almost all common CNN models adopt \"train-and-use\" strategy where the structure is pre-defined and the kernel parameters are fixed after the training with the same structure and set of parameters used for all data without considering the content complexity. In this paper, we propose a new CNN framework, named as $\\textit{Puppet-CNN}$, which contains two modules: a $\\textit{puppet module}$ and a $\\textit{puppeteer module}$. The puppet module is a CNN model used to actually process the input data just like other works, but its depth and kernels are generated by the puppeteer module (realized with Ordinary Differential Equation (ODE)) based on the input complexity each time. By recurrently generating kernel parameters in the puppet module, we can take advantage of the dependence among kernels of different convolutional layers to significantly reduce the size of CNN model by only storing and training the parameters of the much smaller puppeteer ODE module. Through experiments on several datasets, our method has proven to be superior than the traditional CNNs on both performance and efficiency. The model size can be reduced more than 10 times.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12913",
        "abstract url": "https://arxiv.org/abs/2411.12913",
        "title": "MLDGG: Meta-Learning for Domain Generalization on Graphs",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graphs"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Domain generalization on graphs aims to develop models with robust generalization capabilities, ensuring effective performance on the testing set despite disparities between testing and training distributions. However, existing methods often rely on static encoders directly applied to the target domain, constraining its flexible adaptability. In contrast to conventional methodologies, which concentrate on developing specific generalized models, our framework, MLDGG, endeavors to achieve adaptable generalization across diverse domains by integrating cross-multi-domain meta-learning with structure learning and semantic identification. Initially, it introduces a generalized structure learner to mitigate the adverse effects of task-unrelated edges, enhancing the comprehensiveness of representations learned by Graph Neural Networks (GNNs) while capturing shared structural information across domains. Subsequently, a representation learner is designed to disentangle domain-invariant semantic and domain-specific variation information in node embedding by leveraging causal reasoning for semantic identification, further enhancing generalization. In the context of meta-learning, meta-parameters for both learners are optimized to facilitate knowledge transfer and enable effective adaptation to graphs through fine-tuning within the target domains, where target graphs are inaccessible during training. Our empirical results demonstrate that MLDGG surpasses baseline methods, showcasing its effectiveness in three different distribution shift settings.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Accepted in KDD 2025 (research track)"
    },
    {
        "paper id": "2411.12914",
        "abstract url": "https://arxiv.org/abs/2411.12914",
        "title": "Trojan Cleansing with Neural Collapse",
        "rating": "-0.5",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Trojan attacks are sophisticated training-time attacks on neural networks that embed backdoor triggers which force the network to produce a specific output on any input which includes the trigger. With the increasing relevance of deep networks which are too large to train with personal resources and which are trained on data too large to thoroughly audit, these training-time attacks pose a significant risk. In this work, we connect trojan attacks to Neural Collapse, a phenomenon wherein the final feature representations of over-parameterized neural networks converge to a simple geometric structure. We provide experimental evidence that trojan attacks disrupt this convergence for a variety of datasets and architectures. We then use this disruption to design a lightweight, broadly generalizable mechanism for cleansing trojan attacks from a wide variety of different network architectures and experimentally demonstrate its efficacy.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12940",
        "abstract url": "https://arxiv.org/abs/2411.12940",
        "title": "On the relationship between Koopman operator approximations and neural ordinary differential equations for data-driven time-evolution predictions",
        "rating": "-0.5",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This work explores the relationship between state space methods and Koopman operator-based methods for predicting the time-evolution of nonlinear dynamical systems. We demonstrate that extended dynamic mode decomposition with dictionary learning (EDMD-DL), when combined with a state space projection, is equivalent to a neural network representation of the nonlinear discrete-time flow map on the state space. We highlight how this projection step introduces nonlinearity into the evolution equations, enabling significantly improved EDMD-DL predictions. With this projection, EDMD-DL leads to a nonlinear dynamical system on the state space, which can be represented in either discrete or continuous time. This system has a natural structure for neural networks, where the state is first expanded into a high dimensional feature space followed by a linear mapping which represents the discrete-time map or the vector field as a linear combination of these features. Inspired by these observations, we implement several variations of neural ordinary differential equations (ODEs) and EDMD-DL, developed by combining different aspects of their respective model structures and training procedures. We evaluate these methods using numerical experiments on chaotic dynamics in the Lorenz system and a nine-mode model of turbulent shear flow, showing comparable performance across methods in terms of short-time trajectory prediction, reconstruction of long-time statistics, and prediction of rare events. We also show that these methods provide comparable performance to a non-Markovian approach in terms of prediction of extreme events.",
        "subjects": [
            "nlin.CD",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12943",
        "abstract url": "https://arxiv.org/abs/2411.12943",
        "title": "Enhancing Thermal MOT: A Novel Box Association Method Leveraging Thermal Identity and Motion Similarity",
        "rating": "-0.5",
        "keywords": [
            [
                "Thermal"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Multiple Object Tracking (MOT) in thermal imaging presents unique challenges due to the lack of visual features and the complexity of motion patterns. This paper introduces an innovative approach to improve MOT in the thermal domain by developing a novel box association method that utilizes both thermal object identity and motion similarity. Our method merges thermal feature sparsity and dynamic object tracking, enabling more accurate and robust MOT performance. Additionally, we present a new dataset comprised of a large-scale collection of thermal and RGB images captured in diverse urban environments, serving as both a benchmark for our method and a new resource for thermal imaging. We conduct extensive experiments to demonstrate the superiority of our approach over existing methods, showing significant improvements in tracking accuracy and robustness under various conditions. Our findings suggest that incorporating thermal identity with motion data enhances MOT performance. The newly collected dataset and source code is available at https://github.com/wassimea/thermalMOT",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Workshop on Towards a Complete Analysis of People, part of the European Conference on Computer Vision (ECCV) 2024"
    },
    {
        "paper id": "2411.12949",
        "abstract url": "https://arxiv.org/abs/2411.12949",
        "title": "Epidemiology-informed Network for Robust Rumor Detection",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "The rapid spread of rumors on social media has posed significant challenges to maintaining public trust and information integrity. Since an information cascade process is essentially a propagation tree, recent rumor detection models leverage graph neural networks to additionally capture information propagation patterns, thus outperforming text-only solutions. Given the variations in topics and social impact of the root node, different source information naturally has distinct outreach capabilities, resulting in different heights of propagation trees. This variation, however, impedes the data-driven design of existing graph-based rumor detectors. Given a shallow propagation tree with limited interactions, it is unlikely for graph-based approaches to capture sufficient cascading patterns, questioning their ability to handle less popular news or early detection needs. In contrast, a deep propagation tree is prone to noisy user responses, and this can in turn obfuscate the predictions. In this paper, we propose a novel Epidemiology-informed Network (EIN) that integrates epidemiological knowledge to enhance performance by overcoming data-driven methods sensitivity to data quality. Meanwhile, to adapt epidemiology theory to rumor detection, it is expected that each users stance toward the source information will be annotated. To bypass the costly and time-consuming human labeling process, we take advantage of large language models to generate stance labels, facilitating optimization objectives for learning epidemiology-informed representations. Our experimental results demonstrate that the proposed EIN not only outperforms state-of-the-art methods on real-world datasets but also exhibits enhanced robustness across varying tree depths.",
        "subjects": [
            "cs.SI",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12950",
        "abstract url": "https://arxiv.org/abs/2411.12950",
        "title": "KAAE: Numerical Reasoning for Knowledge Graphs via Knowledge-aware Attributes Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Numerical reasoning is pivotal in various artificial intelligence applications, such as natural language processing and recommender systems, where it involves using entities, relations, and attribute values (e.g., weight, length) to infer new factual relations (e.g., the Nile is longer than the Amazon). However, existing approaches encounter two critical challenges in modeling: (1) semantic relevance-the challenge of insufficiently capturing the necessary contextual interactions among entities, relations, and numerical attributes, often resulting in suboptimal inference; and (2) semantic ambiguity-the difficulty in accurately distinguishing ordinal relationships during numerical reasoning, which compromises the generation of high-quality samples and limits the effectiveness of contrastive learning. To address these challenges, we propose the novel Knowledge-Aware Attributes Embedding model (KAAE) for knowledge graph embeddings in numerical reasoning. Specifically, to overcome the challenge of semantic relevance, we introduce a Mixture-of-Experts-Knowledge-Aware (MoEKA) Encoder, designed to integrate the semantics of entities, relations, and numerical attributes into a joint semantic space. To tackle semantic ambiguity, we implement a new ordinal knowledge contrastive learning (OKCL) strategy that generates high-quality ordinal samples from the original data with the aid of ordinal relations, capturing fine-grained semantic nuances essential for accurate numerical reasoning. Experiments on three public benchmark datasets demonstrate the superior performance of KAAE across various attribute value distributions.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "This paper was decided to be withdrawn due to failure to resolve collaborative disputes within the research team or authorship issues. We are actively communicating to reach an agreement and avoid a recurrence of similar issues"
    },
    {
        "paper id": "2411.12964",
        "abstract url": "https://arxiv.org/abs/2411.12964",
        "title": "Real-Time Energy-Optimal Path Planning for Electric Vehicles",
        "rating": "-0.5",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The rapid adoption of electric vehicles (EVs) in modern transport systems has made energy-aware routing a critical task in their successful integration, especially within large-scale networks. In cases where an EV's remaining energy is limited and charging locations are not easily accessible, some destinations may only be reachable through an energy-optimal path: a route that consumes less energy than all other alternatives. The feasibility of such energy-efficient paths depends heavily on the accuracy of the energy model used for planning, and thus failing to account for vehicle dynamics can lead to inaccurate energy estimates, rendering some planned routes infeasible in reality. This paper explores the impact of vehicle dynamics on energy-optimal path planning for EVs. We develop an accurate energy model that incorporates key vehicle dynamics parameters into energy calculations, thereby reducing the risk of planning infeasible paths under battery constraints. The paper also introduces two novel online reweighting functions that allow for a faster, pre-processing free, pathfinding in the presence of negative energy costs resulting from regenerative braking, making them ideal for real-time applications. Through extensive experimentation on real-world transport networks, we demonstrate that our approach considerably enhances energy-optimal pathfinding for EVs in both computational efficiency and energy estimation accuracy.",
        "subjects": [
            "cs.AI"
        ],
        "comment": "12 pages, 7 figures, 5 tables"
    },
    {
        "paper id": "2411.12972",
        "abstract url": "https://arxiv.org/abs/2411.12972",
        "title": "A Foundation Model for Unified Urban Spatio-Temporal Flow Prediction",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Urban spatio-temporal flow prediction, encompassing traffic flows and crowd flows, is crucial for optimizing city infrastructure and managing traffic and emergency responses. Traditional approaches have relied on separate models tailored to either grid-based data, representing cities as uniform cells, or graph-based data, modeling cities as networks of nodes and edges. In this paper, we build UniFlow, a foundational model for general urban flow prediction that unifies both grid-based and graphbased data. We first design a multi-view spatio-temporal patching mechanism to standardize different data into a consistent sequential format and then introduce a spatio-temporal transformer architecture to capture complex correlations and dynamics. To leverage shared spatio-temporal patterns across different data types and facilitate effective cross-learning, we propose SpatioTemporal Memory Retrieval Augmentation (ST-MRA). By creating structured memory modules to store shared spatio-temporal patterns, ST-MRA enhances predictions through adaptive memory retrieval. Extensive experiments demonstrate that UniFlow outperforms existing models in both grid-based and graph-based flow prediction, excelling particularly in scenarios with limited data availability, showcasing its superior performance and broad applicability. The datasets and code implementation have been released on https://github.com/YuanYuan98/UniFlow.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13000",
        "abstract url": "https://arxiv.org/abs/2411.13000",
        "title": "NCAirFL: CSI-Free Over-the-Air Federated Learning Based on Non-Coherent Detection",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Over-the-air federated learning (FL), i.e., AirFL, leverages computing primitively over multiple access channels. A long-standing challenge in AirFL is to achieve coherent signal alignment without relying on expensive channel estimation and feedback. This paper proposes NCAirFL, a CSI-free AirFL scheme based on unbiased non-coherent detection at the edge server. By exploiting binary dithering and a long-term memory based error-compensation mechanism, NCAirFL achieves a convergence rate of order $\\mathcal{O}(1/\\sqrt{T})$ in terms of the average square norm of the gradient for general non-convex and smooth objectives, where $T$ is the number of communication rounds. Experiments demonstrate the competitive performance of NCAirFL compared to vanilla FL with ideal communications and to coherent transmission-based benchmarks.",
        "subjects": [
            "cs.IT",
            "cs.LG",
            "eess.SP"
        ],
        "comment": "6 pages, 2 figures, submitted for possible publication"
    },
    {
        "paper id": "2411.13014",
        "abstract url": "https://arxiv.org/abs/2411.13014",
        "title": "Scalable Deep Metric Learning on Attributed Graphs",
        "rating": "-0.5",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider the problem of constructing embeddings of large attributed graphs and supporting multiple downstream learning tasks. We develop a graph embedding method, which is based on extending deep metric and unbiased contrastive learning techniques to 1) work with attributed graphs, 2) enabling a mini-batch based approach, and 3) achieving scalability. Based on a multi-class tuplet loss function, we present two algorithms -- DMT for semi-supervised learning and DMAT-i for the unsupervised case. Analyzing our methods, we provide a generalization bound for the downstream node classification task and for the first time relate tuplet loss to contrastive learning. Through extensive experiments, we show high scalability of representation construction, and in applying the method for three downstream tasks (node clustering, node classification, and link prediction) better consistency over any single existing method.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "This is the complete version of a published paper with appendix including detailed proofs"
    },
    {
        "paper id": "2411.13028",
        "abstract url": "https://arxiv.org/abs/2411.13028",
        "title": "A Theory for Compressibility of Graph Transformers for Transductive Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Transductive tasks on graphs differ fundamentally from typical supervised machine learning tasks, as the independent and identically distributed (i.i.d.) assumption does not hold among samples. Instead, all train/test/validation samples are present during training, making them more akin to a semi-supervised task. These differences make the analysis of the models substantially different from other models. Recently, Graph Transformers have significantly improved results on these datasets by overcoming long-range dependency problems. However, the quadratic complexity of full Transformers has driven the community to explore more efficient variants, such as those with sparser attention patterns. While the attention matrix has been extensively discussed, the hidden dimension or width of the network has received less attention. In this work, we establish some theoretical bounds on how and under what conditions the hidden dimension of these networks can be compressed. Our results apply to both sparse and dense variants of Graph Transformers.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13029",
        "abstract url": "https://arxiv.org/abs/2411.13029",
        "title": "Probably Approximately Precision and Recall Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Precision and Recall are foundational metrics in machine learning where both accurate predictions and comprehensive coverage are essential, such as in recommender systems and multi-label learning. In these tasks, balancing precision (the proportion of relevant items among those predicted) and recall (the proportion of relevant items successfully predicted) is crucial. A key challenge is that one-sided feedback--where only positive examples are observed during training--is inherent in many practical problems. For instance, in recommender systems like YouTube, training data only consists of videos that a user has actively selected, while unselected items remain unseen. Despite this lack of negative feedback in training, avoiding undesirable recommendations at test time is essential. We introduce a PAC learning framework where each hypothesis is represented by a graph, with edges indicating positive interactions, such as between users and items. This framework subsumes the classical binary and multi-class PAC learning models as well as multi-label learning with partial feedback, where only a single random correct label per example is observed, rather than all correct labels. Our work uncovers a rich statistical and algorithmic landscape, with nuanced boundaries on what can and cannot be learned. Notably, classical methods like Empirical Risk Minimization fail in this setting, even for simple hypothesis classes with only two hypotheses. To address these challenges, we develop novel algorithms that learn exclusively from positive data, effectively minimizing both precision and recall losses. Specifically, in the realizable setting, we design algorithms that achieve optimal sample complexity guarantees. In the agnostic case, we show that it is impossible to achieve additive error guarantees--as is standard in PAC learning--and instead obtain meaningful multiplicative approximations.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13603",
        "abstract url": "https://arxiv.org/abs/2411.13603",
        "title": "A Full-History Network Dataset for BTC Asset Decentralization Profiling",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Since its advent in 2009, Bitcoin (BTC) has garnered increasing attention from both academia and industry. However, due to the massive transaction volume, no systematic study has quantitatively measured the asset decentralization degree specifically from a network perspective. In this paper, by conducting a thorough analysis of the BTC transaction network, we first address the significant gap in the availability of full-history BTC graph and network property dataset, which spans over 15 years from the genesis block (1st March, 2009) to the 845651-th block (29, May 2024). We then present the first systematic investigation to profile BTC's asset decentralization and design several decentralization degrees for quantification. Through extensive experiments, we emphasize the significant role of network properties and our network-based decentralization degree in enhancing Bitcoin analysis. Our findings demonstrate the importance of our comprehensive dataset and analysis in advancing research on Bitcoin's transaction dynamics and decentralization, providing valuable insights into the network's structure and its implications.",
        "subjects": [
            "q-fin.ST",
            "cs.SI"
        ],
        "comment": "IEEE BigData 2024"
    },
    {
        "paper id": "2411.13613",
        "abstract url": "https://arxiv.org/abs/2411.13613",
        "title": "SuPLE: Robot Learning with Lyapunov Rewards",
        "rating": "-0.5",
        "keywords": [
            [
                "Robot"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The reward function is an essential component in robot learning. Reward directly affects the sample and computational complexity of learning, and the quality of a solution. The design of informative rewards requires domain knowledge, which is not always available. We use the properties of the dynamics to produce system-appropriate reward without adding external assumptions. Specifically, we explore an approach to utilize the Lyapunov exponents of the system dynamics to generate a system-immanent reward. We demonstrate that the `Sum of the Positive Lyapunov Exponents' (SuPLE) is a strong candidate for the design of such a reward. We develop a computational framework for the derivation of this reward, and demonstrate its effectiveness on classical benchmarks for sample-based stabilization of various dynamical systems. It eliminates the need to start the training trajectories at arbitrary states, also known as auxiliary exploration. While the latter is a common practice in simulated robot learning, it is unpractical to consider to use it in real robotic systems, since they typically start from natural rest states such as a pendulum at the bottom, a robot on the ground, etc. and can not be easily initialized at arbitrary states. Comparing the performance of SuPLE to commonly-used reward functions, we observe that the latter fail to find a solution without auxiliary exploration, even for the task of swinging up the double pendulum and keeping it stable at the upright position, a prototypical scenario for multi-linked robots. SuPLE-induced rewards for robot learning offer a novel route for effective robot learning in typical as opposed to highly specialized or fine-tuned scenarios. Our code is publicly available for reproducibility and further research.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "7 pages, 4 figures"
    },
    {
        "paper id": "2411.12254",
        "abstract url": "https://arxiv.org/abs/2411.12254",
        "title": "Predicting User Intents and Musical Attributes from Music Discovery Conversations",
        "rating": "-1",
        "keywords": [
            [
                "Music"
            ],
            [
                "cs.LG",
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Intent classification is a text understanding task that identifies user needs from input text queries. While intent classification has been extensively studied in various domains, it has not received much attention in the music domain. In this paper, we investigate intent classification models for music discovery conversation, focusing on pre-trained language models. Rather than only predicting functional needs: intent classification, we also include a task for classifying musical needs: musical attribute classification. Additionally, we propose a method of concatenating previous chat history with just single-turn user queries in the input text, allowing the model to understand the overall conversation context better. Our proposed model significantly improves the F1 score for both user intent and musical attribute classification, and surpasses the zero-shot and few-shot performance of the pretrained Llama 3 model.",
        "subjects": [
            "cs.CL",
            "cs.LG",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "8 pages, 4 figures"
    },
    {
        "paper id": "2411.12262",
        "abstract url": "https://arxiv.org/abs/2411.12262",
        "title": "Low-resource Machine Translation: what for? who for? An observational study on a dedicated Tetun language translation service",
        "rating": "-1",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The impact of machine translation (MT) on low-resource languages remains poorly understood. In particular, observational studies of actual usage patterns are scarce. Such studies could provide valuable insights into user needs and behaviours, complementing survey-based methods. Here we present an observational analysis of real-world MT usage for Tetun, the lingua franca of Timor-Leste, using server logs from a widely-used MT service with over $70,000$ monthly active users. Our analysis of $100,000$ translation requests reveals patterns that challenge assumptions based on existing corpora. We find that users, many of them students on mobile devices, typically translate short texts into Tetun across diverse domains including science, healthcare, and daily life. This contrasts sharply with available Tetun corpora, which are dominated by news articles covering government and social issues. Our results suggest that MT systems for languages like Tetun should prioritise translating into the low-resource language, handling brief inputs effectively, and covering a wide range of domains relevant to educational contexts. More broadly, this study demonstrates how observational analysis can inform low-resource language technology development, by grounding research in practical community needs.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12290",
        "abstract url": "https://arxiv.org/abs/2411.12290",
        "title": "SSEditor: Controllable Mask-to-Scene Generation with Diffusion Model",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in 3D diffusion-based semantic scene generation have gained attention. However, existing methods rely on unconditional generation and require multiple resampling steps when editing scenes, which significantly limits their controllability and flexibility. To this end, we propose SSEditor, a controllable Semantic Scene Editor that can generate specified target categories without multiple-step resampling. SSEditor employs a two-stage diffusion-based framework: (1) a 3D scene autoencoder is trained to obtain latent triplane features, and (2) a mask-conditional diffusion model is trained for customizable 3D semantic scene generation. In the second stage, we introduce a geometric-semantic fusion module that enhance the model's ability to learn geometric and semantic information. This ensures that objects are generated with correct positions, sizes, and categories. Extensive experiments on SemanticKITTI and CarlaSC demonstrate that SSEditor outperforms previous approaches in terms of controllability and flexibility in target generation, as well as the quality of semantic scene generation and reconstruction. More importantly, experiments on the unseen Occ-3D Waymo dataset show that SSEditor is capable of generating novel urban scenes, enabling the rapid construction of 3D scenes.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12301",
        "abstract url": "https://arxiv.org/abs/2411.12301",
        "title": "Physics-Guided Detector for SAR Airplanes",
        "rating": "-1",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The disperse structure distributions (discreteness) and variant scattering characteristics (variability) of SAR airplane targets lead to special challenges of object detection and recognition. The current deep learning-based detectors encounter challenges in distinguishing fine-grained SAR airplanes against complex backgrounds. To address it, we propose a novel physics-guided detector (PGD) learning paradigm for SAR airplanes that comprehensively investigate their discreteness and variability to improve the detection performance. It is a general learning paradigm that can be extended to different existing deep learning-based detectors with \"backbone-neck-head\" architectures. The main contributions of PGD include the physics-guided self-supervised learning, feature enhancement, and instance perception, denoted as PGSSL, PGFE, and PGIP, respectively. PGSSL aims to construct a self-supervised learning task based on a wide range of SAR airplane targets that encodes the prior knowledge of various discrete structure distributions into the embedded space. Then, PGFE enhances the multi-scale feature representation of a detector, guided by the physics-aware information learned from PGSSL. PGIP is constructed at the detection head to learn the refined and dominant scattering point of each SAR airplane instance, thus alleviating the interference from the complex background. We propose two implementations, denoted as PGD and PGD-Lite, and apply them to various existing detectors with different backbones and detection heads. The experiments demonstrate the flexibility and effectiveness of the proposed PGD, which can improve existing detectors on SAR airplane detection with fine-grained classification task (an improvement of 3.1\\% mAP most), and achieve the state-of-the-art performance (90.7\\% mAP) on SAR-AIRcraft-1.0 dataset. The project is open-source at \\url{https://github.com/XAI4SAR/PGD}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12304",
        "abstract url": "https://arxiv.org/abs/2411.12304",
        "title": "Emergence of Implicit World Models from Mortal Agents",
        "rating": "-1",
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We discuss the possibility of world models and active exploration as emergent properties of open-ended behavior optimization in autonomous agents. In discussing the source of the open-endedness of living things, we start from the perspective of biological systems as understood by the mechanistic approach of theoretical biology and artificial life. From this perspective, we discuss the potential of homeostasis in particular as an open-ended objective for autonomous agents and as a general, integrative extrinsic motivation. We then discuss the possibility of implicitly acquiring a world model and active exploration through the internal dynamics of a network, and a hypothetical architecture for this, by combining meta-reinforcement learning, which assumes domain adaptation as a system that achieves robust homeostasis.",
        "subjects": [
            "cs.NE",
            "cs.LG"
        ],
        "comment": "Accepted as a 1-page tiny paper in the Intrinsically Motivated Open-ended Learning workshop at NeurIPS 2024"
    },
    {
        "paper id": "2411.12307",
        "abstract url": "https://arxiv.org/abs/2411.12307",
        "title": "Balancing Accuracy and Efficiency in Multi-Turn Intent Classification for LLM-Powered Dialog Systems in Production",
        "rating": "-1",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Accurate multi-turn intent classification is essential for advancing conversational AI systems. However, challenges such as the scarcity of comprehensive datasets and the complexity of contextual dependencies across dialogue turns hinder progress. This paper presents two novel approaches leveraging Large Language Models (LLMs) to enhance scalability and reduce latency in production dialogue systems. First, we introduce Symbol Tuning, which simplifies intent labels to reduce task complexity and improve performance in multi-turn dialogues. Second, we propose C-LARA (Consistency-aware, Linguistics Adaptive Retrieval Augmentation), a framework that employs LLMs for data augmentation and pseudo-labeling to generate synthetic multi-turn dialogues. These enriched datasets are used to fine-tune a small, efficient model suitable for deployment. Experiments conducted on multilingual dialogue datasets demonstrate significant improvements in classification accuracy and resource efficiency. Our methods enhance multi-turn intent classification accuracy by 5.09%, reduce annotation costs by 40%, and enable scalable deployment in low-resource multilingual industrial systems, highlighting their practicality and impact.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12350",
        "abstract url": "https://arxiv.org/abs/2411.12350",
        "title": "DiM: $f$-Divergence Minimization Guided Sharpness-Aware Optimization for Semi-supervised Medical Image Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "Medical"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "As a technique to alleviate the pressure of data annotation, semi-supervised learning (SSL) has attracted widespread attention. In the specific domain of medical image segmentation, semi-supervised methods (SSMIS) have become a research hotspot due to their ability to reduce the need for large amounts of precisely annotated data. SSMIS focuses on enhancing the model's generalization performance by leveraging a small number of labeled samples and a large number of unlabeled samples. The latest sharpness-aware optimization (SAM) technique, which optimizes the model by reducing the sharpness of the loss function, has shown significant success in SSMIS. However, SAM and its variants may not fully account for the distribution differences between different datasets. To address this issue, we propose a sharpness-aware optimization method based on $f$-divergence minimization (DiM) for semi-supervised medical image segmentation. This method enhances the model's stability by fine-tuning the sensitivity of model parameters and improves the model's adaptability to different datasets through the introduction of $f$-divergence. By reducing $f$-divergence, the DiM method not only improves the performance balance between the source and target datasets but also prevents performance degradation due to overfitting on the source dataset.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "8page"
    },
    {
        "paper id": "2411.12354",
        "abstract url": "https://arxiv.org/abs/2411.12354",
        "title": "Scalable and Effective Negative Sample Generation for Hyperedge Prediction",
        "rating": "-1",
        "keywords": [
            [
                "diffusion"
            ]
        ],
        "abstract": "Hyperedge prediction is crucial in hypergraph analysis for understanding complex multi-entity interactions in various web-based applications, including social networks and e-commerce systems. Traditional methods often face difficulties in generating high-quality negative samples due to the imbalance between positive and negative instances. To address this, we present the Scalable and Effective Negative Sample Generation for Hyperedge Prediction (SEHP) framework, which utilizes diffusion models to tackle these challenges. SEHP employs a boundary-aware loss function that iteratively refines negative samples, moving them closer to decision boundaries to improve classification performance. SEHP samples positive instances to form sub-hypergraphs for scalable batch processing. By using structural information from sub-hypergraphs as conditions within the diffusion process, SEHP effectively captures global patterns. To enhance efficiency, our approach operates directly in latent space, avoiding the need for discrete ID generation and resulting in significant speed improvements while preserving accuracy. Extensive experiments show that SEHP outperforms existing methods in accuracy, efficiency, and scalability, representing a substantial advancement in hyperedge prediction techniques. Our code is available here.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "11"
    },
    {
        "paper id": "2411.12359",
        "abstract url": "https://arxiv.org/abs/2411.12359",
        "title": "TactV: A Class of Hybrid Terrestrial/Aerial Coaxial Tilt-Rotor Vehicles",
        "rating": "-1",
        "keywords": [
            [
                "vehicle"
            ]
        ],
        "abstract": "To enhance the obstacle-crossing and endurance capabilities of vehicles operating in complex environments, this paper presents the design of a hybrid terrestrial/aerial coaxial tilt-rotor vehicle, TactV, which integrates advantages such as lightweight construction and high maneuverability. Unlike existing tandem dual-rotor vehicles, TactV employs a tiltable coaxial dual-rotor design and features a spherical cage structure that encases the body, allowing for omnidirectional movement while further reducing its overall dimensions. To enable TactV to maneuver flexibly in aerial, planar, and inclined surfaces, we established corresponding dynamic and control models for each mode. Additionally, we leveraged TactV's tiltable center of gravity to design energy-saving and high-mobility modes for ground operations, thereby further enhancing its endurance. Experimental designs for both aerial and ground tests corroborated the superiority of TactV's movement capabilities and control strategies.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12360",
        "abstract url": "https://arxiv.org/abs/2411.12360",
        "title": "An Affine Equivalence Algorithm for S-boxes based on Matrix Invariants",
        "rating": "-1",
        "keywords": [
            [
                "depth"
            ]
        ],
        "abstract": "We investigate the affine equivalence (AE) problem of S-boxes. Given two S-boxes denoted as $S_1$ and $S_2$, we aim to seek two invertible AE transformations $A,B$ such that $S_1\\circ A = B\\circ S_2$ holds. Due to important applications in the analysis and design of block ciphers, the investigation of AE algorithms has performed growing significance. In this paper, we propose zeroization on S-box firstly, and the AE problem can be transformed into $2^n$ linear equivalence problems by this zeroization operation. Secondly, we propose standard orthogonal spatial matrix (SOSM), and the rank of the SOSM is invariant under AE transformations. Finally, based on the zeroization operation and the SOSM method, we propose a depth first search (DFS) method for determining AE of S-boxes, named the AE\\_SOSM\\_DFS algorithm. Using this matrix invariant, we optimize the temporal complexity of the algorithm to approximately $\\frac{1}{2^n}$ of the complexity without SOSM. Specifically, the complexity of our algorithm is $O(2^{3n})$. In addition, we also conducted experiments with non-invertible S-boxes, and the performance is similar to that of invertible S-boxes. Moreover, our proposed algorithm can effectively handle S-boxes with low algebraic degree or certain popular S-boxes such as namely AES and ARIA\\_s2, which are difficult to be handled by the algorithm proposed by Dinur (2018). Using our algorithm, it only takes 5.5 seconds to find out that the seven popular S-boxes namely AES, ARIA\\_s2, Camellia, Chiasmus, DBlock, SEED\\_S0, and SMS4 are affine equivalent and the AE transformations of these S-boxes are provided.",
        "subjects": [
            "cs.CR",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12363",
        "abstract url": "https://arxiv.org/abs/2411.12363",
        "title": "DGSNA: prompt-based Dynamic Generative Scene-based Noise Addition method",
        "rating": "-1",
        "keywords": [
            [
                "Text-To-Audio"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "This paper addresses the challenges of accurately enumerating and describing scenes and the labor-intensive process required to replicate acoustic environments using non-generative methods. We introduce the prompt-based Dynamic Generative Sce-ne-based Noise Addition method (DGSNA), which innovatively combines the Dynamic Generation of Scene Information (DGSI) with Scene-based Noise Addition for Audio (SNAA). Employing generative chat models structured within the Back-ground-Examples-Task (BET) prompt framework, DGSI com-ponent facilitates the dynamic synthesis of tailored Scene Infor-mation (SI) for specific acoustic environments. Additionally, the SNAA component leverages Room Impulse Response (RIR) fil-ters and Text-To-Audio (TTA) systems to generate realistic, scene-based noise that can be adapted for both indoor and out-door environments. Through comprehensive experiments, the adaptability of DGSNA across different generative chat models was demonstrated. The results, assessed through both objective and subjective evaluations, show that DGSNA provides robust performance in dynamically generating precise SI and effectively enhancing scene-based noise addition capabilities, thus offering significant improvements over traditional methods in acoustic scene simulation. Our implementation and demos are available at https://dgsna.github.io.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12383",
        "abstract url": "https://arxiv.org/abs/2411.12383",
        "title": "Automatic staff reconstruction within SIMSSA proect",
        "rating": "-1",
        "keywords": [
            [
                "music"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The automatic analysis of scores has been a research topic of interest for the last few decades and still is since music databases that include musical scores are currently being created to make musical content available to the public, including scores of ancient music. For the correct analysis of music elements and their interpretation, the identification of staff lines is of key importance. In this paper, a scheme to post-process the output of a previous musical object identification system is described. This system allows the reconstruction by means of detection, tracking and interpolation of the staff lines of ancient scores from the digital Salzinnes Database. The scheme developed shows a remarkable performance on the specific task it was created for.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "15 pages"
    },
    {
        "paper id": "2411.12385",
        "abstract url": "https://arxiv.org/abs/2411.12385",
        "title": "A Stable-Set Bound and Maximal Numbers of Nash Equilibria in Bimatrix Games",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Quint and Shubik (1997) conjectured that a non-degenerate n-by-n game has at most 2^n-1 Nash equilibria in mixed strategies. The conjecture is true for n at most 4 but false for n=6 or larger. We answer it positively for the remaining case n=5, which had been open since 1999. The problem can be translated to a combinatorial question about the vertices of a pair of simple n-polytopes with 2n facets. We introduce a novel obstruction based on the index of an equilibrium, which states that equilibrium vertices belong to two equal-sized disjoint stable sets of the graph of the polytope. This bound is verified directly using the known classification of the 159,375 combinatorial types of dual neighborly polytopes in dimension 5 with 10 facets. Non-neighborly polytopes are analyzed with additional combinatorial techniques where the bound is used for their disjoint facets.",
        "subjects": [
            "cs.GT"
        ],
        "comment": "Only lemmas and theorems with separate counters, as per journal requirement. Minor changes otherwise"
    },
    {
        "paper id": "2411.12415",
        "abstract url": "https://arxiv.org/abs/2411.12415",
        "title": "Classification of Geographical Land Structure Using Convolution Neural Network and Transfer Learning",
        "rating": "-1",
        "keywords": [
            [
                "Satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Satellite imagery has dramatically revolutionized the field of geography by giving academics, scientists, and policymakers unprecedented global access to spatial data. Manual methods typically require significant time and effort to detect the generic land structure in satellite images. This study can produce a set of applications such as urban planning and development, environmental monitoring, disaster management, etc. Therefore, the research presents a methodology to minimize human labor, reducing the expenses and duration needed to identify the land structure. This article developed a deep learning-based approach to automate the process of classifying geographical land structures. We used a satellite image dataset acquired from MLRSNet. The study compared the performance of three architectures, namely CNN, ResNet-50, and Inception-v3. We used three optimizers with any model: Adam, SGD, and RMSProp. We conduct the training process for a fixed number of epochs, specifically 100 epochs, with a batch size of 64. The ResNet-50 achieved an accuracy of 76.5% with the ADAM optimizer, the Inception-v3 with RMSProp achieved an accuracy of 93.8%, and the proposed approach, CNN with RMSProp optimizer, achieved the highest level of performance and an accuracy of 94.8%. Moreover, a thorough examination of the CNN model demonstrated its exceptional accuracy, recall, and F1 scores for all categories, confirming its resilience and dependability in precisely detecting various terrain formations. The results highlight the potential of deep learning models in scene understanding, as well as their significance in efficiently identifying and categorizing land structures from satellite imagery.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12425",
        "abstract url": "https://arxiv.org/abs/2411.12425",
        "title": "Behaviour diversity in a walking and climbing centipede-like virtual creature",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "Robot controllers are often optimised for a single robot in a single environment. This approach proves brittle, as such a controller will often fail to produce sensible behavior for a new morphology or environment. In comparison, animal gaits are robust and versatile. By observing animals, and attempting to extract general principles of locomotion from their movement, we aim to design a single decentralised controller applicable to diverse morphologies and environments. The controller implements the three components 1) undulation, 2) peristalsis, and 3) leg motion, which we believe are the essential elements in most animal gaits. The controller is tested on a variety of simulated centipede-like robots. The centipede is chosen as inspiration because it moves using both body contractions and legged locomotion. For a controller to work in qualitatively different settings, it must also be able to exhibit qualitatively different behaviors. We find that six different modes of locomotion emerge from our controller in response to environmental and morphological changes. We also find that different parts of the centipede model can exhibit different modes of locomotion, simultaneously, based on local morphological features. This controller can potentially aid in the design or evolution of robots, by quickly testing the potential of a morphology, or be used to get insights about underlying locomotion principles in the centipede.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12426",
        "abstract url": "https://arxiv.org/abs/2411.12426",
        "title": "Motif Channel Opened in a White-Box: Stereo Matching via Motif Correlation Graph",
        "rating": "-1",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "Graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Real-world applications of stereo matching, such as autonomous driving, place stringent demands on both safety and accuracy. However, learning-based stereo matching methods inherently suffer from the loss of geometric structures in certain feature channels, creating a bottleneck in achieving precise detail matching. Additionally, these methods lack interpretability due to the black-box nature of deep learning. In this paper, we propose MoCha-V2, a novel learning-based paradigm for stereo matching. MoCha-V2 introduces the Motif Correlation Graph (MCG) to capture recurring textures, which are referred to as ``motifs\" within feature channels. These motifs reconstruct geometric structures and are learned in a more interpretable way. Subsequently, we integrate features from multiple frequency domains through wavelet inverse transformation. The resulting motif features are utilized to restore geometric structures in the stereo matching process. Experimental results demonstrate the effectiveness of MoCha-V2. MoCha-V2 achieved 1st place on the Middlebury benchmark at the time of its release. Code is available at https://github.com/ZYangChen/MoCha-Stereo.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12431",
        "abstract url": "https://arxiv.org/abs/2411.12431",
        "title": "CV-Cities: Advancing Cross-View Geo-Localization in Global Cities",
        "rating": "-1",
        "keywords": [
            [
                "satellite"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Cross-view geo-localization (CVGL), which involves matching and retrieving satellite images to determine the geographic location of a ground image, is crucial in GNSS-constrained scenarios. However, this task faces significant challenges due to substantial viewpoint discrepancies, the complexity of localization scenarios, and the need for global localization. To address these issues, we propose a novel CVGL framework that integrates the vision foundational model DINOv2 with an advanced feature mixer. Our framework introduces the symmetric InfoNCE loss and incorporates near-neighbor sampling and dynamic similarity sampling strategies, significantly enhancing localization accuracy. Experimental results show that our framework surpasses existing methods across multiple public and self-built datasets. To further improve globalscale performance, we have developed CV-Cities, a novel dataset for global CVGL. CV-Cities includes 223,736 ground-satellite image pairs with geolocation data, spanning sixteen cities across six continents and covering a wide range of complex scenarios, providing a challenging benchmark for CVGL. The framework trained with CV-Cities demonstrates high localization accuracy in various test cities, highlighting its strong globalization and generalization capabilities. Our datasets and codes are available at https://github.com/GaoShuang98/CVCities.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Datasets and codes are available, accepted by IEEE JSTARS"
    },
    {
        "paper id": "2411.12442",
        "abstract url": "https://arxiv.org/abs/2411.12442",
        "title": "Online RMLSA in EONs with $A^3G$: Adaptive ACO with Augmentation of Graph",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "Routing and Spectrum Assignment (RSA) represents a significant challenge within Elastic Optical Networks (EONs), particularly in dynamic traffic scenarios where the network undergoes continuous changes. Integrating multiple modulation formats transforms it into Routing Modulation Level and Spectrum Assignment (RMLSA) problem, thereby making it more challenging. Traditionally, addressing the RSA problem involved identifying a fixed number of paths and subsequently allocating spectrum among them. Numerous heuristic and metaheuristic approaches have been proposed for RSA using this two-step methodology. However, solving for routing and assignment of spectrum independently is not recommended due to their interdependencies and their impact on resource utilization, fragmentation and bandwidth blocking probability. In this paper, we propose a novel approach to solve the RMLSA problem jointly in dynamic traffic scenarios, inspired by Ant Colony Optimization (ACO). This approach involves augmenting the network into an Auxiliary Graph and transforming conventional ACO into a constraint-based ACO variant that adapts to the constraints of EONs. This adaptation also includes an adaptive initiation process and an aggressive termination strategy aimed at achieving faster convergence. Moreover, we have introduced a novel objective/fitness function, to minimize average network fragmentation while ensuring optimal spectrum resource utilization, thereby reducing overall blocking probability.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "11 pages, 10 figures"
    },
    {
        "paper id": "2411.12454",
        "abstract url": "https://arxiv.org/abs/2411.12454",
        "title": "StrTune: Data Dependence-based Code Slicing for Binary Similarity Detection with Fine-tuned Representation",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Binary Code Similarity Detection (BCSD) is significant for software security as it can address binary tasks such as malicious code snippets identification and binary patch analysis by comparing code patterns. Recently, there has been a growing focus on artificial intelligence-based approaches in BCSD due to their scalability and generalization. Because binaries are compiled with different compilation configurations, existing approaches still face notable limitations when comparing binary similarity. First, BCSD requires analysis on code behavior, and existing work claims to extract semantic, but actually still makes analysis in terms of syntax. Second, directly extracting features from assembly sequences, existing work cannot address the issues of instruction reordering and different syntax expressions caused by various compilation configurations. In this paper, we propose StrTune, which slices binary code based on data dependence and perform slice-level fine-tuning. To address the first limitation, StrTune performs backward slicing based on data dependence to capture how a value is computed along the execution. Each slice reflects the collecting semantics of the code, which is stable across different compilation configurations. StrTune introduces flow types to emphasize the independence of computations between slices, forming a graph representation. To overcome the second limitation, based on slices corresponding to the same value computation but having different syntax representation, StrTune utilizes a Siamese Network to fine-tune such pairs, making their representations closer in the feature space.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12458",
        "abstract url": "https://arxiv.org/abs/2411.12458",
        "title": "Variation between Credible and Non-Credible News Across Topics",
        "rating": "-1",
        "keywords": [
            [
                "Health"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "'Fake News' continues to undermine trust in modern journalism and politics. Despite continued efforts to study fake news, results have been conflicting. Previous attempts to analyse and combat fake news have largely focused on distinguishing fake news from truth, or differentiating between its various sub-types (such as propaganda, satire, misinformation, etc.) This paper conducts a linguistic and stylistic analysis of fake news, focusing on variation between various news topics. It builds on related work identifying features from discourse and linguistics in deception detection by analysing five distinct news topics: Economy, Entertainment, Health, Science, and Sports. The results emphasize that linguistic features vary between credible and deceptive news in each domain and highlight the importance of adapting classification tasks to accommodate variety-based stylistic and linguistic differences in order to achieve better real-world performance.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "9 pages, 1 figure"
    },
    {
        "paper id": "2411.12464",
        "abstract url": "https://arxiv.org/abs/2411.12464",
        "title": "Resolution Improvement in OFDM-based Joint Communication and Sensing through Combined Tracking and Interpolation",
        "rating": "-1",
        "keywords": [
            [
                "radar"
            ]
        ],
        "abstract": "We investigate a monostatic orthogonal frequency-division multiplexing (OFDM)-based joint communication and sensing (JCAS) system with multiple antennas for object tracking. The native resolution of OFDM sensing, and radar sensing in general, is limited by the observation time and bandwidth. In this work, we improve the resolution through interpolation methods and tracking algorithms. We verify the resolution enhancement by comparing the root mean squared error (RMSE) of the estimated range, velocity and angle and by comparing the mean Euclidean distance between the estimated and true position. We demonstrate how both a Kalman filter for tracking, and interpolation methods using zero-padding and the chirp Z-transform (CZT) improve the estimation error. We discuss the computational complexity of the different methods. We propose the KalmanCZT approach that combines tracking via Kalman filtering and interpolation via the CZT, resulting in a solution with flexible resolution that significantly improves the range RMSE.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Submitted to SCC2025"
    },
    {
        "paper id": "2411.12503",
        "abstract url": "https://arxiv.org/abs/2411.12503",
        "title": "ManiSkill-ViTac 2025: Challenge on Manipulation Skill Learning With Vision and Tactile Sensing",
        "rating": "-1",
        "keywords": [
            [
                "robotics",
                "robotic manipulation"
            ]
        ],
        "abstract": "This article introduces the ManiSkill-ViTac Challenge 2025, which focuses on learning contact-rich manipulation skills using both tactile and visual sensing. Expanding upon the 2024 challenge, ManiSkill-ViTac 2025 includes 3 independent tracks: tactile manipulation, tactile-vision fusion manipulation, and tactile sensor structure design. The challenge aims to push the boundaries of robotic manipulation skills, emphasizing the integration of tactile and visual data to enhance performance in complex, real-world tasks. Participants will be evaluated using standardized metrics across both simulated and real-world environments, spurring innovations in sensor design and significantly advancing the field of vision-tactile fusion in robotics.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Challenge webpage: https://ai-workshops.github.io/maniskill-vitac-challenge-2025/"
    },
    {
        "paper id": "2411.12514",
        "abstract url": "https://arxiv.org/abs/2411.12514",
        "title": "3D Reconstruction by Looking: Instantaneous Blind Spot Detector for Indoor SLAM through Mixed Reality",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "LiDAR",
                "SLAM"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Indoor SLAM often suffers from issues such as scene drifting, double walls, and blind spots, particularly in confined spaces with objects close to the sensors (e.g. LiDAR and cameras) in reconstruction tasks. Real-time visualization of point cloud registration during data collection may help mitigate these issues, but a significant limitation remains in the inability to in-depth compare the scanned data with actual physical environments. These challenges obstruct the quality of reconstruction products, frequently necessitating revisit and rescan efforts. For this regard, we developed the LiMRSF (LiDAR-MR-RGB Sensor Fusion) system, allowing users to perceive the in-situ point cloud registration by looking through a Mixed-Reality (MR) headset. This tailored framework visualizes point cloud meshes as holograms, seamlessly matching with the real-time scene on see-through glasses, and automatically highlights errors detected while they overlap. Such holographic elements are transmitted via a TCP server to an MR headset, where it is calibrated to align with the world coordinate, the physical location. This allows users to view the localized reconstruction product instantaneously, enabling them to quickly identify blind spots and errors, and take prompt action on-site. Our blind spot detector achieves an error detection precision with an F1 Score of 75.76% with acceptably high fidelity of monitoring through the LiMRSF system (highest SSIM of 0.5619, PSNR of 14.1004, and lowest MSE of 0.0389 in the five different sections of the simplified mesh model which users visualize through the LiMRSF device see-through glasses). This method ensures the creation of detailed, high-quality datasets for 3D models, with potential applications in Building Information Modeling (BIM) but not limited.",
        "subjects": [
            "cs.HC",
            "cs.CV",
            "cs.GR"
        ],
        "comment": "21 pages, 13 figures, 3 tables"
    },
    {
        "paper id": "2411.12547",
        "abstract url": "https://arxiv.org/abs/2411.12547",
        "title": "S3TU-Net: Structured Convolution and Superpixel Transformer for Lung Nodule Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "CT",
                "lesion"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The irregular and challenging characteristics of lung adenocarcinoma nodules in computed tomography (CT) images complicate staging diagnosis, making accurate segmentation critical for clinicians to extract detailed lesion information. In this study, we propose a segmentation model, S3TU-Net, which integrates multi-dimensional spatial connectors and a superpixel-based visual transformer. S3TU-Net is built on a multi-view CNN-Transformer hybrid architecture, incorporating superpixel algorithms, structured weighting, and spatial shifting techniques to achieve superior segmentation performance. The model leverages structured convolution blocks (DWF-Conv/D2BR-Conv) to extract multi-scale local features while mitigating overfitting. To enhance multi-scale feature fusion, we introduce the S2-MLP Link, integrating spatial shifting and attention mechanisms at the skip connections. Additionally, the residual-based superpixel visual transformer (RM-SViT) effectively merges global and local features by employing sparse correlation learning and multi-branch attention to capture long-range dependencies, with residual connections enhancing stability and computational efficiency. Experimental results on the LIDC-IDRI dataset demonstrate that S3TU-Net achieves a DSC, precision, and IoU of 89.04%, 90.73%, and 90.70%, respectively. Compared to recent methods, S3TU-Net improves DSC by 4.52% and sensitivity by 3.16%, with other metrics showing an approximate 2% increase. In addition to comparison and ablation studies, we validated the generalization ability of our model on the EPDB private dataset, achieving a DSC of 86.40%.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12560",
        "abstract url": "https://arxiv.org/abs/2411.12560",
        "title": "Topological Symmetry Enhanced Graph Convolution for Skeleton-Based Action Recognition",
        "rating": "-1",
        "keywords": [
            [
                "Skeleton"
            ],
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Skeleton-based action recognition has achieved remarkable performance with the development of graph convolutional networks (GCNs). However, most of these methods tend to construct complex topology learning mechanisms while neglecting the inherent symmetry of the human body. Additionally, the use of temporal convolutions with certain fixed receptive fields limits their capacity to effectively capture dependencies in time sequences. To address the issues, we (1) propose a novel Topological Symmetry Enhanced Graph Convolution (TSE-GC) to enable distinct topology learning across different channel partitions while incorporating topological symmetry awareness and (2) construct a Multi-Branch Deformable Temporal Convolution (MBDTC) for skeleton-based action recognition. The proposed TSE-GC emphasizes the inherent symmetry of the human body while enabling efficient learning of dynamic topologies. Meanwhile, the design of MBDTC introduces the concept of deformable modeling, leading to more flexible receptive fields and stronger modeling capacity of temporal dependencies. Combining TSE-GC with MBDTC, our final model, TSE-GCN, achieves competitive performance with fewer parameters compared with state-of-the-art methods on three large datasets, NTU RGB+D, NTU RGB+D 120, and NW-UCLA. On the cross-subject and cross-set evaluations of NTU RGB+D 120, the accuracies of our model reach 90.0\\% and 91.1\\%, with 1.1M parameters and 1.38 GFLOPS for one stream.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12582",
        "abstract url": "https://arxiv.org/abs/2411.12582",
        "title": "Reconfiguration Using Generalized Token Jumping",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "In reconfiguration, we are given two solutions to a graph problem, such as Vertex Cover or Dominating Set, with each solu tion represented by a placement of tokens on vertices of the graph. Our task is to reconfigure one into the other using small steps while ensuring the intermediate configurations of tokens are also valid solutions. The two commonly studied settings are Token Jumping and Token Sliding, which allows moving a single token to an arbitrary or an adjacent vertex, respectively. We introduce new rules that generalize Token Jumping, parameterized by the number of tokens allowed to move at once and by the maximum distance of each move. Our main contribution is identifying minimal rules that allow reconfiguring any possible given solution into any other for Independent Set, Vertex Cover, and Dominating Set. For each minimal rule, we also provide an efficient algorithm that finds a corresponding reconfiguration sequence. We further focus on the rule that allows each token to move to an adjacent vertex in a single step. This natural variant turns out to be the minimal rule that guarantees reconfigurability for Vertex Cover. We determine the computational complexity of deciding whether a (shortest) reconfiguration sequence exists under this rule for the three studied problems. While reachability for Vertex Cover is shown to be in P, finding a shortest sequence is shown to be NP-complete. For Independent Set and Dominating Set, even reachability is shown to be PSPACE-complete.",
        "subjects": [
            "math.CO",
            "cs.DS"
        ],
        "comment": "To appear at WALCOM 2025"
    },
    {
        "paper id": "2411.12602",
        "abstract url": "https://arxiv.org/abs/2411.12602",
        "title": "SAM Carries the Burden: A Semi-Supervised Approach Refining Pseudo Labels for Medical Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "Medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Semantic segmentation is a crucial task in medical imaging. Although supervised learning techniques have proven to be effective in performing this task, they heavily depend on large amounts of annotated training data. The recently introduced Segment Anything Model (SAM) enables prompt-based segmentation and offers zero-shot generalization to unfamiliar objects. In our work, we leverage SAM's abstract object understanding for medical image segmentation to provide pseudo labels for semi-supervised learning, thereby mitigating the need for extensive annotated training data. Our approach refines initial segmentations that are derived from a limited amount of annotated data (comprising up to 43 cases) by extracting bounding boxes and seed points as prompts forwarded to SAM. Thus, it enables the generation of dense segmentation masks as pseudo labels for unlabelled data. The results show that training with our pseudo labels yields an improvement in Dice score from $74.29\\,\\%$ to $84.17\\,\\%$ and from $66.63\\,\\%$ to $74.87\\,\\%$ for the segmentation of bones of the paediatric wrist and teeth in dental radiographs, respectively. As a result, our method outperforms intensity-based post-processing methods, state-of-the-art supervised learning for segmentation (nnU-Net), and the semi-supervised mean teacher approach. Our Code is available on GitHub.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Presented at MICCAI Workshop on Advancing Data Solutions in Medical Imaging AI 2024; Code and data: https://github.com/multimodallearning/SamCarriesTheBurden"
    },
    {
        "paper id": "2411.12604",
        "abstract url": "https://arxiv.org/abs/2411.12604",
        "title": "SG-LRA: Self-Generating Automatic Scoliosis Cobb Angle Measurement with Low-Rank Approximation",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "X-ray"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Automatic Cobb angle measurement from X-ray images is crucial for scoliosis screening and diagnosis. However, most existing regression-based methods and segmentation-based methods struggle with inaccurate spine representations or mask connectivity/fragmentation issues. Besides, landmark-based methods suffer from insufficient training data and annotations. To address these challenges, we propose a novel framework including Self-Generation pipeline and Low-Rank Approximation representation (SG-LRA) for automatic Cobb angle measurement. Specifically, we propose a parameterized spine contour representation based on LRA, which enables eigen-spine decomposition and spine contour reconstruction. We can directly obtain spine contour with only regressed LRA coefficients, which form a more accurate spine representation than rectangular boxes. Also, we combine LRA coefficient regression with anchor box classification to solve inaccurate predictions and mask connectivity issues. Moreover, we develop a data engine with automatic annotation and automatic selection in an iterative manner, which is trained on a private Spinal2023 dataset. With our data engine, we generate the largest scoliosis X-ray dataset named Spinal-AI2024 largely without privacy leaks. Extensive experiments on public AASCE2019, private Spinal2023, and generated Spinal-AI2024 datasets demonstrate that our method achieves state-of-the-art Cobb angle measurement performance. Our code and Spinal-AI2024 dataset are available at https://github.com/Ernestchenchen/SG-LRA and https://github.com/Ernestchenchen/Spinal-AI2024, respectively.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12606",
        "abstract url": "https://arxiv.org/abs/2411.12606",
        "title": "Generation of Cycle Permutation Graphs and Permutation Snarks",
        "rating": "-1",
        "keywords": [
            [
                "Graphs"
            ]
        ],
        "abstract": "We present an algorithm for the efficient generation of all pairwise non-isomorphic cycle permutation graphs, i.e. cubic graphs with a $2$-factor consisting of two chordless cycles, and non-hamiltonian cycle permutation graphs, from which the permutation snarks can easily be computed. This allows us to generate all cycle permutation graphs up to order $34$ and all permutation snarks up to order $46$, improving upon previous computational results by Brinkmann et al. Moreover, we give several improved lower bounds for interesting permutation snarks, such as for a smallest permutation snark of order $6 \\bmod 8$ or a smallest permutation snark of girth at least $6$. These computational results also allow us to complete a characterisation of the orders for which non-hamiltonian cycle permutation graphs exist, answering an open question by Klee from 1972, and yield many more counterexamples to a conjecture by Zhang.",
        "subjects": [
            "math.CO",
            "cs.DM"
        ],
        "comment": "22 pages"
    },
    {
        "paper id": "2411.12615",
        "abstract url": "https://arxiv.org/abs/2411.12615",
        "title": "A Multimodal Approach Combining Structural and Cross-domain Textual Guidance for Weakly Supervised OCT Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "diagnosing",
                "retinal",
                "lesion"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Accurate segmentation of Optical Coherence Tomography (OCT) images is crucial for diagnosing and monitoring retinal diseases. However, the labor-intensive nature of pixel-level annotation limits the scalability of supervised learning with large datasets. Weakly Supervised Semantic Segmentation (WSSS) provides a promising alternative by leveraging image-level labels. In this study, we propose a novel WSSS approach that integrates structural guidance with text-driven strategies to generate high-quality pseudo labels, significantly improving segmentation performance. In terms of visual information, our method employs two processing modules that exchange raw image features and structural features from OCT images, guiding the model to identify where lesions are likely to occur. In terms of textual information, we utilize large-scale pretrained models from cross-domain sources to implement label-informed textual guidance and synthetic descriptive integration with two textual processing modules that combine local semantic features with consistent synthetic descriptions. By fusing these visual and textual components within a multimodal framework, our approach enhances lesion localization accuracy. Experimental results on three OCT datasets demonstrate that our method achieves state-of-the-art performance, highlighting its potential to improve diagnostic accuracy and efficiency in medical imaging.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "21 pages, 9 figures, 8 tables"
    },
    {
        "paper id": "2411.12643",
        "abstract url": "https://arxiv.org/abs/2411.12643",
        "title": "DLBacktrace: A Model Agnostic Explainability for any Deep Learning Models",
        "rating": "-1",
        "keywords": [
            [
                "tabular"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The rapid advancement of artificial intelligence has led to increasingly sophisticated deep learning models, which frequently operate as opaque 'black boxes' with limited transparency in their decision-making processes. This lack of interpretability presents considerable challenges, especially in high-stakes applications where understanding the rationale behind a model's outputs is as essential as the outputs themselves. This study addresses the pressing need for interpretability in AI systems, emphasizing its role in fostering trust, ensuring accountability, and promoting responsible deployment in mission-critical fields. To address the interpretability challenge in deep learning, we introduce DLBacktrace, an innovative technique developed by the AryaXAI team to illuminate model decisions across a wide array of domains, including simple Multi Layer Perceptron (MLPs), Convolutional Neural Networks (CNNs), Large Language Models (LLMs), Computer Vision Models, and more. We provide a comprehensive overview of the DLBacktrace algorithm and present benchmarking results, comparing its performance against established interpretability methods, such as SHAP, LIME, GradCAM, Integrated Gradients, SmoothGrad, and Attention Rollout, using diverse task-based metrics. The proposed DLBacktrace technique is compatible with various model architectures built in PyTorch and TensorFlow, supporting models like Llama 3.2, other NLP architectures such as BERT and LSTMs, computer vision models like ResNet and U-Net, as well as custom deep neural network (DNN) models for tabular data. This flexibility underscores DLBacktrace's adaptability and effectiveness in enhancing model transparency across a broad spectrum of applications. The library is open-sourced and available at https://github.com/AryaXAI/DLBacktrace .",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12670",
        "abstract url": "https://arxiv.org/abs/2411.12670",
        "title": "Reconstructing Graph Signals from Noisy Dynamical Samples",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "We investigate the dynamical sampling space-time trade-off problem within a graph setting. Specifically, we derive necessary and sufficient conditions for space-time sampling that enable the reconstruction of an initial band-limited signal on a graph. Additionally, we develop and test numerical algorithms for approximating the optimal placement of sensors on the graph to minimize the mean squared error when recovering signals from time-space measurements corrupted by i.i.d.~additive noise. Our numerical experiments demonstrate that our approach outperforms previously proposed algorithms for related problems.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12674",
        "abstract url": "https://arxiv.org/abs/2411.12674",
        "title": "OrigamiPlot: An R Package and Shiny Web App Enhanced Visualizations for Multivariate Data",
        "rating": "-1",
        "keywords": [
            [
                "radar"
            ]
        ],
        "abstract": "We introduce OrigamiPlot, an open-source R package and Shiny web application designed to enhance the visualization of multivariate data. This package implements the origami plot, a novel visualization technique proposed by Duan et al. in 2023, which improves upon traditional radar charts by ensuring that the area of the connected region is invariant to the ordering of attributes, addressing a key limitation of radar charts. The software facilitates multivariate decision-making by supporting comparisons across multiple objects and attributes, offering customizable features such as auxiliary axes and weighted attributes for enhanced clarity. Through the R package and user-friendly Shiny interface, researchers can efficiently create and customize plots without requiring extensive programming knowledge. Demonstrated using network meta-analysis as a real-world example, OrigamiPlot proves to be a versatile tool for visualizing multivariate data across various fields. This package opens new opportunities for simplifying decision-making processes with complex data.",
        "subjects": [
            "cs.HC",
            "stat.ME"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12685",
        "abstract url": "https://arxiv.org/abs/2411.12685",
        "title": "Enhanced Sign Language Translation between American Sign Language (ASL) and Indian Sign Language (ISL) Using LLMs",
        "rating": "-1",
        "keywords": [
            [
                "Sign Language"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "We have come up with a research that hopes to provide a bridge between the users of American Sign Language and the users of spoken language and Indian Sign Language (ISL). The research enabled us to create a novel framework that we have developed for Learner Systems. Leveraging art of Large models to create key features including: - Real-time translation between these two sign languages in an efficient manner. Making LLM's capability available for seamless translations to ISL. Here is the full study showing its implementation in this paper. The core of the system is a sophisticated pipeline that begins with reclassification and recognition of ASL gestures based on a strong Random Forest Classifier. By recognizing the ASL, it is translated into text which can be more easily processed. Highly evolved natural language NLP (Natural Language Processing) techniques come in handy as they play a role in our LLM integration where you then use LLMs to be able to convert the ASL text to ISL which provides you with the intent of sentence or phrase. The final step is to synthesize the translated text back into ISL gestures, creating an end-to-end translation experience using RIFE-Net. This framework is tasked with key challenges such as automatically dealing with gesture variability and overcoming the linguistic differences between ASL and ISL. By automating the translation process, we hope to vastly improve accessibility for sign language users. No longer will the communication gap between ASL and ISL create barriers; this totally cool innovation aims to bring our communities closer together. And we believe, with full confidence in our framework, that we're able to apply the same principles across a wide variety of sign language dialects.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12694",
        "abstract url": "https://arxiv.org/abs/2411.12694",
        "title": "Local Density and its Distributed Approximation",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "The densest subgraph problem is a classic problem in combinatorial optimisation. Danisch, Chan, and Sozio propose a definition for \\emph{local density} that assigns to each vertex $v$ a value $\u03c1^*(v)$. This local density is a generalisation of the maximum subgraph density of a graph. I.e., if $\u03c1(G)$ is the subgraph density of a finite graph $G$, then $\u03c1(G)$ equals the maximum local density $\u03c1^*(v)$ over vertices $v$ in $G$. They approximate the local density of each vertex with no theoretical (asymptotic) guarantees. We provide an extensive study of this local density measure. Just as with (global) maximum subgraph density, we show that there is a dual relation between the local out-degrees and the minimum out-degree orientations of the graph. We introduce the definition of the local out-degree $g^*(v)$ of a vertex $v$, and show it to be equal to the local density $\u03c1^*(v)$. We consider the local out-degree to be conceptually simpler, shorter to define, and easier to compute. Using the local out-degree we show a previously unknown fact: that existing algorithms already dynamically approximate the local density. Next, we provide the first distributed algorithms that compute the local density with provable guarantees: given any $\\varepsilon$ such that $\\varepsilon^{-1} \\in O(poly \\, n)$, we show a deterministic distributed algorithm in the LOCAL model where, after $O(\\varepsilon^{-2} \\log^2 n)$ rounds, every vertex $v$ outputs a $(1 + \\varepsilon)$-approximation of their local density $\u03c1^*(v)$. In CONGEST, we show a deterministic distributed algorithm that requires $\\text{poly}(\\log n,\\varepsilon^{-1}) \\cdot 2^{O(\\sqrt{\\log n})}$ rounds, which is sublinear in $n$. As a corollary, we obtain the first deterministic algorithm running in a sublinear number of rounds for $(1+\\varepsilon)$-approximate densest subgraph detection in the CONGEST model.",
        "subjects": [
            "cs.DS",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12709",
        "abstract url": "https://arxiv.org/abs/2411.12709",
        "title": "Dimensions of Generative AI Evaluation Design",
        "rating": "-1",
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.CY"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "There are few principles or guidelines to ensure evaluations of generative AI (GenAI) models and systems are effective. To help address this gap, we propose a set of general dimensions that capture critical choices involved in GenAI evaluation design. These dimensions include the evaluation setting, the task type, the input source, the interaction style, the duration, the metric type, and the scoring method. By situating GenAI evaluations within these dimensions, we aim to guide decision-making during GenAI evaluation design and provide a structure for comparing different evaluations. We illustrate the utility of the proposed set of general dimensions using two examples: a hypothetical evaluation of the fairness of a GenAI system and three real-world GenAI evaluations of biological threats.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "NeurIPS 2024 Workshop on Evaluating Evaluations (EvalEval)"
    },
    {
        "paper id": "2411.12712",
        "abstract url": "https://arxiv.org/abs/2411.12712",
        "title": "Enhancing Multi-Class Disease Classification: Neoplasms, Cardiovascular, Nervous System, and Digestive Disorders Using Advanced LLMs",
        "rating": "-1",
        "keywords": [
            [
                "BioBERT",
                "Medical",
                "cancer",
                "Disease"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In this research, we explored the improvement in terms of multi-class disease classification via pre-trained language models over Medical-Abstracts-TC-Corpus that spans five medical conditions. We excluded non-cancer conditions and examined four specific diseases. We assessed four LLMs, BioBERT, XLNet, and BERT, as well as a novel base model (Last-BERT). BioBERT, which was pre-trained on medical data, demonstrated superior performance in medical text classification (97% accuracy). Surprisingly, XLNet followed closely (96% accuracy), demonstrating its generalizability across domains even though it was not pre-trained on medical data. LastBERT, a custom model based on the lighter version of BERT, also proved competitive with 87.10% accuracy (just under BERT's 89.33%). Our findings confirm the importance of specialized models such as BioBERT and also support impressions around more general solutions like XLNet and well-tuned transformer architectures with fewer parameters (in this case, LastBERT) in medical domain tasks.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "7 Pages, 4 tables and 11 figures. Under review in a IEEE conference"
    },
    {
        "paper id": "2411.12719",
        "abstract url": "https://arxiv.org/abs/2411.12719",
        "title": "Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation",
        "rating": "-1",
        "keywords": [
            [
                "Text-to-Speech"
            ],
            [
                "cs.LG",
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Despite rapid advancements in TTS models, a consistent and robust human evaluation framework is still lacking. For example, MOS tests fail to differentiate between similar models, and CMOS's pairwise comparisons are time-intensive. The MUSHRA test is a promising alternative for evaluating multiple TTS systems simultaneously, but in this work we show that its reliance on matching human reference speech unduly penalises the scores of modern TTS systems that can exceed human speech quality. More specifically, we conduct a comprehensive assessment of the MUSHRA test, focusing on its sensitivity to factors such as rater variability, listener fatigue, and reference bias. Based on our extensive evaluation involving 471 human listeners across Hindi and Tamil we identify two primary shortcomings: (i) reference-matching bias, where raters are unduly influenced by the human reference, and (ii) judgement ambiguity, arising from a lack of clear fine-grained guidelines. To address these issues, we propose two refined variants of the MUSHRA test. The first variant enables fairer ratings for synthesized samples that surpass human reference quality. The second variant reduces ambiguity, as indicated by the relatively lower variance across raters. By combining these approaches, we achieve both more reliable and more fine-grained assessments. We also release MANGO, a massive dataset of 47,100 human ratings, the first-of-its-kind collection for Indian languages, aiding in analyzing human preferences and developing automatic metrics for evaluating TTS systems.",
        "subjects": [
            "cs.CL",
            "cs.LG",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "19 pages, 12 Figures"
    },
    {
        "paper id": "2411.12721",
        "abstract url": "https://arxiv.org/abs/2411.12721",
        "title": "An AI-Enabled Side Channel Power Analysis Based Hardware Trojan Detection Method for Securing the Integrated Circuits in Cyber-Physical Systems",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Cyber-physical systems rely on sensors, communication, and computing, all powered by integrated circuits (ICs). ICs are largely susceptible to various hardware attacks with malicious intents. One of the stealthiest threats is the insertion of a hardware trojan into the IC, causing the circuit to malfunction or leak sensitive information. Due to supply chain vulnerabilities, ICs face risks of trojan insertion during various design and fabrication stages. These trojans typically remain inactive until triggered. Once triggered, trojans can severely compromise system safety and security. This paper presents a non-invasive method for hardware trojan detection based on side-channel power analysis. We utilize the dynamic power measurements for twelve hardware trojans from IEEE DataPort. Our approach applies to signal processing techniques to extract crucial time-domain and frequency-domain features from the power traces, which are then used for trojan detection leveraging Artificial Intelligence (AI) models. Comparison with a baseline detection approach indicates that our approach achieves higher detection accuracy than the baseline models used on the same side-channel power dataset.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "19 pages, 7 figures"
    },
    {
        "paper id": "2411.12786",
        "abstract url": "https://arxiv.org/abs/2411.12786",
        "title": "Off-policy estimation with adaptively collected data: the power of online learning",
        "rating": "-1",
        "keywords": [
            [
                "tabular"
            ],
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We consider estimation of a linear functional of the treatment effect using adaptively collected data. This task finds a variety of applications including the off-policy evaluation (\\textsf{OPE}) in contextual bandits, and estimation of the average treatment effect (\\textsf{ATE}) in causal inference. While a certain class of augmented inverse propensity weighting (\\textsf{AIPW}) estimators enjoys desirable asymptotic properties including the semi-parametric efficiency, much less is known about their non-asymptotic theory with adaptively collected data. To fill in the gap, we first establish generic upper bounds on the mean-squared error of the class of AIPW estimators that crucially depends on a sequentially weighted error between the treatment effect and its estimates. Motivated by this, we also propose a general reduction scheme that allows one to produce a sequence of estimates for the treatment effect via online learning to minimize the sequentially weighted estimation error. To illustrate this, we provide three concrete instantiations in (\\romannumeral 1) the tabular case; (\\romannumeral 2) the case of linear function approximation; and (\\romannumeral 3) the case of general function approximation for the outcome model. We then provide a local minimax lower bound to show the instance-dependent optimality of the \\textsf{AIPW} estimator using no-regret online learning algorithms.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.OC",
            "math.ST"
        ],
        "comment": "37 pages. Accepted to the 38th Annual Conference on Neural Information Processing Systems (NeurIPS 2024), Vancouver, British Columbia, Canada"
    },
    {
        "paper id": "2411.12791",
        "abstract url": "https://arxiv.org/abs/2411.12791",
        "title": "Mitigating Perception Bias: A Training-Free Approach to Enhance LMM for Image Quality Assessment",
        "rating": "-1",
        "keywords": [
            [
                "Quality Assessment"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Despite the impressive performance of large multimodal models (LMMs) in high-level visual tasks, their capacity for image quality assessment (IQA) remains limited. One main reason is that LMMs are primarily trained for high-level tasks (e.g., image captioning), emphasizing unified image semantics extraction under varied quality. Such semantic-aware yet quality-insensitive perception bias inevitably leads to a heavy reliance on image semantics when those LMMs are forced for quality rating. In this paper, instead of retraining or tuning an LMM costly, we propose a training-free debiasing framework, in which the image quality prediction is rectified by mitigating the bias caused by image semantics. Specifically, we first explore several semantic-preserving distortions that can significantly degrade image quality while maintaining identifiable semantics. By applying these specific distortions to the query or test images, we ensure that the degraded images are recognized as poor quality while their semantics remain. During quality inference, both a query image and its corresponding degraded version are fed to the LMM along with a prompt indicating that the query image quality should be inferred under the condition that the degraded one is deemed poor quality.This prior condition effectively aligns the LMM's quality perception, as all degraded images are consistently rated as poor quality, regardless of their semantic difference.Finally, the quality scores of the query image inferred under different prior conditions (degraded versions) are aggregated using a conditional probability model. Extensive experiments on various IQA datasets show that our debiasing framework could consistently enhance the LMM performance and the code will be publicly available.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12814",
        "abstract url": "https://arxiv.org/abs/2411.12814",
        "title": "Interactive Medical Image Segmentation: A Benchmark Dataset and Baseline",
        "rating": "-1",
        "keywords": [
            [
                "Medical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Interactive Medical Image Segmentation (IMIS) has long been constrained by the limited availability of large-scale, diverse, and densely annotated datasets, which hinders model generalization and consistent evaluation across different models. In this paper, we introduce the IMed-361M benchmark dataset, a significant advancement in general IMIS research. First, we collect and standardize over 6.4 million medical images and their corresponding ground truth masks from multiple data sources. Then, leveraging the strong object recognition capabilities of a vision foundational model, we automatically generated dense interactive masks for each image and ensured their quality through rigorous quality control and granularity management. Unlike previous datasets, which are limited by specific modalities or sparse annotations, IMed-361M spans 14 modalities and 204 segmentation targets, totaling 361 million masks-an average of 56 masks per image. Finally, we developed an IMIS baseline network on this dataset that supports high-quality mask generation through interactive inputs, including clicks, bounding boxes, text prompts, and their combinations. We evaluate its performance on medical image segmentation tasks from multiple perspectives, demonstrating superior accuracy and scalability compared to existing interactive segmentation models. To facilitate research on foundational models in medical computer vision, we release the IMed-361M and model at https://github.com/uni-medical/IMIS-Bench.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12846",
        "abstract url": "https://arxiv.org/abs/2411.12846",
        "title": "Towards Fairness in AI for Melanoma Detection: Systemic Review and Recommendations",
        "rating": "-1",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.CY",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Early and accurate melanoma detection is crucial for improving patient outcomes. Recent advancements in artificial intelligence AI have shown promise in this area, but the technologys effectiveness across diverse skin tones remains a critical challenge. This study conducts a systematic review and preliminary analysis of AI based melanoma detection research published between 2013 and 2024, focusing on deep learning methodologies, datasets, and skin tone representation. Our findings indicate that while AI can enhance melanoma detection, there is a significant bias towards lighter skin tones. To address this, we propose including skin hue in addition to skin tone as represented by the LOreal Color Chart Map for a more comprehensive skin tone assessment technique. This research highlights the need for diverse datasets and robust evaluation metrics to develop AI models that are equitable and effective for all patients. By adopting best practices outlined in a PRISMA Equity framework tailored for healthcare and melanoma detection, we can work towards reducing disparities in melanoma outcomes.",
        "subjects": [
            "cs.CY",
            "cs.CV",
            "cs.HC",
            "eess.IV"
        ],
        "comment": "22 pages, 4 figures, 7 tables,accepted for publication in Future of Information and Communication Conference (FICC) 2025, whose proceedings will be published in 'Lecture Notes in Networks and Systems' by Springer Nature"
    },
    {
        "paper id": "2411.12863",
        "abstract url": "https://arxiv.org/abs/2411.12863",
        "title": "On corona of Konig-Egervary graphs",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "Let $\u03b1(G)$ denote the cardinality of a maximum independent set and $\u03bc(G)$ be the size of a maximum matching of a graph $G=\\left( V,E\\right) $. If $\u03b1(G)+\u03bc(G)=\\left\\vert V\\right\\vert $, then $G$ is a K\u00f6nig-Egerv\u00e1ry graph, and $G$ is a $1$-K\u00f6nig-Egerv\u00e1ry graph whenever $\u03b1(G)+\u03bc(G)=\\left\\vert V\\right\\vert -1$. The corona $H\\circ\\mathcal{X}$ of a graph $H$ and a family of graphs $\\mathcal{X}=\\left\\{ X_{i}:1\\leq i\\leq\\left\\vert V(H)\\right\\vert \\right\\} $ is obtained by joining each vertex $v_{i}$ of $H$ to all the vertices of the corresponding graph $X_{i},i=1,2,...,\\left\\vert V(H)\\right\\vert $. In this paper we completely characterize graphs whose coronas are $k$-K\u00f6nig-Egerv\u00e1ry graphs, where $k\\in\\left\\{ 0,1\\right\\} $.",
        "subjects": [
            "math.CO",
            "cs.DM"
        ],
        "comment": "11 pages, 3 figures"
    },
    {
        "paper id": "2411.12865",
        "abstract url": "https://arxiv.org/abs/2411.12865",
        "title": "AzSLD: Azerbaijani Sign Language Dataset for Fingerspelling, Word, and Sentence Translation with Baseline Software",
        "rating": "-1",
        "keywords": [
            [
                "Sign Language"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "Sign language processing technology development relies on extensive and reliable datasets, instructions, and ethical guidelines. We present a comprehensive Azerbaijani Sign Language Dataset (AzSLD) collected from diverse sign language users and linguistic parameters to facilitate advancements in sign recognition and translation systems and support the local sign language community. The dataset was created within the framework of a vision-based AzSL translation project. This study introduces the dataset as a summary of the fingerspelling alphabet and sentence- and word-level sign language datasets. The dataset was collected from signers of different ages, genders, and signing styles, with videos recorded from two camera angles to capture each sign in full detail. This approach ensures robust training and evaluation of gesture recognition models. AzSLD contains 30,000 videos, each carefully annotated with accurate sign labels and corresponding linguistic translations. The dataset is accompanied by technical documentation and source code to facilitate its use in training and testing. This dataset offers a valuable resource of labeled data for researchers and developers working on sign language recognition, translation, or synthesis. Ethical guidelines were strictly followed throughout the project, with all participants providing informed consent for collecting, publishing, and using the data.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12874",
        "abstract url": "https://arxiv.org/abs/2411.12874",
        "title": "Residual Vision Transformer (ResViT) Based Self-Supervised Learning Model for Brain Tumor Classification",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "MRI",
                "Tumor"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Deep learning has proven very promising for interpreting MRI in brain tumor diagnosis. However, deep learning models suffer from a scarcity of brain MRI datasets for effective training. Self-supervised learning (SSL) models provide data-efficient and remarkable solutions to limited dataset problems. Therefore, this paper introduces a generative SSL model for brain tumor classification in two stages. The first stage is designed to pre-train a Residual Vision Transformer (ResViT) model for MRI synthesis as a pretext task. The second stage includes fine-tuning a ResViT-based classifier model as a downstream task. Accordingly, we aim to leverage local features via CNN and global features via ViT, employing a hybrid CNN-transformer architecture for ResViT in pretext and downstream tasks. Moreover, synthetic MRI images are utilized to balance the training set. The proposed model performs on public BraTs 2023, Figshare, and Kaggle datasets. Furthermore, we compare the proposed model with various deep learning models, including A-UNet, ResNet-9, pix2pix, pGAN for MRI synthesis, and ConvNeXtTiny, ResNet101, DenseNet12, Residual CNN, ViT for classification. According to the results, the proposed model pretraining on the MRI dataset is superior compared to the pretraining on the ImageNet dataset. Overall, the proposed model attains the highest accuracy, achieving 90.56% on the BraTs dataset with T1 sequence, 98.53% on the Figshare, and 98.47% on the Kaggle brain tumor datasets. As a result, the proposed model demonstrates a robust, effective, and successful approach to handling insufficient dataset challenges in MRI analysis by incorporating SSL, fine-tuning, data augmentation, and combining CNN and ViT.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12899",
        "abstract url": "https://arxiv.org/abs/2411.12899",
        "title": "Adaptive Control Barrier Functions with Vanishing Conservativeness Under Persistency of Excitation",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "This article presents a closed-form adaptive controlbarrier-function (CBF) approach for satisfying state constraints in systems with parametric uncertainty. This approach uses a sampled-data recursive-least-squares algorithm to estimate the unknown model parameters and construct a nonincreasing upper bound on the norm of the estimation error. Together, this estimate and upper bound are used to construct a CBF-based constraint that has nonincreasing conservativeness. Furthermore, if a persistency of excitation condition is satisfied, then the CBFbased constraint has vanishing conservativeness in the sense that the CBF-based constraint converges to the ideal constraint corresponding to the case where the uncertainty is known. In addition, the approach incorporates a monotonically improving estimate of the unknown model parameters thus, this estimate can be effectively incorporated into a desired control law. We demonstrate constraint satisfaction and performance using 2 two numerical examples, namely, a nonlinear pendulum and a nonholonomic robot.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "8 pages, 11 figures , submitted for conference"
    },
    {
        "paper id": "2411.12901",
        "abstract url": "https://arxiv.org/abs/2411.12901",
        "title": "Signformer is all you need: Towards Edge AI for Sign Language",
        "rating": "-1",
        "keywords": [
            [
                "Sign Language"
            ],
            [
                "cs.LG",
                "cs.CY",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Sign language translation, especially in gloss-free paradigm, is confronting a dilemma of impracticality and unsustainability due to growing resource-intensive methodologies. Contemporary state-of-the-arts (SOTAs) have significantly hinged on pretrained sophiscated backbones such as Large Language Models (LLMs), embedding sources, or extensive datasets, inducing considerable parametric and computational inefficiency for sustainable use in real-world scenario. Despite their success, following this research direction undermines the overarching mission of this domain to create substantial value to bridge hard-hearing and common populations. Committing to the prevailing trend of LLM and Natural Language Processing (NLP) studies, we pursue a profound essential change in architecture to achieve ground-up improvements without external aid from pretrained models, prior knowledge transfer, or any NLP strategies considered not-from-scratch. Introducing Signformer, a from-scratch Feather-Giant transforming the area towards Edge AI that redefines extremities of performance and efficiency with LLM-competence and edgy-deployable compactness. In this paper, we present nature analysis of sign languages to inform our algorithmic design and deliver a scalable transformer pipeline with convolution and attention novelty. We achieve new 2nd place on leaderboard with a parametric reduction of 467-1807x against the finests as of 2024 and outcompete almost every other methods in a lighter configuration of 0.57 million parameters.",
        "subjects": [
            "cs.CL",
            "cs.CV",
            "cs.CY",
            "cs.HC",
            "cs.LG"
        ],
        "comment": "Official Code at: https://github.com/EtaEnding/Signformer/tree/main"
    },
    {
        "paper id": "2411.12911",
        "abstract url": "https://arxiv.org/abs/2411.12911",
        "title": "On large Sidon sets",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "A Sidon set $M$ is a subset of $\\mathbb{F}_2^t$ such that the sum of four distinct elements of $M$ is never 0. The goal is to find Sidon sets of large size. In this note we show that the graphs of almost perfect nonlinear (APN) functions with high linearity can be used to construct large Sidon sets. Thanks to recently constructed APN functions $\\mathbb{F}_2^8\\to \\mathbb{F}_2^8$ with high linearity, we can construct Sidon sets of size 192 in $\\mathbb{F}_2^{15}$, where the largest sets so far had size 152. Using the inverse and the Dobbertin function also gives larger Sidon sets as previously known. Moreover, we improve the upper bound for the linearity of arbitrary APN functions.",
        "subjects": [
            "math.CO",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12955",
        "abstract url": "https://arxiv.org/abs/2411.12955",
        "title": "Matrix-Scheduling of QSR-Dissipative Systems",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "This paper considers gain-scheduling of QSR-dissipative subsystems using scheduling matrices. The corresponding QSR-dissipative properties of the overall matrix-gain-scheduled system, which depends on the QSR properties of the subsystems scheduled, are explicitly derived. The use of scheduling matrices is a generalization of the scalar scheduling signals used in the literature, and allows for greater design freedom when scheduling systems, such as in the case of gain-scheduled control. Furthermore, this work extends the existing gain-scheduling results to a broader class of QSR-dissipative systems. The matrix-scheduling of important special cases, such as passive, input strictly passive, output strictly passive, finite L2 gain, very strictly passive, and conic systems are presented. The proposed gain-scheduling architecture is used in the context of controlling a planar three-link robot subject to model uncertainty. A novel control synthesis technique is used to design QSR-dissipative subcontrollers that are gain-scheduled using scheduling matrices. Numerical simulation results highlight the greater design freedom of scheduling matrices, leading to improved performance.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Submitted to IEEE Transactions on Automatic Control (TAC)"
    },
    {
        "paper id": "2411.12960",
        "abstract url": "https://arxiv.org/abs/2411.12960",
        "title": "I Can Tell What I am Doing: Toward Real-World Natural Language Grounding of Robot Experiences",
        "rating": "-1",
        "keywords": [
            [
                "Robot"
            ]
        ],
        "abstract": "Understanding robot behaviors and experiences through natural language is crucial for developing intelligent and transparent robotic systems. Recent advancement in large language models (LLMs) makes it possible to translate complex, multi-modal robotic experiences into coherent, human-readable narratives. However, grounding real-world robot experiences into natural language is challenging due to many reasons, such as multi-modal nature of data, differing sample rates, and data volume. We introduce RONAR, an LLM-based system that generates natural language narrations from robot experiences, aiding in behavior announcement, failure analysis, and human interaction to recover failure. Evaluated across various scenarios, RONAR outperforms state-of-the-art methods and improves failure recovery efficiency. Our contributions include a multi-modal framework for robot experience narration, a comprehensive real-robot dataset, and empirical evidence of RONAR's effectiveness in enhancing user experience in system transparency and failure analysis.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12976",
        "abstract url": "https://arxiv.org/abs/2411.12976",
        "title": "Oblivious Algorithms for Maximum Directed Cut: New Upper and Lower Bounds",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "In the maximum directed cut problem, the input is a directed graph $G=(V,E)$, and the goal is to pick a partition $V = S \\cup (V \\setminus S)$ of the vertices such that as many edges as possible go from $S$ to $V\\setminus S$. Oblivious algorithms, introduced by Feige and Jozeph (Algorithmica'17), are a simple class of algorithms for this problem. These algorithms independently and randomly assign each vertex $v$ to either $S$ or $V \\setminus S$, and the distribution of $v$'s assignment is determined using only extremely local information about $v$: its bias, i.e., the relative difference between its out- and in-degrees. These algorithms have natural implementations in certain graph streaming models, where they have important implications (Saxena, Singer, Sudan, and Velusamy, SODA'23, FOCS'23, Kallaugher, Parekh, and Voronova, STOC'24). In this work, we narrow the gap between upper and lower bounds on the best approximation ratio achievable by oblivious algorithms for Max-Directed-Cut. We show that there exists an oblivious algorithm achieving an approximation ratio of at least $0.4853$, while every oblivious algorithm obeying a natural symmetry property achieves an approximation ratio of at most $0.4889$. The previous known bounds were $0.4844$ and $0.4899$, due to Singer (APPROX'23) and Feige and Jozeph, respectively. Our techniques involve designing principled parameterizations of the spaces of algorithms and lower bounds and then executing computer searches through these spaces.",
        "subjects": [
            "cs.DS"
        ],
        "comment": "17 pages, 7 figures"
    },
    {
        "paper id": "2411.12985",
        "abstract url": "https://arxiv.org/abs/2411.12985",
        "title": "Disco Intelligent Omni-Surfaces: 360-degree Fully-Passive Jamming Attacks",
        "rating": "-1",
        "keywords": [
            [
                "Attacks"
            ]
        ],
        "abstract": "Intelligent omni-surfaces (IOSs) with 360-degree electromagnetic radiation significantly improves the performance of wireless systems, while an adversarial IOS also poses a significant potential risk for physical layer security. In this paper, we propose a \"DISCO\" IOS (DIOS) based fully-passive jammer (FPJ) that can launch omnidirectional fully-passive jamming attacks. In the proposed DIOS-based FPJ, the interrelated refractive and reflective (R&R) coefficients of the adversarial IOS are randomly generated, acting like a \"DISCO\" that distributes wireless energy radiated by the base station. By introducing active channel aging (ACA) during channel coherence time, the DIOS-based FPJ can perform omnidirectional fully-passive jamming without neither jamming power nor channel knowledge of legitimate users (LUs). To characterize the impact of the DIOS-based PFJ, we derive the statistical characteristics of DIOS-jammed channels based on two widely-used IOS models, i.e., the constant-amplitude model and the variable-amplitude model. Consequently, the asymptotic analysis of the ergodic achievable sum rates under the DIOS-based omnidirectional fully-passive jamming is given based on the derived stochastic characteristics for both the two IOS models. Based on the derived analysis, the omnidirectional jamming impact of the proposed DIOS-based FPJ implemented by a constant-amplitude IOS does not depend on either the quantization number or the stochastic distribution of the DIOS coefficients, while the conclusion does not hold on when a variable-amplitude IOS is used. Numerical results based on one-bit quantization of the IOS phase shifts are provided to verify the effectiveness of the derived theoretical analysis. The proposed DIOS-based FPJ can not only launch omnidirectional fully-passive jamming, but also improve the jamming impact by about 55% at 10 dBm transmit power per LU.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "This paper has been submitted to IEEE TWC for possible publication"
    },
    {
        "paper id": "2411.13006",
        "abstract url": "https://arxiv.org/abs/2411.13006",
        "title": "Automating Sonologists USG Commands with AI and Voice Interface",
        "rating": "-1",
        "keywords": [
            [
                "clinical",
                "organ"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "This research presents an advanced AI-powered ultrasound imaging system that incorporates real-time image processing, organ tracking, and voice commands to enhance the efficiency and accuracy of diagnoses in clinical practice. Traditional ultrasound diagnostics often require significant time and introduce a degree of subjectivity due to user interaction. The goal of this innovative solution is to provide Sonologists with a more predictable and productive imaging procedure utilizing artificial intelligence, computer vision, and voice technology. The functionality of the system employs computer vision and deep learning algorithms, specifically adopting the Mask R-CNN model from Detectron2 for semantic segmentation of organs and key landmarks. This automation improves diagnostic accuracy by enabling the extraction of valuable information with minimal human input. Additionally, it includes a voice recognition feature that allows for hands-free operation, enabling users to control the system with commands such as freeze or liver, all while maintaining their focus on the patient. The architecture comprises video processing and real-time segmentation modules that prepare the system to perform essential imaging functions, such as freezing and zooming in on frames. The liver histopathology module, optimized for detecting fibrosis, achieved an impressive accuracy of 98.6%. Furthermore, the organ segmentation module produces output confidence levels between 50% and 95%, demonstrating its efficacy in organ detection.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13019",
        "abstract url": "https://arxiv.org/abs/2411.13019",
        "title": "Open-World Amodal Appearance Completion",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "inpainting",
                "image editing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Understanding and reconstructing occluded objects is a challenging problem, especially in open-world scenarios where categories and contexts are diverse and unpredictable. Traditional methods, however, are typically restricted to closed sets of object categories, limiting their use in complex, open-world scenes. We introduce Open-World Amodal Appearance Completion, a training-free framework that expands amodal completion capabilities by accepting flexible text queries as input. Our approach generalizes to arbitrary objects specified by both direct terms and abstract queries. We term this capability reasoning amodal completion, where the system reconstructs the full appearance of the queried object based on the provided image and language query. Our framework unifies segmentation, occlusion analysis, and inpainting to handle complex occlusions and generates completed objects as RGBA elements, enabling seamless integration into applications such as 3D reconstruction and image editing. Extensive evaluations demonstrate the effectiveness of our approach in generalizing to novel objects and occlusions, establishing a new benchmark for amodal completion in open-world settings. The code and datasets will be released after paper acceptance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13024",
        "abstract url": "https://arxiv.org/abs/2411.13024",
        "title": "Prior-based Objective Inference Mining Potential Uncertainty for Facial Expression Recognition",
        "rating": "-1",
        "keywords": [
            [
                "Facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Annotation ambiguity caused by the inherent subjectivity of visual judgment has always been a major challenge for Facial Expression Recognition (FER) tasks, particularly for largescale datasets from in-the-wild scenarios. A potential solution is the evaluation of relatively objective emotional distributions to help mitigate the ambiguity of subjective annotations. To this end, this paper proposes a novel Prior-based Objective Inference (POI) network. This network employs prior knowledge to derive a more objective and varied emotional distribution and tackles the issue of subjective annotation ambiguity through dynamic knowledge transfer. POI comprises two key networks: Firstly, the Prior Inference Network (PIN) utilizes the prior knowledge of AUs and emotions to capture intricate motion details. To reduce over-reliance on priors and facilitate objective emotional inference, PIN aggregates inferential knowledge from various key facial subregions, encouraging mutual learning. Secondly, the Target Recognition Network (TRN) integrates subjective emotion annotations and objective inference soft labels provided by the PIN, fostering an understanding of inherent facial expression diversity, thus resolving annotation ambiguity. Moreover, we introduce an uncertainty estimation module to quantify and balance facial expression confidence. This module enables a flexible approach to dealing with the uncertainties of subjective annotations. Extensive experiments show that POI exhibits competitive performance on both synthetic noisy datasets and multiple real-world datasets. All codes and training logs will be publicly available at https://github.com/liuhw01/POI.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13602",
        "abstract url": "https://arxiv.org/abs/2411.13602",
        "title": "Large-scale cross-modality pretrained model enhances cardiovascular state estimation and cardiomyopathy detection from electrocardiograms: An AI system development and multi-center validation study",
        "rating": "-1",
        "keywords": [
            [
                "Biobank",
                "diagnosis",
                "disease",
                "cardiac"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Cardiovascular diseases (CVDs) present significant challenges for early and accurate diagnosis. While cardiac magnetic resonance imaging (CMR) is the gold standard for assessing cardiac function and diagnosing CVDs, its high cost and technical complexity limit accessibility. In contrast, electrocardiography (ECG) offers promise for large-scale early screening. This study introduces CardiacNets, an innovative model that enhances ECG analysis by leveraging the diagnostic strengths of CMR through cross-modal contrastive learning and generative pretraining. CardiacNets serves two primary functions: (1) it evaluates detailed cardiac function indicators and screens for potential CVDs, including coronary artery disease, cardiomyopathy, pericarditis, heart failure and pulmonary hypertension, using ECG input; and (2) it enhances interpretability by generating high-quality CMR images from ECG data. We train and validate the proposed CardiacNets on two large-scale public datasets (the UK Biobank with 41,519 individuals and the MIMIC-IV-ECG comprising 501,172 samples) as well as three private datasets (FAHZU with 410 individuals, SAHZU with 464 individuals, and QPH with 338 individuals), and the findings demonstrate that CardiacNets consistently outperforms traditional ECG-only models, substantially improving screening accuracy. Furthermore, the generated CMR images provide valuable diagnostic support for physicians of all experience levels. This proof-of-concept study highlights how ECG can facilitate cross-modal insights into cardiac function assessment, paving the way for enhanced CVD screening and diagnosis at a population level.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "23 pages, 8 figures"
    },
    {
        "paper id": "2411.13604",
        "abstract url": "https://arxiv.org/abs/2411.13604",
        "title": "RadPhi-3: Small Language Models for Radiology",
        "rating": "-1",
        "keywords": [
            [
                "Radiology"
            ],
            [
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "LLM based copilot assistants are useful in everyday tasks. There is a proliferation in the exploration of AI assistant use cases to support radiology workflows in a reliable manner. In this work, we present RadPhi-3, a Small Language Model instruction tuned from Phi-3-mini-4k-instruct with 3.8B parameters to assist with various tasks in radiology workflows. While impression summary generation has been the primary task which has been explored in prior works w.r.t radiology reports of Chest X-rays, we also explore other useful tasks like change summary generation comparing the current radiology report and its prior report, section extraction from radiology reports, tagging the reports with various pathologies and tubes, lines or devices present in them etc. In-addition, instruction tuning RadPhi-3 involved learning from a credible knowledge source used by radiologists, Radiopaedia.org. RadPhi-3 can be used both to give reliable answers for radiology related queries as well as perform useful tasks related to radiology reports. RadPhi-3 achieves SOTA results on the RaLEs radiology report generation benchmark.",
        "subjects": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13609",
        "abstract url": "https://arxiv.org/abs/2411.13609",
        "title": "What You See Is What Matters: A Novel Visual and Physics-Based Metric for Evaluating Video Generation Quality",
        "rating": "-1",
        "keywords": [
            [
                "Physics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "As video generation models advance rapidly, assessing the quality of generated videos has become increasingly critical. Existing metrics, such as Fr\u00e9chet Video Distance (FVD), Inception Score (IS), and ClipSim, measure quality primarily in latent space rather than from a human visual perspective, often overlooking key aspects like appearance and motion consistency to physical laws. In this paper, we propose a novel metric, VAMP (Visual Appearance and Motion Plausibility), that evaluates both the visual appearance and physical plausibility of generated videos. VAMP is composed of two main components: an appearance score, which assesses color, shape, and texture consistency across frames, and a motion score, which evaluates the realism of object movements. We validate VAMP through two experiments: corrupted video evaluation and generated video evaluation. In the corrupted video evaluation, we introduce various types of corruptions into real videos and measure the correlation between corruption severity and VAMP scores. In the generated video evaluation, we use state-of-the-art models to generate videos from carefully designed prompts and compare VAMP's performance to human evaluators' rankings. Our results demonstrate that VAMP effectively captures both visual fidelity and temporal consistency, offering a more comprehensive evaluation of video quality than traditional methods.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.14476",
        "abstract url": "https://arxiv.org/abs/2411.14476",
        "title": "StreetviewLLM: Extracting Geographic Information Using a Chain-of-Thought Multimodal Large Language Model",
        "rating": "-1",
        "keywords": [
            [
                "health",
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Geospatial predictions are crucial for diverse fields such as disaster management, urban planning, and public health. Traditional machine learning methods often face limitations when handling unstructured or multi-modal data like street view imagery. To address these challenges, we propose StreetViewLLM, a novel framework that integrates a large language model with the chain-of-thought reasoning and multimodal data sources. By combining street view imagery with geographic coordinates and textual data, StreetViewLLM improves the precision and granularity of geospatial predictions. Using retrieval-augmented generation techniques, our approach enhances geographic information extraction, enabling a detailed analysis of urban environments. The model has been applied to seven global cities, including Hong Kong, Tokyo, Singapore, Los Angeles, New York, London, and Paris, demonstrating superior performance in predicting urban indicators, including population density, accessibility to healthcare, normalized difference vegetation index, building height, and impervious surface. The results show that StreetViewLLM consistently outperforms baseline models, offering improved predictive accuracy and deeper insights into the built environment. This research opens new opportunities for integrating the large language model into urban analytics, decision-making in urban planning, infrastructure management, and environmental monitoring.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.14486",
        "abstract url": "https://arxiv.org/abs/2411.14486",
        "title": "The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz",
        "rating": "-1",
        "keywords": [
            [
                "biology",
                "psychological"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "This research introduces a novel evaluation framework designed to assess large language models' (LLMs) ability to acknowledge uncertainty on 675 fundamentally unsolvable problems. Using a curated dataset of graduate-level grand challenge questions with intentionally unknowable answers, we evaluated twelve state-of-the-art LLMs, including both open and closed-source models, on their propensity to admit ignorance rather than generate plausible but incorrect responses. The best models scored in 62-68% accuracy ranges for admitting the problem solution was unknown in fields ranging from biology to philosophy and mathematics. We observed an inverse relationship between problem difficulty and model accuracy, with GPT-4 demonstrating higher rates of uncertainty acknowledgment on more challenging problems (35.8%) compared to simpler ones (20.0%). This pattern indicates that models may be more prone to generate speculative answers when problems appear more tractable. The study also revealed significant variations across problem categories, with models showing difficulty in acknowledging uncertainty in invention and NP-hard problems while performing relatively better on philosophical and psychological challenges. These results contribute to the growing body of research on artificial general intelligence (AGI) assessment by highlighting the importance of uncertainty recognition as a critical component of future machine intelligence evaluation. This impossibility test thus extends previous theoretical frameworks for universal intelligence testing by providing empirical evidence of current limitations in LLMs' ability to recognize their own knowledge boundaries, suggesting new directions for improving model training architectures and evaluation approaches.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12308",
        "abstract url": "https://arxiv.org/abs/2411.12308",
        "title": "SNN-Based Online Learning of Concepts and Action Laws in an Open World",
        "rating": "-1.5",
        "keywords": [
            [
                "bio-inspired"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We present the architecture of a fully autonomous, bio-inspired cognitive agent built around a spiking neural network (SNN) implementing the agent's semantic memory. The agent explores its universe and learns concepts of objects/situations and of its own actions in a one-shot manner. While object/situation concepts are unary, action concepts are triples made up of an initial situation, a motor activity, and an outcome. They embody the agent's knowledge of its universe's actions laws. Both kinds of concepts have different degrees of generality. To make decisions the agent queries its semantic memory for the expected outcomes of envisaged actions and chooses the action to take on the basis of these predictions. Our experiments show that the agent handles new situations by appealing to previously learned general concepts and rapidly modifies its concepts to adapt to environment changes.",
        "subjects": [
            "cs.AI",
            "cs.LG",
            "cs.NE",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12335",
        "abstract url": "https://arxiv.org/abs/2411.12335",
        "title": "Cities beyond proximity",
        "rating": "-1.5",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "The concept of `proximity-based cities' has gained attention as a new urban organizational model. Most prominently, the 15-minute city contends that cities can function more effectively, equitably and sustainably if essential, everyday services and key amenities are within a 15-minute walk or cycle. However, focusing solely on travel time risks overlooking disparities in service quality, as the proximity paradigm tends to emphasize the mere presence of an element in a location rather than bringing up more complex questions of identity, diversity, quality, value or relationships. Transitioning to value-based cities by considering more than just proximity can enhance local identity, resilience and urban democracy. Fostering bottom-up initiatives can create a culture of local care and value, while predominantly top-down governing strategies can lead to large inequalities. Balancing these approaches can maximize resilience, health and sustainability. This equilibrium has the potential to accompany sustainable growth, by encouraging the creation of innovative urban solutions and reducing inequalities.",
        "subjects": [
            "physics.soc-ph",
            "cs.SI"
        ],
        "comment": "This article is part of the theme issue 'Cocreating the future: participatory cities and digital governance', an opinion piece in Phil. Trans. R. Soc. A"
    },
    {
        "paper id": "2411.12352",
        "abstract url": "https://arxiv.org/abs/2411.12352",
        "title": "Perfecting Imperfect Physical Neural Networks with Transferable Robustness using Sharpness-Aware Training",
        "rating": "-1.5",
        "keywords": [
            [
                "thermal"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "AI models are essential in science and engineering, but recent advances are pushing the limits of traditional digital hardware. To address these limitations, physical neural networks (PNNs), which use physical substrates for computation, have gained increasing attention. However, developing effective training methods for PNNs remains a significant challenge. Current approaches, regardless of offline and online training, suffer from significant accuracy loss. Offline training is hindered by imprecise modeling, while online training yields device-specific models that can't be transferred to other devices due to manufacturing variances. Both methods face challenges from perturbations after deployment, such as thermal drift or alignment errors, which make trained models invalid and require retraining. Here, we address the challenges with both offline and online training through a novel technique called Sharpness-Aware Training (SAT), where we innovatively leverage the geometry of the loss landscape to tackle the problems in training physical systems. SAT enables accurate training using efficient backpropagation algorithms, even with imprecise models. PNNs trained by SAT offline even outperform those trained online, despite modeling and fabrication errors. SAT also overcomes online training limitations by enabling reliable transfer of models between devices. Finally, SAT is highly resilient to perturbations after deployment, allowing PNNs to continuously operate accurately under perturbations without retraining. We demonstrate SAT across three types of PNNs, showing it is universally applicable, regardless of whether the models are explicitly known. This work offers a transformative, efficient approach to training PNNs, addressing critical challenges in analog computing and enabling real-world deployment.",
        "subjects": [
            "physics.optics",
            "cs.ET",
            "cs.LG"
        ],
        "comment": "24 pages, 4 figures"
    },
    {
        "paper id": "2411.12436",
        "abstract url": "https://arxiv.org/abs/2411.12436",
        "title": "Coevolution of relationship-driven cooperation under recommendation protocol on multiplex networks",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "While traditional game models often simplify interactions among agents as static, real-world social relationships are inherently dynamic, influenced by both immediate payoffs and alternative information. Motivated by this fact, we introduce a coevolutionary multiplex network model that incorporates the concepts of a relationship threshold and a recommendation mechanism to explore how the strength of relationships among agents interacts with their strategy choices within the framework of weak prisoner's dilemma games. In the relationship layer, the relationship strength between agents varies based on interaction outcomes. In return, the strategy choice of agents in the game layer is influenced by both payoffs and relationship indices, and agents can interact with distant agents through a recommendation mechanism. Simulation of various network topologies reveals that a higher average degree supports cooperation, although increased randomness in interactions may inhibit its formation. Interestingly, a higher threshold value of interaction quality is detrimental, while the applied recommendation protocol can improve global cooperation. The best results are obtained when the relative weight of payoff is minimal and the individual fitness is dominated by the relationship indices gained from the quality of links to neighbors. As a consequence, the changes in the distribution of relationship indices are closely correlated with overall levels of cooperation.",
        "subjects": [
            "cs.SI",
            "physics.soc-ph"
        ],
        "comment": "9 pages, 5 figures"
    },
    {
        "paper id": "2411.12450",
        "abstract url": "https://arxiv.org/abs/2411.12450",
        "title": "Frequency-Aware Guidance for Blind Image Restoration via Diffusion Models",
        "rating": "-1.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "Image Restoration"
            ],
            [
                "cs.CV",
                "eess.IV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Blind image restoration remains a significant challenge in low-level vision tasks. Recently, denoising diffusion models have shown remarkable performance in image synthesis. Guided diffusion models, leveraging the potent generative priors of pre-trained models along with a differential guidance loss, have achieved promising results in blind image restoration. However, these models typically consider data consistency solely in the spatial domain, often resulting in distorted image content. In this paper, we propose a novel frequency-aware guidance loss that can be integrated into various diffusion models in a plug-and-play manner. Our proposed guidance loss, based on 2D discrete wavelet transform, simultaneously enforces content consistency in both the spatial and frequency domains. Experimental results demonstrate the effectiveness of our method in three blind restoration tasks: blind image deblurring, imaging through turbulence, and blind restoration for multiple degradations. Notably, our method achieves a significant improvement in PSNR score, with a remarkable enhancement of 3.72\\,dB in image deblurring. Moreover, our method exhibits superior capability in generating images with rich details and reduced distortion, leading to the best visual quality.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": "17 pages, 6 figures, has been accepted by the ECCV 2024: AIM workshop"
    },
    {
        "paper id": "2411.12451",
        "abstract url": "https://arxiv.org/abs/2411.12451",
        "title": "Empirical Privacy Evaluations of Generative and Predictive Machine Learning Models -- A review and challenges for practice",
        "rating": "-1.5",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Synthetic data generators, when trained using privacy-preserving techniques like differential privacy, promise to produce synthetic data with formal privacy guarantees, facilitating the sharing of sensitive data. However, it is crucial to empirically assess the privacy risks associated with the generated synthetic data before deploying generative technologies. This paper outlines the key concepts and assumptions underlying empirical privacy evaluation in machine learning-based generative and predictive models. Then, this paper explores the practical challenges for privacy evaluations of generative models for use cases with millions of training records, such as data from statistical agencies and healthcare providers. Our findings indicate that methods designed to verify the correct operation of the training algorithm are effective for large datasets, but they often assume an adversary that is unrealistic in many scenarios. Based on the findings, we highlight a crucial trade-off between the computational feasibility of the evaluation and the level of realism of the assumed threat model. Finally, we conclude with ideas and suggestions for future research.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12502",
        "abstract url": "https://arxiv.org/abs/2411.12502",
        "title": "Transformer Neural Processes -- Kernel Regression",
        "rating": "-1.5",
        "keywords": [
            [
                "disease"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Stochastic processes model various natural phenomena from disease transmission to stock prices, but simulating and quantifying their uncertainty can be computationally challenging. For example, modeling a Gaussian Process with standard statistical methods incurs an $\\mathcal{O}(n^3)$ penalty, and even using state-of-the-art Neural Processes (NPs) incurs an $\\mathcal{O}(n^2)$ penalty due to the attention mechanism. We introduce the Transformer Neural Process - Kernel Regression (TNP-KR), a new architecture that incorporates a novel transformer block we call a Kernel Regression Block (KRBlock), which reduces the computational complexity of attention in transformer-based Neural Processes (TNPs) from $\\mathcal{O}((n_C+n_T)^2)$ to $O(n_C^2+n_Cn_T)$ by eliminating masked computations, where $n_C$ is the number of context, and $n_T$ is the number of test points, respectively, and a fast attention variant that further reduces all attention calculations to $\\mathcal{O}(n_C)$ in space and time complexity. In benchmarks spanning such tasks as meta-regression, Bayesian optimization, and image completion, we demonstrate that the full variant matches the performance of state-of-the-art methods while training faster and scaling two orders of magnitude higher in number of test points, and the fast variant nearly matches that performance while scaling to millions of both test and context points on consumer hardware.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12556",
        "abstract url": "https://arxiv.org/abs/2411.12556",
        "title": "UMGAD: Unsupervised Multiplex Graph Anomaly Detection",
        "rating": "-1.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "Anomaly Detection"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Graph anomaly detection (GAD) is a critical task in graph machine learning, with the primary objective of identifying anomalous nodes that deviate significantly from the majority. This task is widely applied in various real-world scenarios, including fraud detection and social network analysis. However, existing GAD methods still face two major challenges: (1) They are often limited to detecting anomalies in single-type interaction graphs and struggle with multiple interaction types in multiplex heterogeneous graphs; (2) In unsupervised scenarios, selecting appropriate anomaly score thresholds remains a significant challenge for accurate anomaly detection. To address the above challenges, we propose a novel Unsupervised Multiplex Graph Anomaly Detection method, named UMGAD. We first learn multi-relational correlations among nodes in multiplex heterogeneous graphs and capture anomaly information during node attribute and structure reconstruction through graph-masked autoencoder (GMAE). Then, to further weaken the influence of noise and redundant information on abnormal information extraction, we generate attribute-level and subgraph-level augmented-view graphs respectively, and perform attribute and structure reconstruction through GMAE. Finally, We learn to optimize node attributes and structural features through contrastive learning between original-view and augmented-view graphs to improve the model's ability to capture anomalies. Meanwhile, we also propose a new anomaly score threshold selection strategy, which allows the model to be independent of the ground truth in real unsupervised scenarios. Extensive experiments on four datasets show that our \\model significantly outperforms state-of-the-art methods, achieving average improvements of 13.48% in AUC and 11.68% in Macro-F1 across all datasets.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12563",
        "abstract url": "https://arxiv.org/abs/2411.12563",
        "title": "Stream-Based Active Learning for Process Monitoring",
        "rating": "-1.5",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Statistical process monitoring (SPM) methods are essential tools in quality management to check the stability of industrial processes, i.e., to dynamically classify the process state as in control (IC), under normal operating conditions, or out of control (OC), otherwise. Traditional SPM methods are based on unsupervised approaches, which are popular because in most industrial applications the true OC states of the process are not explicitly known. This hampered the development of supervised methods that could instead take advantage of process data containing labels on the true process state, although they still need improvement in dealing with class imbalance, as OC states are rare in high-quality processes, and the dynamic recognition of unseen classes, e.g., the number of possible OC states. This article presents a novel stream-based active learning strategy for SPM that enhances partially hidden Markov models to deal with data streams. The ultimate goal is to optimize labeling resources constrained by a limited budget and dynamically update the possible OC states. The proposed method performance in classifying the true state of the process is assessed through a simulation and a case study on the SPM of a resistance spot welding process in the automotive industry, which motivated this research.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12612",
        "abstract url": "https://arxiv.org/abs/2411.12612",
        "title": "Reward driven workflows for unsupervised explainable analysis of phases and ferroic variants from atomically resolved imaging data",
        "rating": "-1.5",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Rapid progress in aberration corrected electron microscopy necessitates development of robust methods for the identification of phases, ferroic variants, and other pertinent aspects of materials structure from imaging data. While unsupervised methods for clustering and classification are widely used for these tasks, their performance can be sensitive to hyperparameter selection in the analysis workflow. In this study, we explore the effects of descriptors and hyperparameters on the capability of unsupervised ML methods to distill local structural information, exemplified by discovery of polarization and lattice distortion in Sm doped BiFeO3 (BFO) thin films. We demonstrate that a reward-driven approach can be used to optimize these key hyperparameters across the full workflow, where rewards were designed to reflect domain wall continuity and straightness, ensuring that the analysis aligns with the material's physical behavior. This approach allows us to discover local descriptors that are best aligned with the specific physical behavior, providing insight into the fundamental physics of materials. We further extend the reward driven workflows to disentangle structural factors of variation via optimized variational autoencoder (VAE). Finally, the importance of well-defined rewards was explored as a quantifiable measure of success of the workflow.",
        "subjects": [
            "cond-mat.mtrl-sci",
            "cs.HC",
            "cs.LG"
        ],
        "comment": "19 pages, 6 figures"
    },
    {
        "paper id": "2411.12626",
        "abstract url": "https://arxiv.org/abs/2411.12626",
        "title": "Exploring the Manifold of Neural Networks Using Diffusion Geometry",
        "rating": "-1.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "architecture search"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Drawing motivation from the manifold hypothesis, which posits that most high-dimensional data lies on or near low-dimensional manifolds, we apply manifold learning to the space of neural networks. We learn manifolds where datapoints are neural networks by introducing a distance between the hidden layer representations of the neural networks. These distances are then fed to the non-linear dimensionality reduction algorithm PHATE to create a manifold of neural networks. We characterize this manifold using features of the representation, including class separation, hierarchical cluster structure, spectral entropy, and topological structure. Our analysis reveals that high-performing networks cluster together in the manifold, displaying consistent embedding patterns across all these features. Finally, we demonstrate the utility of this approach for guiding hyperparameter optimization and neural architecture search by sampling from the manifold.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12689",
        "abstract url": "https://arxiv.org/abs/2411.12689",
        "title": "IMUVIE: Pickup Timeline Action Localization via Motion Movies",
        "rating": "-1.5",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Falls among seniors due to difficulties with tasks such as picking up objects pose significant health and safety risks, impacting quality of life and independence. Reliable, accessible assessment tools are critical for early intervention but often require costly clinic-based equipment and trained personnel, limiting their use in daily life. Existing wearable-based pickup measurement solutions address some needs but face limitations in generalizability. We present IMUVIE, a wearable system that uses motion movies and a machine-learning model to automatically detect and measure pickup events, providing a practical solution for frequent monitoring. IMUVIE's design principles-data normalization, occlusion handling, and streamlined visuals-enhance model performance and are adaptable to tasks beyond pickup classification. In rigorous leave one subject out cross validation evaluations, IMUVIE achieves exceptional window level localization accuracy of 91-92% for pickup action classification on 256,291 motion movie frame candidates while maintaining an event level recall of 97% when evaluated on 129 pickup events. IMUVIE has strong generalization and performs well on unseen subjects. In an interview survey, IMUVIE demonstrated strong user interest and trust, with ease of use identified as the most critical factor for adoption. IMUVIE offers a practical, at-home solution for fall risk assessment, facilitating early detection of movement deterioration, and supporting safer, independent living for seniors.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "This is a preprint version, 12 pages, 20 figures, 3 tables"
    },
    {
        "paper id": "2411.12808",
        "abstract url": "https://arxiv.org/abs/2411.12808",
        "title": "Conversational Medical AI: Ready for Practice",
        "rating": "-1.5",
        "keywords": [
            [
                "Medical",
                "healthcare"
            ],
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "The shortage of doctors is creating a critical squeeze in access to medical expertise. While conversational Artificial Intelligence (AI) holds promise in addressing this problem, its safe deployment in patient-facing roles remains largely unexplored in real-world medical settings. We present the first large-scale evaluation of a physician-supervised LLM-based conversational agent in a real-world medical setting. Our agent, Mo, was integrated into an existing medical advice chat service. Over a three-week period, we conducted a randomized controlled experiment with 926 cases to evaluate patient experience and satisfaction. Among these, Mo handled 298 complete patient interactions, for which we report physician-assessed measures of safety and medical accuracy. Patients reported higher clarity of information (3.73 vs 3.62 out of 4, p < 0.05) and overall satisfaction (4.58 vs 4.42 out of 5, p < 0.05) with AI-assisted conversations compared to standard care, while showing equivalent levels of trust and perceived empathy. The high opt-in rate (81% among respondents) exceeded previous benchmarks for AI acceptance in healthcare. Physician oversight ensured safety, with 95% of conversations rated as \"good\" or \"excellent\" by general practitioners experienced in operating a medical advice chat service. Our findings demonstrate that carefully implemented AI medical assistants can enhance patient experience while maintaining safety standards through physician supervision. This work provides empirical evidence for the feasibility of AI deployment in healthcare communication and insights into the requirements for successful integration into existing healthcare services.",
        "subjects": [
            "cs.AI",
            "cs.CY",
            "cs.HC"
        ],
        "comment": "14 pages, 7 figures, 3 tables"
    },
    {
        "paper id": "2411.12820",
        "abstract url": "https://arxiv.org/abs/2411.12820",
        "title": "Declare and Justify: Explicit assumptions in AI evaluations are necessary for effective regulation",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "As AI systems advance, AI evaluations are becoming an important pillar of regulations for ensuring safety. We argue that such regulation should require developers to explicitly identify and justify key underlying assumptions about evaluations as part of their case for safety. We identify core assumptions in AI evaluations (both for evaluating existing models and forecasting future models), such as comprehensive threat modeling, proxy task validity, and adequate capability elicitation. Many of these assumptions cannot currently be well justified. If regulation is to be based on evaluations, it should require that AI development be halted if evaluations demonstrate unacceptable danger or if these assumptions are inadequately justified. Our presented approach aims to enhance transparency in AI development, offering a practical path towards more effective governance of advanced AI systems.",
        "subjects": [
            "cs.AI",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12853",
        "abstract url": "https://arxiv.org/abs/2411.12853",
        "title": "Integrating Secondary Structures Information into Triangular Spatial Relationships (TSR) for Advanced Protein Classification",
        "rating": "-1.5",
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Protein structures represent the key to deciphering biological functions. The more detailed form of similarity among these proteins is sometimes overlooked by the conventional structural comparison methods. In contrast, further advanced methods, such as Triangular Spatial Relationship (TSR), have been demonstrated to make finer differentiations. Still, the classical implementation of TSR does not provide for the integration of secondary structure information, which is important for a more detailed understanding of the folding pattern of a protein. To overcome these limitations, we developed the SSE-TSR approach. The proposed method integrates secondary structure elements (SSEs) into TSR-based protein representations. This allows an enriched representation of protein structures by considering 18 different combinations of helix, strand, and coil arrangements. Our results show that using SSEs improves the accuracy and reliability of protein classification to varying degrees. We worked with two large protein datasets of 9.2K and 7.8K samples, respectively. We applied the SSE-TSR approach and used a neural network model for classification. Interestingly, introducing SSEs improved performance statistics for Dataset 1, with accuracy moving from 96.0% to 98.3%. For Dataset 2, where the performance statistics were already good, further small improvements were found with the introduction of SSE, giving an accuracy of 99.5% compared to 99.4%. These results show that SSE integration can dramatically improve TSR key discrimination, with significant benefits in datasets with low initial accuracies and only incremental gains in those with high baseline performance. Thus, SSE-TSR is a powerful bioinformatics tool that improves protein classification and understanding of protein function and interaction.",
        "subjects": [
            "cs.LG",
            "q-bio.BM"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12858",
        "abstract url": "https://arxiv.org/abs/2411.12858",
        "title": "CDI: Copyrighted Data Identification in Diffusion Models",
        "rating": "-1.5",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "attacks"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Diffusion Models (DMs) benefit from large and diverse datasets for their training. Since this data is often scraped from the Internet without permission from the data owners, this raises concerns about copyright and intellectual property protections. While (illicit) use of data is easily detected for training samples perfectly re-created by a DM at inference time, it is much harder for data owners to verify if their data was used for training when the outputs from the suspect DM are not close replicas. Conceptually, membership inference attacks (MIAs), which detect if a given data point was used during training, present themselves as a suitable tool to address this challenge. However, we demonstrate that existing MIAs are not strong enough to reliably determine the membership of individual images in large, state-of-the-art DMs. To overcome this limitation, we propose CDI, a framework for data owners to identify whether their dataset was used to train a given DM. CDI relies on dataset inference techniques, i.e., instead of using the membership signal from a single data point, CDI leverages the fact that most data owners, such as providers of stock photography, visual media companies, or even individual artists, own datasets with multiple publicly exposed data points which might all be included in the training of a given DM. By selectively aggregating signals from existing MIAs and using new handcrafted methods to extract features for these datasets, feeding them to a scoring model, and applying rigorous statistical testing, CDI allows data owners with as little as 70 data points to identify with a confidence of more than 99% whether their data was used to train a given DM. Thereby, CDI represents a valuable tool for data owners to claim illegitimate use of their copyrighted data.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": "Coda available at https://github.com/sprintml/copyrighted_data_identification"
    },
    {
        "paper id": "2411.12880",
        "abstract url": "https://arxiv.org/abs/2411.12880",
        "title": "Advancing Large Language Models for Spatiotemporal and Semantic Association Mining of Similar Environmental Events",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Retrieval and recommendation are two essential tasks in modern search tools. This paper introduces a novel retrieval-reranking framework leveraging Large Language Models (LLMs) to enhance the spatiotemporal and semantic associated mining and recommendation of relevant unusual climate and environmental events described in news articles and web posts. This framework uses advanced natural language processing techniques to address the limitations of traditional manual curation methods in terms of high labor cost and lack of scalability. Specifically, we explore an optimized solution to employ cutting-edge embedding models for semantically analyzing spatiotemporal events (news) and propose a Geo-Time Re-ranking (GT-R) strategy that integrates multi-faceted criteria including spatial proximity, temporal association, semantic similarity, and category-instructed similarity to rank and identify similar spatiotemporal events. We apply the proposed framework to a dataset of four thousand Local Environmental Observer (LEO) Network events, achieving top performance in recommending similar events among multiple cutting-edge dense retrieval models. The search and recommendation pipeline can be applied to a wide range of similar data search tasks dealing with geospatial and temporal data. We hope that by linking relevant events, we can better aid the general public to gain an enhanced understanding of climate change and its impact on different communities.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12935",
        "abstract url": "https://arxiv.org/abs/2411.12935",
        "title": "Improving Low-Fidelity Models of Li-ion Batteries via Hybrid Sparse Identification of Nonlinear Dynamics",
        "rating": "-1.5",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Accurate modeling of lithium ion (li-ion) batteries is essential for enhancing the safety, and efficiency of electric vehicles and renewable energy systems. This paper presents a data-inspired approach for improving the fidelity of reduced-order li-ion battery models. The proposed method combines a Genetic Algorithm with Sequentially Thresholded Ridge Regression (GA-STRidge) to identify and compensate for discrepancies between a low-fidelity model (LFM) and data generated either from testing or a high-fidelity model (HFM). The hybrid model, combining physics-based and data-driven methods, is tested across different driving cycles to demonstrate the ability to significantly reduce the voltage prediction error compared to the baseline LFM, while preserving computational efficiency. The model robustness is also evaluated under various operating conditions, showing low prediction errors and high Pearson correlation coefficients for terminal voltage in unseen environments.",
        "subjects": [
            "eess.SY",
            "cs.LG",
            "cs.NE"
        ],
        "comment": "6 pages"
    },
    {
        "paper id": "2411.12965",
        "abstract url": "https://arxiv.org/abs/2411.12965",
        "title": "On adaptivity and minimax optimality of two-sided nearest neighbors",
        "rating": "-1.5",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Nearest neighbor (NN) algorithms have been extensively used for missing data problems in recommender systems and sequential decision-making systems. Prior theoretical analysis has established favorable guarantees for NN when the underlying data is sufficiently smooth and the missingness probabilities are lower bounded. Here we analyze NN with non-smooth non-linear functions with vast amounts of missingness. In particular, we consider matrix completion settings where the entries of the underlying matrix follow a latent non-linear factor model, with the non-linearity belonging to a \\Holder function class that is less smooth than Lipschitz. Our results establish following favorable properties for a suitable two-sided NN: (1) The mean squared error (MSE) of NN adapts to the smoothness of the non-linearity, (2) under certain regularity conditions, the NN error rate matches the rate obtained by an oracle equipped with the knowledge of both the row and column latent factors, and finally (3) NN's MSE is non-trivial for a wide range of settings even when several matrix entries might be missing deterministically. We support our theoretical findings via extensive numerical simulations and a case study with data from a mobile health study, HeartSteps.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "math.ST",
            "stat.ME"
        ],
        "comment": "29 pages, 7 figures"
    },
    {
        "paper id": "2411.12973",
        "abstract url": "https://arxiv.org/abs/2411.12973",
        "title": "Adaptive Process-Guided Learning: An Application in Predicting Lake DO Concentrations",
        "rating": "-1.5",
        "keywords": [
            [
                "biomedicine",
                "health"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper introduces a \\textit{Process-Guided Learning (Pril)} framework that integrates physical models with recurrent neural networks (RNNs) to enhance the prediction of dissolved oxygen (DO) concentrations in lakes, which is crucial for sustaining water quality and ecosystem health. Unlike traditional RNNs, which may deliver high accuracy but often lack physical consistency and broad applicability, the \\textit{Pril} method incorporates differential DO equations for each lake layer, modeling it as a first-order linear solution using a forward Euler scheme with a daily timestep. However, this method is sensitive to numerical instabilities. When drastic fluctuations occur, the numerical integration is neither mass-conservative nor stable. Especially during stratified conditions, exogenous fluxes into each layer cause significant within-day changes in DO concentrations. To address this challenge, we further propose an \\textit{Adaptive Process-Guided Learning (April)} model, which dynamically adjusts timesteps from daily to sub-daily intervals with the aim of mitigating the discrepancies caused by variations in entrainment fluxes. \\textit{April} uses a generator-discriminator architecture to identify days with significant DO fluctuations and employs a multi-step Euler scheme with sub-daily timesteps to effectively manage these variations. We have tested our methods on a wide range of lakes in the Midwestern USA, and demonstrated robust capability in predicting DO concentrations even with limited training data. While primarily focused on aquatic ecosystems, this approach is broadly applicable to diverse scientific and engineering disciplines that utilize process-based models, such as power engineering, climate science, and biomedicine.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13025",
        "abstract url": "https://arxiv.org/abs/2411.13025",
        "title": "ORID: Organ-Regional Information Driven Framework for Radiology Report Generation",
        "rating": "-1.5",
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "diagnosis",
                "Organ",
                "Radiology"
            ],
            [
                "cs.CV"
            ],
            [
                "WACV"
            ]
        ],
        "abstract": "The objective of Radiology Report Generation (RRG) is to automatically generate coherent textual analyses of diseases based on radiological images, thereby alleviating the workload of radiologists. Current AI-based methods for RRG primarily focus on modifications to the encoder-decoder model architecture. To advance these approaches, this paper introduces an Organ-Regional Information Driven (ORID) framework which can effectively integrate multi-modal information and reduce the influence of noise from unrelated organs. Specifically, based on the LLaVA-Med, we first construct an RRG-related instruction dataset to improve organ-regional diagnosis description ability and get the LLaVA-Med-RRG. After that, we propose an organ-based cross-modal fusion module to effectively combine the information from the organ-regional diagnosis description and radiology image. To further reduce the influence of noise from unrelated organs on the radiology report generation, we introduce an organ importance coefficient analysis module, which leverages Graph Neural Network (GNN) to examine the interconnections of the cross-modal information of each organ region. Extensive experiments an1d comparisons with state-of-the-art methods across various evaluation metrics demonstrate the superior performance of our proposed method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "13 pages, 11 figures, WACV2025"
    },
    {
        "paper id": "2411.13599",
        "abstract url": "https://arxiv.org/abs/2411.13599",
        "title": "Can ChatGPT Overcome Behavioral Biases in the Financial Sector? Classify-and-Rethink: Multi-Step Zero-Shot Reasoning in the Gold Investment",
        "rating": "-1.5",
        "keywords": [
            [
                "clinical"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have achieved remarkable success recently, displaying exceptional capabilities in creating understandable and organized text. These LLMs have been utilized in diverse fields, such as clinical research, where domain-specific models like Med-Palm have achieved human-level performance. Recently, researchers have employed advanced prompt engineering to enhance the general reasoning ability of LLMs. Despite the remarkable success of zero-shot Chain-of-Thoughts (CoT) in solving general reasoning tasks, the potential of these methods still remains paid limited attention in the financial reasoning task.To address this issue, we explore multiple prompt strategies and incorporated semantic news information to improve LLMs' performance on financial reasoning tasks.To the best of our knowledge, we are the first to explore this important issue by applying ChatGPT to the gold investment.In this work, our aim is to investigate the financial reasoning capabilities of LLMs and their capacity to generate logical and persuasive investment opinions. We will use ChatGPT, one of the most powerful LLMs recently, and prompt engineering to achieve this goal. Our research will focus on understanding the ability of LLMs in sophisticated analysis and reasoning within the context of investment decision-making. Our study finds that ChatGPT with CoT prompt can provide more explainable predictions and overcome behavioral biases, which is crucial in finance-related tasks and can achieve higher investment returns.",
        "subjects": [
            "q-fin.ST",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13607",
        "abstract url": "https://arxiv.org/abs/2411.13607",
        "title": "VioPose: Violin Performance 4D Pose Estimation by Hierarchical Audiovisual Inference",
        "rating": "-1.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "music"
            ],
            [
                "cs.CV"
            ],
            [
                "WACV"
            ]
        ],
        "abstract": "Musicians delicately control their bodies to generate music. Sometimes, their motions are too subtle to be captured by the human eye. To analyze how they move to produce the music, we need to estimate precise 4D human pose (3D pose over time). However, current state-of-the-art (SoTA) visual pose estimation algorithms struggle to produce accurate monocular 4D poses because of occlusions, partial views, and human-object interactions. They are limited by the viewing angle, pixel density, and sampling rate of the cameras and fail to estimate fast and subtle movements, such as in the musical effect of vibrato. We leverage the direct causal relationship between the music produced and the human motions creating them to address these challenges. We propose VioPose: a novel multimodal network that hierarchically estimates dynamics. High-level features are cascaded to low-level features and integrated into Bayesian updates. Our architecture is shown to produce accurate pose sequences, facilitating precise motion analysis, and outperforms SoTA. As part of this work, we collected the largest and the most diverse calibrated violin-playing dataset, including video, sound, and 3D motion capture poses. Code and dataset can be found in our project page \\url{https://sj-yoo.info/viopose/}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by WACV 2025 in Round 1. First two authors contributed equally"
    },
    {
        "paper id": "2411.14480",
        "abstract url": "https://arxiv.org/abs/2411.14480",
        "title": "Associative Knowledge Graphs for Efficient Sequence Storage and Retrieval",
        "rating": "-1.5",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "anomaly detection"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "This paper presents a novel approach for constructing associative knowledge graphs that are highly effective for storing and recognizing sequences. The graph is created by representing overlapping sequences of objects, as tightly connected clusters within the larger graph. Individual objects (represented as nodes) can be a part of multiple sequences or appear repeatedly within a single sequence. To retrieve sequences, we leverage context, providing a subset of objects that triggers an association with the complete sequence. The system's memory capacity is determined by the size of the graph and the density of its connections. We have theoretically derived the relationships between the critical density of the graph and the memory capacity for storing sequences. The critical density is the point beyond which error-free sequence reconstruction becomes impossible. Furthermore, we have developed an efficient algorithm for ordering elements within a sequence. Through extensive experiments with various types of sequences, we have confirmed the validity of these relationships. This approach has potential applications in diverse fields, such as anomaly detection in financial transactions or predicting user behavior based on past actions.",
        "subjects": [
            "cs.AI",
            "cs.DB"
        ],
        "comment": "10 pages, 6 figures"
    },
    {
        "paper id": "2411.15186",
        "abstract url": "https://arxiv.org/abs/2411.15186",
        "title": "Preliminary Evaluation of the Test-Time Training Layers in Recommendation System (Student Abstract)",
        "rating": "-1.5",
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "This paper explores the application and effectiveness of Test-Time Training (TTT) layers in improving the performance of recommendation systems. We developed a model, TTT4Rec, utilizing TTT-Linear as the feature extraction layer. Our tests across multiple datasets indicate that TTT4Rec, as a base model, performs comparably or even surpasses other baseline models in similar environments.",
        "subjects": [
            "cs.IR"
        ],
        "comment": "To be published in AAAI-25 Student Abstract and Poster Program"
    },
    {
        "paper id": "2411.12274",
        "abstract url": "https://arxiv.org/abs/2411.12274",
        "title": "A Review on Generative AI Models for Synthetic Medical Text, Time Series, and Longitudinal Data",
        "rating": "-2",
        "keywords": [
            [
                "re-identification"
            ],
            [
                "Medical",
                "health"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "This paper presents the results of a novel scoping review on the practical models for generating three different types of synthetic health records (SHRs): medical text, time series, and longitudinal data. The innovative aspects of the review, which incorporate study objectives, data modality, and research methodology of the reviewed studies, uncover the importance and the scope of the topic for the digital medicine context. In total, 52 publications met the eligibility criteria for generating medical time series (22), longitudinal data (17), and medical text (13). Privacy preservation was found to be the main research objective of the studied papers, along with class imbalance, data scarcity, and data imputation as the other objectives. The adversarial network-based, probabilistic, and large language models exhibited superiority for generating synthetic longitudinal data, time series, and medical texts, respectively. Finding a reliable performance measure to quantify SHR re-identification risk is the major research gap of the topic.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": "27 pages, 3 figures"
    },
    {
        "paper id": "2411.12284",
        "abstract url": "https://arxiv.org/abs/2411.12284",
        "title": "DT-RaDaR: Digital Twin Assisted Robot Navigation using Differential Ray-Tracing",
        "rating": "-2",
        "keywords": [
            [
                "LiDAR",
                "RaDaR"
            ],
            [
                "Robot",
                "Navigation"
            ]
        ],
        "abstract": "Autonomous system navigation is a well-researched and evolving field. Recent advancements in improving robot navigation have sparked increased interest among researchers and practitioners, especially in the use of sensing data. However, this heightened focus has also raised significant privacy concerns, particularly for robots that rely on cameras and LiDAR for navigation. Our innovative concept of Radio Frequency (RF) map generation through ray-tracing (RT) within digital twin environments effectively addresses these concerns. In this paper, we propose DT-RaDaR, a robust privacy-preserving, deep reinforcement learning-based framework for robot navigation that leverages RF ray-tracing in both static and dynamic indoor scenarios as well as in smart cities. We introduce a streamlined framework for generating RF digital twins using open-source tools like Blender and NVIDIA's Sionna RT. This approach allows for high-fidelity replication of real-world environments and RF propagation models, optimized for service robot navigation. Several experimental validations and results demonstrate the feasibility of the proposed framework in indoor environments and smart cities, positioning our work as a significant advancement toward the practical implementation of robot navigation using ray-tracing-generated data.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "15 pages, 25 figures, 7 tables, under review to IEEE transactions"
    },
    {
        "paper id": "2411.12288",
        "abstract url": "https://arxiv.org/abs/2411.12288",
        "title": "Construction of the UXAR-CT -- a User eXperience Questionnaire for Augmented Reality in Corporate Training",
        "rating": "-2",
        "keywords": [
            [
                "CT"
            ]
        ],
        "abstract": "Measuring User Experience (UX) with questionnaires is essential for developing and improving products. However, no domain-specific standardized UX questionnaire exists for Augmented Reality (AR) in Corporate Training (CT). Thus, this study introduces the UXAR-CT questionnaire - an AR-specific UX questionnaire for CT environments. We describe the construction procedure and the evaluation process of the questionnaire. A set of candidate items was constructed, and a larger sample of participants evaluated several AR-based learning scenarios with these items. Based on the results, we performed a Principal Component Analysis (PCA) to identify relevant measurement items for each scale. The three best-fitting items were selected based on the results to form the final questionnaire. The first results regarding scale quality indicate a high level of internal consistency. The final version of the UXAR-CT questionnaire is provided and will be evaluated in further research.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "9 pages, 9 figures, CENTRIC 2024 : The Seventeenth International Conference on Advances in Human-oriented and Personalized Mechanisms, Technologies, and Services"
    },
    {
        "paper id": "2411.12292",
        "abstract url": "https://arxiv.org/abs/2411.12292",
        "title": "The Soft-PVTOL: modeling and control",
        "rating": "-2",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "robot"
            ]
        ],
        "abstract": "This paper presents, for the first time, the soft planar vertical take-off and landing (Soft-PVTOL) aircraft. This concept captures the soft aerial vehicle's fundamental dynamics with a minimum number of states and inputs but retains the main features to consider when designing control laws. Unlike conventional PVTOL and multi-rotors, where altering position inevitably impacts orientation due to their underactuated design, the Soft-PVTOL offers the unique advantage of separating these dynamics, opening doors to unparalleled maneuverability and precision. We demonstrate that the Soft-PVTOL can be modeled using the Euler-Lagrange equations by assuming a constant curvature model in the aerial robot's arms. Such a mathematical model is presented in detail and can be extended to several constant curvature segments in each Soft-PVTOL arm. Moreover, we design a passivity-based control law that exploits the flexibility of the robot's arms. We solve the tracking control problem, proving that the error equilibrium globally exponentially converges to zero. The controller is tested in numerical simulations, demonstrating robust performance and ensuring the efficacy of the closed-loop system.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "This manuscript has been submitted for peer review"
    },
    {
        "paper id": "2411.12295",
        "abstract url": "https://arxiv.org/abs/2411.12295",
        "title": "Consistency Regularization for Complementary Clothing Recommendations",
        "rating": "-2",
        "keywords": [
            [
                "recommendation"
            ]
        ],
        "abstract": "This paper reports on the development of a Consistency Regularized model for Bayesian Personalized Ranking (CR-BPR), addressing to the drawbacks in existing complementary clothing recommendation methods, namely limited consistency and biased learning caused by diverse feature scale of multi-modal data. Compared to other product types, fashion preferences are inherently subjective and more personal, and fashion are often presented, not by individual clothing product, but with other complementary product(s) in a well coordinated fashion outfit. Current complementary-product recommendation studies primarily focus on user preference and product matching, this study further emphasizes the consistency observed in user-product interactions as well as product-product interactions, in the specific context of clothing matching. Most traditional approaches often underplayed the impact of existing wardrobe items on future matching choices, resulting in less effective preference prediction models. Moreover, many multi-modal information based models overlook the limitations arising from various feature scales being involved. To address these gaps, the CR-BPR model integrates collaborative filtering techniques to incorporate both user preference and product matching modeling, with a unique focus on consistency regularization for each aspect. Additionally, the incorporation of a feature scaling process further addresses the imbalances caused by different feature scales, ensuring that the model can effectively handle multi-modal data without being skewed by any particular type of feature. The effectiveness of the CR-BPR model was validated through detailed analysis involving two benchmark datasets. The results confirmed that the proposed approach significantly outperforms existing models.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12321",
        "abstract url": "https://arxiv.org/abs/2411.12321",
        "title": "Enhancing Blind Source Separation with Dissociative Principal Component Analysis",
        "rating": "-2",
        "keywords": [
            [
                "inpainting"
            ],
            [
                "fMRI"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Sparse principal component analysis (sPCA) enhances the interpretability of principal components (PCs) by imposing sparsity constraints on loading vectors (LVs). However, when used as a precursor to independent component analysis (ICA) for blind source separation (BSS), sPCA may underperform due to its focus on simplicity, potentially disregarding some statistical information essential for effective ICA. To overcome this limitation, a sophisticated approach is proposed that preserves the interpretability advantages of sPCA while significantly enhancing its source extraction capabilities. This consists of two tailored algorithms, dissociative PCA (DPCA1 and DPCA2), which employ adaptive and firm thresholding alongside gradient and coordinate descent approaches to optimize the proposed model dynamically. These algorithms integrate left and right singular vectors from singular value decomposition (SVD) through dissociation matrices (DMs) that replace traditional singular values, thus capturing latent interdependencies effectively to model complex source relationships. This leads to refined PCs and LVs that more accurately represent the underlying data structure. The proposed approach avoids focusing on individual eigenvectors, instead, it collaboratively combines multiple eigenvectors to disentangle interdependencies within each SVD variate. The superior performance of the proposed DPCA algorithms is demonstrated across four varied imaging applications including functional magnetic resonance imaging (fMRI) source retrieval, foreground-background separation, image reconstruction, and image inpainting. They outperformed traditional methods such as PCA+ICA, PPCA+ICA, SPCA+ICA, PMD, and GPower.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "1. 13 pages with 6 figures, this work has not bee published before. 2. The paper is yet to be peer-reviewed and I am planning to submit it to IEEE Transactions on Image Processing. 3. There is no supplementary material. 4. There is no funding for this work as of now"
    },
    {
        "paper id": "2411.12342",
        "abstract url": "https://arxiv.org/abs/2411.12342",
        "title": "Temperature-Aware Phase-shift Design of LC-RIS for Secure Communication",
        "rating": "-2",
        "keywords": [
            [
                "thermal"
            ]
        ],
        "abstract": "Liquid crystal (LC) technology enables low-power and cost-effective solutions for implementing the reconfigurable intelligent surface (RIS). However, the phase-shift response of LC-RISs is temperature-dependent, which, if unaddressed, can degrade the performance. This issue is particularly critical in applications such as secure communications, where variations in phase-shift response may lead to significant information leakage. In this paper, we consider secure communication through an LC-RIS and developed a temperature-aware algorithm adapting the RIS phase shifts to thermal conditions. Our simulation results demonstrate that the proposed algorithm significantly improves the secure data rate compared to scenarios where temperature variations are not accounted for.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12346",
        "abstract url": "https://arxiv.org/abs/2411.12346",
        "title": "A computational model for inelastic behaviour and fracture of refractory industrial components under high-temperature conditions, application to slide gate plates",
        "rating": "-2",
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "This work aims to provide a computational model that can describe the complex behaviour of refractory industrial components under working conditions. Special attention is given to the asymmetric tension-compression behaviour and its evolution in the full range of working temperatures. The model accounts for inelastic flow in compression and brittle fracture behaviour in tension by leveraging the continuum-mechanics theory of plasticity and phase-field fracture damage. The model is implemented in the Finite Element open-source platform FEniCS and is used to analyze the fracture phenomenon in the refractory plate used in ladle slide gate systems to control the liquid steel flow from the ladle to the tundish.",
        "subjects": [
            "cs.CE",
            "physics.class-ph"
        ],
        "comment": "26 pages, 4 figures"
    },
    {
        "paper id": "2411.12347",
        "abstract url": "https://arxiv.org/abs/2411.12347",
        "title": "Leveraging NFTs for Spectrum Securitization in 6G Networks",
        "rating": "-2",
        "keywords": [
            [
                "6G"
            ]
        ],
        "abstract": "Dynamic Spectrum Sharing can enhance spectrum resource utilization by promoting the dynamic distribution of spectrum resources. However, to effectively implement dynamic spectrum resource allocation, certain mechanisms are needed to incentivize primary users to proactively share their spectrum resources. This paper, based on the ERC404 standard and integrating Non-Fungible Token and Fungible Token technologies, proposes a spectrum securitization model to incentivize spectrum resource sharing and implements it on the Ethereum test net.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12386",
        "abstract url": "https://arxiv.org/abs/2411.12386",
        "title": "Semi-Automatic Extraction of Formal Models from Object Oriented Code",
        "rating": "-2",
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "Behavioral models are incredibly useful for understanding and validating software. However, the automatic extraction of such models from actual industrial code remains a largely unsolved problem with current solutions often not scaling well with the complexity and size of industrial systems or having to rely on approximations. To enable the extraction of useful models from code, we provide a framework for transforming object-oriented code into processes from which, when paired with minimal user input, models can be automatically generated and composed. Paired with this, we introduce the novel SSTraGen (StateSpace Transformation & Generation) tool, which provides an implementation of this framework. Through case studies at Philips Image Guided Therapy Systems, we showcase the practical applicability and usefulness of this tool, including the transformation of a component with >1000 LOC.",
        "subjects": [
            "cs.SE",
            "cs.FL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12389",
        "abstract url": "https://arxiv.org/abs/2411.12389",
        "title": "Combinational Backdoor Attack against Customized Text-to-Image Models",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion",
                "Text-to-Image"
            ],
            [
                "Attack"
            ]
        ],
        "abstract": "Recently, Text-to-Image (T2I) synthesis technology has made tremendous strides. Numerous representative T2I models have emerged and achieved promising application outcomes, such as DALL-E, Stable Diffusion, Imagen, etc. In practice, it has become increasingly popular for model developers to selectively adopt various pre-trained text encoders and conditional diffusion models from third-party platforms, integrating them to build customized (personalized) T2I models. However, such an adoption approach is vulnerable to backdoor attacks. In this work, we propose a Combinational Backdoor Attack against Customized T2I models (CBACT2I) targeting this application scenario. Different from previous backdoor attacks against T2I models, CBACT2I embeds the backdoor into the text encoder and the conditional diffusion model separately. The customized T2I model exhibits backdoor behaviors only when the backdoor text encoder is used in combination with the backdoor conditional diffusion model. These properties make CBACT2I more stealthy and flexible than prior backdoor attacks against T2I models. Extensive experiments demonstrate the effectiveness of CBACT2I with different backdoor triggers and different backdoor targets on the open-sourced Stable Diffusion model. This work reveals the backdoor vulnerabilities of customized T2I models and urges countermeasures to mitigate backdoor threats in this scenario.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12400",
        "abstract url": "https://arxiv.org/abs/2411.12400",
        "title": "Bi-LSTM neural network for EEG-based error detection in musicians' performance",
        "rating": "-2",
        "keywords": [
            [
                "EEG"
            ]
        ],
        "abstract": "Electroencephalography (EEG) is a tool that allows us to analyze brain activity with high temporal resolution. These measures, combined with deep learning and digital signal processing, are widely used in neurological disorder detection and emotion and mental activity recognition. In this paper, a new method for mental activity recognition is presented; instantaneous frequency, spectral entropy and Mel-frequency cepstral coefficients (MFCC) are used to classify EEG signals using bidirectional LSTM neural networks. It is shown that this method can be used for intra-subject or inter-subject analysis and has been applied to error detection in musician performance reaching compelling accuracy.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2411.12478",
        "abstract url": "https://arxiv.org/abs/2411.12478",
        "title": "Robotic transcatheter tricuspid valve replacement with hybrid enhanced intelligence: a new paradigm and first-in-vivo study",
        "rating": "-2",
        "keywords": [
            [
                "time efficiency"
            ],
            [
                "robotics",
                "robot"
            ],
            [
                "surgical",
                "clinical"
            ]
        ],
        "abstract": "Transcatheter tricuspid valve replacement (TTVR) is the latest treatment for tricuspid regurgitation and is in the early stages of clinical adoption. Intelligent robotic approaches are expected to overcome the challenges of surgical manipulation and widespread dissemination, but systems and protocols with high clinical utility have not yet been reported. In this study, we propose a complete solution that includes a passive stabilizer, robotic drive, detachable delivery catheter and valve manipulation mechanism. Working towards autonomy, a hybrid augmented intelligence approach based on reinforcement learning, Monte Carlo probabilistic maps and human-robot co-piloted control was introduced. Systematic tests in phantom and first-in-vivo animal experiments were performed to verify that the system design met the clinical requirement. Furthermore, the experimental results confirmed the advantages of co-piloted control over conventional master-slave control in terms of time efficiency, control efficiency, autonomy and stability of operation. In conclusion, this study provides a comprehensive pathway for robotic TTVR and, to our knowledge, completes the first animal study that not only successfully demonstrates the application of hybrid enhanced intelligence in interventional robotics, but also provides a solution with high application value for a cutting-edge procedure.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12510",
        "abstract url": "https://arxiv.org/abs/2411.12510",
        "title": "PR-ENDO: Physically Based Relightable Gaussian Splatting for Endoscopy",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "diagnosis",
                "cancer",
                "Endoscopic"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Endoscopic procedures are crucial for colorectal cancer diagnosis, and three-dimensional reconstruction of the environment for real-time novel-view synthesis can significantly enhance diagnosis. We present PR-ENDO, a framework that leverages 3D Gaussian Splatting within a physically based, relightable model tailored for the complex acquisition conditions in endoscopy, such as restricted camera rotations and strong view-dependent illumination. By exploiting the connection between the camera and light source, our approach introduces a relighting model to capture the intricate interactions between light and tissue using physically based rendering and MLP. Existing methods often produce artifacts and inconsistencies under these conditions, which PR-ENDO overcomes by incorporating a specialized diffuse MLP that utilizes light angles and normal vectors, achieving stable reconstructions even with limited training camera rotations. We benchmarked our framework using a publicly available dataset and a newly introduced dataset with wider camera rotations. Our methods demonstrated superior image quality compared to baseline approaches.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12557",
        "abstract url": "https://arxiv.org/abs/2411.12557",
        "title": "Power Efficient Cooperative Communication within IIoT Subnetworks: Relay or RIS?",
        "rating": "-2",
        "keywords": [
            [
                "6G",
                "industrial",
                "IoT"
            ]
        ],
        "abstract": "The forthcoming sixth-generation (6G) industrial Internet-of-Things (IIoT) subnetworks are expected to support ultra-fast control communication cycles for numerous IoT devices. However, meeting the stringent requirements for low latency and high reliability poses significant challenges, particularly due to signal fading and physical obstructions. In this paper, we propose novel time division multiple access (TDMA) and frequency division multiple access (FDMA) communication protocols for cooperative transmission in IIoT subnetworks. These protocols leverage secondary access points (sAPs) as Decode-and-Forward (DF) and Amplify-and-Forward (AF) relays, enabling shorter cycle times while minimizing overall transmit power. A classification mechanism determines whether the highest-gain link for each IoT device is a single-hop or two-hop connection, and selects the corresponding sAP. We then formulate the problem of minimizing transmit power for DF/AF relaying while adhering to the delay and maximum power constraints. In the FDMA case, an additional constraint is introduced for bandwidth allocation to IoT devices during the first and second phases of cooperative transmission. To tackle the nonconvex problem, we employ the sequential parametric convex approximation (SPCA) method. We extend our analysis to a system model with reconfigurable intelligent surfaces (RISs), enabling transmission through direct and RIS-assisted channels, and optimizing for a multi-RIS scenario for comparative analysis. Simulation results show that our cooperative communication approach reduces the emitted power by up to 7.5 dB while maintaining an outage probability and a resource overflow rate below $10^{-6}$. While the RIS-based solution achieves greater power savings, the relay-based protocol outperforms RIS in terms of outage probability.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2112.05989, arXiv:2005.00269 by other authors"
    },
    {
        "paper id": "2411.12559",
        "abstract url": "https://arxiv.org/abs/2411.12559",
        "title": "Emulating a computing grid in a local environment for feature evaluation",
        "rating": "-2",
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "The necessity for complex calculations in high-energy physics and large-scale data analysis has led to the development of computing grids, such as the ALICE computing grid at CERN. These grids outperform traditional supercomputers but present challenges in directly evaluating new features, as changes can disrupt production operations and require comprehensive assessments, entailing significant time investments across all components. This paper proposes a solution to this challenge by introducing a novel approach for emulating a computing grid within a local environment. This emulation, resembling a mini clone of the original computing grid, encompasses its essential components and functionalities. Local environments provide controlled settings for emulating grid components, enabling researchers to evaluate system features without impacting production environments. This investigation contributes to the evolving field of computing grids and distributed systems, offering insights into the emulation of a computing grid in a local environment for feature evaluation.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "16 pages 4 figures 2 tables Computer Science & Information Technology (CS & IT) ISSN : 2231 - 5403 Volume 14, Number 22, November 2024"
    },
    {
        "paper id": "2411.12575",
        "abstract url": "https://arxiv.org/abs/2411.12575",
        "title": "Stochastic BIQA: Median Randomized Smoothing for Certified Blind Image Quality Assessment",
        "rating": "-2",
        "keywords": [
            [
                "attacks"
            ],
            [
                "Quality Assessment"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Most modern No-Reference Image-Quality Assessment (NR-IQA) metrics are based on neural networks vulnerable to adversarial attacks. Attacks on such metrics lead to incorrect image/video quality predictions, which poses significant risks, especially in public benchmarks. Developers of image processing algorithms may unfairly increase the score of a target IQA metric without improving the actual quality of the adversarial image. Although some empirical defenses for IQA metrics were proposed, they do not provide theoretical guarantees and may be vulnerable to adaptive attacks. This work focuses on developing a provably robust no-reference IQA metric. Our method is based on Median Smoothing (MS) combined with an additional convolution denoiser with ranking loss to improve the SROCC and PLCC scores of the defended IQA metric. Compared with two prior methods on three datasets, our method exhibited superior SROCC and PLCC scores while maintaining comparable certified guarantees.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12619",
        "abstract url": "https://arxiv.org/abs/2411.12619",
        "title": "Leveraging Virtual Reality and AI Tutoring for Language Learning: A Case Study of a Virtual Campus Environment with OpenAI GPT Integration with Unity 3D",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "text to speech"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "This paper presents a new approach to multiple language learning, with Hindi the language to be learnt in our case, by using the integration of virtual reality environments and AI enabled tutoring systems using OpenAIs GPT api calls. We have developed a scenario which has a virtual campus environment using Unity which focuses on a detailed representation of our universitys buildings 11th floor, where most of the cultural and technological activities take place. Within this virtual environment that we have created, we have an AI tutor powered by OpenAI's GPT model which was called using an api which moves around with the user. This provided language learning support in Hindi, as GPT is able to take care of language translation. Our approach mainly involves utilising speech to text, text to text conversion and text to speech capabilities to facilitate real time interaction between users and the AI tutor in the presence of internet. This research demonstrates the use of combining VR technology with AI tutoring for immersive language learning experiences and provides interaction.",
        "subjects": [
            "cs.HC",
            "cs.CL"
        ],
        "comment": "5 pages, 2 tables, 8 figures"
    },
    {
        "paper id": "2411.12621",
        "abstract url": "https://arxiv.org/abs/2411.12621",
        "title": "Meeting Future Mobile Traffic Needs by Peak-Throughput Design of Next-Gen RAN",
        "rating": "-2",
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "Growing congestion in current mobile networks necessitates innovative solutions. This paper explores the potential of mmWave 5G networks in urban settings, focusing on Integrated Access and Backhaul (IAB) and the Smart Radio Environment (SRE). The mmWave traffic will be mainly made of short bursts to transfer large volumes of data and long idle periods where data are processed. This must change the way of designing mobile radio networks. To this extent, we propose network planning models leveraging the maximization of the achievable peak throughput. Results highlight the advantages of this approach during the network planning phase, providing insights into better accommodating the demands of mobile traffic without sacrificing the overall network capacity.",
        "subjects": [
            "cs.NI",
            "eess.SP"
        ],
        "comment": "18 pages, submitted to Transactions on Mobile Computing"
    },
    {
        "paper id": "2411.12633",
        "abstract url": "https://arxiv.org/abs/2411.12633",
        "title": "Instant Policy: In-Context Imitation Learning via Graph Diffusion",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "robotics",
                "robot"
            ],
            [
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Following the impressive capabilities of in-context learning with large transformers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics. We introduce Instant Policy, which learns new tasks instantly (without further training) from just one or two demonstrations, achieving ICIL through two key components. First, we introduce inductive biases through a graph representation and model ICIL as a graph generation problem with a learned diffusion process, enabling structured reasoning over demonstrations, observations, and actions. Second, we show that such a model can be trained using pseudo-demonstrations - arbitrary trajectories generated in simulation - as a virtually infinite pool of training data. Simulated and real experiments show that Instant Policy enables rapid learning of various everyday robot tasks. We also show how it can serve as a foundation for cross-embodiment and zero-shot transfer to language-defined tasks. Code and videos are available at https://www.robot-learning.uk/instant-policy.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Code and videos are available on our project webpage at https://www.robot-learning.uk/instant-policy"
    },
    {
        "paper id": "2411.12635",
        "abstract url": "https://arxiv.org/abs/2411.12635",
        "title": "M3D: Dual-Stream Selective State Spaces and Depth-Driven Framework for High-Fidelity Single-View 3D Reconstruction",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "Depth"
            ],
            [
                "autonomous driving"
            ],
            [
                "robotics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The precise reconstruction of 3D objects from a single RGB image in complex scenes presents a critical challenge in virtual reality, autonomous driving, and robotics. Existing neural implicit 3D representation methods face significant difficulties in balancing the extraction of global and local features, particularly in diverse and complex environments, leading to insufficient reconstruction precision and quality. We propose M3D, a novel single-view 3D reconstruction framework, to tackle these challenges. This framework adopts a dual-stream feature extraction strategy based on Selective State Spaces to effectively balance the extraction of global and local features, thereby improving scene comprehension and representation precision. Additionally, a parallel branch extracts depth information, effectively integrating visual and geometric features to enhance reconstruction quality and preserve intricate details. Experimental results indicate that the fusion of multi-scale features with depth information via the dual-branch feature extraction significantly boosts geometric consistency and fidelity, achieving state-of-the-art reconstruction performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "9 pages, 4 figures"
    },
    {
        "paper id": "2411.12641",
        "abstract url": "https://arxiv.org/abs/2411.12641",
        "title": "Improving Controllability and Editability for Pretrained Text-to-Music Generation Models",
        "rating": "-2",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "Music",
                "Text-to-Music"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "The field of AI-assisted music creation has made significant strides, yet existing systems often struggle to meet the demands of iterative and nuanced music production. These challenges include providing sufficient control over the generated content and allowing for flexible, precise edits. This thesis tackles these issues by introducing a series of advancements that progressively build upon each other, enhancing the controllability and editability of text-to-music generation models. First, we introduce Loop Copilot, a system that tries to address the need for iterative refinement in music creation. Loop Copilot leverages a large language model (LLM) to coordinate multiple specialised AI models, enabling users to generate and refine music interactively through a conversational interface. Central to this system is the Global Attribute Table, which records and maintains key musical attributes throughout the iterative process, ensuring that modifications at any stage preserve the overall coherence of the music. While Loop Copilot excels in orchestrating the music creation process, it does not directly address the need for detailed edits to the generated content. To overcome this limitation, MusicMagus is presented as a further solution for editing AI-generated music. MusicMagus introduces a zero-shot text-to-music editing approach that allows for the modification of specific musical attributes, such as genre, mood, and instrumentation, without the need for retraining. By manipulating the latent space within pre-trained diffusion models, MusicMagus ensures that these edits are stylistically coherent and that non-targeted attributes remain unchanged. This system is particularly effective in maintaining the structural integrity of the music during edits, but it encounters challenges with more complex and real-world audio scenarios. ...",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": "PhD Thesis"
    },
    {
        "paper id": "2411.12666",
        "abstract url": "https://arxiv.org/abs/2411.12666",
        "title": "Steady-State Initialization of Object-Oriented Advanced Thermal Power Generation System Models with Application to the Case of the SOS-CO2 Cycle",
        "rating": "-2",
        "keywords": [
            [
                "Thermal"
            ]
        ],
        "abstract": "The forthcoming energy transition calls for a new generation of thermal power generation systems with low- or zero-emission and highly flexible operation. Dynamic modelling and simulation is a key enabling factor in this field, as controlling such plants is a difficult task for which there is no previous experience and very short design times are expected. The steady-state initialization of those dynamic models is an essential step in the design process, but is unfortunately a difficult task which involves the numerical solution of large systems of nonlinear equations with iterative Newton methods, which is often prone to numerical failures. In this work, several strategies and methodologies are discussed to successfully achieve steady-state initialization of first-principles equation-based, object-oriented models of advanced thermal power generation systems. These are presented in the context of the Modelica modelling language, but could be applied to other equation-based, object-oriented modelling and simulation environments. Finally, the successful application of such strategies and methodologies to the SOS-CO2 advanced power generation system is presented.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Submitted to Simulation Modelling Practice and Theory"
    },
    {
        "paper id": "2411.12667",
        "abstract url": "https://arxiv.org/abs/2411.12667",
        "title": "Machine Learning Approaches on Crop Pattern Recognition a Comparative Analysis",
        "rating": "-2",
        "keywords": [
            [
                "SVM",
                "support vector machine"
            ],
            [
                "Remote sensing",
                "agricultural"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Monitoring agricultural activities is important to ensure food security. Remote sensing plays a significant role for large-scale continuous monitoring of cultivation activities. Time series remote sensing data were used for the generation of the cropping pattern. Classification algorithms are used to classify crop patterns and mapped agriculture land used. Some conventional classification methods including support vector machine (SVM) and decision trees were applied for crop pattern recognition. However, in this paper, we are proposing Deep Neural Network (DNN) based classification to improve the performance of crop pattern recognition and make a comparative analysis with two (2) other machine learning approaches including Naive Bayes and Random Forest.",
        "subjects": [
            "cs.LG",
            "cs.CV"
        ],
        "comment": "Published in ICNTET2018: International Conference on New Trends in Engineering & Technology Tirupathi Highway, Tiruvallur Dist Chennai, India, September 7-8, 2018"
    },
    {
        "paper id": "2411.12669",
        "abstract url": "https://arxiv.org/abs/2411.12669",
        "title": "Constrained Coding and Deep Learning Aided Threshold Detection for Resistive Memories",
        "rating": "-2",
        "keywords": [
            [
                "physics"
            ]
        ],
        "abstract": "Resistive random access memory (ReRAM) is a promising emerging non-volatile memory (NVM) technology that shows high potential for both data storage and computing. However, its crossbar array architecture leads to the sneak path problem, which may severely degrade the reliability of data stored in the ReRAM cell. Due to the complication of memory physics and unique features of the sneak path induced interference (SPI), it is difficult to derive an accurate channel model for it. The deep learning (DL)-based detection scheme \\cite{zhong2020sneakdl} can better mitigate the SPI, at the cost of additional power consumption and read latency. In this letter, we first propose a novel CC scheme which can not only reduce the SPI in the memory array, but also effectively differentiate the memory arrays into two categories of sneak-path-free and sneak-path-affected arrays. For the sneak-path-free arrays, we can use a simple middle-point threshold detector to detect the low and high resistance cells of ReRAM. For the sneak-path-affected arrays, a DL detector is first trained off-line (prior to the data detection of ReRAM). To avoid the additional power consumption and latency introduced by the DL detector, we further propose a DL-based threshold detector, whose detection threshold can be derived based on the outputs of the DL detector. It is then utilized for the online data detection of all the identified sneak-path-affected arrays. Simulation results demonstrate that the above CC and DL aided threshold detection scheme can effectively mitigate the SPI of the ReRAM array and achieve better error rate performance than the prior art detection schemes, without the prior knowledge of the channel.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12672",
        "abstract url": "https://arxiv.org/abs/2411.12672",
        "title": "ISAC Super-Resolution Receivers: The Effect of Different Dictionary Matrices",
        "rating": "-2",
        "keywords": [
            [
                "Super-Resolution"
            ],
            [
                "radar"
            ]
        ],
        "abstract": "This paper presents an off-the-grid estimator for ISAC systems using lifted atomic norm minimization (LANM). The main challenge in the ISAC systems is the unknown nature of both transmitted signals and radar-communication channels. We use a known dictionary to encode transmit signals and show that LANM can localize radar targets and decode communication symbols when the number of observations is proportional to the system's degrees of freedom and the coherence of the dictionary matrix. We reformulate LANM using a dual method and solve it with semidefinite relaxation (SDR) for different dictionary matrices to reduce the number of observations required at the receiver. Simulations demonstrate that the proposed LANM accurately estimates communication data and target parameters under varying complexity by selecting different dictionary matrices.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12676",
        "abstract url": "https://arxiv.org/abs/2411.12676",
        "title": "IoT-Based 3D Pose Estimation and Motion Optimization for Athletes: Application of C3D and OpenPose",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "IoT"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "This study proposes the IoT-Enhanced Pose Optimization Network (IE-PONet) for high-precision 3D pose estimation and motion optimization of track and field athletes. IE-PONet integrates C3D for spatiotemporal feature extraction, OpenPose for real-time keypoint detection, and Bayesian optimization for hyperparameter tuning. Experimental results on NTURGB+D and FineGYM datasets demonstrate superior performance, with AP\\(^p50\\) scores of 90.5 and 91.0, and mAP scores of 74.3 and 74.0, respectively. Ablation studies confirm the essential roles of each module in enhancing model accuracy. IE-PONet provides a robust tool for athletic performance analysis and optimization, offering precise technical insights for training and injury prevention. Future work will focus on further model optimization, multimodal data integration, and developing real-time feedback mechanisms to enhance practical applications.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "17 pages"
    },
    {
        "paper id": "2411.12678",
        "abstract url": "https://arxiv.org/abs/2411.12678",
        "title": "Deep Learning-Driven Heat Map Analysis for Evaluating thickness of Wounded Skin Layers",
        "rating": "-2",
        "keywords": [
            [
                "depth"
            ],
            [
                "diagnosis",
                "clinical"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Understanding the appropriate skin layer thickness in wounded sites is an important tool to move forward on wound healing practices and treatment protocols. Methods to measure depth often are invasive and less specific. This paper introduces a novel method that is non-invasive with deep learning techniques using classifying of skin layers that helps in measurement of wound depth through heatmap analysis. A set of approximately 200 labeled images of skin allows five classes to be distinguished: scars, wounds, and healthy skin, among others. Each image has annotated key layers, namely the stratum cornetum, the epidermis, and the dermis, in the software Roboflow. In the preliminary stage, the Heatmap generator VGG16 was used to enhance the visibility of tissue layers, based upon which their annotated images were used to train ResNet18 with early stopping techniques. It ended up at a very high accuracy rate of 97.67%. To do this, the comparison of the models ResNet18, VGG16, DenseNet121, and EfficientNet has been done where both EfficientNet and ResNet18 have attained accuracy rates of almost 95.35%. For further hyperparameter tuning, EfficientNet and ResNet18 were trained at six different learning rates to determine the best model configuration. It has been noted that the accuracy has huge variations with different learning rates. In the case of EfficientNet, the maximum achievable accuracy was 95.35% at the rate of 0.0001. The same was true for ResNet18, which also attained its peak value of 95.35% at the same rate. These facts indicate that the model can be applied and utilized in actual-time, non-invasive wound assessment, which holds a great promise to improve clinical diagnosis and treatment planning.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12681",
        "abstract url": "https://arxiv.org/abs/2411.12681",
        "title": "AI Guided Early Screening of Cervical Cancer",
        "rating": "-2",
        "keywords": [
            [
                "anomaly detection"
            ],
            [
                "medical",
                "Cancer",
                "clinical",
                "pathological"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "In order to support the creation of reliable machine learning models for anomaly detection, this project focuses on preprocessing, enhancing, and organizing a medical imaging dataset. There are two classifications in the dataset: normal and abnormal, along with extra noise fluctuations. In order to improve the photographs' quality, undesirable artifacts, including visible medical equipment at the edges, were eliminated using central cropping. Adjusting the brightness and contrast was one of the additional preprocessing processes. Normalization was then performed to normalize the data. To make classification jobs easier, the dataset was methodically handled by combining several image subsets into two primary categories: normal and pathological. To provide a strong training set that adapts well to real-world situations, sophisticated picture preprocessing techniques were used, such as contrast enhancement and real-time augmentation (including rotations, zooms, and brightness modifications). To guarantee efficient model evaluation, the data was subsequently divided into training and testing subsets. In order to create precise and effective machine learning models for medical anomaly detection, high-quality input data is ensured via this thorough approach. Because of the project pipeline's flexible and scalable design, it can be easily integrated with bigger clinical decision-support systems.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12682",
        "abstract url": "https://arxiv.org/abs/2411.12682",
        "title": "Distributed Coordination of Grid-Forming and Grid-Following Inverter-Based Resources for Optimal Frequency Control in Power Systems",
        "rating": "-2",
        "keywords": [
            [
                "thermal"
            ]
        ],
        "abstract": "With the fast-growing penetration of power inverter-interfaced renewable generation, power systems face significant challenges in maintaining power balance and the nominal frequency. This paper studies the grid-level coordinated control of a mix of grid-forming (GFM) and grid-following (GFL) inverter-based resources (IBRs) for power system frequency regulation at scale. Specifically, a fully distributed optimal frequency control algorithm is proposed by leveraging the projected primal-dual gradient method and the structure of the physical system dynamics. This algorithm 1) restores the nominal frequency, 2) minimizes the total control cost, 3) respects the IBR power limits and the line thermal constraints, and 4) is implemented in a distributed fashion that only needs local measurement and local communication. The effectiveness and optimality of the proposed algorithm are demonstrated through high-fidelity electromagnetic transient (EMT) simulations on the IEEE 39-bus system.",
        "subjects": [
            "eess.SY",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12783",
        "abstract url": "https://arxiv.org/abs/2411.12783",
        "title": "Med-2E3: A 2D-Enhanced 3D Medical Multimodal Large Language Model",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "Medical",
                "healthcare",
                "clinical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The analysis of 3D medical images is crucial for modern healthcare, yet traditional task-specific models are becoming increasingly inadequate due to limited generalizability across diverse clinical scenarios. Multimodal large language models (MLLMs) offer a promising solution to these challenges. However, existing MLLMs have limitations in fully leveraging the rich, hierarchical information embedded in 3D medical images. Inspired by clinical practice, where radiologists focus on both 3D spatial structure and 2D planar content, we propose Med-2E3, a novel MLLM for 3D medical image analysis that integrates 3D and 2D encoders. To aggregate 2D features more effectively, we design a Text-Guided Inter-Slice (TG-IS) scoring module, which scores the attention of each 2D slice based on slice contents and task instructions. To the best of our knowledge, Med-2E3 is the first MLLM to integrate both 3D and 2D features for 3D medical image analysis. Experiments on a large-scale, open-source 3D medical multimodal benchmark demonstrate that Med-2E3 exhibits task-specific attention distribution and significantly outperforms current state-of-the-art models, with a 14% improvement in report generation and a 5% gain in medical visual question answering (VQA), highlighting the model's potential in addressing complex multimodal clinical tasks. The code will be released upon acceptance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12789",
        "abstract url": "https://arxiv.org/abs/2411.12789",
        "title": "Automated 3D Physical Simulation of Open-world Scene with Gaussian Splatting",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "physics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in 3D generation models have opened new possibilities for simulating dynamic 3D object movements and customizing behaviors, yet creating this content remains challenging. Current methods often require manual assignment of precise physical properties for simulations or rely on video generation models to predict them, which is computationally intensive. In this paper, we rethink the usage of multi-modal large language model (MLLM) in physics-based simulation, and present Sim Anything, a physics-based approach that endows static 3D objects with interactive dynamics. We begin with detailed scene reconstruction and object-level 3D open-vocabulary segmentation, progressing to multi-view image in-painting. Inspired by human visual reasoning, we propose MLLM-based Physical Property Perception (MLLM-P3) to predict mean physical properties of objects in a zero-shot manner. Based on the mean values and the object's geometry, the Material Property Distribution Prediction model (MPDP) model then estimates the full distribution, reformulating the problem as probability distribution estimation to reduce computational costs. Finally, we simulate objects in an open-world scene with particles sampled via the Physical-Geometric Adaptive Sampling (PGAS) strategy, efficiently capturing complex deformations and significantly reducing computational costs. Extensive experiments and user studies demonstrate our Sim Anything achieves more realistic motion than state-of-the-art methods within 2 minutes on a single GPU.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12833",
        "abstract url": "https://arxiv.org/abs/2411.12833",
        "title": "Efficient Medicinal Image Transmission and Resolution Enhancement via GAN",
        "rating": "-2",
        "keywords": [
            [
                "GAN",
                "super-resolution"
            ],
            [
                "medical",
                "diagnosis",
                "X-ray"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "While X-ray imaging is indispensable in medical diagnostics, it inherently carries with it those noises and limitations on resolution that mask the details necessary for diagnosis. B/W X-ray images require a careful balance between noise suppression and high-detail preservation to ensure clarity in soft-tissue structures and bone edges. While traditional methods, such as CNNs and early super-resolution models like ESRGAN, have enhanced image resolution, they often perform poorly regarding high-frequency detail preservation and noise control for B/W imaging. We are going to present one efficient approach that improves the quality of an image with the optimization of network transmission in the following paper. The pre-processing of X-ray images into low-resolution files by Real-ESRGAN, a version of ESRGAN elucidated and improved, helps reduce the server load and transmission bandwidth. Lower-resolution images are upscaled at the receiving end using Real-ESRGAN, fine-tuned for real-world image degradation. The model integrates Residual-in-Residual Dense Blocks with perceptual and adversarial loss functions for high-quality upscaled images with low noise. We further fine-tune Real-ESRGAN by adapting it to the specific B/W noise and contrast characteristics. This suppresses noise artifacts without compromising detail. The comparative evaluation conducted shows that our approach achieves superior noise reduction and detail clarity compared to state-of-the-art CNN-based and ESRGAN models, apart from reducing network bandwidth requirements. These benefits are confirmed both by quantitative metrics, including Peak Signal-to-Noise Ratio and Structural Similarity Index, and by qualitative assessments, which indicate the potential of Real-ESRGAN for diagnostic-quality X-ray imaging and for efficient medical data transmission.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12851",
        "abstract url": "https://arxiv.org/abs/2411.12851",
        "title": "GenAI Assistance for Deep Reinforcement Learning-based VNF Placement and SFC Provisioning in 5G Cores",
        "rating": "-2",
        "keywords": [
            [
                "5G"
            ]
        ],
        "abstract": "Virtualization technology, Network Function Virtualization (NFV), gives flexibility to communication and 5G core network technologies for dynamic and efficient resource allocation while reducing the cost and dependability of the physical infrastructure. In the NFV context, Service Function Chain (SFC) refers to the ordered arrangement of various Virtual Network Functions (VNFs). To provide an automated SFC provisioning algorithm that satisfies high demands of SFC requests having ultra-reliable and low latency communication (URLLC) requirements, in the literature, Artificial Intelligence (AI) modules and Deep Reinforcement Learning (DRL) algorithms are investigated in detail. This research proposes a generative Variational Autoencoder (VAE) assisted advanced-DRL module for handling SFC requests in a dynamic environment where network configurations and request amounts can be changed. Using the hybrid approach, including generative VAE and DRL, the algorithm leverages several advantages, such as dimensionality reduction, better generalization on the VAE side, exploration, and trial-error learning from the DRL model. Results show that GenAI-assisted DRL surpasses the state-of-the-art model of DRL in SFC provisioning in terms of SFC acceptance ratio, E2E delay, and throughput maximization.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "6 pages, 5 figures"
    },
    {
        "paper id": "2411.12852",
        "abstract url": "https://arxiv.org/abs/2411.12852",
        "title": "Enhanced Cross-Dataset Electroencephalogram-based Emotion Recognition using Unsupervised Domain Adaptation",
        "rating": "-2",
        "keywords": [
            [
                "healthcare",
                "EEG"
            ]
        ],
        "abstract": "Emotion recognition has significant potential in healthcare and affect-sensitive systems such as brain-computer interfaces (BCIs). However, challenges such as the high cost of labeled data and variability in electroencephalogram (EEG) signals across individuals limit the applicability of EEG-based emotion recognition models across domains. These challenges are exacerbated in cross-dataset scenarios due to differences in subject demographics, recording devices, and presented stimuli. To address these issues, we propose a novel approach to improve cross-domain EEG-based emotion classification. Our method, Gradual Proximity-guided Target Data Selection (GPTDS), incrementally selects reliable target domain samples for training. By evaluating their proximity to source clusters and the models confidence in predicting them, GPTDS minimizes negative transfer caused by noisy and diverse samples. Additionally, we introduce Prediction Confidence-aware Test-Time Augmentation (PC-TTA), a cost-effective augmentation technique. Unlike traditional TTA methods, which are computationally intensive, PC-TTA activates only when model confidence is low, improving inference performance while drastically reducing computational costs. Experiments on the DEAP and SEED datasets validate the effectiveness of our approach. When trained on DEAP and tested on SEED, our model achieves 67.44% accuracy, a 7.09% improvement over the baseline. Conversely, training on SEED and testing on DEAP yields 59.68% accuracy, a 6.07% improvement. Furthermore, PC-TTA reduces computational time by a factor of 15 compared to traditional TTA methods. Our method excels in detecting both positive and negative emotions, demonstrating its practical utility in healthcare applications. Code available at: https://github.com/RyersonMultimediaLab/EmotionRecognitionUDA",
        "subjects": [
            "eess.SP"
        ],
        "comment": "In press: Computers in Biology and Medicine"
    },
    {
        "paper id": "2411.12869",
        "abstract url": "https://arxiv.org/abs/2411.12869",
        "title": "Omnidirectional Wireless Power Transfer for Millimetric Magnetoelectric Biomedical Implants",
        "rating": "-2",
        "keywords": [
            [
                "Biomedical",
                "surgery"
            ]
        ],
        "abstract": "Miniature bioelectronic implants promise revolutionary therapies for cardiovascular and neurological disorders. Wireless power transfer (WPT) is a significant method for miniaturization, eliminating the need for bulky batteries in devices. Despite successful demonstrations of millimetric battery free implants in animal models, the robustness and efficiency of WPT are known to degrade significantly under misalignment incurred by body movements, respiration, heart beating, and limited control of implant orientation during surgery. This article presents an omnidirectional WPT platform for millimetric bioelectronic implants, employing the emerging magnetoelectric (ME) WPT modality, and magnetic field steering technique based on multiple transmitter (TX) coils. To accurately sense the weak coupling in a miniature implant and adaptively control the multicoil TX array in a closed loop, we develop an active echo (AE) scheme using a tiny coil on the implant. Our prototype comprises a fully integrated 14.2 mm3 implantable stimulator embedding a custom low power system on chip (SoC) powered by an ME film, a TX with a custom three channel AE RX chip, and a multicoil TX array with mutual inductance cancellation. The AE RX achieves negative 161 dBm per Hz input referred noise with 64 dB gain tuning range to reliably sense the AE signal, and offers fast polarity detection for driver control. AE simultaneously enhances the robustness, efficiency, and charging range of ME WPT. Under 90 degree rotation from the ideal position, our omnidirectional WPT system achieves 6.8x higher power transfer efficiency (PTE) than a single coil baseline. The tracking error of AE negligibly degrades the PTE by less than 2 percent from using ideal control.",
        "subjects": [
            "eess.SY",
            "physics.med-ph"
        ],
        "comment": "13 pages, 27 figures"
    },
    {
        "paper id": "2411.12879",
        "abstract url": "https://arxiv.org/abs/2411.12879",
        "title": "Wireless Sensor Networks based on TSCH/TDMA with Power Consumption and Latency Constraints",
        "rating": "-2",
        "keywords": [
            [
                "industrial"
            ]
        ],
        "abstract": "One of the main goals of wireless sensor networks is to permit the involved nodes to communicate with low energy budgets, as they are typically battery-powered. When such networks are employed in industrial scenarios, constraints about latency may have a significant role, too. The TSCH mechanism, and more in general TDMA schemes, rely on traffic scheduling, and consequently they can feature low power consumption and more predictable latency. Some recent proposals like PRIL-M enable further consistent energy savings, but unfortunately they cause at the same time a dramatic increase in latency. This work presents an extension of PRIL-M, we named PRIL-ML, that achieves a significantly shorter latency in exchange for a slight increase in power consumption. Its operating principles are first illustrated, then some approximate equations are provided for assessing analytically the improvements it achieves, starting from simulation results obtained for both standard TSCH and the original PRIL-M technique.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "preprint, 4 pages, 2024"
    },
    {
        "paper id": "2411.12888",
        "abstract url": "https://arxiv.org/abs/2411.12888",
        "title": "An Experimental Multi-Band Channel Characterization in the Upper Mid-Band",
        "rating": "-2",
        "keywords": [
            [
                "MUSIC"
            ]
        ],
        "abstract": "The following paper provides a multi-band channel measurement analysis on the frequency range (FR)3. This study focuses on the FR3 low frequencies 6.5 GHz and 8.75 GHz with a setup tailored to the context of integrated sensing and communication (ISAC), where the data are collected with and without the presence of a target. A method based on multiple signal classification (MUSIC) is used to refine the delays of the channel impulse response estimates. The results reveal that the channel at the lower frequency 6.5 GHz has additional distinguishable multipath components in the presence of the target, while the one associated with the higher frequency 8.75 GHz has more blockage. The set of results reported in this paper serves as a benchmark for future multi-band studies in the FR3 spectrum.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12919",
        "abstract url": "https://arxiv.org/abs/2411.12919",
        "title": "Enhancing Deep Learning-Driven Multi-Coil MRI Reconstruction via Self-Supervised Denoising",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "MRI"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "We examine the effect of incorporating self-supervised denoising as a pre-processing step for training deep learning (DL) based reconstruction methods on data corrupted by Gaussian noise. K-space data employed for training are typically multi-coil and inherently noisy. Although DL-based reconstruction methods trained on fully sampled data can enable high reconstruction quality, obtaining large, noise-free datasets is impractical. We leverage Generalized Stein's Unbiased Risk Estimate (GSURE) for denoising. We evaluate two DL-based reconstruction methods: Diffusion Probabilistic Models (DPMs) and Model-Based Deep Learning (MoDL). We evaluate the impact of denoising on the performance of these DL-based methods in solving accelerated multi-coil magnetic resonance imaging (MRI) reconstruction. The experiments were carried out on T2-weighted brain and fat-suppressed proton-density knee scans. We observed that self-supervised denoising enhances the quality and efficiency of MRI reconstructions across various scenarios. Specifically, employing denoised images rather than noisy counterparts when training DL networks results in lower normalized root mean squared error (NRMSE), higher structural similarity index measure (SSIM) and peak signal-to-noise ratio (PSNR) across different SNR levels, including 32dB, 22dB, and 12dB for T2-weighted brain data, and 24dB, 14dB, and 4dB for fat-suppressed knee data. Overall, we showed that denoising is an essential pre-processing technique capable of improving the efficacy of DL-based MRI reconstruction methods under diverse conditions. By refining the quality of input data, denoising can enable the training of more effective DL networks, potentially bypassing the need for noise-free reference MRI scans.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12920",
        "abstract url": "https://arxiv.org/abs/2411.12920",
        "title": "Quantum Mini-Apps for Engineering Applications: A Case Study",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "In this work, we present a case study in implementing a variational quantum algorithm for solving the Poisson equation, which is a commonly encountered partial differential equation in science and engineering. We highlight the practical challenges encountered in mapping the algorithm to physical hardware, and the software engineering considerations needed to achieve realistic results on today's non-fault-tolerant systems.",
        "subjects": [
            "quant-ph",
            "cs.ET"
        ],
        "comment": "8 pages, 6 figures, submitted on 2024-10-10 to IEEE Quantum Software and its Engineering"
    },
    {
        "paper id": "2411.12981",
        "abstract url": "https://arxiv.org/abs/2411.12981",
        "title": "GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "NeRF",
                "radiance fields"
            ],
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Gaze estimation encounters generalization challenges when dealing with out-of-distribution data. To address this problem, recent methods use neural radiance fields (NeRF) to generate augmented data. However, existing methods based on NeRF are computationally expensive and lack facial details. 3D Gaussian Splatting (3DGS) has become the prevailing representation of neural fields. While 3DGS has been extensively examined in head avatars, it faces challenges with accurate gaze control and generalization across different subjects. In this work, we propose GazeGaussian, a high-fidelity gaze redirection method that uses a two-stream 3DGS model to represent the face and eye regions separately. By leveraging the unstructured nature of 3DGS, we develop a novel eye representation for rigid eye rotation based on the target gaze direction. To enhance synthesis generalization across various subjects, we integrate an expression-conditional module to guide the neural renderer. Comprehensive experiments show that GazeGaussian outperforms existing methods in rendering speed, gaze redirection accuracy, and facial synthesis across multiple datasets. We also demonstrate that existing gaze estimation methods can leverage GazeGaussian to improve their generalization performance. The code will be available at: https://ucwxb.github.io/GazeGaussian/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12989",
        "abstract url": "https://arxiv.org/abs/2411.12989",
        "title": "Data Watermarking for Sequential Recommender Systems",
        "rating": "-2",
        "keywords": [
            [
                "recommendation",
                "Watermarking"
            ]
        ],
        "abstract": "In the era of large foundation models, data has become a crucial component for building high-performance AI systems. As the demand for high-quality and large-scale data continues to rise, data copyright protection is attracting increasing attention. In this work, we explore the problem of data watermarking for sequential recommender systems, where a watermark is embedded into the target dataset and can be detected in models trained on that dataset. We address two specific challenges: dataset watermarking, which protects the ownership of the entire dataset, and user watermarking, which safeguards the data of individual users. We systematically define these problems and present a method named DWRS to address them. Our approach involves randomly selecting unpopular items to create a watermark sequence, which is then inserted into normal users' interaction sequences. Extensive experiments on five representative sequential recommendation models and three benchmark datasets demonstrate the effectiveness of DWRS in protecting data copyright while preserving model utility.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.15200",
        "abstract url": "https://arxiv.org/abs/2411.15200",
        "title": "Deep Learning-Based Classification of Hyperkinetic Movement Disorders in Children",
        "rating": "-2",
        "keywords": [
            [
                "Graph"
            ],
            [
                "diagnosis",
                "clinical"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Hyperkinetic movement disorders (HMDs) in children, including dystonia (abnormal twisting) and chorea (irregular, random movements), pose significant diagnostic challenges due to overlapping clinical features. The prevalence of dystonia ranges from 2 to 50 per million, and chorea from 5 to 10 per 100,000. These conditions are often diagnosed with delays averaging 4.75 to 7.83 years. Traditional diagnostic methods depend on clinical history and expert physical examinations, but specialized tests are ineffective due to the complex pathophysiology of these disorders. This study develops a neural network model to differentiate between dystonia and chorea from video recordings of paediatric patients performing motor tasks. The model integrates a Graph Convolutional Network (GCN) to capture spatial relationships and Long Short-Term Memory (LSTM) networks to account for temporal dynamics. Attention mechanisms were incorporated to improve model interpretability. The model was trained and validated on a dataset of 50 videos (31 chorea-predominant, 19 dystonia-predominant) collected under regulatory approval from Guy's and St Thomas' NHS Foundation Trust. The model achieved 85% accuracy, 81% sensitivity, and 88% specificity at 15 frames per second. Attention maps highlighted the model's ability to correctly identify involuntary movement patterns, with misclassifications often due to occluded body parts or subtle movement variations. This work demonstrates the potential of deep learning to improve the accuracy and efficiency of HMD diagnosis and could contribute to more reliable, interpretable clinical tools.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "eess.IV"
        ],
        "comment": "59 pages, 20 figures"
    },
    {
        "paper id": "2411.12244",
        "abstract url": "https://arxiv.org/abs/2411.12244",
        "title": "Hyper-parameter Optimization for Federated Learning with Step-wise Adaptive Mechanism",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "IoT"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL) is a decentralized learning approach that protects sensitive information by utilizing local model parameters rather than sharing clients' raw datasets. While this privacy-preserving method is widely employed across various applications, it still requires significant development and optimization. Automated Machine Learning (Auto-ML) has been adapted for reducing the need for manual adjustments. Previous studies have explored the integration of AutoML with different FL algorithms to evaluate their effectiveness in enhancing FL settings. However, Automated FL (Auto-FL) faces additional challenges due to the involvement of a large cohort of clients and global training rounds between clients and the server, rendering the tuning process time-consuming and nearly impossible on resource-constrained edge devices (e.g., IoT devices). This paper investigates the deployment and integration of two lightweight Hyper-Parameter Optimization (HPO) tools, Raytune and Optuna, within the context of FL settings. A step-wise feedback mechanism has also been designed to accelerate the hyper-parameter tuning process and coordinate AutoML toolkits with the FL server. To this end, both local and global feedback mechanisms are integrated to limit the search space and expedite the HPO process. Further, a novel client selection technique is introduced to mitigate the straggler effect in Auto-FL. The selected hyper-parameter tuning tools are evaluated using two benchmark datasets, FEMNIST, and CIFAR10. Further, the paper discusses the essential properties of successful HPO tools, the integration mechanism with the FL pipeline, and the challenges posed by the distributed and heterogeneous nature of FL environments.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12640",
        "abstract url": "https://arxiv.org/abs/2411.12640",
        "title": "Leadsee-Precip: A Deep Learning Diagnostic Model for Precipitation",
        "rating": "-2.5",
        "keywords": [
            [
                "radar"
            ],
            [
                "forecasting",
                "satellite"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recently, deep-learning weather forecasting models have surpassed traditional numerical models in terms of the accuracy of meteorological variables. However, there is considerable potential for improvements in precipitation forecasts, especially for heavy precipitation events. To address this deficiency, we propose Leadsee-Precip, a global deep learning model to generate precipitation from meteorological circulation fields. The model utilizes an information balance scheme to tackle the challenges of predicting heavy precipitation caused by the long-tail distribution of precipitation data. Additionally, more accurate satellite and radar-based precipitation retrievals are used as training targets. Compared to artificial intelligence global weather models, the heavy precipitation from Leadsee-Precip is more consistent with observations and shows competitive performance against global numerical weather prediction models. Leadsee-Precip can be integrated with any global circulation model to generate precipitation forecasts. But the deviations between the predicted and the ground-truth circulation fields may lead to a weakened precipitation forecast, which could potentially be mitigated by further fine-tuning based on the predicted circulation fields.",
        "subjects": [
            "physics.ao-ph",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12948",
        "abstract url": "https://arxiv.org/abs/2411.12948",
        "title": "Machine learned reconstruction of tsunami dynamics from sparse observations",
        "rating": "-2.5",
        "keywords": [
            [
                "Radar"
            ],
            [
                "remote sensing",
                "forecasting",
                "satellite"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We investigate the use of the Senseiver, a transformer neural network designed for sparse sensing applications, to estimate full-field surface height measurements of tsunami waves from sparse observations. The model is trained on a large ensemble of simulated data generated via a shallow water equations solver, which we show to be a faithful reproduction for the underlying dynamics by comparison to historical events. We train the model on a dataset consisting of 8 tsunami simulations whose epicenters correspond to historical USGS earthquake records, and where the model inputs are restricted to measurements obtained at actively deployed buoy locations. We test the Senseiver on a dataset consisting of 8 simulations not included in training, demonstrating its capability for extrapolation. The results show remarkable resolution of fine scale phase and amplitude features from the true field, provided that at least a few of the sensors have obtained a non-zero signal. Throughout, we discuss which forecasting techniques can be improved by this method, and suggest ways in which the flexibility of the architecture can be leveraged to incorporate arbitrary remote sensing data (eg. HF Radar and satellite measurements) as well as investigate optimal sensor placements.",
        "subjects": [
            "cs.LG",
            "physics.flu-dyn"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12967",
        "abstract url": "https://arxiv.org/abs/2411.12967",
        "title": "Shrinking POMCP: A Framework for Real-Time UAV Search and Rescue",
        "rating": "-2.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "UAV"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Efficient path optimization for drones in search and rescue operations faces challenges, including limited visibility, time constraints, and complex information gathering in urban environments. We present a comprehensive approach to optimize UAV-based search and rescue operations in neighborhood areas, utilizing both a 3D AirSim-ROS2 simulator and a 2D simulator. The path planning problem is formulated as a partially observable Markov decision process (POMDP), and we propose a novel ``Shrinking POMCP'' approach to address time constraints. In the AirSim environment, we integrate our approach with a probabilistic world model for belief maintenance and a neurosymbolic navigator for obstacle avoidance. The 2D simulator employs surrogate ROS2 nodes with equivalent functionality. We compare trajectories generated by different approaches in the 2D simulator and evaluate performance across various belief types in the 3D AirSim-ROS simulator. Experimental results from both simulators demonstrate that our proposed shrinking POMCP solution achieves significant improvements in search times compared to alternative methods, showcasing its potential for enhancing the efficiency of UAV-assisted search and rescue operations.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "Accepted to the The 3rd International Conference on Assured Autonomy"
    },
    {
        "paper id": "2411.12248",
        "abstract url": "https://arxiv.org/abs/2411.12248",
        "title": "Neuro-3D: Towards 3D Visual Decoding from EEG Signals",
        "rating": "-3",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "diffusion"
            ],
            [
                "EEG"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Human's perception of the visual world is shaped by the stereo processing of 3D information. Understanding how the brain perceives and processes 3D visual stimuli in the real world has been a longstanding endeavor in neuroscience. Towards this goal, we introduce a new neuroscience task: decoding 3D visual perception from EEG signals, a neuroimaging technique that enables real-time monitoring of neural dynamics enriched with complex visual cues. To provide the essential benchmark, we first present EEG-3D, a pioneering dataset featuring multimodal analysis data and extensive EEG recordings from 12 subjects viewing 72 categories of 3D objects rendered in both videos and images. Furthermore, we propose Neuro-3D, a 3D visual decoding framework based on EEG signals. This framework adaptively integrates EEG features derived from static and dynamic stimuli to learn complementary and robust neural representations, which are subsequently utilized to recover both the shape and color of 3D objects through the proposed diffusion-based colored point cloud decoder. To the best of our knowledge, we are the first to explore EEG-based 3D visual decoding. Experiments indicate that Neuro-3D not only reconstructs colored 3D objects with high fidelity, but also learns effective neural representations that enable insightful brain region analysis. The dataset and associated code will be made publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12273",
        "abstract url": "https://arxiv.org/abs/2411.12273",
        "title": "Acquire Precise and Comparable Fundus Image Quality Score: FTHNet and FQS Dataset",
        "rating": "-3",
        "keywords": [
            [
                "medical",
                "diagnosis",
                "retinal"
            ],
            [
                "quality assessment"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The retinal fundus images are utilized extensively in the diagnosis, and their quality can directly affect the diagnosis results. However, due to the insufficient dataset and algorithm application, current fundus image quality assessment (FIQA) methods are not powerful enough to meet ophthalmologists` demands. In this paper, we address the limitations of datasets and algorithms in FIQA. First, we establish a new FIQA dataset, Fundus Quality Score(FQS), which includes 2246 fundus images with two labels: a continuous Mean Opinion Score varying from 0 to 100 and a three-level quality label. Then, we propose a FIQA Transformer-based Hypernetwork (FTHNet) to solve these tasks with regression results rather than classification results in conventional FIQA works. The FTHNet is optimized for the FIQA tasks with extensive experiments. Results on our FQS dataset show that the FTHNet can give quality scores for fundus images with PLCC of 0.9423 and SRCC of 0.9488, significantly outperforming other methods with fewer parameters and less computation complexity.We successfully build a dataset and model addressing the problems of current FIQA methods. Furthermore, the model deployment experiments demonstrate its potential in automatic medical image quality control. All experiments are carried out with 10-fold cross-validation to ensure the significance of the results.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "11 pages, 7 figures"
    },
    {
        "paper id": "2411.12298",
        "abstract url": "https://arxiv.org/abs/2411.12298",
        "title": "Dual-Functional FMCW Waveform for Terahertz Space Debris Detection and Inter-Satellite Communications",
        "rating": "-3",
        "keywords": [
            [
                "radar"
            ],
            [
                "Satellite"
            ]
        ],
        "abstract": "Terahertz (THz) band communication, ranging from 0.1 THz to 10 THz, is envisioned as a key enabling technology for next-generation networks and future applications such as inter-satellite communications and environmental sensing. The surging number of space debris in Low Earth Orbit poses a big threat to orbital infrastructure and the development of the space economy. In particular, despite the ability to detect and track large-scale space debris, millions of space debris with a radius within the range of 0.1-10 cm and velocity exceeding 1 km/s remains hard to detect with conventional ground-based radars and optical telescopes. In this study, a dual-functional frequency modulated continuous waveform (FMCW) operating in the THz band is adopted for space debris sensing and inter-satellite communications. Specifically, the radar cross section of space debris with various sizes in the THz band is analyzed to demonstrate the feasibility of THz space debris detection. A joint space debris detection and inter-satellite communications based on the FMCW waveform is derived. Then, the parameter estimation and demodulation algorithms are illustrated. Extensive simulations demonstrate that the proposed method can realize high-accuracy parameter estimation of hypervelocity space debris while achieving high reliability for inter-satellite communications.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12312",
        "abstract url": "https://arxiv.org/abs/2411.12312",
        "title": "Age of Information Minimization in UAV-Assisted Covert Communication: Trajectory and Beamforming Design",
        "rating": "-3",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "UAV"
            ]
        ],
        "abstract": "Unmanned aerial vehicles (UAVs) have the potential for time-sensitive applications. Due to wireless channel variation, received data may have an expiration time, particularly in critical situations such as rescue operations, natural disasters, or the military. Age of Information (AoI) is a metric that measures the freshness of received packets to specify the validity period of information. In addition, it is necessary to guarantee the privacy of confidential information transmission through air-to-ground links against eavesdroppers. This paper investigates UAV-assisted covert communication to minimize AoI in the presence of an aerial eavesdropper for the first time. However, to ensure the eavesdropper's error detection rate, UAV-enabled beamforming employs the power-domain non-orthogonal multiple access (PD-NOMA) technique to cover the covert user by a public user. PD-NOMA technique significantly improves the user's AoI, too. The joint optimization problem contains non-convex constraints and coupled optimization variables, including UAV trajectory, beamforming design, and the user's AoI which is challenging to derive a direct solution. We have developed an efficient alternating optimization technique to address the formulated optimization problem. Numerical results demonstrate the impact of the main parameters on the performance of the proposed communication system.",
        "subjects": [
            "eess.SY",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12516",
        "abstract url": "https://arxiv.org/abs/2411.12516",
        "title": "MAViS: Modular Autonomous Virtualization System for Two-Dimensional Semiconductor Quantum Dot Arrays",
        "rating": "-3",
        "keywords": [
            [
                "chemical"
            ],
            [
                "Quantum"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Arrays of gate-defined semiconductor quantum dots are among the leading candidates for building scalable quantum processors. High-fidelity initialization, control, and readout of spin qubit registers require exquisite and targeted control over key Hamiltonian parameters that define the electrostatic environment. However, due to the tight gate pitch, capacitive crosstalk between gates hinders independent tuning of chemical potentials and interdot couplings. While virtual gates offer a practical solution, determining all the required cross-capacitance matrices accurately and efficiently in large quantum dot registers is an open challenge. Here, we establish a Modular Automated Virtualization System (MAViS) -- a general and modular framework for autonomously constructing a complete stack of multi-layer virtual gates in real time. Our method employs machine learning techniques to rapidly extract features from two-dimensional charge stability diagrams. We then utilize computer vision and regression models to self-consistently determine all relative capacitive couplings necessary for virtualizing plunger and barrier gates in both low- and high-tunnel-coupling regimes. Using MAViS, we successfully demonstrate accurate virtualization of a dense two-dimensional array comprising ten quantum dots defined in a high-quality Ge/SiGe heterostructure. Our work offers an elegant and practical solution for the efficient control of large-scale semiconductor quantum dot systems.",
        "subjects": [
            "cond-mat.mes-hall",
            "cs.CV",
            "cs.ET",
            "cs.LG",
            "quant-ph"
        ],
        "comment": "14 pages, 5 figures, 8 pages of supplemental material"
    },
    {
        "paper id": "2411.12535",
        "abstract url": "https://arxiv.org/abs/2411.12535",
        "title": "Multilayer occupancy grid for obstacle avoidance in an autonomous ground vehicle using RGB-D camera",
        "rating": "-3",
        "keywords": [
            [
                "RGB-D",
                "depth"
            ],
            [
                "LIDAR",
                "vehicle"
            ],
            [
                "navigation"
            ]
        ],
        "abstract": "This work describes the process of integrating a depth camera into the navigation system of a self-driving ground vehicle (SDV) and the implementation of a multilayer costmap that enhances the vehicle's obstacle identification process by expanding its two-dimensional field of view, based on 2D LIDAR, to a three-dimensional perception system using an RGB-D camera. This approach lays the foundation for a robust vision-based navigation and obstacle detection system. A theoretical review is presented and implementation results are discussed for future work.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12549",
        "abstract url": "https://arxiv.org/abs/2411.12549",
        "title": "Tactile interaction with social robots influences attitudes and behaviour",
        "rating": "-3",
        "keywords": [
            [
                "robot"
            ],
            [
                "physiological"
            ]
        ],
        "abstract": "Tactile interaction plays an essential role in human-to-human interaction. People gain comfort and support from tactile interactions with others and touch is an important predictor for trust. While touch has been explored as a communicative modality in HCI and HRI, we here report on two studies in which touching a social robot is used to regulate people's stress levels and consequently their actions. In the first study, we look at whether different intensities of tactile interaction result in a physiological response related to stress, and whether the interaction impacts risk-taking behaviour and trust. We let 38 participants complete a Balloon Analogue Risk Task (BART), a computer-based game that serves as a proxy for risk-taking behaviour. In our study, participants are supported by a robot during the BART task. The robot builds trust and encourages participants to take more risk. The results show that affective tactile interaction with the robot increases participants' risk-taking behaviour, but gentle affective tactile interaction increases comfort and lowers stress whereas high-intensity touch does not. We also find that male participants exhibit more risk-taking behaviour than females while being less stressed. Based on this experiment, a second study is used to ascertain whether these effects are caused by the social nature of tactile interaction or by the physical interaction alone. For this, instead of a social robot, participants now have a tactile interaction with a non-social device. The non-social interaction does not result in any effect, leading us to conclude that tactile interaction with humanoid robots is a social phenomenon rather than a mere physical phenomenon.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12632",
        "abstract url": "https://arxiv.org/abs/2411.12632",
        "title": "Securing Satellite Link Segment: A Secure-by-Component Design",
        "rating": "-3",
        "keywords": [
            [
                "attack"
            ],
            [
                "Satellite"
            ]
        ],
        "abstract": "The rapid evolution of communication technologies, compounded by recent geopolitical events such as the Viasat cyberattack in February 2022, has highlighted the urgent need for fast and reliable satellite missions for military and civil security operations. Consequently, this paper examines two Earth observation (EO) missions: one utilizing a single low Earth orbit (LEO) satellite and another through a network of LEO satellites, employing a secure-by-component design strategy. This approach begins by defining the scope of technical security engineering, decomposing the system into components and data flows, and enumerating attack surfaces. Then it proceeds by identifying threats to low-level components, applying secure-by-design principles, redesigning components into secure blocks in alignment with the Space Attack Research & Tactic Analysis (SPARTA) framework, and crafting shall statements to refactor the system design, with a particular focus on improving the security of the link segment.",
        "subjects": [
            "cs.CR",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12690",
        "abstract url": "https://arxiv.org/abs/2411.12690",
        "title": "MORE-Stress: Model Order Reduction based Efficient Numerical Algorithm for Thermal Stress Simulation of TSV Arrays in 2.5D/3D IC",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "Thermal"
            ]
        ],
        "abstract": "Thermomechanical stress induced by through-silicon vias (TSVs) plays an important role in the performance and reliability analysis of 2.5D/3D ICs. While the finite element method (FEM) adopted by commercial software can provide accurate simulation results, it is very time- and memory-consuming for large-scale analysis. Over the past decade, the linear superposition method has been utilized to perform fast thermal stress estimations of TSV arrays, but it suffers from a lack of accuracy. In this paper, we propose MORE-Stress, a novel strict numerical algorithm for efficient thermal stress simulation of TSV arrays based on model order reduction. Extensive experimental results demonstrate that our algorithm can realize a 153-504 times reduction in computational time and a 39-115 times reduction in memory usage compared with the commercial software ANSYS, with negligible errors less than 1%. Our algorithm is as efficient as the linear superposition method, with an order of magnitude smaller errors and fast convergence.",
        "subjects": [
            "cs.CE"
        ],
        "comment": "2025 Design, Automation & Test in Europe Conference & Exhibition (DATE)"
    },
    {
        "paper id": "2411.12707",
        "abstract url": "https://arxiv.org/abs/2411.12707",
        "title": "Barttender: An approachable & interpretable way to compare medical imaging and non-imaging data",
        "rating": "-3",
        "keywords": [
            [
                "medical",
                "health",
                "healthcare",
                "disease",
                "clinical"
            ],
            [
                "tabular"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Imaging-based deep learning has transformed healthcare research, yet its clinical adoption remains limited due to challenges in comparing imaging models with traditional non-imaging and tabular data. To bridge this gap, we introduce Barttender, an interpretable framework that uses deep learning for the direct comparison of the utility of imaging versus non-imaging tabular data for tasks like disease prediction. Barttender converts non-imaging tabular features, such as scalar data from electronic health records, into grayscale bars, facilitating an interpretable and scalable deep learning based modeling of both data modalities. Our framework allows researchers to evaluate differences in utility through performance measures, as well as local (sample-level) and global (population-level) explanations. We introduce a novel measure to define global feature importances for image-based deep learning models, which we call gIoU. Experiments on the CheXpert and MIMIC datasets with chest X-rays and scalar data from electronic health records show that Barttender performs comparably to traditional methods and offers enhanced explainability using deep learning models.",
        "subjects": [
            "q-bio.QM",
            "cs.CV"
        ],
        "comment": "Accepted to the Proceedings Track at Machine Learning for Health (ML4H 2024) conference, held on December 15-16, 2024 in Vancouver, Canada"
    },
    {
        "paper id": "2411.12837",
        "abstract url": "https://arxiv.org/abs/2411.12837",
        "title": "Anticipatory Planning for Performant Long-Lived Robot in Large-Scale Home-Like Environments",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "Robot"
            ],
            [
                "GNN",
                "Graph"
            ]
        ],
        "abstract": "We consider the setting where a robot must complete a sequence of tasks in a persistent large-scale environment, given one at a time. Existing task planners often operate myopically, focusing solely on immediate goals without considering the impact of current actions on future tasks. Anticipatory planning, which reduces the joint objective of the immediate planning cost of the current task and the expected cost associated with future subsequent tasks, offers an approach for improving long-lived task planning. However, applying anticipatory planning in large-scale environments presents significant challenges due to the sheer number of assets involved, which strains the scalability of learning and planning. In this research, we introduce a model-based anticipatory task planning framework designed to scale to large-scale realistic environments. Our framework uses a GNN in particular via a representation inspired by a 3D Scene Graph to learn the essential properties of the environment crucial to estimating the state's expected cost and a sampling-based procedure for practical large-scale anticipatory planning. Our experimental results show that our planner reduces the cost of task sequence by 5.38% in home and 31.5% in restaurant settings. If given time to prepare in advance using our model reduces task sequence costs by 40.6% and 42.5%, respectively.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Submitted to 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2025"
    },
    {
        "paper id": "2411.12963",
        "abstract url": "https://arxiv.org/abs/2411.12963",
        "title": "Probabilistic Dynamic Line Rating Forecasting with Line Graph Convolutional LSTM",
        "rating": "-3",
        "keywords": [
            [
                "Graph"
            ],
            [
                "Forecasting"
            ]
        ],
        "abstract": "Dynamic line rating (DLR) is a promising solution to increase the utilization of transmission lines by adjusting ratings based on real-time weather conditions. Accurate DLR forecast at the scheduling stage is thus necessary for system operators to proactively optimize power flows, manage congestion, and reduce the cost of grid operations. However, the DLR forecast remains challenging due to weather uncertainty. To reliably predict DLRs, we propose a new probabilistic forecasting model based on line graph convolutional LSTM. Like standard LSTM networks, our model accounts for temporal correlations between DLRs across the planning horizon. The line graph-structured network additionally allows us to leverage the spatial correlations of DLR features across the grid to improve the quality of predictions. Simulation results on the synthetic Texas 123-bus system demonstrate that the proposed model significantly outperforms the baseline probabilistic DLR forecasting models regarding reliability and sharpness while using the fewest parameters.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "5 pages, 5 figures"
    },
    {
        "paper id": "2411.12968",
        "abstract url": "https://arxiv.org/abs/2411.12968",
        "title": "Quadratic Programming Optimization for Bio-Inspired Thruster-Assisted Bipedal Locomotion on Inclined Slopes",
        "rating": "-3",
        "keywords": [
            [
                "robot"
            ],
            [
                "Bio-Inspired"
            ]
        ],
        "abstract": "Our work aims to make significant strides in understanding unexplored locomotion control paradigms based on the integration of posture manipulation and thrust vectoring. These techniques are commonly seen in nature, such as Chukar birds using their wings to run on a nearly vertical wall. In this work, we show quadratic programming with contact constraints which is then given to the whole body controller to map on robot states to produce a thruster-assisted slope walking controller for our state-of-the-art Harpy platform. Harpy is a bipedal robot capable of legged-aerial locomotion using its legs and thrusters attached to its main frame. The optimization-based walking controller has been used for dynamic locomotion such as slope walking, but the addition of thrusters to perform inclined slope walking has not been extensively explored. In this work, we derive a thruster-assisted bipedal walking with the quadratic programming (QP) controller and implement it in simulation to study its performance.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Submitted to ACC2025. arXiv admin note: text overlap with arXiv:2406.14799"
    },
    {
        "paper id": "2411.12970",
        "abstract url": "https://arxiv.org/abs/2411.12970",
        "title": "Validation of Tumbling Robot Dynamics with Posture Manipulation for Closed-Loop Heading Angle Control",
        "rating": "-3",
        "keywords": [
            [
                "Robot"
            ],
            [
                "Bio-inspired"
            ]
        ],
        "abstract": "Navigating rugged terrain and steep slopes is a challenge for mobile robots. Conventional legged and wheeled systems struggle with these environments due to limited traction and stability. Northeastern University's COBRA (Crater Observing Bio-inspired Rolling Articulator), a novel multi-modal snake-like robot, addresses these issues by combining traditional snake gaits for locomotion on flat and inclined surfaces with a tumbling mode for controlled descent on steep slopes. Through dynamic posture manipulation, COBRA can modulate its heading angle and velocity during tumbling. This paper presents a reduced-order cascade model for COBRA's tumbling locomotion and validates it against a high-fidelity rigid-body simulation, presenting simulation results that show that the model captures key system dynamics.",
        "subjects": [
            "cs.RO",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13022",
        "abstract url": "https://arxiv.org/abs/2411.13022",
        "title": "Training Physics-Driven Deep Learning Reconstruction without Raw Data Access for Equitable Fast MRI",
        "rating": "-3",
        "keywords": [
            [
                "MRI",
                "clinical"
            ],
            [
                "Physics"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Physics-driven deep learning (PD-DL) approaches have become popular for improved reconstruction of fast magnetic resonance imaging (MRI) scans. Even though PD-DL offers higher acceleration rates compared to existing clinical fast MRI techniques, their use has been limited outside specialized MRI centers. One impediment for their deployment is the difficulties with generalization to pathologies or population groups that are not well-represented in training sets. This has been noted in several studies, and fine-tuning on target populations to improve reconstruction has been suggested. However, current approaches for PD-DL training require access to raw k-space measurements, which is typically only available at specialized MRI centers that have research agreements for such data access. This is especially an issue for rural and underserved areas, where commercial MRI scanners only provide access to a final reconstructed image. To tackle these challenges, we propose Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity (CUPID) for high-quality PD-DL training, using only routine clinical reconstructed images exported from an MRI scanner. CUPID evaluates the goodness of the output with a compressibility-based approach, while ensuring that the output stays consistent with the clinical parallel imaging reconstruction through well-designed perturbations. Our results show that CUPID achieves similar quality compared to well-established PD-DL training strategies that require raw k-space data access, while outperforming conventional compressed sensing (CS) and state-of-the-art generative methods. We also demonstrate its effectiveness in a zero-shot training setup for retrospectively and prospectively sub-sampled acquisitions, attesting to its minimal training burden.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13023",
        "abstract url": "https://arxiv.org/abs/2411.13023",
        "title": "Enhancing Transportation Cyber-Physical Systems Security: A Shift to Post-Quantum Cryptography",
        "rating": "-3",
        "keywords": [
            [
                "attacks"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "The rise of quantum computing threatens traditional cryptographic algorithms that secure Transportation Cyber-Physical Systems (TCPS). Shor's algorithm poses a significant threat to RSA and ECC, while Grover's algorithm reduces the security of symmetric encryption schemes, such as AES. The objective of this paper is to underscore the urgency of transitioning to post-quantum cryptography (PQC) to mitigate these risks in TCPS by analyzing the vulnerabilities of traditional cryptographic schemes and the applicability of standardized PQC schemes in TCPS. We analyzed vulnerabilities in traditional cryptography against quantum attacks and reviewed the applicability of NIST-standardized PQC schemes, including CRYSTALS-Kyber, CRYSTALS-Dilithium, and SPHINCS+, in TCPS. We conducted a case study to analyze the vulnerabilities of a TCPS application from the Architecture Reference for Cooperative and Intelligent Transportation (ARC-IT) service package, i.e., Electronic Toll Collection, leveraging the Microsoft Threat Modeling tool. This case study highlights the cryptographic vulnerabilities of a TCPS application and presents how PQC can effectively counter these threats. Additionally, we evaluated CRYSTALS-Kyber's performance across wired and wireless TCPS data communication scenarios. While CRYSTALS-Kyber proves effective in securing TCPS applications over high-bandwidth, low-latency Ethernet networks, our analysis highlights challenges in meeting the stringent latency requirements of safety-critical wireless applications within TCPS. Future research should focus on developing lightweight PQC solutions and hybrid schemes that integrate traditional and PQC algorithms, to enhance compatibility, scalability, and real-time performance, ensuring robust protection against emerging quantum threats in TCPS.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "This version has been submitted to ACM Transactions on Cyber-Physical Systems (Special Issue on Security and Privacy in Safety-Critical Cyber-Physical Systems) and is currently under peer review. Please note that the abstract in this version has been revised from the ACM-submitted version to comply with arXiv's 1920-character limit"
    },
    {
        "paper id": "2411.15203",
        "abstract url": "https://arxiv.org/abs/2411.15203",
        "title": "Multimodal large language model for wheat breeding: a new exploration of smart breeding",
        "rating": "-3",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "remote sensing",
                "UAV"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "UAV remote sensing technology has become a key technology in crop breeding, which can achieve high-throughput and non-destructive collection of crop phenotyping data. However, the multidisciplinary nature of breeding has brought technical barriers and efficiency challenges to knowledge mining. Therefore, it is important to develop a smart breeding goal tool to mine cross-domain multimodal data. Based on different pre-trained open-source multimodal large language models (MLLMs) (e.g., Qwen-VL, InternVL, Deepseek-VL), this study used supervised fine-tuning (SFT), retrieval-augmented generation (RAG), and reinforcement learning from human feedback (RLHF) technologies to inject cross-domain knowledge into MLLMs, thereby constructing multiple multimodal large language models for wheat breeding (WBLMs). The above WBLMs were evaluated using the newly created evaluation benchmark in this study. The results showed that the WBLM constructed using SFT, RAG and RLHF technologies and InternVL2-8B has leading performance. Then, subsequent experiments were conducted using the WBLM. Ablation experiments indicated that the combination of SFT, RAG, and RLHF technologies can improve the overall generation performance, enhance the generated quality, balance the timeliness and adaptability of the generated answer, and reduce hallucinations and biases. The WBLM performed best in wheat yield prediction using cross-domain data (remote sensing, phenotyping, weather, germplasm) simultaneously, with R2 and RMSE of 0.821 and 489.254 kg/ha, respectively. Furthermore, the WBLM can generate professional decision support answers for phenotyping estimation, environmental stress assessment, target germplasm screening, cultivation technique recommendation, and seed price query tasks.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12697",
        "abstract url": "https://arxiv.org/abs/2411.12697",
        "title": "Attribute Inference Attacks for Federated Regression Tasks",
        "rating": "-3.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Attacks"
            ],
            [
                "IoT"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL) enables multiple clients, such as mobile phones and IoT devices, to collaboratively train a global machine learning model while keeping their data localized. However, recent studies have revealed that the training phase of FL is vulnerable to reconstruction attacks, such as attribute inference attacks (AIA), where adversaries exploit exchanged messages and auxiliary public information to uncover sensitive attributes of targeted clients. While these attacks have been extensively studied in the context of classification tasks, their impact on regression tasks remains largely unexplored. In this paper, we address this gap by proposing novel model-based AIAs specifically designed for regression tasks in FL environments. Our approach considers scenarios where adversaries can either eavesdrop on exchanged messages or directly interfere with the training process. We benchmark our proposed attacks against state-of-the-art methods using real-world datasets. The results demonstrate a significant increase in reconstruction accuracy, particularly in heterogeneous client datasets, a common scenario in FL. The efficacy of our model-based AIAs makes them better candidates for empirically quantifying privacy leakage for federated regression tasks.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12824",
        "abstract url": "https://arxiv.org/abs/2411.12824",
        "title": "Generalized Prompt Tuning: Adapting Frozen Univariate Time Series Foundation Models for Multivariate Healthcare Time Series",
        "rating": "-3.5",
        "keywords": [
            [
                "medical",
                "Healthcare"
            ],
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Time series foundation models are pre-trained on large datasets and are able to achieve state-of-the-art performance in diverse tasks. However, to date, there has been limited work demonstrating how well these models perform in medical applications, where labeled data can be scarce. Further, we observe that currently, the majority of time series foundation models either are univariate in nature, or assume channel independence, meaning that they handle multivariate time series but do not model how the different variables relate. In this paper, we propose a prompt-tuning-inspired fine-tuning technique, Generalized Prompt Tuning (Gen-P-Tuning), that enables us to adapt an existing univariate time series foundation model (treated as frozen) to handle multivariate time series prediction. Our approach provides a way to combine information across channels (variables) of multivariate time series. We demonstrate the effectiveness of our fine-tuning approach against various baselines on two MIMIC classification tasks, and on influenza-like illness forecasting.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Machine Learning for Health (ML4H 2024)"
    },
    {
        "paper id": "2411.12886",
        "abstract url": "https://arxiv.org/abs/2411.12886",
        "title": "NPGPT: Natural Product-Like Compound Generation with GPT-based Chemical Language Models",
        "rating": "-3.5",
        "keywords": [
            [
                "biological"
            ],
            [
                "Chemical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Natural products are substances produced by organisms in nature and often possess biological activity and structural diversity. Drug development based on natural products has been common for many years. However, the intricate structures of these compounds present challenges in terms of structure determination and synthesis, particularly compared to the efficiency of high-throughput screening of synthetic compounds. In recent years, deep learning-based methods have been applied to the generation of molecules. In this study, we trained chemical language models on a natural product dataset and generated natural product-like compounds. The results showed that the distribution of the compounds generated was similar to that of natural products. We also evaluated the effectiveness of the generated compounds as drug candidates. Our method can be used to explore the vast chemical space and reduce the time and cost of drug discovery of natural products.",
        "subjects": [
            "q-bio.BM",
            "cs.LG",
            "q-bio.QM",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2411.14157",
        "abstract url": "https://arxiv.org/abs/2411.14157",
        "title": "DrugGen: Advancing Drug Discovery with Large Language Models and Reinforcement Learning Feedback",
        "rating": "-3.5",
        "keywords": [
            [
                "biological",
                "clinical"
            ],
            [
                "chemical"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Traditional drug design faces significant challenges due to inherent chemical and biological complexities, often resulting in high failure rates in clinical trials. Deep learning advancements, particularly generative models, offer potential solutions to these challenges. One promising algorithm is DrugGPT, a transformer-based model, that generates small molecules for input protein sequences. Although promising, it generates both chemically valid and invalid structures and does not incorporate the features of approved drugs, resulting in time-consuming and inefficient drug discovery. To address these issues, we introduce DrugGen, an enhanced model based on the DrugGPT structure. DrugGen is fine-tuned on approved drug-target interactions and optimized with proximal policy optimization. By giving reward feedback from protein-ligand binding affinity prediction using pre-trained transformers (PLAPT) and a customized invalid structure assessor, DrugGen significantly improves performance. Evaluation across multiple targets demonstrated that DrugGen achieves 100% valid structure generation compared to 95.5% with DrugGPT and produced molecules with higher predicted binding affinities (7.22 [6.30-8.07]) compared to DrugGPT (5.81 [4.97-6.63]) while maintaining diversity and novelty. Docking simulations further validate its ability to generate molecules targeting binding sites effectively. For example, in the case of fatty acid-binding protein 5 (FABP5), DrugGen generated molecules with superior docking scores (FABP5/11, -9.537 and FABP5/5, -8.399) compared to the reference molecule (Palmitic acid, -6.177). Beyond lead compound generation, DrugGen also shows potential for drug repositioning and creating novel pharmacophores for existing targets. By producing high-quality small molecules, DrugGen provides a high-performance medium for advancing pharmaceutical research and drug discovery.",
        "subjects": [
            "q-bio.QM",
            "cs.AI"
        ],
        "comment": "20 pages, 5 figures, 3 tables, and 7 supplementary files. To use the model, see https://huggingface.co/alimotahharynia/DrugGen"
    },
    {
        "paper id": "2411.17715",
        "abstract url": "https://arxiv.org/abs/2411.17715",
        "title": "Hybrid Quantum Deep Learning Model for Emotion Detection using raw EEG Signal Analysis",
        "rating": "-3.5",
        "keywords": [
            [
                "health",
                "EEG"
            ],
            [
                "Quantum"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Applications in behavioural research, human-computer interaction, and mental health depend on the ability to recognize emotions. In order to improve the accuracy of emotion recognition using electroencephalography (EEG) data, this work presents a hybrid quantum deep learning technique. Conventional EEG-based emotion recognition techniques are limited by noise and high-dimensional data complexity, which make feature extraction difficult. To tackle these issues, our method combines traditional deep learning classification with quantum-enhanced feature extraction. To identify important brain wave patterns, Bandpass filtering and Welch method are used as preprocessing techniques on EEG data. Intricate inter-band interactions that are essential for determining emotional states are captured by mapping frequency band power attributes (delta, theta, alpha, and beta) to quantum representations. Entanglement and rotation gates are used in a hybrid quantum circuit to maximize the model's sensitivity to EEG patterns associated with different emotions. Promising results from evaluation on a test dataset indicate the model's potential for accurate emotion recognition. The model will be extended for real-time applications and multi-class categorization in future study, which could improve EEG-based mental health screening instruments. This method offers a promising tool for applications in adaptive human-computer systems and mental health monitoring by showcasing the possibilities of fusing traditional deep learning with quantum processing for reliable, scalable emotion recognition.",
        "subjects": [
            "eess.SP",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12278",
        "abstract url": "https://arxiv.org/abs/2411.12278",
        "title": "Versatile Cataract Fundus Image Restoration Model Utilizing Unpaired Cataract and High-quality Images",
        "rating": "-4",
        "keywords": [
            [
                "GAN"
            ],
            [
                "medical",
                "surgery",
                "diagnosis"
            ],
            [
                "Image Restoration"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Cataract is one of the most common blinding eye diseases and can be treated by surgery. However, because cataract patients may also suffer from other blinding eye diseases, ophthalmologists must diagnose them before surgery. The cloudy lens of cataract patients forms a hazy degeneration in the fundus images, making it challenging to observe the patient's fundus vessels, which brings difficulties to the diagnosis process. To address this issue, this paper establishes a new cataract image restoration method named Catintell. It contains a cataract image synthesizing model, Catintell-Syn, and a restoration model, Catintell-Res. Catintell-Syn uses GAN architecture with fully unsupervised data to generate paired cataract-like images with realistic style and texture rather than the conventional Gaussian degradation algorithm. Meanwhile, Catintell-Res is an image restoration network that can improve the quality of real cataract fundus images using the knowledge learned from synthetic cataract images. Extensive experiments show that Catintell-Res outperforms other cataract image restoration methods in PSNR with 39.03 and SSIM with 0.9476. Furthermore, the universal restoration ability that Catintell-Res gained from unpaired cataract images can process cataract images from various datasets. We hope the models can help ophthalmologists identify other blinding eye diseases of cataract patients and inspire more medical image restoration methods in the future.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": "12 pages, 8 figures"
    },
    {
        "paper id": "2411.12328",
        "abstract url": "https://arxiv.org/abs/2411.12328",
        "title": "HW/SW Implementation of MiRitH on Embedded Platforms",
        "rating": "-4",
        "keywords": [
            [
                "FPGA"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "Multi-Party Computation in the Head (MPCitH) algorithms are appealing candidates in the additional US NIST standardization rounds for Post-Quantum Cryptography (PQC) with respect to key sizes and mathematical hardness assumptions. However, their complexity presents a significant challenge for platforms with limited computational capabilities. To address this issue, we present, to the best of our knowledge, the first design space exploration of MiRitH, a promising MPCitH algorithm, for embedded devices. We develop a library of mixed HW/SW blocks on the Xilinx ZYNQ 7000, and, based on this library, we explore optimal solutions under runtime or FPGA resource constraints for a given public key infrastructure. Our results show that MiRitH is a viable algorithm for embedded devices in terms of runtime and FPGA resource requirements.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12361",
        "abstract url": "https://arxiv.org/abs/2411.12361",
        "title": "Breathless: An 8-hour Performance Contrasting Human and Robot Expressiveness",
        "rating": "-4",
        "keywords": [
            [
                "Robot"
            ],
            [
                "industrial"
            ],
            [
                "music"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper describes the robot technology behind an original performance that pairs a human dancer (Cuan) with an industrial robot arm for an eight-hour dance that unfolds over the timespan of an American workday. To control the robot arm, we combine a range of sinusoidal motions with varying amplitude, frequency and offset at each joint to evoke human motions common in physical labor such as stirring, digging, and stacking. More motions were developed using deep learning techniques for video-based human-pose tracking and extraction. We combine these pre-recorded motions with improvised robot motions created live by putting the robot into teach-mode and triggering force sensing from the robot joints onstage. All motions are combined with commercial and original music using a custom suite of python software with AppleScript, Keynote, and Zoom to facilitate on-stage communication with the dancer. The resulting performance contrasts the expressivity of the human body with the precision of robot machinery. Video, code and data are available on the project website: https://sites.google.com/playing.studio/breathless",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": "15 pages, 9 figures, accepted for ISRR (International Symposium of Robotics Research) 2024"
    },
    {
        "paper id": "2411.12401",
        "abstract url": "https://arxiv.org/abs/2411.12401",
        "title": "Design of an FPGA-Based Neutral Atom Rearrangement Accelerator for Quantum Computing",
        "rating": "-4",
        "keywords": [
            [
                "FPGA"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "Neutral atoms have emerged as a promising technology for implementing quantum computers due to their scalability and long coherence times. However, the execution frequency of neutral atom quantum computers is constrained by image processing procedures, particularly the assembly of defect-free atom arrays, which is a crucial step in preparing qubits (atoms) for execution. To optimize this assembly process, we propose a novel quadrant-based rearrangement algorithm that employs a divide-and-conquer strategy and also enables the simultaneous movement of multiple atoms, even across different columns and rows. We implement the algorithm on FPGA to handle each quadrant independently (hardware-level optimization) while maximizing parallelization. To the best of our knowledge, this is the first hardware acceleration work for atom rearrangement, and it significantly reduces the processing time. This achievement also contributes to the ongoing efforts of tightly integrating quantum accelerators into High-Performance Computing (HPC) systems. Tested on a Zynq RFSoC FPGA at 250 MHz, our hardware implementation is able to complete the rearrangement process of a 30$\\times$30 compact target array, derived from a 50$\\times$50 initial loaded array, in approximately 1.0 $\u03bcs$. Compared to a comparable CPU implementation and to state-of-the-art FPGA work, we achieved about 54$\\times$ and 300$\\times$ speedups in the rearrangement analysis time, respectively. Additionally, the FPGA-based acceleration demonstrates good scalability, allowing for seamless adaptation to varying sizes of the atom array, which makes this algorithm a promising solution for large-scale quantum systems.",
        "subjects": [
            "quant-ph",
            "cs.AR",
            "cs.ET"
        ],
        "comment": "The paper is accepted by Design, Automation and Test in Europe Conference (DATE), 2025"
    },
    {
        "paper id": "2411.12439",
        "abstract url": "https://arxiv.org/abs/2411.12439",
        "title": "Efficient terabyte-scale text compression via stable local consistency and parallel grammar processing",
        "rating": "-4",
        "keywords": [
            [
                "satellite"
            ],
            [
                "grammar"
            ]
        ],
        "abstract": "We present a highly parallelizable text compression algorithm that scales efficiently to terabyte-sized datasets. Our method builds on locally consistent grammars, a lightweight form of compression, combined with simple recompression techniques to achieve further space reductions. Locally consistent grammar algorithms are particularly suitable for scaling, as they need minimal satellite information to compact the text. We introduce a novel concept to enable parallelisation, stable local consistency. A grammar algorithm ALG is stable, if for any pattern $P$ occurring in a collection $\\mathcal{T}=\\{T_1, T_2, \\ldots, T_k\\}$, the instances $ALG(T_1), ALG(T_2), \\ldots, ALG(T_k)$ independently produce cores for $P$ with the same topology. In a locally consistent grammar, the core of $P$ is a subset of nodes and edges in $\\mathcal{T}$'s parse tree that remains the same in all the occurrences of $P$. This feature is important to achieve compression, but it only holds if ALG synchronises the parsing of the strings, for instance, by defining a common set of nonterminal symbols for them. Stability removes the need for synchronisation during the parsing phase. Consequently, we can run $ALG(T_1), ALG(T_2), \\ldots, ALG(T_k)$ fully in parallel and then merge the resulting grammars into a single compressed output equivalent to $ALG(\\mathcal{T})$. We implemented our ideas and tested them on massive datasets. Our results showed that our method could process a diverse collection of bacterial genomes (7.9 TB) in around nine hours, requiring 16 threads and 0.43 bits/symbol of working memory, producing a compressed representation 85 times smaller than the original input.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12711",
        "abstract url": "https://arxiv.org/abs/2411.12711",
        "title": "UBSoft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments",
        "rating": "-4",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "robot"
            ],
            [
                "physics"
            ]
        ],
        "abstract": "It is desired to equip robots with the capability of interacting with various soft materials as they are ubiquitous in the real world. While physics simulations are one of the predominant methods for data collection and robot training, simulating soft materials presents considerable challenges. Specifically, it is significantly more costly than simulating rigid objects in terms of simulation speed and storage requirements. These limitations typically restrict the scope of studies on soft materials to small and bounded areas, thereby hindering the learning of skills in broader spaces. To address this issue, we introduce UBSoft, a new simulation platform designed to support unbounded soft environments for robot skill acquisition. Our platform utilizes spatially adaptive resolution scales, where simulation resolution dynamically adjusts based on proximity to active robotic agents. Our framework markedly reduces the demand for extensive storage space and computation costs required for large-scale scenarios involving soft materials. We also establish a set of benchmark tasks in our platform, including both locomotion and manipulation tasks, and conduct experiments to evaluate the efficacy of various reinforcement learning algorithms and trajectory optimization techniques, both gradient-based and sampling-based. Preliminary results indicate that sampling-based trajectory optimization generally achieves better results for obtaining one trajectory to solve the task. Additionally, we conduct experiments in real-world environments to demonstrate that advancements made in our UBSoft simulator could translate to improved robot interactions with large-scale soft material. More videos can be found at https://vis-www.cs.umass.edu/ubsoft/.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "CoRL 2024. The first two authors contributed equally to this paper"
    },
    {
        "paper id": "2411.12812",
        "abstract url": "https://arxiv.org/abs/2411.12812",
        "title": "DIETS: Diabetic Insulin Management System in Everyday Life",
        "rating": "-4",
        "keywords": [
            [
                "medical"
            ],
            [
                "recommendation"
            ]
        ],
        "abstract": "People with diabetes need insulin delivery to effectively manage their blood glucose levels, especially after meals, because their bodies either do not produce enough insulin or cannot fully utilize it. Accurate insulin delivery starts with estimating the nutrients in meals and is followed by developing a detailed, personalized insulin injection strategy. These tasks are particularly challenging in daily life, especially without professional guidance. Existing solutions usually assume the prior knowledge of nutrients in meals and primarily rely on feedback from professional clinicians or simulators to develop Reinforcement Learning-based models for insulin management, leading to extensive consumption of medical resources and difficulties in adapting the models to new patients due to individual differences. In this paper, we propose DIETS, a novel diabetic insulin management framework built on the transformer architecture, to help people with diabetes effectively manage insulin delivery in everyday life. Specifically, DIETS tailors a Large Language Model (LLM) to estimate the nutrients in meals and employs a titration model to generate recommended insulin injection strategies, which are further validated by a glucose prediction model to prevent potential risks of hyperglycemia or hypoglycemia. DIETS has been extensively evaluated on three public datasets, and the results show it achieves superior performance in providing effective insulin delivery recommendation to control blood glucose levels.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12982",
        "abstract url": "https://arxiv.org/abs/2411.12982",
        "title": "Hierarchical Diffusion Policy: manipulation trajectory generation via contact guidance",
        "rating": "-4",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "trajectory"
            ],
            [
                "robotics",
                "robot"
            ]
        ],
        "abstract": "Decision-making in robotics using denoising diffusion processes has increasingly become a hot research topic, but end-to-end policies perform poorly in tasks with rich contact and have limited controllability. This paper proposes Hierarchical Diffusion Policy (HDP), a new imitation learning method of using objective contacts to guide the generation of robot trajectories. The policy is divided into two layers: the high-level policy predicts the contact for the robot's next object manipulation based on 3D information, while the low-level policy predicts the action sequence toward the high-level contact based on the latent variables of observation and contact. We represent both level policies as conditional denoising diffusion processes, and combine behavioral cloning and Q-learning to optimize the low level policy for accurately guiding actions towards contact. We benchmark Hierarchical Diffusion Policy across 6 different tasks and find that it significantly outperforms the existing state of-the-art imitation learning method Diffusion Policy with an average improvement of 20.8%. We find that contact guidance yields significant improvements, including superior performance, greater interpretability, and stronger controllability, especially on contact-rich tasks. To further unlock the potential of HDP, this paper proposes a set of key technical contributions including snapshot gradient optimization, 3D conditioning, and prompt guidance, which improve the policy's optimization efficiency, spatial awareness, and controllability respectively. Finally, real world experiments verify that HDP can handle both rigid and deformable objects.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2303.04137 by other authors"
    },
    {
        "paper id": "2411.13610",
        "abstract url": "https://arxiv.org/abs/2411.13610",
        "title": "Video2BEV: Transforming Drone Videos to BEVs for Video-based Geo-localization",
        "rating": "-4",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "diffusion"
            ],
            [
                "flight"
            ],
            [
                "satellite",
                "BEV",
                "Drone"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Existing approaches to drone visual geo-localization predominantly adopt the image-based setting, where a single drone-view snapshot is matched with images from other platforms. Such task formulation, however, underutilizes the inherent video output of the drone and is sensitive to occlusions and environmental constraints. To address these limitations, we formulate a new video-based drone geo-localization task and propose the Video2BEV paradigm. This paradigm transforms the video into a Bird's Eye View (BEV), simplifying the subsequent matching process. In particular, we employ Gaussian Splatting to reconstruct a 3D scene and obtain the BEV projection. Different from the existing transform methods, \\eg, polar transform, our BEVs preserve more fine-grained details without significant distortion. To further improve model scalability toward diverse BEVs and satellite figures, our Video2BEV paradigm also incorporates a diffusion-based module for generating hard negative samples, which facilitates discriminative feature learning. To validate our approach, we introduce UniV, a new video-based geo-localization dataset that extends the image-based University-1652 dataset. UniV features flight paths at $30^\\circ$ and $45^\\circ$ elevation angles with increased frame rates of up to 10 frames per second (FPS). Extensive experiments on the UniV dataset show that our Video2BEV paradigm achieves competitive recall rates and outperforms conventional video-based methods. Compared to other methods, our proposed approach exhibits robustness at lower elevations with more occlusions.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12530",
        "abstract url": "https://arxiv.org/abs/2411.12530",
        "title": "Contourlet Refinement Gate Framework for Thermal Spectrum Distribution Regularized Infrared Image Super-Resolution",
        "rating": "-5",
        "keywords": [
            [
                "diffusion",
                "Super-Resolution"
            ],
            [
                "Infrared"
            ],
            [
                "Thermal"
            ],
            [
                "image enhancement"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Image super-resolution (SR) is a classical yet still active low-level vision problem that aims to reconstruct high-resolution (HR) images from their low-resolution (LR) counterparts, serving as a key technique for image enhancement. Current approaches to address SR tasks, such as transformer-based and diffusion-based methods, are either dedicated to extracting RGB image features or assuming similar degradation patterns, neglecting the inherent modal disparities between infrared and visible images. When directly applied to infrared image SR tasks, these methods inevitably distort the infrared spectral distribution, compromising the machine perception in downstream tasks. In this work, we emphasize the infrared spectral distribution fidelity and propose a Contourlet refinement gate framework to restore infrared modal-specific features while preserving spectral distribution fidelity. Our approach captures high-pass subbands from multi-scale and multi-directional infrared spectral decomposition to recover infrared-degraded information through a gate architecture. The proposed Spectral Fidelity Loss regularizes the spectral frequency distribution during reconstruction, which ensures the preservation of both high- and low-frequency components and maintains the fidelity of infrared-specific features. We propose a two-stage prompt-learning optimization to guide the model in learning infrared HR characteristics from LR degradation. Extensive experiments demonstrate that our approach outperforms existing image SR models in both visual and perceptual tasks while notably enhancing machine perception in downstream tasks. Our code is available at https://github.com/hey-it-s-me/CoRPLE.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "13 figures, 6 tables"
    },
    {
        "paper id": "2411.12637",
        "abstract url": "https://arxiv.org/abs/2411.12637",
        "title": "ChemSICal: Evaluating a Stochastic Chemical Reaction Network for Molecular Multiple Access",
        "rating": "-5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "bio-nano-things"
            ],
            [
                "Chemical"
            ]
        ],
        "abstract": "Proposals for molecular communication networks as part of a future internet of bio-nano-things have become more intricate and the question of practical implementation is gaining more importance. One option is to apply detailed chemical modeling to capture more realistic effects of computing processes in biological systems. In this paper, we present ChemSICal, a detailed model for implementing the successive interference cancellation (SIC) algorithm for molecular multiple access in diffusion-based molecular communication networks as a chemical reaction network (CRN). We describe the structure of the model as a number of smaller reaction blocks, their speed controlled by reaction rate constants (RRCs). Deterministic and stochastic methods are utilized to first iteratively improve the choice of RRCs and subsequently investigate the performance of the model in terms of an error probability. We analyze the model's sensitivity to parameter changes and find that the analytically optimal values for the non-chemical model do not necessarily translate to the chemical domain. This necessitates careful optimization, especially of the RRCs, which are crucial for the successful operation of the ChemSICal system.",
        "subjects": [
            "cs.ET"
        ],
        "comment": "6 pages, 8 figures"
    },
    {
        "paper id": "2411.12897",
        "abstract url": "https://arxiv.org/abs/2411.12897",
        "title": "Tree Species Classification using Machine Learning and 3D Tomographic SAR -- a case study in Northern Europe",
        "rating": "-5",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "LiDAR",
                "Radar"
            ],
            [
                "remote sensing"
            ],
            [
                "tabular"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Tree species classification plays an important role in nature conservation, forest inventories, forest management, and the protection of endangered species. Over the past four decades, remote sensing technologies have been extensively utilized for tree species classification, with Synthetic Aperture Radar (SAR) emerging as a key technique. In this study, we employed TomoSense, a 3D tomographic dataset, which utilizes a stack of single-look complex (SLC) images, a byproduct of SAR, captured at different incidence angles to generate a three-dimensional representation of the terrain. Our research focuses on evaluating multiple tabular machine-learning models using the height information derived from the tomographic image intensities to classify eight distinct tree species. The SLC data and tomographic imagery were analyzed across different polarimetric configurations and geosplit configurations. We investigated the impact of these variations on classification accuracy, comparing the performance of various tabular machine-learning models and optimizing them using Bayesian optimization. Additionally, we incorporated a proxy for actual tree height using point cloud data from Light Detection and Ranging (LiDAR) to provide height statistics associated with the model's predictions. This comparison offers insights into the reliability of tomographic data in predicting tree species classification based on height.",
        "subjects": [
            "cs.LG",
            "cs.CV",
            "physics.data-an"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12239",
        "abstract url": "https://arxiv.org/abs/2411.12239",
        "title": "A Control Lyapunov Function Approach to Event-Triggered Parameterized Control for Discrete-Time Linear Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper proposes an event-triggered parameterized control method using a control Lyapunov function approach for discrete time linear systems with external disturbances. In this control method, each control input to the plant is a linear combination of a fixed set of linearly independent scalar functions. The controller updates the coefficients of the parameterized control input in an event-triggered manner so as to minimize a quadratic cost function subject to quadratic constraints and communicates the same to the actuator. We design an event-triggering rule that guarantees global uniform ultimate boundedness of trajectories of the closed loop system and non-trivial inter-event times. We illustrate our results through numerical examples and we also compare the performance of the proposed control method with other existing control methods in the literature.",
        "subjects": [
            "math.OC",
            "eess.SY"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2402.16337"
    },
    {
        "paper id": "2411.12241",
        "abstract url": "https://arxiv.org/abs/2411.12241",
        "title": "Extending the Burrows-Wheeler Transform for Cartesian Tree Matching and Constructing It",
        "rating": "-10",
        "keywords": [],
        "abstract": "Cartesian tree matching is a form of generalized pattern matching where a substring of the text matches with the pattern if they share the same Cartesian tree. This form of matching finds application for time series of stock prices and can be of interest for melody matching between musical scores. For the indexing problem, the state-of-the-art data structure is a Burrows-Wheeler transform based solution due to [Kim and Cho, CPM'21], which uses nearly succinct space and can count the number of substrings that Cartesian tree match with a pattern in time linear in the pattern length. The authors address the construction of their data structure with a straight-forward solution that, however, requires pointer-based data structures, which asymptotically need more space than compact solutions [Kim and Cho, CPM'21, Section A.4]. We address this bottleneck by a construction that requires compact space and has a time complexity linear in the product of the text length with some logarithmic terms. Additionally, we can extend this index for indexing multiple circular texts in the spirit of the extended Burrows-Wheeler transform without sacrificing the time and space complexities. We present this index in a dynamic variant, where we pay a logarithmic slowdown and need compact space for the extra functionality that we can incrementally add texts. Our extended setting is of interest for finding repetitive motifs common in the aforementioned applications, independent of offsets and scaling.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12271",
        "abstract url": "https://arxiv.org/abs/2411.12271",
        "title": "SMT-Layout: A MaxSMT-based Approach Supporting Real-time Interaction of Real-world GUI Layout",
        "rating": "-10",
        "keywords": [],
        "abstract": "Leveraging the flexible expressive ability of (Max)SMT and the powerful solving ability of SMT solvers, we propose a novel layout model named SMT-Layout. SMT-Layout is the first constraint-based layout model that can support real-time interaction for real-world GUI layout adapting to various screen sizes with only one specification. Previous works neglect the hierarchy information among widgets and thus cannot exploit the reasoning ability of solvers. For the first time, we introduce Boolean variables to encode the hierarchy relationship, boosting the reasoning ability of SMT solvers. The workflow is divided into two stages. At the development end, two novel preprocessing methods are proposed to simplify constraints and extract useful information in advance, easing the solving burden. After deploying constraints to the terminal end, SMT solvers are applied to solve constraints incrementally. Besides mainstream SMT solvers, a local search solver is customized to this scenario. Experiments show that SMT-Layout can support millisecond-level interaction for real-world layouts, even on devices with low computing power and rigorous memory limitations.",
        "subjects": [
            "cs.LO",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12289",
        "abstract url": "https://arxiv.org/abs/2411.12289",
        "title": "Enhancing UX Research Activities Using GenAI -- Potential Applications and Challenges",
        "rating": "-10",
        "keywords": [],
        "abstract": "User Experience (UX) Research covers various methods for gathering the users' subjective impressions of a product. For this, practitioners face different activities and tasks related to the research process. This includes processing a large amount of data based on qualitative and quantitative data. However, this can be very laborious in practice. Thus, the application of GenAI can support UX research activities. This paper provides a practical perspective on this topic. Based on previous studies, we present different use cases indicating the potential of GenAI in UX research. Moreover, we provide insights into an exploratory study using GenAI along an entire UX research process. Results show that Large Language Models (LLMs) are useful for various tasks. Thus, the research activities can be carried out more efficiently. However, the researcher should always review results to ensure quality. In summary, we want to express the potential of GenAI enhancing UX research",
        "subjects": [
            "cs.HC"
        ],
        "comment": "13 pages, 4 figures, CENTRIC 2024 : The Seventeenth International Conference on Advances in Human-oriented and Personalized Mechanisms, Technologies, and Services"
    },
    {
        "paper id": "2411.12296",
        "abstract url": "https://arxiv.org/abs/2411.12296",
        "title": "Modeling and Analysis of Terahertz Wave Propagation in Charged Dust Using Extended Mie Scattering Theory",
        "rating": "-10",
        "keywords": [],
        "abstract": "Terahertz (THz) band (0.1-10 THz) possesses multi-gigahertz continuous bandwidth resources, making it a promising frequency band for high-speed wireless communications and environment sensing. The interaction between the THz wave and the external environment has been studied for various scenarios. However, it has recently been revealed that the friction forces in dust storms as well as the irradiation of sunlight and solar wind lead to the electrification of dust particles on Earth and the Moon. The THz wave propagation in these charged dust has not been fully investigated, which is essential for THz aerial communications in dust storms and lunar communications. In this paper, a channel model for THz wave propagation in charged dust is developed for wireless communications. Specifically, an extended Mie scattering model for charged dust is first introduced, which captures the electrodynamic feature of the interaction between THz wave and charged particles. Then, the diameter and density distributions of dust particles are modeled, based on which the propagation loss of THz wave in charged dust is modeled and elaborated. Finally, numerical results on the additional loss caused by these charged dust with different sizes in the THz band are evaluated and compared. Extensive results demonstrate that as the number of dust charges increases, the extinction cross section of smaller-sized particles significantly increases, and the overall attenuation led by charged dust increases by at most 50% at 0.3 THz.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12299",
        "abstract url": "https://arxiv.org/abs/2411.12299",
        "title": "Could Humans Outshine AI in Visual Data Analysis?",
        "rating": "-10",
        "keywords": [],
        "abstract": "People often use visualizations not only to explore a dataset but also to draw generalizable conclusions about underlying models or phenomena. While previous research has viewed deviations from rational analysis as problematic, we hypothesize that human reliance on non-normative heuristics may be advantageous in certain situations. In this study, we investigate scenarios where human intuition might outperform idealized statistical rationality. Our experiment assesses participants' accuracy in characterizing the parameters of known data-generating models from bivariate visualizations. Our findings show that, while participants generally demonstrated lower accuracy than statistical models, they often outperformed Bayesian agents, particularly when dealing with extreme samples. These results suggest that, even when deviating from rationality, human gut reactions to visualizations can provide an advantage. Our findings offer insights into how analyst intuition and statistical models can be integrated to improve inference and decision-making, with important implications for the design of visual analytics tools.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "Accepted to the Workshop on Trust and Reliance in Evolving Human-AI Workflows (TREW) at CHI 2024. arXiv admin note: text overlap with arXiv:2407.16871"
    },
    {
        "paper id": "2411.12303",
        "abstract url": "https://arxiv.org/abs/2411.12303",
        "title": "An Integrated (Crop Model, Cloud and Big Data Analytic) Framework to support Agriculture Activity Monitoring System",
        "rating": "-10",
        "keywords": [],
        "abstract": "Agriculture activity monitoring needs to deal with large amounts of data originating from various organizations (weather stations, agriculture repositories, field management, farm management, universities, etc.) and mass people. Therefore, a scalable environment with flexible information access, easy communication, and real-time collaboration from all types of computing devices, including mobile handheld devices such as smartphones, PDAs and iPads, Geo-sensor devices, etc. are essential. The system must be accessible, scalable, and transparent from location, migration, and resources. In addition, the framework should support modern information retrieval and management systems, unstructured information to structured information processing, task prioritization, task distribution, workflow and task scheduling systems, processing power, and data storage. Thus, High Scalability Computing (HSC) or Cloud-based systems with Big data analytics can be a prominent and convincing solution for this circumstance. In this paper, we are going to propose an integrated (crop model, cloud, and big data analytics) geo-information framework to support agriculture activity monitoring systems.",
        "subjects": [
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12310",
        "abstract url": "https://arxiv.org/abs/2411.12310",
        "title": "Variable-Frequency Imitation Learning for Variable-Speed Motion",
        "rating": "-10",
        "keywords": [],
        "abstract": "Conventional methods of imitation learning for variable-speed motion have difficulty extrapolating speeds because they rely on learning models running at a constant sampling frequency. This study proposes variable-frequency imitation learning (VFIL), a novel method for imitation learning with learning models trained to run at variable sampling frequencies along with the desired speeds of motion. The experimental results showed that the proposed method improved the velocity-wise accuracy along both the interpolated and extrapolated frequency labels, in addition to a 12.5 % increase in the overall success rate.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages, 9 figures, 2 tables. Submitted to IEEE ICM 2025"
    },
    {
        "paper id": "2411.12351",
        "abstract url": "https://arxiv.org/abs/2411.12351",
        "title": "Multipacking in Euclidean Plane",
        "rating": "-10",
        "keywords": [],
        "abstract": "We initiate the study of multipacking problems for geometric point sets with respect to their Euclidean distances. We consider a set of $n$ points $P$ and define $N_s[v]$ as the subset of $P$ that includes the $s$ nearest points of $v \\in P$ and the point $v$ itself. We assume that the \\emph{$s$-th neighbor} of each point is unique, for every $s \\in \\{0, 1, 2, \\dots , n-1\\}$. For a natural number $r \\leq n$, an $r$-multipacking is a set $ M \\subseteq P $ such that for each point $ v \\in P $ and for every integer $ 1\\leq s \\leq r $, $|N_s[v]\\cap M|\\leq (s+1)/2$. The $r$-multipacking number of $ P $ is the maximum cardinality of an $r$-multipacking of $ P $ and is denoted by $ \\MP_{r}(P) $. For $r=n-1$, an $r$-multipacking is called a multipacking and $r$-multipacking number is called as multipacking number. We study the problem of computing a maximum $r$-multipacking for point sets in $\\mathbb{R}^2$. We show that a maximum $1$-multipacking can be computed in polynomial time but computing a maximum $2$-multipacking is NP complete. Further, we provide approximation and parameterized solutions to the $2$-multipacking problem.",
        "subjects": [
            "cs.CG",
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12353",
        "abstract url": "https://arxiv.org/abs/2411.12353",
        "title": "Service Restoration for Distribution Systems Based on Semi-Analytical Metamodeling of Decision-Dependent Interruption Cost and Cold Load Pickup",
        "rating": "-10",
        "keywords": [],
        "abstract": "Developing optimized restoration strategies for power distribution systems (PDSs) is essential to meet the pressing demand for enhanced resilience. Prior knowledge of customer interruption cost (CIC) and load restoration behaviors, particularly cold load pickup (CLPU), is crucial for guiding effective restoration; however, both are reciprocally affected by the realized customer interruption duration (CID), making them decision-dependent and challenging to model especially given the limited understanding of underlying physical mechanisms. This paper presents a novel approach by constructing tractable metamodels to capture the varying patterns of CIC and CLPU with CID - patterns which can be derived from limited data and reflect observed surface-level correlations rather than underlying mechanisms, thereby enabling practical surrogate modeling of these decision-dependencies. Specifically, quadratic functions are used to model the increasing rate of CIC with CID based on data fitting. Several defining characteristics of CLPU are extracted, each modeled in a piecewise linear form relative to CID, and the actual restored load accounting for CLPU is subsequently retrieved. Building on these metamodels, a PDS restoration optimization model is constructed, incorporating mobile energy storage systems (MESSs) and network reconfiguration. Case studies validate our approach and also highlight MESS's unique potential to accelerate CLPU-related restoration.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "10 pages, 10 figures, submitted to IEEE Transactions on Smart Grid"
    },
    {
        "paper id": "2411.12365",
        "abstract url": "https://arxiv.org/abs/2411.12365",
        "title": "Brief Announcement: Parallel Construction of Bumped Ribbon Retrieval",
        "rating": "-10",
        "keywords": [],
        "abstract": "A retrieval data structure stores a static function f : S -> {0,1}^r . For all x in S, it returns the r-bit value f(x), while for other inputs it may return an arbitrary result. The structure cannot answer membership queries, so it does not have to encode S. The information theoretic space lower bound for arbitrary inputs is r|S| bits. Retrieval data structures have widespread applications. They can be used as an approximate membership filter for S by storing fingerprints of the keys in S, where they are faster and more space efficient than Bloom filters. They can also be used as a basic building block of succinct data structures like perfect hash functions. Bumped Ribbon Retrieval (BuRR) [Dillinger et al., SEA'22] is a recently developed retrieval data structure that is fast to construct with a space overhead of less than 1%. The idea is to solve a nearly diagonal system of linear equations to determine a matrix that, multiplied with the hash of each key, gives the desired output values. During solving, BuRR might bump lines of the equation system to another layer of the same data structure. While the paper describes a simple parallel construction based on bumping the keys on thread boundaries, it does not give an implementation. In this brief announcement, we now fill this gap. Our parallel implementation is transparent to the queries. It achieves a speedup of 14 on 32 cores for 8-bit filters. The additional space overhead is 105 bytes per thread, or 105 slots. This matches 0.0007% of the total space consumption when constructing with 1 billion input keys. A large portion of the construction time is spent on parallel sorting.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12380",
        "abstract url": "https://arxiv.org/abs/2411.12380",
        "title": "Instrumentation of Software Systems with OpenTelemetry for Software Visualization",
        "rating": "-10",
        "keywords": [],
        "abstract": "As software systems grow in complexity, data and tools that provide valuable insights for easier program comprehension become increasingly important. OpenTelemetry has become a standard for the collection of monitoring data. In this work we present our experiences with different ways how OpenTelemetry can be leveraged to automatically instrument software systems for the purpose of software visualization. Particularly, we explore automatic instrumentation with the OpenTelemetry SDKs, and both application and unit test instrumentation with the Java agent inspectIT Ocelot. The collected data is exported to our live trace visualization tool ExplorViz.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "submitted to SSP 2024, see https://www.performance-symposium.org/ssp-2024/program/"
    },
    {
        "paper id": "2411.12441",
        "abstract url": "https://arxiv.org/abs/2411.12441",
        "title": "Towards Unifying Feature Interaction Models for Click-Through Rate Prediction",
        "rating": "-10",
        "keywords": [],
        "abstract": "Modeling feature interactions plays a crucial role in accurately predicting click-through rates (CTR) in advertising systems. To capture the intricate patterns of interaction, many existing models employ matrix-factorization techniques to represent features as lower-dimensional embedding vectors, enabling the modeling of interactions as products between these embeddings. In this paper, we propose a general framework called IPA to systematically unify these models. Our framework comprises three key components: the Interaction Function, which facilitates feature interaction; the Layer Pooling, which constructs higher-level interaction layers; and the Layer Aggregator, which combines the outputs of all layers to serve as input for the subsequent classifier. We demonstrate that most existing models can be categorized within our framework by making specific choices for these three components. Through extensive experiments and a dimensional collapse analysis, we evaluate the performance of these choices. Furthermore, by leveraging the most powerful components within our framework, we introduce a novel model that achieves competitive results compared to state-of-the-art CTR models. PFL gets significant GMV lift during online A/B test in Tencent's advertising platform and has been deployed as the production model in several primary scenarios.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12444",
        "abstract url": "https://arxiv.org/abs/2411.12444",
        "title": "Advancing Cloud Computing Capabilities on gem5 by Implementing the RISC-V Hypervisor Extension",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents the implementation and evaluation of the H (hypervisor) extension for the RISC-V instruction set architecture (ISA) on top of the gem5 microarchitectural simulator. The RISC-V ISA, known for its simplicity and modularity, has seen widespread adoption in various computing domains. The H extension aims to enhance RISC-V's capabilities for cloud computing and virtualization. In this paper, we present the architectural integration of the H extension into gem5, an open-source, modular platform for computer system architecture research. We detail the modifications required in gem5's CPU models and virtualization support to accommodate the H extension. We also present evaluation results regarding the performance impact and functional correctness of the extension's implementation on gem5. This study not only provides a pathway for further research and development of RISC-V extensions but also contributes valuable insights into the optimization of the gem5 simulator for advanced architectural features.",
        "subjects": [
            "cs.AR"
        ],
        "comment": "8 pages, 6 figures, CARRV '24, November 3rd, 2024, Co-located with IEEE/ACM MICRO 2024"
    },
    {
        "paper id": "2411.12456",
        "abstract url": "https://arxiv.org/abs/2411.12456",
        "title": "Ichnos: A Carbon Footprint Estimator for Scientific Workflows",
        "rating": "-10",
        "keywords": [],
        "abstract": "We propose Ichnos, a novel and flexible tool to estimate the carbon footprint of Nextflow workflows based on detailed workflow traces, CI time series, and power models. First, Ichnos takes as input the automatically-generated workflow trace produced by Nextflow. Use of these traces is an original contribution, ensuring that users do not need to manually monitor power consumption and enabling analysis of previously executed workflows. Next, Ichnos allows users to provide their own resource power model for utilised compute resources to accurately reflect processor settings, such as the processor frequency, instead of solely relying on a linear function. Finally, Ichnos converts estimated energy consumption to overall carbon emissions using fine-grained time-series CI data for each workflow task and only resorts to coarse-grained yearly averages where high-resolution location-based CI data are not available. Additionally, Ichnos reports estimated energy consumption and carbon emissions per task, providing greater granularity than existing methodologies and allowing users to identify which of their tasks have the largest footprint to address. We provide the implementation of Ichnos as open-source. We demonstrate our tool on traces of two real-world Nextflow workflows, compare the estimated energy consumption against RAPL and the GA methodology, and show the tool's functionality by varying the granularity of provided CI data and varying the processor frequency settings of assigned compute resources.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "Extended Abstract for LOCO 2024. GitHub Repository: https://github.com/westkath/ichnos"
    },
    {
        "paper id": "2411.12480",
        "abstract url": "https://arxiv.org/abs/2411.12480",
        "title": "Probabilistic Day-Ahead Battery Scheduling based on Mixed Random Variables for Enhanced Grid Operation",
        "rating": "-10",
        "keywords": [],
        "abstract": "The increasing penetration of renewable energy sources introduces significant challenges to power grid stability, primarily due to their inherent variability. A new opportunity for grid operation is the smart integration of electricity production combined with battery storages in residential buildings. This study explores how residential battery systems can aid in stabilizing the power grid by flexibly managing deviations from forecasted residential power consumption and PV generation. The key contribution of this work is the development of an analytical approach that enables the asymmetric allocation of quantified power uncertainties between a residential battery system and the power grid, introducing a new degree of freedom into the scheduling problem. This is accomplished by employing mixed random variables - characterized by both continuous and discrete events - to model battery and grid power uncertainties. These variables are embedded into a continuous stochastic optimization framework, which computes probabilistic schedules for battery operation and power exchange with the grid. Test cases demonstrate that the proposed framework can be used effectively to reduce and quantify grid uncertainties while minimizing electricity costs. It is also shown that residential battery systems can be actively used to provide flexibility during critical periods of grid operation. Overall, this framework empowers prosumers to take an active role in grid stabilization, contributing to a more resilient and adaptive energy system.",
        "subjects": [
            "math.OC",
            "eess.SY"
        ],
        "comment": "12 pages, 7 figures, submitted to IREP 2025 Symposium"
    },
    {
        "paper id": "2411.12508",
        "abstract url": "https://arxiv.org/abs/2411.12508",
        "title": "Probe-Me-Not: Protecting Pre-trained Encoders from Malicious Probing",
        "rating": "-10",
        "keywords": [],
        "abstract": "Adapting pre-trained deep learning models to customized tasks has become a popular choice for developers to cope with limited computational resources and data volume. More specifically, probing--training a downstream head on a pre-trained encoder--has been widely adopted in transfer learning, which helps to prevent overfitting and catastrophic forgetting. However, such generalizability of pre-trained encoders raises concerns about the potential misuse of probing for harmful intentions, such as discriminatory speculation and warfare applications. In this work, we introduce EncoderLock, a novel applicability authorization method designed to protect pre-trained encoders from malicious probing, i.e., yielding poor performance on specified prohibited domains while maintaining their utility in authorized ones. Achieving this balance is challenging because of the opposite optimization objectives and the variety of downstream heads that adversaries can utilize adaptively. To address these challenges, EncoderLock employs two techniques: domain-aware weight selection and updating to restrict applications on prohibited domains/tasks, and self-challenging training scheme that iteratively strengthens resistance against any potential downstream classifiers that adversaries may apply. Moreover, recognizing the potential lack of data from prohibited domains in practical scenarios, we introduce three EncoderLock variants with different levels of data accessibility: supervised (prohibited domain data with labels), unsupervised (prohibited domain data without labels), and zero-shot (no data or labels available). We verify EncoderLock's effectiveness and practicality with a real-world pre-trained Vision Transformer (ViT) encoder from Facebook. These results underscore the valuable contributions EncoderLock brings to the development of responsible AI.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Accepted by Network and Distributed System Security (NDSS) Symposium 2025"
    },
    {
        "paper id": "2411.12512",
        "abstract url": "https://arxiv.org/abs/2411.12512",
        "title": "Near-Optimal Time-Sparsity Trade-Offs for Solving Noisy Linear Equations",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present a polynomial-time reduction from solving noisy linear equations over $\\mathbb{Z}/q\\mathbb{Z}$ in dimension $\u0398(k\\log n/\\mathsf{poly}(\\log k,\\log q,\\log\\log n))$ with a uniformly random coefficient matrix to noisy linear equations over $\\mathbb{Z}/q\\mathbb{Z}$ in dimension $n$ where each row of the coefficient matrix has uniformly random support of size $k$. This allows us to deduce the hardness of sparse problems from their dense counterparts. In particular, we derive hardness results in the following canonical settings. 1) Assuming the $\\ell$-dimensional (dense) LWE over a polynomial-size field takes time $2^{\u03a9(\\ell)}$, $k$-sparse LWE in dimension $n$ takes time $n^{\u03a9({k}/{(\\log k \\cdot (\\log k + \\log \\log n))})}.$ 2) Assuming the $\\ell$-dimensional (dense) LPN over $\\mathbb{F}_2$ takes time $2^{\u03a9(\\ell/\\log \\ell)}$, $k$-sparse LPN in dimension $n$ takes time $n^{\u03a9(k/(\\log k \\cdot (\\log k + \\log \\log n)^2))}~.$ These running time lower bounds are nearly tight as both sparse problems can be solved in time $n^{O(k)},$ given sufficiently many samples. We further give a reduction from $k$-sparse LWE to noisy tensor completion. Concretely, composing the two reductions implies that order-$k$ rank-$2^{k-1}$ noisy tensor completion in $\\mathbb{R}^{n^{\\otimes k}}$ takes time $n^{\u03a9(k/ \\log k \\cdot (\\log k + \\log \\log n))}$, assuming the exponential hardness of standard worst-case lattice problems.",
        "subjects": [
            "cs.CC",
            "cs.CR",
            "cs.DM",
            "math.ST"
        ],
        "comment": "Abstract shortened to match arXiv requirements"
    },
    {
        "paper id": "2411.12527",
        "abstract url": "https://arxiv.org/abs/2411.12527",
        "title": "Human-AI Co-Creativity: Exploring Synergies Across Levels of Creative Collaboration",
        "rating": "-10",
        "keywords": [],
        "abstract": "Human-AI co-creativity represents a transformative shift in how humans and generative AI tools collaborate in creative processes. This chapter explores the synergies between human ingenuity and AI capabilities across four levels of interaction: Digital Pen, AI Task Specialist, AI Assistant, and AI Co-Creator. While earlier digital tools primarily facilitated creativity, generative AI systems now contribute actively, demonstrating autonomous creativity in producing novel and valuable outcomes. Empirical evidence from mathematics showcases how AI can extend human creative potential, from computational problem-solving to co-creative partnerships yielding breakthroughs in longstanding challenges. By analyzing these collaborations, the chapter highlights AI's potential to enhance human creativity without replacing it, underscoring the importance of balancing AI's contributions with human oversight and contextual understanding. This integration pushes the boundaries of creative achievements, emphasizing the need for human-centered AI systems that foster collaboration while preserving the unique qualities of human creativity.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12541",
        "abstract url": "https://arxiv.org/abs/2411.12541",
        "title": "High-Throughput Blind Co-Channel Interference Cancellation for Edge Devices Using Depthwise Separable Convolutions, Quantization, and Pruning",
        "rating": "-10",
        "keywords": [],
        "abstract": "Co-channel interference cancellation (CCI) is the process used to reduce interference from other signals using the same frequency channel, thereby enhancing the performance of wireless communication systems. An improvement to this approach is blind CCI, which reduces interference without relying on prior knowledge of the interfering signal characteristics. Recent work suggested using machine learning (ML) models for this purpose, but high-throughput ML solutions are still lacking, especially for edge devices with limited resources. This work explores the adaptation of U-Net Convolutional Neural Network models for high-throughput blind source separation. Our approach is established on architectural modifications, notably through quantization and the incorporation of depthwise separable convolution, to achieve a balance between computational efficiency and performance. Our results demonstrate that the proposed models achieve superior MSE scores when removing unknown interference sources from the signals while maintaining significantly lower computational complexity compared to baseline models. One of our proposed models is deeper and fully convolutional, while the other is shallower with a convolutional structure incorporating an LSTM. Depthwise separable convolution and quantization further reduce the memory footprint and computational demands, albeit with some performance trade-offs. Specifically, applying depthwise separable convolutions to the model with the LSTM results in only a 0.72% degradation in MSE score while reducing MACs by 58.66%. For the fully convolutional model, we observe a 0.63% improvement in MSE score with even 61.10% fewer MACs. Overall, our findings underscore the feasibility of using optimized machine-learning models for interference cancellation in devices with limited resources.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12542",
        "abstract url": "https://arxiv.org/abs/2411.12542",
        "title": "Virtual Reality for Action Evaluation",
        "rating": "-10",
        "keywords": [],
        "abstract": "Physical rehabilitation plays a crucial role in restoring functional abilities, but traditional approaches often face challenges in terms of cost, accessibility, and personalized monitoring. Asynchronous physical rehabilitation has gained traction as a cost-effective and convenient alternative, but it lacks real-time monitoring and assessment capabilities. This study investigates the feasibility of using low-cost Virtual Reality (VR) devices for action evaluation in rehabilitation exercises. We leverage state-of-the-art deep learning models and evaluate their performance on three data streams (head and hands) derived from existing rehabilitation datasets that approximate VR headset and hand data. Our results demonstrate that VR tracking data can be effectively utilized for action evaluation, paving the way for more accessible and affordable remote monitoring solutions in physical therapy. By leveraging artificial intelligence techniques and consumer-grade virtual reality technology, this study proposes an approach that could potentially address some of the challenges in asynchronous rehabilitation, such as the need for expensive motion capture systems or in-person sessions.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "8 pages, 5 figures"
    },
    {
        "paper id": "2411.12573",
        "abstract url": "https://arxiv.org/abs/2411.12573",
        "title": "Locomotion Mode Transitions: Tackling System- and User-Specific Variability in Lower-Limb Exoskeletons",
        "rating": "-10",
        "keywords": [],
        "abstract": "Accurate detection of locomotion transitions, such as walk to sit, walk to stair ascent, and descent, is crucial to effectively control robotic assistive devices, such as lower-limb exoskeletons, as each locomotion mode requires specific assistance. Variability in collected sensor data introduced by user- or system-specific characteristics makes it challenging to maintain high transition detection accuracy while avoiding latency using non-adaptive classification models. In this study, we identified key factors influencing transition detection performance, including variations in user behavior, and different mechanical designs of the exoskeletons. To boost the transition detection accuracy, we introduced two methods for adapting a finite-state machine classifier to system- and user-specific variability: a Statistics-Based approach and Bayesian Optimization. Our experimental results demonstrate that both methods remarkably improve transition detection accuracy across diverse users, achieving up to an 80% increase in certain scenarios compared to the non-personalized threshold method. These findings emphasize the importance of personalization in adaptive control systems, underscoring the potential for enhanced user experience and effectiveness in assistive devices. By incorporating subject- and system-specific data into the model training process, our approach offers a precise and reliable solution for detecting locomotion transitions, catering to individual user needs, and ultimately improving the performance of assistive devices.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "10 pages, 11 figures"
    },
    {
        "paper id": "2411.12649",
        "abstract url": "https://arxiv.org/abs/2411.12649",
        "title": "PseudoSeer: a Search Engine for Pseudocode",
        "rating": "-10",
        "keywords": [],
        "abstract": "A novel pseudocode search engine is designed to facilitate efficient retrieval and search of academic papers containing pseudocode. By leveraging Elasticsearch, the system enables users to search across various facets of a paper, such as the title, abstract, author information, and LaTeX code snippets, while supporting advanced features like combined facet searches and exact-match queries for more targeted results. A description of the data acquisition process is provided, with arXiv as the primary data source, along with methods for data extraction and text-based indexing, highlighting how different data elements are stored and optimized for search. A weighted BM25-based ranking algorithm is used by the search engine, and factors considered when prioritizing search results for both single and combined facet searches are described. We explain how each facet is weighted in a combined search. Several search engine results pages are displayed. Finally, there is a brief overview of future work and potential evaluation methodology for assessing the effectiveness and performance of the search engine is described.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12653",
        "abstract url": "https://arxiv.org/abs/2411.12653",
        "title": "Smart Predict-then-Optimize Method with Dependent Data: Risk Bounds and Calibration of Autoregression",
        "rating": "-10",
        "keywords": [],
        "abstract": "The predict-then-optimize (PTO) framework is indispensable for addressing practical stochastic decision-making tasks. It consists of two crucial steps: initially predicting unknown parameters of an optimization model and subsequently solving the problem based on these predictions. Elmachtoub and Grigas [1] introduced the Smart Predict-then-Optimize (SPO) loss for the framework, which gauges the decision error arising from predicted parameters, and a convex surrogate, the SPO+ loss, which incorporates the underlying structure of the optimization model. The consistency of these different loss functions is guaranteed under the assumption of i.i.d. training data. Nevertheless, various types of data are often dependent, such as power load fluctuations over time. This dependent nature can lead to diminished model performance in testing or real-world applications. Motivated to make intelligent predictions for time series data, we present an autoregressive SPO method directly targeting the optimization problem at the decision stage in this paper, where the conditions of consistency are no longer met. Therefore, we first analyze the generalization bounds of the SPO loss within our autoregressive model. Subsequently, the uniform calibration results in Liu and Grigas [2] are extended in the proposed model. Finally, we conduct experiments to empirically demonstrate the effectiveness of the SPO+ surrogate compared to the absolute loss and the least squares loss, especially when the cost vectors are determined by stationary dynamical systems and demonstrate the relationship between normalized regret and mixing coefficients.",
        "subjects": [
            "eess.SY",
            "stat.ML"
        ],
        "comment": "10 pages"
    },
    {
        "paper id": "2411.12658",
        "abstract url": "https://arxiv.org/abs/2411.12658",
        "title": "Data-efficient Tactile Sensing with Electrical Impedance Tomography",
        "rating": "-10",
        "keywords": [],
        "abstract": "Electrical Impedance Tomography (EIT)-inspired tactile sensors are gaining attention in robotic tactile sensing due to their cost-effectiveness, safety, and scalability with sparse electrode configurations. This paper presents a data augmentation strategy for learning-based tactile reconstruction that amplifies the original single-frame signal measurement into 32 distinct, effective signal data for training. This approach supplements uncollected conditions of position information, resulting in more accurate and high-resolution tactile reconstructions. Data augmentation for EIT significantly reduces the required EIT measurements and achieves promising performance with even limited samples. Simulation results show that the proposed method improves the correlation coefficient by over 12% and reduces the relative error by over 21% under various noise levels. Furthermore, we demonstrate that a standard deep neural network (DNN) utilizing the proposed data augmentation reduces the required data down to 1/31 while achieving a similar tactile reconstruction quality. Real-world tests further validate the approach's effectiveness on a flexible EIT-based tactile sensor. These results could help address the challenge of training tactile sensing networks with limited available measurements, improving the accuracy and applicability of EIT-based tactile sensing systems.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12664",
        "abstract url": "https://arxiv.org/abs/2411.12664",
        "title": "Identifying patterns of proprioception and target matching acuity in healthy humans",
        "rating": "-10",
        "keywords": [],
        "abstract": "Traditional approaches to measurement in upper-limb therapy have gaps that electronic sensing and recording can help fill. We highlight shortcomings in current kinematic recording devices, and we introduce a wrist sensing device that performs multimodal sensing during single-axis rotation. Our goal is to characterize normative kinesthetic perception and real-world performance as a multimodal sensory \"fingerprint\" that can serve as a reference point for identifying deficit in persons affected by stroke, and then as a jumping point for later neuroscientific interrogation. We present an experiment involving psychophysical measurements of passive stimuli discrimination, matching adjustment acuity, and ADL performance in 11 neurologically-intact persons. We found that passive velocity sense and active position sense of healthy controls, measured by velocity discrimination and position matching respectively, correlated in rank with each other, but other score comparisons of acuity or task performance had no statistically significant correlations. We also found that participants differed in acuity between passive and active velocity sense, which supports current understanding about muscle spindle activation being modulated by conscious motor command. The potential for our null correlation results to reveal dissociable aspects of deficit is discussed, as well as implications for future neuroscientific study with more kinematic measures and larger datasets.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "14 pages, 15 figures; A newer version of this work has been submitted to the 2024 IEEE EMBC for possible publication in their conference proceedings"
    },
    {
        "paper id": "2411.12688",
        "abstract url": "https://arxiv.org/abs/2411.12688",
        "title": "An Algorithm to Speed up the Spatial Power Profile Calculation in Backward Raman Amplified Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "As data transmission demands grow, long-haul optical transmission links face increasing pressure to increase their throughput. Expanding usable bandwidth through Ultra-Wide Band (UWB) systems has become the primary strategy for increasing transmission capacity. However, UWB systems present challenges, such as the reliance on backward Raman amplification and the complications posed by inter-channel stimulated Raman scattering (ISRS), which causes uneven signal propagation across bands. To address these issues, accurate and efficient physical models are required for real-time optimization, which rely on the knowledge of the power profile. This paper develops a novel, more efficient method for computing the power profile of signals and pumps, utilizing the integral form of the equations with matrix-based approximations. The algorithm achieves up to a thirty-fold average speed increase over conventional approaches while maintaining an error margin under 0.05 dBm. These results represent a significant step forward towards reducing optimization times and enabling more extensive studies in ultra-wide band long haul optical transmission, further facilitating research and commercialization of UWB systems, in an effort to address the growing demand for higher throughput.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12692",
        "abstract url": "https://arxiv.org/abs/2411.12692",
        "title": "SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference",
        "rating": "-10",
        "keywords": [],
        "abstract": "Leveraging sparsity is crucial for optimizing large language model inference. however, modern LLMs employing SiLU as their activation function exhibit minimal activation sparsity. Recent research has proposed replacing SiLU with ReLU to induce significant activation sparsity and showed no downstream task accuracy degradation through fine tuning. However, taking full advantage of it required training a predictor to estimate this sparsity. In this paper, we introduce SparseInfer, a simple, light weight, and training free predictor for activation sparsity of ReLU field LLMs, in which activation sparsity is predicted by comparing only the sign bits of inputs and weights. To compensate for possible prediction inaccuracy, an adaptive tuning of the predictor's conservativeness is enabled, which can also serve as a control knob for optimizing LLM inference. The proposed method achieves approximately faster inference speed over the state of the art, with negligible accuracy loss of within 1%p.",
        "subjects": [
            "cs.PF"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12696",
        "abstract url": "https://arxiv.org/abs/2411.12696",
        "title": "Weighted Envy Freeness With Limited Subsidies",
        "rating": "-10",
        "keywords": [],
        "abstract": "We explore solutions for fairly allocating indivisible items among agents assigned weights representing their entitlements. Our fairness goal is weighted-envy-freeness (WEF), where each agent deems their allocated portion relative to their entitlement at least as favorable as any other's relative to their own. In many cases, achieving WEF necessitates monetary transfers, which can be modeled as third-party subsidies. The goal is to attain WEF with bounded subsidies. Previous work in the unweighted setting of subsidies relied on basic characterizations of EF that fail in the weighted settings. This makes our new setting challenging and theoretically intriguing. We present polynomial-time algorithms that compute WEF-able allocations with an upper bound on the subsidy per agent in three distinct additive valuation scenarios: (1) general, (2) identical, and (3) binary. When all weights are equal, our bounds reduce to the bounds derived in the literature for the unweighted setting.",
        "subjects": [
            "cs.GT",
            "cs.DS"
        ],
        "comment": "45 pages, 1 table"
    },
    {
        "paper id": "2411.12710",
        "abstract url": "https://arxiv.org/abs/2411.12710",
        "title": "Travel Time Based Task Mapping for NoC-Based DNN Accelerator",
        "rating": "-10",
        "keywords": [],
        "abstract": "Network-on-Chip (NoC) based architectures are recently proposed to accelerate deep neural networks in specialized hardware. Given that the hardware configuration is fixed post-manufacture, proper task mapping attracts researchers' interest. We propose a travel time-based task mapping method that allocates uneven counts of tasks across different Processing Elements (PEs). This approach utilizes the travel time recorded in the sampling window and implicitly makes use of static NoC architecture information and dynamic NoC congestion status. Furthermore, we examine the effectiveness of our method under various configurations, including different mapping iterations, flit sizes, and NoC architecture. Our method achieves up to 12.1% improvement compared with even mapping and static distance mapping for one layer. For a complete NN example, our method achieves 10.37% and 13.75% overall improvements to row-major mapping and distance-based mapping, respectively. While ideal travel time-based mapping (post-run) achieves 10.37% overall improvements to row-major mapping, we adopt a sampling window to efficiently map tasks during the running, achieving 8.17% (sampling window 10) improvement.",
        "subjects": [
            "cs.AR"
        ],
        "comment": "Accepted to SAMOS 2024 (SAMOS XXIV International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation June 29 - July 4, 2024)"
    },
    {
        "paper id": "2411.12725",
        "abstract url": "https://arxiv.org/abs/2411.12725",
        "title": "Reinforcement Learning, Collusion, and the Folk Theorem",
        "rating": "-10",
        "keywords": [],
        "abstract": "We explore the behaviour emerging from learning agents repeatedly interacting strategically for a wide range of learning dynamics that includes projected gradient, replicator and log-barrier dynamics. Going beyond the better-understood classes of potential games and zero-sum games, we consider the setting of a general repeated game with finite recall, for different forms of monitoring. We obtain a Folk Theorem-like result and characterise the set of payoff vectors that can be obtained by these dynamics, discovering a wide range of possibilities for the emergence of algorithmic collusion.",
        "subjects": [
            "cs.GT",
            "econ.TH",
            "stat.ML"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2210.08857 by other authors"
    },
    {
        "paper id": "2411.12734",
        "abstract url": "https://arxiv.org/abs/2411.12734",
        "title": "Soft Robotic Dynamic In-Hand Pen Spinning",
        "rating": "-10",
        "keywords": [],
        "abstract": "Dynamic in-hand manipulation remains a challenging task for soft robotic systems that have demonstrated advantages in safe compliant interactions but struggle with high-speed dynamic tasks. In this work, we present SWIFT, a system for learning dynamic tasks using a soft and compliant robotic hand. Unlike previous works that rely on simulation, quasi-static actions and precise object models, the proposed system learns to spin a pen through trial-and-error using only real-world data without requiring explicit prior knowledge of the pen's physical attributes. With self-labeled trials sampled from the real world, the system discovers the set of pen grasping and spinning primitive parameters that enables a soft hand to spin a pen robustly and reliably. After 130 sampled actions per object, SWIFT achieves 100% success rate across three pens with different weights and weight distributions, demonstrating the system's generalizability and robustness to changes in object properties. The results highlight the potential for soft robotic end-effectors to perform dynamic tasks including rapid in-hand manipulation. We also demonstrate that SWIFT generalizes to spinning items with different shapes and weights such as a brush and a screwdriver which we spin with 10/10 and 5/10 success rates respectively. Videos, data, and code are available at https://soft-spin.github.io.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12735",
        "abstract url": "https://arxiv.org/abs/2411.12735",
        "title": "The More the Merrier: On Evolving Five-valued Spectra Boolean Functions",
        "rating": "-10",
        "keywords": [],
        "abstract": "Evolving Boolean functions with specific properties is an interesting optimization problem since, depending on the combination of properties and Boolean function size, the problem can range from very simple to (almost) impossible to solve. Moreover, some problems are more interesting as there may be only a few options for generating the required Boolean functions. This paper investigates one such problem: evolving five-valued spectra Boolean functions, which are the functions whose Walsh-Hadamard coefficients can only take five distinct values. We experimented with three solution encodings, two fitness functions, and 12 Boolean function sizes and showed that the tree encoding is superior to other choices, as we can obtain five-valued Boolean functions with high nonlinearity.",
        "subjects": [
            "cs.NE"
        ],
        "comment": "18 pages, 2 figures, 2 tables"
    },
    {
        "paper id": "2411.12740",
        "abstract url": "https://arxiv.org/abs/2411.12740",
        "title": "WIA-SZZ: Work Item Aware SZZ",
        "rating": "-10",
        "keywords": [],
        "abstract": "Many software engineering maintenance tasks require linking a commit that induced a bug with the commit that later fixed that bug. Several existing SZZ algorithms provide a way to identify the potential commit that induced a bug when given a fixing commit as input. Prior work introduced the notion of a \"work item\", a logical grouping of commits that could be a single unit of work. Our key insight in this work is to recognize that a bug-inducing commit and the fix(es) for that bug together represent a \"work item.\" It is not currently understood how these work items, which are logical groups of revisions addressing a single issue or feature, could impact the performance of algorithms such as SZZ. In this paper, we propose a heuristic that, given an input commit, uses information about changed methods to identify related commits that form a work item with the input commit. We hypothesize that given such a work item identifying heuristic, we can identify bug-inducing commits more accurately than existing SZZ approaches. We then build a new variant of SZZ that we call Work Item Aware SZZ (WIA-SZZ), that leverages our work item detecting heuristic to first suggest bug-inducing commits. If our heuristic fails to find any candidates, we then fall back to baseline variants of SZZ. We conduct a manual evaluation to assess the accuracy of our heuristic to identify work items. Our evaluation reveals the heuristic is 64% accurate in finding work items, but most importantly it is able to find many bug-inducing commits. We then evaluate our approach on 821 repositories that have been previously used to study the performance of SZZ, comparing our work against six SZZ variants. That evaluation shows an improvement in F1 scores ranging from 2% to 9%, or when looking only at the subset of cases that found work item improved 3% to 14%.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12777",
        "abstract url": "https://arxiv.org/abs/2411.12777",
        "title": "User Experience Evaluation of Augmented Reality: A Systematic Literature Review",
        "rating": "-10",
        "keywords": [],
        "abstract": "Due to technological development, Augmented Reality (AR) can be applied in different domains. However, innovative technologies refer to new interaction paradigms, thus creating a new experience for the user. This so-called User Experience (UX) is essential for developing and designing interactive products. Moreover, UX must be measured to get insights into the user's perception and, thus, to improve innovative technologies. We conducted a Systematic Literature Review (SLR) to provide an overview of the current research concerning UX evaluation of AR. In particular, we aim to identify (1) research referring to UX evaluation of AR and (2) articles containing AR-specific UX models or frameworks concerning the theoretical foundation. The SLR is a five-step approach including five scopes. From a total of 498 records based on eight search terms referring to two databases, 30 relevant articles were identified and further analyzed. Results show that most approaches concerning UX evaluation of AR are quantitative. In summary, five UX models/frameworks were identified. Concerning the UX evaluation results of AR in Training and Education, the UX was consistently positive. Negative aspects refer to errors and deficiencies concerning the AR system and its functionality. No specific metric for UX evaluation of AR in the field of Training and Education exists. Only three AR-specific standardized UX questionnaires could be found. However, the questionnaires do not refer to the field of Training and Education. Thus, there is a lack of research in the field of UX evaluation of AR in Training and Education.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "18 pages, 11 figures, CENTRIC 2024 : The Seventeenth International Conference on Advances in Human-oriented and Personalized Mechanisms, Technologies, and Services"
    },
    {
        "paper id": "2411.12822",
        "abstract url": "https://arxiv.org/abs/2411.12822",
        "title": "Denotational Semantics of Gradual Typing using Synthetic Guarded Domain Theory (Extended Version)",
        "rating": "-10",
        "keywords": [],
        "abstract": "Gradually typed programming languages, which allow for soundly mixing static and dynamically typed programming styles, present a strong challenge for metatheorists. Even the simplest sound gradually typed languages feature at least recursion and errors, with realistic languages featuring furthermore runtime allocation of memory locations and dynamic type tags. Further, the desired metatheoretic properties of gradually typed languages have become increasingly sophisticated: validity of type-based equational reasoning as well as the relational property known as graduality. Many recent works have tackled verifying these properties, but the resulting mathematical developments are highly repetitive and tedious, with few reusable theorems persisting across different developments. In this work, we present a new denotational semantics for gradual typing developed using guarded domain theory. Guarded domain theory combines the generality of step-indexed logical relations for modeling advanced programming features with the modularity and reusability of denotational semantics. We demonstrate the feasibility of this approach with a model of a simple gradually typed lambda calculus and prove the validity of beta-eta equality and the graduality theorem for the denotational model. This model should provide the basis for a reusable mathematical theory of gradually typed program semantics. Finally, we have mechanized most of the core theorems of our development in Guarded Cubical Agda, a recent extension of Agda with support for the guarded recursive constructions we use.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12825",
        "abstract url": "https://arxiv.org/abs/2411.12825",
        "title": "TopoCode: Topologically Informed Error Detection and Correction in Communication Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Traditional error detection and correction codes focus on bit-level fidelity, which is insufficient for emerging technologies like eXtended Reality (XR) and holographic communications requiring high-data-rate, low-latency systems. Bit-level metrics cannot comprehensively evaluate Quality-of-Service (QoS) in these scenarios. This letter proposes TopoCode which leverages Topological Data Analysis (TDA) and persistent homology to encode topological information for message-level error detection and correction. It introduces minimal redundancy while enabling effective data reconstruction, especially in low Signal-to-Noise Ratio (SNR) conditions. TopoCode offers a promising approach to meet the demands of next-generation communication systems prioritizing semantic accuracy and message-level integrity.",
        "subjects": [
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12840",
        "abstract url": "https://arxiv.org/abs/2411.12840",
        "title": "The Aldous--Hoover Theorem in Categorical Probability",
        "rating": "-10",
        "keywords": [],
        "abstract": "The Aldous-Hoover Theorem concerns an infinite matrix of random variables whose distribution is invariant under finite permutations of rows and columns. It states that, up to equality in distribution, each random variable in the matrix can be expressed as a function only depending on four key variables: one common to the entire matrix, one that encodes information about its row, one that encodes information about its column, and a fourth one specific to the matrix entry. We state and prove the theorem within a category-theoretic approach to probability, namely the theory of Markov categories. This makes the proof more transparent and intuitive when compared to measure-theoretic ones. A key role is played by a newly identified categorical property, the Cauchy--Schwarz axiom, which also facilitates a new synthetic de Finetti Theorem. We further provide a variant of our proof using the ordered Markov property and the d-separation criterion, both generalized from Bayesian networks to Markov categories. We expect that this approach will facilitate a systematic development of more complex results in the future, such as categorical approaches to hierarchical exchangeability.",
        "subjects": [
            "math.ST",
            "cs.LO",
            "math.CT",
            "math.PR"
        ],
        "comment": "39 pages"
    },
    {
        "paper id": "2411.12857",
        "abstract url": "https://arxiv.org/abs/2411.12857",
        "title": "An Abstract Domain for Heap Commutativity (Extended Version)",
        "rating": "-10",
        "keywords": [],
        "abstract": "Commutativity of program code (i.e. the equivalence of two code fragments composed in alternate orders) is of ongoing interest in many settings such as program verification, scalable concurrency, and security analysis. While some have explored static analysis for code commutativity, few have specifically catered to heap-manipulating programs. We introduce an abstract domain in which commutativity synthesis or verification techniques can safely be performed on abstract mathematical models and, from those results, one can directly obtain commutativity conditions for concrete heap programs. This approach offloads challenges of concrete heap reasoning into the simpler abstract space. We show this reasoning supports framing and composition, and conclude with commutativity analysis of programs operating on example heap data structures. Our work has been mechanized in Coq and is available in the supplement.",
        "subjects": [
            "cs.PL",
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12893",
        "abstract url": "https://arxiv.org/abs/2411.12893",
        "title": "Fast and Efficient Memory Reclamation For Serverless MicroVMs",
        "rating": "-10",
        "keywords": [],
        "abstract": "Resource elasticity is one of the key defining characteristics of the Function-as-a-Service (FaaS) serverless computing paradigm. In order to provide strong multi-tenant isolation, FaaS providers commonly sandbox functions inside virtual machines (VMs or microVMs). While compute resources assigned to VM-sandboxed functions can be seamlessly adjusted on the fly, memory elasticity remains challenging, especially when scaling down. State-of-the-art mechanisms for VM memory elasticity suffer from increased reclaim latency when memory needs to be released, compounded by CPU and memory bandwidth overheads. We identify the obliviousness of the Linux memory manager to the virtually hotplugged memory as the key issue hindering hot-unplug performance, and design HotMem, a novel approach for fast and efficient VM memory hot(un)plug, targeting VM-sandboxed serverless functions. Our key insight is that by segregating virtually hotplugged memory regions from regular VM memory, we are able to bound the lifetimes of allocations within these regions thus enabling their fast and efficient reclamation. We implement HotMem in Linux v6.6 and our evaluation shows that it is an order of magnitude faster than state-of-practice to reclaim VM memory, while achieving the same P99 function latency with a model that statically over-provisions VMs.",
        "subjects": [
            "cs.OS"
        ],
        "comment": null
    },
    {
        "paper id": "2411.12906",
        "abstract url": "https://arxiv.org/abs/2411.12906",
        "title": "Experimental Study of Underwater Acoustic Reconfigurable Intelligent Surfaces with In-Phase and Quadrature Modulation",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents an underwater acoustic reconfigurable intelligent surfaces (UA-RIS) designed for long-range, high-speed, and environmentally friendly communication in oceanic environments. The proposed UA-RIS comprises multiple pairs of acoustic reflectors that utilize in-phase and quadrature (IQ) modulation to flexibly control the amplitude and phase of reflected waves. This capability enables precise beam steering to enhance or attenuate sound levels in specific directions. A prototype UA-RIS with 4*6 acoustic reflection units is constructed and tested in both tank and lake environments to evaluate performance. The experimental results indicate that the prototype is capable of effectively pointing reflected waves to targeted directions while minimizing side lobes using passive IQ modulation. Field tests reveal that deploying the UA-RIS on the sender side considerably extends communication ranges by 28% in deep water and 46% in shallow waters. Furthermore, with a fixed communication distance, positioning the UA-RIS at the transmitter side substantially boosts data rates, with an average increase of 63.8% and peaks up to 96%. When positioned on the receiver side, the UA-RIS can expand the communication range in shallow and deep water environments by 40.6% and 66%, respectively. Moreover, placing the UA-RIS close to the receiver enhances data rates by an average of 80.3%, reaching up to 163% under certain circumstances.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "12 pages, 17 figures"
    },
    {
        "paper id": "2411.12923",
        "abstract url": "https://arxiv.org/abs/2411.12923",
        "title": "Towards Automated Verification of Logarithmic Arithmetic",
        "rating": "-10",
        "keywords": [],
        "abstract": "Correctness proofs for floating point programs are difficult to verify. To simplify the task, a similar, but less complex system, known as logarithmic arithmetic can be used. The Boyer-Moore Theorem Prover, NQTHM, mechanically verified the correctness of a simple implementation of logarithmic arithmetic. It also verified some useful theorems about accumulated relative error bounds for addition, multiplication and division in this logarithmic number system. These theorems were used to verify a program that approximates e^x using a truncated Taylor series. Axioms that characterize the finite precision of the logarithmic system using a rational base, b, were shown by the prover to be satisfiable for any choice of 1 < b < 2. The prover verified the correctness of a function for converting an arbitrary rational value to a logarithmic representation. It also verified that multiplication and division implementations produce exact results for exact inputs, and that addition implementation produces a result as accurate as possible for exact inputs. When these operations are used in combination by a program, such as evaluating a polynomial, the relative error increases in a way that can be bounded by simple expressions, referred to here as tolerances. Several mechanically verified theorems about tolerances allow us to construct mechanically verified proofs about logarithmic arithmetic programs. Although similar to interval arithmetic, tolerances are especially suited to logarithmic arithmetic.",
        "subjects": [
            "cs.LO",
            "cs.AR"
        ],
        "comment": "21 pages, Originally a 1994 technical report from University of Wyoming and Computational Logic, Inc"
    },
    {
        "paper id": "2411.12939",
        "abstract url": "https://arxiv.org/abs/2411.12939",
        "title": "Stabilization of Switched Affine Systems With Dwell-Time Constraint",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper addresses the problem of stabilization of switched affine systems under dwell-time constraint, giving guarantees on the bound of the quadratic cost associated with the proposed state switching control law. Specifically, two switching rules are presented relying on the solution of differential Lyapunov inequalities and Lyapunov-Metzler inequalities, from which the stability conditions are expressed. The first one allows to regulate the state of linear switched systems to zero, whereas the second one is designed for switched affine systems proving practical stability of the origin. In both cases, the determination of a guaranteed cost associated with each control strategy is shown. In the cases of linear and affine systems, the existence of the solution for the Lyapunov-Metzler condition is discussed and guidelines for the selection of a solution ensuring suitable performance of the system evolution are provided. The theoretical results are finally assessed by means of three examples.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "12 pages, 10 figures"
    },
    {
        "paper id": "2411.12983",
        "abstract url": "https://arxiv.org/abs/2411.12983",
        "title": "Veryl: A New Hardware Description Language as an Altarnative to SystemVerilog",
        "rating": "-10",
        "keywords": [],
        "abstract": "Veryl, a hardware description language based on SystemVerilog, offers optimized syntax tailored for logic design, ensuring synthesizability and simplifying common constructs. It prioritizes interoperability with SystemVerilog, allowing for smooth integration with existing projects while maintaining high readability. Additionally, Veryl includes a comprehensive set of development support tools, such as package managers and real-time checkers, to boost productivity and streamline the design process. These features empower designers to conduct high-quality hardware design efficiently.",
        "subjects": [
            "cs.AR"
        ],
        "comment": "6 pages, 6 figures, Published in DVCon Japan 2024"
    },
    {
        "paper id": "2411.12993",
        "abstract url": "https://arxiv.org/abs/2411.12993",
        "title": "HapKnob -- A Motorized Shape-changing Haptic Knob Interface",
        "rating": "-10",
        "keywords": [],
        "abstract": "The absence of physical interfaces creates challenges when interacting with touchscreen technology. This study aims to investigate an innovative haptic solution for interacting with graphical user interfaces. A motorized shape-changing rotary knob interface, HapKnob, has been developed, achieving seven distinctive shape configurations and various force feedback renderings. HapKnob presents a compact design and can provide additional configurations and combinations based on user needs. The anticipated results hold the potential to advance the development of future user interfaces, especially in situations where visual interaction is unavailable.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "IEEE Haptics Symposium 2024 Work-in-process Paper (Accepted)"
    },
    {
        "paper id": "2411.12999",
        "abstract url": "https://arxiv.org/abs/2411.12999",
        "title": "From Signal Space To STP-CS",
        "rating": "-10",
        "keywords": [],
        "abstract": "Under the assumption that a finite signal with different sampling lengths or different sampling frequencies is considered as equivalent, the signal space is considered as the quotient space of $\\mathbb{R}^{\\infty}$ over equivalence. The topological structure and the properties of signal space are investigated. Using them some characteristics of semi-tensor product based compressed sensing (STP-CS) are revealed. Finally, a systematic analysis of the construction of sensing matrix based on balanced incomplete block design (BIBD) is presented.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13015",
        "abstract url": "https://arxiv.org/abs/2411.13015",
        "title": "Strong XOR Lemma for Information Complexity",
        "rating": "-10",
        "keywords": [],
        "abstract": "For any $\\{0,1\\}$-valued function $f$, its \\emph{$n$-folded XOR} is the function $f^{\\oplus n}$ where $f^{\\oplus n}(X_1, \\ldots, X_n) = f(X_1) \\oplus \\cdots \\oplus f(X_n)$. Given a procedure for computing the function $f$, one can apply a ``naive\" approach to compute $f^{\\oplus n}$ by computing each $f(X_i)$ independently, followed by XORing the outputs. This approach uses $n$ times the resources required for computing $f$. In this paper, we prove a strong XOR lemma for \\emph{information complexity} in the two-player randomized communication model: if computing $f$ with an error probability of $O(n^{-1})$ requires revealing $I$ bits of information about the players' inputs, then computing $f^{\\oplus n}$ with a constant error requires revealing $\u03a9(n) \\cdot (I - 1 - o_n(1))$ bits of information about the players' inputs. Our result demonstrates that the naive protocol for computing $f^{\\oplus n}$ is both information-theoretically optimal and asymptotically tight in error trade-offs.",
        "subjects": [
            "cs.CC",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2411.13020",
        "abstract url": "https://arxiv.org/abs/2411.13020",
        "title": "AsymDex: Leveraging Asymmetry and Relative Motion in Learning Bimanual Dexterity",
        "rating": "-10",
        "keywords": [],
        "abstract": "We present Asymmetric Dexterity (AsymDex), a novel reinforcement learning (RL) framework that can efficiently learn asymmetric bimanual skills for multi-fingered hands without relying on demonstrations, which can be cumbersome to collect. Two crucial ingredients enable AsymDex to reduce the observation and action space dimensions and improve sample efficiency. First, AsymDex leverages the natural asymmetry found in human bimanual manipulation and assigns specific and interdependent roles to each hand: a facilitating hand that moves and reorients the object, and a dominant hand that performs complex manipulations on said object. Second, AsymDex defines and operates over relative observation and action spaces, facilitating responsive coordination between the two hands. Further, AsymDex can be easily integrated with recent advances in grasp learning to handle both the object acquisition phase and the interaction phase of bimanual dexterity. Unlike existing RL-based methods for bimanual dexterity, which are tailored to a specific task, AsymDex can be used to learn a wide variety of bimanual tasks that exhibit asymmetry. Detailed experiments on four simulated asymmetric bimanual dexterous manipulation tasks reveal that AsymDex consistently outperforms strong baselines that challenge its design choices, in terms of success rate and sample efficiency. The project website is at https://sites.google.com/view/asymdex-2024/.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "Accepted by CoRL 2024 Workshop WCBM"
    },
    {
        "paper id": "2411.13601",
        "abstract url": "https://arxiv.org/abs/2411.13601",
        "title": "Error Analysis of Sum-Product Algorithms under Stochastic Rounding",
        "rating": "-10",
        "keywords": [],
        "abstract": "The quality of numerical computations can be measured through their forward error, for which finding good error bounds is challenging in general. For several algorithms and using stochastic rounding (SR), probabilistic analysis has been shown to be an effective alternative for obtaining tight error bounds. This analysis considers the distribution of errors and evaluates the algorithm's performance on average. Using martingales and the Azuma-Hoeffding inequality, it provides error bounds that are valid with a certain probability and in $\\mathcal{O}(\\sqrt{n}u)$ instead of deterministic worst-case bounds in $\\mathcal{O}(nu)$, where $n$ is the number of operations and $u$ is the unit roundoff.In this paper, we present a general method that automatically constructs a martingale for any computation scheme with multi-linear errors based on additions, subtractions, and multiplications. We apply this generalization to algorithms previously studied with SR, such as pairwise summation and the Horner algorithm, and prove equivalent results. We also analyze a previously unstudied algorithm, Karatsuba polynomial multiplication, which illustrates that the method can handle reused intermediate computations.",
        "subjects": [
            "stat.CO",
            "cs.DS"
        ],
        "comment": null
    }
]