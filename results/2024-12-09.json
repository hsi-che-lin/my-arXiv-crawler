[
    {
        "paper id": "2412.07077",
        "abstract url": "https://arxiv.org/abs/2412.07077",
        "title": "Retaining and Enhancing Pre-trained Knowledge in Vision-Language Models with Prompt Ensembling",
        "rating": "2.5",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ],
            [
                "WACV"
            ]
        ],
        "abstract": "The advancement of vision-language models, particularly the Contrastive Language-Image Pre-training (CLIP) model, has revolutionized the field of machine learning by enabling robust zero-shot learning capabilities. These capabilities allow models to understand and respond to previously unseen data without task-specific training. However, adapting CLIP to integrate specialized knowledge from various domains while retaining its zero-shot capabilities remains a significant challenge. To address this, we introduce a novel prompt ensemble learning approach called Group-wise Prompt Ensemble (GPE). This method aims to enhance CLIP's zero-shot capabilities by incorporating new domain knowledge while improving its adaptability and robustness against data distribution shifts. Our approach hinges on three main strategies: prompt grouping with masked attention to optimize CLIP's adaptability while safeguarding its zero-shot capabilities; the incorporation of auxiliary prompts for the seamless integration of new domain insights without disrupting the original model's representation; and an ensemble learning strategy that effectively merges original and new knowledge. Through rigorous experimentation, including more challenging cross-dataset transfer evaluations, our GPE method redefines the benchmarks for the adaptability and efficiency of vision-language models, surpassing existing models across various scenarios.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2025"
    },
    {
        "paper id": "2412.07157",
        "abstract url": "https://arxiv.org/abs/2412.07157",
        "title": "Multi-Scale Contrastive Learning for Video Temporal Grounding",
        "rating": "2.5",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Temporal grounding, which localizes video moments related to a natural language query, is a core problem of vision-language learning and video understanding. To encode video moments of varying lengths, recent methods employ a multi-level structure known as a feature pyramid. In this structure, lower levels concentrate on short-range video moments, while higher levels address long-range moments. Because higher levels experience downsampling to accommodate increasing moment length, their capacity to capture information is reduced and consequently leads to degraded information in moment representations. To resolve this problem, we propose a contrastive learning framework to capture salient semantics among video moments. Our key methodology is to leverage samples from the feature space emanating from multiple stages of the video encoder itself requiring neither data augmentation nor online memory banks to obtain positive and negative samples. To enable such an extension, we introduce a sampling process to draw multiple video moments corresponding to a common query. Subsequently, by utilizing these moments' representations across video encoder layers, we instantiate a novel form of multi-scale and cross-scale contrastive learning that links local short-range video moments with global long-range video moments. Extensive experiments demonstrate the effectiveness of our framework for not only long-form but also short-form video grounding.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at AAAI 2025"
    },
    {
        "paper id": "2412.06209",
        "abstract url": "https://arxiv.org/abs/2412.06209",
        "title": "Sound2Vision: Generating Diverse Visuals from Audio through Cross-Modal Latent Alignment",
        "rating": "2",
        "keywords": [
            [
                "audio-visual"
            ],
            [
                "cs.CV",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "How does audio describe the world around us? In this work, we propose a method for generating images of visual scenes from diverse in-the-wild sounds. This cross-modal generation task is challenging due to the significant information gap between auditory and visual signals. We address this challenge by designing a model that aligns audio-visual modalities by enriching audio features with visual information and translating them into the visual latent space. These features are then fed into the pre-trained image generator to produce images. To enhance image quality, we use sound source localization to select audio-visual pairs with strong cross-modal correlations. Our method achieves substantially better results on the VEGAS and VGGSound datasets compared to previous work and demonstrates control over the generation process through simple manipulations to the input waveform or latent space. Furthermore, we analyze the geometric properties of the learned embedding space and demonstrate that our learning approach effectively aligns audio-visual signals for cross-modal generation. Based on this analysis, we show that our method is agnostic to specific design choices, showing its generalizability by integrating various model architectures and different types of audio-visual data.",
        "subjects": [
            "cs.CV",
            "cs.MM",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "Under-review"
    },
    {
        "paper id": "2412.06244",
        "abstract url": "https://arxiv.org/abs/2412.06244",
        "title": "DenseVLM: A Retrieval and Decoupled Alignment Framework for Open-Vocabulary Dense Prediction",
        "rating": "2",
        "keywords": [
            [
                "vision-language",
                "VLMs"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated impressive zero-shot recognition capability, but still underperform in dense prediction tasks. Self-distillation recently is emerging as a promising approach for fine-tuning VLMs to better adapt to local regions without requiring extensive annotations. However, previous state-of-the-art approaches often suffer from significant `foreground bias', where models tend to wrongly identify background regions as foreground objects. To alleviate this issue, we propose DenseVLM, a framework designed to learn unbiased region-language alignment from powerful pre-trained VLM representations. By leveraging the pre-trained VLM to retrieve categories for unlabeled regions, DenseVLM effectively decouples the interference between foreground and background region features, ensuring that each region is accurately aligned with its corresponding category. We show that DenseVLM can be seamlessly integrated into open-vocabulary object detection and image segmentation tasks, leading to notable performance improvements. Furthermore, it exhibits promising zero-shot scalability when training on more extensive and diverse datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06263",
        "abstract url": "https://arxiv.org/abs/2412.06263",
        "title": "iLLaVA: An Image is Worth Fewer Than 1/3 Input Tokens in Large Multimodal Models",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we introduce iLLaVA, a simple method that can be seamlessly deployed upon current Large Vision-Language Models (LVLMs) to greatly increase the throughput with nearly lossless model performance, without a further requirement to train. iLLaVA achieves this by finding and gradually merging the redundant tokens with an accurate and fast algorithm, which can merge hundreds of tokens within only one step. While some previous methods have explored directly pruning or merging tokens in the inference stage to accelerate models, our method excels in both performance and throughput by two key designs. First, while most previous methods only try to save the computations of Large Language Models (LLMs), our method accelerates the forward pass of both image encoders and LLMs in LVLMs, which both occupy a significant part of time during inference. Second, our method recycles the beneficial information from the pruned tokens into existing tokens, which avoids directly dropping context tokens like previous methods to cause performance loss. iLLaVA can nearly 2$\\times$ the throughput, and reduce the memory costs by half with only a 0.2\\% - 0.5\\% performance drop across models of different scales including 7B, 13B and 34B. On tasks across different domains including single-image, multi-images and videos, iLLaVA demonstrates strong generalizability with consistently promising efficiency. We finally offer abundant visualizations to show the merging processes of iLLaVA in each step, which show insights into the distribution of computing resources in LVLMs. Code is available at https://github.com/hulianyuyy/iLLaVA.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06441",
        "abstract url": "https://arxiv.org/abs/2412.06441",
        "title": "BoRA: Bi-dimensional Weight-Decomposed Low-Rank Adaptation",
        "rating": "2",
        "keywords": [
            [
                "Parameter-Efficient",
                "PEFT",
                "Efficient Fine-Tuning"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In recent years, Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) have significantly enhanced the adaptability of large-scale pre-trained models. Weight-Decomposed Low-Rank Adaptation (DoRA) improves upon LoRA by separating the magnitude and direction components of the weight matrix, leading to superior performance. However, DoRA's improvements are limited to the vertical dimension, resulting in an asymmetrical pattern between horizontal and vertical dimensions. This paper introduces BoRA, an innovative extension of LoRA and DoRA, characterized by symmetrical properties across horizontal and vertical dimensions. Our approach optimizes the weight matrix symmetrically by adjusting both column-wise and row-wise magnitudes. Extensive experiments demonstrate that BoRA surpasses state-of-the-art PEFT methods, including LoRA and DoRA, achieving superior results across various benchmarks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06458",
        "abstract url": "https://arxiv.org/abs/2412.06458",
        "title": "Pruning All-Rounder: Rethinking and Improving Inference Efficiency for Large Vision Language Models",
        "rating": "2",
        "keywords": [
            [
                "Vision Language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Although Large Vision-Language Models (LVLMs) have achieved impressive results, their high computational cost poses a significant barrier to wider application. To enhance inference efficiency, most existing approaches depend on parameter-dependent or token-dependent strategies to reduce computational demands. However, these methods typically require complex training processes and struggle to consistently select the most relevant tokens. In this paper, we systematically analyze the above challenges and provide a series of valuable insights for inference acceleration. Based on these findings, we propose a novel framework, the Pruning All-Rounder (PAR). Different from previous works, PAR develops a meta-router to adaptively organize pruning flows across both tokens and layers. With a self-supervised learning manner, our method achieves a superior balance between performance and efficiency. Notably, PAR is highly flexible, offering multiple pruning versions to address a range of pruning scenarios. The code for this work will be made publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06464",
        "abstract url": "https://arxiv.org/abs/2412.06464",
        "title": "Gated Delta Networks: Improving Mamba2 with Delta Rule",
        "rating": "2",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Linear Transformers have gained attention as efficient alternatives to standard Transformers, but their performance in retrieval and long-context tasks has been limited. To address these limitations, recent work has explored two distinct mechanisms: gating for adaptive memory control and the delta update rule for precise memory modifications. We observe that these mechanisms are complementary: gating enables rapid memory erasure while the delta rule facilitates targeted updates. Building on this insight, we introduce the gated delta rule and develop a parallel training algorithm optimized for modern hardware. Our proposed architecture, Gated DeltaNet, consistently surpasses existing models like Mamba2 and DeltaNet across multiple benchmarks, including language modeling, common-sense reasoning, in-context retrieval, length extrapolation, and long-context understanding. We further enhance performance by developing hybrid architectures that combine Gated DeltaNet layers with sliding window attention or Mamba2 layers, achieving both improved training efficiency and superior task performance.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": "Preprint"
    },
    {
        "paper id": "2412.06474",
        "abstract url": "https://arxiv.org/abs/2412.06474",
        "title": "From Uncertainty to Trust: Enhancing Reliability in Vision-Language Models with Uncertainty-Guided Dropout Decoding",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Large vision-language models (LVLMs) demonstrate remarkable capabilities in multimodal tasks but are prone to misinterpreting visual inputs, often resulting in hallucinations and unreliable outputs. To address these challenges, we propose Dropout Decoding, a novel inference-time approach that quantifies the uncertainty of visual tokens and selectively masks uncertain tokens to improve decoding. Our method measures the uncertainty of each visual token by projecting it onto the text space and decomposing it into aleatoric and epistemic components. Specifically, we focus on epistemic uncertainty, which captures perception-related errors more effectively. Inspired by dropout regularization, we introduce uncertainty-guided token dropout, which applies the dropout principle to input visual tokens instead of model parameters, and during inference rather than training. By aggregating predictions from an ensemble of masked decoding contexts, Dropout Decoding robustly mitigates errors arising from visual token misinterpretations. Evaluations on benchmarks including CHAIR, THRONE, and MMBench demonstrate that Dropout Decoding significantly reduces object hallucinations (OH) and enhances both reliability and quality of LVLM outputs across diverse visual contexts.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Code is released at https://github.com/kigb/DropoutDecoding"
    },
    {
        "paper id": "2412.06575",
        "abstract url": "https://arxiv.org/abs/2412.06575",
        "title": "Data Quality Enhancement on the Basis of Diversity with Large Language Models for Text Classification: Uncovered, Difficult, and Noisy",
        "rating": "2",
        "keywords": [
            [
                "training efficiency"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "In recent years, the use of large language models (LLMs) for text classification has attracted widespread attention. Despite this, the classification accuracy of LLMs has not yet universally surpassed that of smaller models. LLMs can enhance their performance in text classification through fine-tuning. However, existing data quality research based on LLMs is challenging to apply directly to solve text classification problems. To further improve the performance of LLMs in classification tasks, this paper proposes a data quality enhancement (DQE) method for text classification based on LLMs. This method starts by using a greedy algorithm to select data, dividing the dataset into sampled and unsampled subsets, and then performing fine-tuning of the LLMs using the sampled data. Subsequently, this model is used to predict the outcomes for the unsampled data, categorizing incorrectly predicted data into uncovered, difficult, and noisy data. Experimental results demonstrate that our method effectively enhances the performance of LLMs in text classification tasks and significantly improves training efficiency, saving nearly half of the training time. Our method has achieved state-of-the-art performance in several open-source classification tasks.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by COLING 2025(main, long paper)"
    },
    {
        "paper id": "2412.06632",
        "abstract url": "https://arxiv.org/abs/2412.06632",
        "title": "MAVias: Mitigate any Visual Bias",
        "rating": "2",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Mitigating biases in computer vision models is an essential step towards the trustworthiness of artificial intelligence models. Existing bias mitigation methods focus on a small set of predefined biases, limiting their applicability in visual datasets where multiple, possibly unknown biases exist. To address this limitation, we introduce MAVias, an open-set bias mitigation approach leveraging foundation models to discover spurious associations between visual attributes and target classes. MAVias first captures a wide variety of visual features in natural language via a foundation image tagging model, and then leverages a large language model to select those visual features defining the target class, resulting in a set of language-coded potential visual biases. We then translate this set of potential biases into vision-language embeddings and introduce an in-processing bias mitigation approach to prevent the model from encoding information related to them. Our experiments on diverse datasets, including CelebA, Waterbirds, ImageNet, and UrbanCars, show that MAVias effectively detects and mitigates a wide range of biases in visual recognition tasks outperforming current state-of-the-art.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06646",
        "abstract url": "https://arxiv.org/abs/2412.06646",
        "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Recent advances in multimodal training have significantly improved the integration of image understanding and generation within a unified model. This study investigates how vision-language models (VLMs) handle image-understanding tasks, specifically focusing on how visual information is processed and transferred to the textual domain. We compare VLMs that generate both images and text with those that output only text, highlighting key differences in information flow. We find that in models with multimodal outputs, image and text embeddings are more separated within the residual stream. Additionally, models vary in how information is exchanged from visual to textual tokens. VLMs that only output text exhibit a distributed communication pattern, where information is exchanged through multiple image tokens. In contrast, models trained for image and text generation rely on a single token that acts as a narrow gate for the visual information. We demonstrate that ablating this single token significantly deteriorates performance on image understanding tasks. Furthermore, modifying this token enables effective steering of the image semantics, showing that targeted, local interventions can reliably control the model's global behavior.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06745",
        "abstract url": "https://arxiv.org/abs/2412.06745",
        "title": "ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities",
        "rating": "2",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Traditional fixed test sets fall short in evaluating open-ended capabilities of foundation models. To address this, we propose ONEBench(OpeN-Ended Benchmarking), a new testing paradigm that consolidates individual evaluation datasets into a unified, ever-expanding sample pool. ONEBench allows users to generate custom, open-ended evaluation benchmarks from this pool, corresponding to specific capabilities of interest. By aggregating samples across test sets, ONEBench enables the assessment of diverse capabilities beyond those covered by the original test sets, while mitigating overfitting and dataset bias. Most importantly, it frames model evaluation as a collective process of selecting and aggregating sample-level tests. The shift from task-specific benchmarks to ONEBench introduces two challenges: (1)heterogeneity and (2)incompleteness. Heterogeneity refers to the aggregation over diverse metrics, while incompleteness describes comparing models evaluated on different data subsets. To address these challenges, we explore algorithms to aggregate sparse measurements into reliable model scores. Our aggregation algorithm ensures identifiability(asymptotically recovering ground-truth scores) and rapid convergence, enabling accurate model ranking with less data. On homogenous datasets, we show our aggregation algorithm provides rankings that highly correlate with those produced by average scores. We also demonstrate robustness to ~95% of measurements missing, reducing evaluation cost by up to 20x with little-to-no change in model rankings. We introduce ONEBench-LLM for language models and ONEBench-LMM for vision-language models, unifying evaluations across these domains. Overall, we present a technique for open-ended evaluation, which can aggregate over incomplete, heterogeneous sample-level measurements to continually grow a benchmark alongside the rapidly developing foundation models.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06775",
        "abstract url": "https://arxiv.org/abs/2412.06775",
        "title": "Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "cs.AI",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To address this, recent approaches apply contrastive decoding to calibrate the model's response via contrasting output distributions with original and visually distorted samples, demonstrating promising hallucination mitigation in a training-free manner. However, the potential of changing information in visual inputs is not well-explored, so a deeper investigation into the behaviors of visual contrastive decoding is of great interest. In this paper, we first explore various methods for contrastive decoding to change visual contents, including image downsampling and editing. Downsampling images reduces the detailed textual information while editing yields new contents in images, providing new aspects as visual contrastive samples. To further study benefits by using different contrastive samples, we analyze probability-level metrics, including entropy and distribution distance. Interestingly, the effect of these samples in mitigating hallucinations varies a lot across LVLMs and benchmarks. Based on our analysis, we propose a simple yet effective method to combine contrastive samples, offering a practical solution for applying contrastive decoding across various scenarios. Extensive experiments are conducted to validate the proposed fusion method among different benchmarks.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "Under review. Project pages: https://github.com/YiLunLee/VCD_Analysis"
    },
    {
        "paper id": "2412.07112",
        "abstract url": "https://arxiv.org/abs/2412.07112",
        "title": "Maya: An Instruction Finetuned Multilingual Multimodal Model",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "The rapid development of large Vision-Language Models (VLMs) has led to impressive results on academic benchmarks, primarily in widely spoken languages. However, significant gaps remain in the ability of current VLMs to handle low-resource languages and varied cultural contexts, largely due to a lack of high-quality, diverse, and safety-vetted data. Consequently, these models often struggle to understand low-resource languages and cultural nuances in a manner free from toxicity. To address these limitations, we introduce Maya, an open-source Multimodal Multilingual model. Our contributions are threefold: 1) a multilingual image-text pretraining dataset in eight languages, based on the LLaVA pretraining dataset; 2) a thorough analysis of toxicity within the LLaVA dataset, followed by the creation of a novel toxicity-free version across eight languages; and 3) a multilingual image-text model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07148",
        "abstract url": "https://arxiv.org/abs/2412.07148",
        "title": "MM-PoE: Multiple Choice Reasoning via. Process of Elimination using Multi-Modal Models",
        "rating": "2",
        "keywords": [
            [
                "Vision-Language",
                "VLMs"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces Multiple Choice Reasoning via. Process of Elimination using Multi-Modal models, herein referred to as Multi-Modal Process of Elimination (MM-PoE). This novel methodology is engineered to augment the efficacy of Vision-Language Models (VLMs) in multiple-choice visual reasoning tasks. Diverging from conventional approaches that evaluate each option independently, MM-PoE employs a dual-step scoring paradigm that initially identifies and excludes implausible choices, subsequently concentrating on the most probable remaining options. This method emulates human test-taking strategies, where individuals typically eliminate clearly incorrect answers prior to selecting the optimal response. Our empirical evaluations, conducted across three benchmark datasets, reveal that MM-PoE significantly improves both zero-shot and few-shot performance of contemporary state-of-the-art VLMs. Critically, this approach not only broadens the application of the elimination process to multi-modal contexts but also allows few-shot experiments, thereby addressing two principal limitations concerning usage of PoE only in zero-shot settings and only with a language-only framework. As a result, MM-PoE not only refines the reasoning capabilities of VLMs but also broadens their applicability to complex visual question-answering scenarios. All code and documentation supporting our work are available at https://pypi.org/project/mm-poe/, enabling researchers and practitioners to easily integrate and further develop these techniques.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06286",
        "abstract url": "https://arxiv.org/abs/2412.06286",
        "title": "No Annotations for Object Detection in Art through Stable Diffusion",
        "rating": "1.5",
        "keywords": [
            [
                "vision-language"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ],
            [
                "WACV"
            ]
        ],
        "abstract": "Object detection in art is a valuable tool for the digital humanities, as it allows for faster identification of objects in artistic and historical images compared to humans. However, annotating such images poses significant challenges due to the need for specialized domain expertise. We present NADA (no annotations for detection in art), a pipeline that leverages diffusion models' art-related knowledge for object detection in paintings without the need for full bounding box supervision. Our method, which supports both weakly-supervised and zero-shot scenarios and does not require any fine-tuning of its pretrained components, consists of a class proposer based on large vision-language models and a class-conditioned detector based on Stable Diffusion. NADA is evaluated on two artwork datasets, ArtDL 2.0 and IconArt, outperforming prior work in weakly-supervised detection, while being the first work for zero-shot object detection in art. Code is available at https://github.com/patrick-john-ramos/nada",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages, 6 figures, to be published in WACV 2025"
    },
    {
        "paper id": "2412.06289",
        "abstract url": "https://arxiv.org/abs/2412.06289",
        "title": "S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity",
        "rating": "1.5",
        "keywords": [
            [
                "training efficiency",
                "PEFT"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. Utilizing this key insight, we propose a family of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which concurrently achieve state-of-the-art fine-tuning performance, training efficiency, and inference scalability. S$^{2}$FT accomplishes this by \"selecting sparsely and computing densely\". It selects a few heads and channels in the MHA and FFN modules for each Transformer block, respectively. Next, it co-permutes weight matrices on both sides of the coupled structures in LLMs to connect the selected components in each layer into a dense submatrix. Finally, S$^{2}$FT performs in-place gradient updates on all submatrices. Through theoretical analysis and empirical results, our method prevents overfitting and forgetting, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6% and 1.3% average improvements compared to LoRA, and surpasses full FT by 11.5% when generalizing to various domains after instruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT saves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$ compared to full FT, while delivering an average 10% improvement over LoRA on both metrics. We further demonstrate that the weight updates in S$^{2}$FT can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism for serving multiple fine-tuned models.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06470",
        "abstract url": "https://arxiv.org/abs/2412.06470",
        "title": "Active Learning with Context Sampling and One-vs-Rest Entropy for Semantic Segmentation",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ],
            [
                "WACV"
            ]
        ],
        "abstract": "Multi-class semantic segmentation remains a cornerstone challenge in computer vision. Yet, dataset creation remains excessively demanding in time and effort, especially for specialized domains. Active Learning (AL) mitigates this challenge by selecting data points for annotation strategically. However, existing patch-based AL methods often overlook boundary pixels critical information, essential for accurate segmentation. We present OREAL, a novel patch-based AL method designed for multi-class semantic segmentation. OREAL enhances boundary detection by employing maximum aggregation of pixel-wise uncertainty scores. Additionally, we introduce one-vs-rest entropy, a novel uncertainty score function that computes class-wise uncertainties while achieving implicit class balancing during dataset creation. Comprehensive experiments across diverse datasets and model architectures validate our hypothesis.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "WACV 2025, 8 pages"
    },
    {
        "paper id": "2412.06483",
        "abstract url": "https://arxiv.org/abs/2412.06483",
        "title": "SafeWorld: Geo-Diverse Safety Alignment",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In the rapidly evolving field of Large Language Models (LLMs), ensuring safety is a crucial and widely discussed topic. However, existing works often overlook the geo-diversity of cultural and legal standards across the world. To demonstrate the challenges posed by geo-diverse safety standards, we introduce SafeWorld, a novel benchmark specifically designed to evaluate LLMs' ability to generate responses that are not only helpful but also culturally sensitive and legally compliant across diverse global contexts. SafeWorld encompasses 2,342 test user queries, each grounded in high-quality, human-verified cultural norms and legal policies from 50 countries and 493 regions/races. On top of it, we propose a multi-dimensional automatic safety evaluation framework that assesses the contextual appropriateness, accuracy, and comprehensiveness of responses. Our evaluations reveal that current LLMs struggle to meet these criteria. To enhance LLMs' alignment with geo-diverse safety standards, we synthesize helpful preference pairs for Direct Preference Optimization (DPO) alignment training. The preference pair construction aims to encourage LLMs to behave appropriately and provide precise references to relevant cultural norms and policies when necessary. Our trained SafeWorldLM outperforms all competing models, including GPT-4o on all three evaluation dimensions by a large margin. Global human evaluators also note a nearly 20% higher winning rate in helpfulness and harmfulness evaluation. Our code and data can be found here: https://github.com/PlusLabNLP/SafeWorld.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "Accepted by NeurIPS 2024"
    },
    {
        "paper id": "2412.06590",
        "abstract url": "https://arxiv.org/abs/2412.06590",
        "title": "Bridging the Divide: Reconsidering Softmax and Linear Attention",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Widely adopted in modern Vision Transformer designs, Softmax attention can effectively capture long-range visual information; however, it incurs excessive computational cost when dealing with high-resolution inputs. In contrast, linear attention naturally enjoys linear complexity and has great potential to scale up to higher-resolution images. Nonetheless, the unsatisfactory performance of linear attention greatly limits its practical application in various scenarios. In this paper, we take a step forward to close the gap between the linear and Softmax attention with novel theoretical analyses, which demystify the core factors behind the performance deviations. Specifically, we present two key perspectives to understand and alleviate the limitations of linear attention: the injective property and the local modeling ability. Firstly, we prove that linear attention is not injective, which is prone to assign identical attention weights to different query vectors, thus adding to severe semantic confusion since different queries correspond to the same outputs. Secondly, we confirm that effective local modeling is essential for the success of Softmax attention, in which linear attention falls short. The aforementioned two fundamental differences significantly contribute to the disparities between these two attention paradigms, which is demonstrated by our substantial empirical validation in the paper. In addition, more experiment results indicate that linear attention, as long as endowed with these two properties, can outperform Softmax attention across various tasks while maintaining lower computation complexity. Code is available at https://github.com/LeapLabTHU/InLine.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "NeurIPS 2024"
    },
    {
        "paper id": "2412.06676",
        "abstract url": "https://arxiv.org/abs/2412.06676",
        "title": "I Don't Know: Explicit Modeling of Uncertainty with an [IDK] Token",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Large Language Models are known to capture real-world knowledge, allowing them to excel in many downstream tasks. Despite recent advances, these models are still prone to what are commonly known as hallucinations, causing them to emit unwanted and factually incorrect text. In this work, we propose a novel calibration method that can be used to combat hallucinations. We add a special [IDK] (\"I don't know\") token to the model's vocabulary and introduce an objective function that shifts probability mass to the [IDK] token for incorrect predictions. This approach allows the model to express uncertainty in its output explicitly. We evaluate our proposed method across multiple model architectures and factual downstream tasks. We find that models trained with our method are able to express uncertainty in places where they would previously make mistakes while suffering only a small loss of encoded knowledge. We further perform extensive ablation studies of multiple variations of our approach and provide a detailed analysis of the precision-recall tradeoff of our method.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": "Published at NeurIPS 2024"
    },
    {
        "paper id": "2412.07030",
        "abstract url": "https://arxiv.org/abs/2412.07030",
        "title": "FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Multimodal multihop question answering is a complex task that requires reasoning over multiple sources of information, such as images and text, to answer questions. While there has been significant progress in visual question answering, the multihop setting remains unexplored due to the lack of high-quality datasets. Current methods focus on single-hop question answering or a single modality, which makes them unsuitable for real-world scenarios such as analyzing multimodal educational materials, summarizing lengthy academic articles, or interpreting scientific studies that combine charts, images, and text. To address this gap, we propose a novel methodology, introducing the first framework for creating a high-quality dataset that enables training models for multimodal multihop question answering. Our approach consists of a 5-stage pipeline that involves acquiring relevant multimodal documents from Wikipedia, synthetically generating high-level questions and answers, and validating them through rigorous criteria to ensure quality data. We evaluate our methodology by training models on our synthesized dataset and testing on two benchmarks, our results demonstrate that, with an equal sample size, models trained on our synthesized data outperform those trained on human-collected data by 1.9 in exact match (EM) on average. We believe our data synthesis method will serve as a strong foundation for training and evaluating multimodal multihop question answering models.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.IR",
            "cs.LG"
        ],
        "comment": "20 pages, 11 figures, 10 tables, Submitted to CVPR 2025"
    },
    {
        "paper id": "2412.07072",
        "abstract url": "https://arxiv.org/abs/2412.07072",
        "title": "Stable Mean Teacher for Semi-supervised Video Action Detection",
        "rating": "1.5",
        "keywords": [
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "In this work, we focus on semi-supervised learning for video action detection. Video action detection requires spatiotemporal localization in addition to classification, and a limited amount of labels makes the model prone to unreliable predictions. We present Stable Mean Teacher, a simple end-to-end teacher-based framework that benefits from improved and temporally consistent pseudo labels. It relies on a novel Error Recovery (EoR) module, which learns from students' mistakes on labeled samples and transfers this knowledge to the teacher to improve pseudo labels for unlabeled samples. Moreover, existing spatiotemporal losses do not take temporal coherency into account and are prone to temporal inconsistencies. To address this, we present Difference of Pixels (DoP), a simple and novel constraint focused on temporal consistency, leading to coherent temporal detections. We evaluate our approach on four different spatiotemporal detection benchmarks: UCF101-24, JHMDB21, AVA, and YouTube-VOS. Our approach outperforms the supervised baselines for action detection by an average margin of 23.5% on UCF101-24, 16% on JHMDB21, and 3.3% on AVA. Using merely 10% and 20% of data, it provides competitive performance compared to the supervised baseline trained on 100% annotations on UCF101-24 and JHMDB21, respectively. We further evaluate its effectiveness on AVA for scaling to large-scale datasets and YouTube-VOS for video object segmentation, demonstrating its generalization capability to other tasks in the video domain. Code and models are publicly available.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "AAAI Conference on Artificial Intelligence, Main Technical Track (AAAI), 2025, Code: https://github.com/AKASH2907/stable_mean_teacher"
    },
    {
        "paper id": "2412.07121",
        "abstract url": "https://arxiv.org/abs/2412.07121",
        "title": "Bridging the Gap for Test-Time Multimodal Sentiment Analysis",
        "rating": "1.5",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Multimodal sentiment analysis (MSA) is an emerging research topic that aims to understand and recognize human sentiment or emotions through multiple modalities. However, in real-world dynamic scenarios, the distribution of target data is always changing and different from the source data used to train the model, which leads to performance degradation. Common adaptation methods usually need source data, which could pose privacy issues or storage overheads. Therefore, test-time adaptation (TTA) methods are introduced to improve the performance of the model at inference time. Existing TTA methods are always based on probabilistic models and unimodal learning, and thus can not be applied to MSA which is often considered as a multimodal regression task. In this paper, we propose two strategies: Contrastive Adaptation and Stable Pseudo-label generation (CASP) for test-time adaptation for multimodal sentiment analysis. The two strategies deal with the distribution shifts for MSA by enforcing consistency and minimizing empirical risk, respectively. Extensive experiments show that CASP brings significant and consistent improvements to the performance of the model across various distribution shift settings and with different backbones, demonstrating its effectiveness and versatility. Our codes are available at https://github.com/zrguo/CASP.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": "Accepted to AAAI 2025"
    },
    {
        "paper id": "2412.07144",
        "abstract url": "https://arxiv.org/abs/2412.07144",
        "title": "Political Actor Agent: Simulating Legislative System for Roll Call Votes Prediction with Large Language Models",
        "rating": "1.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Predicting roll call votes through modeling political actors has emerged as a focus in quantitative political science and computer science. Widely used embedding-based methods generate vectors for legislators from diverse data sets to predict legislative behaviors. However, these methods often contend with challenges such as the need for manually predefined features, reliance on extensive training data, and a lack of interpretability. Achieving more interpretable predictions under flexible conditions remains an unresolved issue. This paper introduces the Political Actor Agent (PAA), a novel agent-based framework that utilizes Large Language Models to overcome these limitations. By employing role-playing architectures and simulating legislative system, PAA provides a scalable and interpretable paradigm for predicting roll-call votes. Our approach not only enhances the accuracy of predictions but also offers multi-view, human-understandable decision reasoning, providing new insights into political actor behaviors. We conducted comprehensive experiments using voting records from the 117-118th U.S. House of Representatives, validating the superior performance and interpretability of PAA. This study not only demonstrates PAA's effectiveness but also its potential in political science research.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": "Accepted at AAAI 2025"
    },
    {
        "paper id": "2412.06206",
        "abstract url": "https://arxiv.org/abs/2412.06206",
        "title": "SiReRAG: Indexing Similar and Related Information for Multihop Reasoning",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Indexing is an important step towards strong performance in retrieval-augmented generation (RAG) systems. However, existing methods organize data based on either semantic similarity (similarity) or related information (relatedness), but do not cover both perspectives comprehensively. Our analysis reveals that modeling only one perspective results in insufficient knowledge synthesis, leading to suboptimal performance on complex tasks requiring multihop reasoning. In this paper, we propose SiReRAG, a novel RAG indexing approach that explicitly considers both similar and related information. On the similarity side, we follow existing work and explore some variances to construct a similarity tree based on recursive summarization. On the relatedness side, SiReRAG extracts propositions and entities from texts, groups propositions via shared entities, and generates recursive summaries to construct a relatedness tree. We index and flatten both similarity and relatedness trees into a unified retrieval pool. Our experiments demonstrate that SiReRAG consistently outperforms state-of-the-art indexing methods on three multihop datasets (MuSiQue, 2WikiMultiHopQA, and HotpotQA), with an average 1.9% improvement in F1 scores. As a reasonably efficient solution, SiReRAG enhances existing reranking methods significantly, with up to 7.8% improvement in average F1 scores.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06207",
        "abstract url": "https://arxiv.org/abs/2412.06207",
        "title": "Skill-Enhanced Reinforcement Learning Acceleration from Demonstrations",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "Learning from Demonstration (LfD) aims to facilitate rapid Reinforcement Learning (RL) by leveraging expert demonstrations to pre-train the RL agent. However, the limited availability of expert demonstration data often hinders its ability to effectively aid downstream RL learning. To address this problem, we propose a novel two-stage method dubbed as Skill-enhanced Reinforcement Learning Acceleration (SeRLA). SeRLA introduces a skill-level adversarial Positive-Unlabeled (PU) learning model to extract useful skill prior knowledge by enabling learning from both limited expert data and general low-cost demonstration data in the offline prior learning stage. Subsequently, it deploys a skill-based soft actor-critic algorithm to leverage this acquired prior knowledge in the downstream online RL stage for efficient training of a skill policy network. Moreover, we develop a simple skill-level data enhancement technique to further alleviate data sparsity and improve both skill prior learning and downstream skill policy training. Our experimental results on multiple standard RL environments show the proposed SeRLA method achieves state-of-the-art performance on accelerating reinforcement learning on downstream tasks, especially in the early learning phase.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "ICML 2024 AutoRL Workshop; 9 pages"
    },
    {
        "paper id": "2412.06208",
        "abstract url": "https://arxiv.org/abs/2412.06208",
        "title": "Pilot-guided Multimodal Semantic Communication for Audio-Visual Event Localization",
        "rating": "1",
        "keywords": [
            [
                "Audio-Visual"
            ],
            [
                "autonomous driving"
            ],
            [
                "cs.CV",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Multimodal semantic communication, which integrates various data modalities such as text, images, and audio, significantly enhances communication efficiency and reliability. It has broad application prospects in fields such as artificial intelligence, autonomous driving, and smart homes. However, current research primarily relies on analog channels and assumes constant channel states (perfect CSI), which is inadequate for addressing dynamic physical channels and noise in real-world scenarios. Existing methods often focus on single modality tasks and fail to handle multimodal stream data, such as video and audio, and their corresponding tasks. Furthermore, current semantic encoding and decoding modules mainly transmit single modality features, neglecting the need for multimodal semantic enhancement and recognition tasks. To address these challenges, this paper proposes a pilot-guided framework for multimodal semantic communication specifically tailored for audio-visual event localization tasks. This framework utilizes digital pilot codes and channel modules to guide the state of analog channels in real-wold scenarios and designs Euler-based multimodal semantic encoding and decoding that consider time-frequency characteristics based on dynamic channel state. This approach effectively handles multimodal stream source data, especially for audio-visual event localization tasks. Extensive numerical experiments demonstrate the robustness of the proposed framework in channel changes and its support for various communication scenarios. The experimental results show that the framework outperforms existing benchmark methods in terms of Signal-to-Noise Ratio (SNR), highlighting its advantage in semantic communication quality.",
        "subjects": [
            "cs.SD",
            "cs.CV",
            "cs.MM",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06224",
        "abstract url": "https://arxiv.org/abs/2412.06224",
        "title": "Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks",
        "rating": "1",
        "keywords": [
            [
                "Vision-Language"
            ],
            [
                "Navigation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "A practical navigation agent must be capable of handling a wide range of interaction demands, such as following instructions, searching objects, answering questions, tracking people, and more. Existing models for embodied navigation fall short of serving as practical generalists in the real world, as they are often constrained by specific task configurations or pre-defined maps with discretized waypoints. In this work, we present Uni-NaVid, the first video-based vision-language-action (VLA) model designed to unify diverse embodied navigation tasks and enable seamless navigation for mixed long-horizon tasks in unseen real-world environments. Uni-NaVid achieves this by harmonizing the input and output data configurations for all commonly used embodied navigation tasks and thereby integrating all tasks in one model. For training Uni-NaVid, we collect 3.6 million navigation data samples in total from four essential navigation sub-tasks and foster synergy in learning across them. Extensive experiments on comprehensive navigation benchmarks clearly demonstrate the advantages of unification modeling in Uni-NaVid and show it achieves state-of-the-art performance. Additionally, real-world experiments confirm the model's effectiveness and efficiency, shedding light on its strong generalizability.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06229",
        "abstract url": "https://arxiv.org/abs/2412.06229",
        "title": "LLMs as Debate Partners: Utilizing Genetic Algorithms and Adversarial Search for Adaptive Arguments",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "This paper introduces DebateBrawl, an innovative AI-powered debate platform that integrates Large Language Models (LLMs), Genetic Algorithms (GA), and Adversarial Search (AS) to create an adaptive and engaging debating experience. DebateBrawl addresses the limitations of traditional LLMs in strategic planning by incorporating evolutionary optimization and game-theoretic techniques. The system demonstrates remarkable performance in generating coherent, contextually relevant arguments while adapting its strategy in real-time. Experimental results involving 23 debates show balanced outcomes between AI and human participants, with the AI system achieving an average score of 2.72 compared to the human average of 2.67 out of 10. User feedback indicates significant improvements in debating skills and a highly satisfactory learning experience, with 85% of users reporting improved debating abilities and 78% finding the AI opponent appropriately challenging. The system's ability to maintain high factual accuracy (92% compared to 78% in human-only debates) while generating diverse arguments addresses critical concerns in AI-assisted discourse. DebateBrawl not only serves as an effective educational tool but also contributes to the broader goal of improving public discourse through AI-assisted argumentation. The paper discusses the ethical implications of AI in persuasive contexts and outlines the measures implemented to ensure responsible development and deployment of the system, including robust fact-checking mechanisms and transparency in decision-making processes.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.CY",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06245",
        "abstract url": "https://arxiv.org/abs/2412.06245",
        "title": "A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic Dimension",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "The performance of Large Language Models (LLMs) on natural language tasks can be improved through both supervised fine-tuning (SFT) and in-context learning (ICL), which operate via distinct mechanisms. Supervised fine-tuning updates the model's weights by minimizing loss on training data, whereas in-context learning leverages task demonstrations embedded in the prompt, without changing the model's parameters. This study investigates the effects of these learning paradigms on the hidden representations of LLMs using Intrinsic Dimension (ID). We use ID to estimate the number of degrees of freedom between representations extracted from LLMs as they perform specific natural language tasks. We first explore how the ID of LLM representations evolves during SFT and how it varies due to the number of demonstrations in ICL. We then compare the IDs induced by SFT and ICL and find that ICL consistently induces a higher ID compared to SFT, suggesting that representations generated during ICL reside in higher dimensional manifolds in the embedding space.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06249",
        "abstract url": "https://arxiv.org/abs/2412.06249",
        "title": "Optimizing Multi-Task Learning for Enhanced Performance in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "This study aims to explore the performance improvement method of large language models based on GPT-4 under the multi-task learning framework and conducts experiments on two tasks: text classification and automatic summary generation. Through the combined design of shared feature extractors and task-specific modules, we achieve knowledge-sharing and optimization of multiple tasks in the same model. The experiment uses multiple subtasks of the GLUE dataset to compare the performance of the multi-task model with the single-task GPT-4, the multi-task version of GPT-3, the BERT basic model, and the classic Bi-LSTM with Attention model. The results show that the proposed multi-task learning model outperforms other comparison models in terms of text classification accuracy and ROUGE value of summary generation, demonstrating the advantages of multi-task learning in improving model generalization ability and collaborative learning between tasks. The model maintains a stable loss convergence rate during training, showing good learning efficiency and adaptability to the test set. This study verifies the applicability of the multi-task learning framework in large language models, especially in improving the model's ability to balance different tasks. In the future, with the combination of large language models and multimodal data and the application of dynamic task adjustment technology, the framework based on multi-task learning is expected to play a greater role in practical applications across fields and provide new ideas for the development of general artificial intelligence.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06251",
        "abstract url": "https://arxiv.org/abs/2412.06251",
        "title": "Fearless unsafe. Safety Property is all you need",
        "rating": "1",
        "keywords": [
            [
                "time efficiency"
            ]
        ],
        "abstract": "Rust, a popular systems-level programming language, has garnered widespread attention due to its features of achieving run-time efficiency and memory safety. With an increasing number of real-world projects adopting Rust, understanding how to assist programmers in correctly writing unsafe code poses a significant challenge. Based on our observations, the current standard library has many unsafe APIs, but their descriptions are not uniform, complete, and intuitive, especially in describing safety requirements. Therefore, we advocate establishing a systematic category of safety requirements for revising those documents. In this paper, we empirically studied the safety requirements across Unsafe Rust. We defined a category of Safety Properties (22 items in total) that learned from the documents of unsafe APIs in the standard library. Then, we labeled all public unsafe APIs (438 in total) and analyzed their correlations. Based on the safety properties, we reorganized all the unsafe documents in the standard library and designed a consultation plugin into rust-analyzer as a complementary tool to assist Rust developers in writing unsafe code. To validate the practical significance, we categorized the root causes of all Rust CVEs up to 2024-01-31 (419 in total) into safety properties and further counted the real-world usage of unsafe APIs in the crates.io ecosystem.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2308.04785"
    },
    {
        "paper id": "2412.06258",
        "abstract url": "https://arxiv.org/abs/2412.06258",
        "title": "Enhanced Multi-Object Tracking Using Pose-based Virtual Markers in 3x3 Basketball",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Multi-object tracking (MOT) is crucial for various multi-agent analyses such as evaluating team sports tactics and player movements and performance. While pedestrian tracking has advanced with Tracking-by-Detection MOT, team sports like basketball pose unique challenges. These challenges include players' unpredictable movements, frequent close interactions, and visual similarities that complicate pose labeling and lead to significant occlusions, frequent ID switches, and high manual annotation costs. To address these challenges, we propose a novel pose-based virtual marker (VM) MOT method for team sports, named Sports-vmTracking. This method builds on the vmTracking approach developed for multi-animal tracking with active learning. First, we constructed a 3x3 basketball pose dataset for VMs and applied active learning to enhance model performance in generating VMs. Then, we overlaid the VMs on video to identify players, extract their poses with unique IDs, and convert these into bounding boxes for comparison with automated MOT methods. Using our 3x3 basketball dataset, we demonstrated that our VM configuration has been highly effective, and reduced the need for manual corrections and labeling during pose model training while maintaining high accuracy. Our approach achieved an average HOTA score of 72.3%, over 10 points higher than other state-of-the-art methods without VM, and resulted in 0 ID switches. Beyond improving performance in handling occlusions and minimizing ID switches, our framework could substantially increase the time and cost efficiency compared to traditional manual annotation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06272",
        "abstract url": "https://arxiv.org/abs/2412.06272",
        "title": "Methods for Legal Citation Prediction in the Age of LLMs: An Australian Law Case Study",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In recent years, Large Language Models (LLMs) have shown great potential across a wide range of legal tasks. Despite these advances, mitigating hallucination remains a significant challenge, with state-of-the-art LLMs still frequently generating incorrect legal references. In this paper, we focus on the problem of legal citation prediction within the Australian law context, where correctly identifying and citing relevant legislations or precedents is critical. We compare several approaches: prompting general purpose and law-specialised LLMs, retrieval-only pipelines with both generic and domain-specific embeddings, task-specific instruction-tuning of LLMs, and hybrid strategies that combine LLMs with retrieval augmentation, query expansion, or voting ensembles. Our findings indicate that domain-specific pre-training alone is insufficient for achieving satisfactory citation accuracy even after law-specialised pre-training. In contrast, instruction tuning on our task-specific dataset dramatically boosts performance reaching the best results across all settings. We also highlight that database granularity along with the type of embeddings play a critical role in the performance of retrieval systems. Among retrieval-based approaches, hybrid methods consistently outperform retrieval-only setups, and among these, ensemble voting delivers the best result by combining the predictive quality of instruction-tuned LLMs with the retrieval system.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.IR"
        ],
        "comment": "For code, data, and models see https://auslawbench.github.io"
    },
    {
        "paper id": "2412.06284",
        "abstract url": "https://arxiv.org/abs/2412.06284",
        "title": "Your Data Is Not Perfect: Towards Cross-Domain Out-of-Distribution Detection in Class-Imbalanced Data",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Previous OOD detection systems only focus on the semantic gap between ID and OOD samples. Besides the semantic gap, we are faced with two additional gaps: the domain gap between source and target domains, and the class-imbalance gap between different classes. In fact, similar objects from different domains should belong to the same class. In this paper, we introduce a realistic yet challenging setting: class-imbalanced cross-domain OOD detection (CCOD), which contains a well-labeled (but usually small) source set for training and conducts OOD detection on an unlabeled (but usually larger) target set for testing. We do not assume that the target domain contains only OOD classes or that it is class-balanced: the distribution among classes of the target dataset need not be the same as the source dataset. To tackle this challenging setting with an OOD detection system, we propose a novel uncertainty-aware adaptive semantic alignment (UASA) network based on a prototype-based alignment strategy. Specifically, we first build label-driven prototypes in the source domain and utilize these prototypes for target classification to close the domain gap. Rather than utilizing fixed thresholds for OOD detection, we generate adaptive sample-wise thresholds to handle the semantic gap. Finally, we conduct uncertainty-aware clustering to group semantically similar target samples to relieve the class-imbalance gap. Extensive experiments on three challenging benchmarks demonstrate that our proposed UASA outperforms state-of-the-art methods by a large margin.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by Expert Systems with Applications"
    },
    {
        "paper id": "2412.06293",
        "abstract url": "https://arxiv.org/abs/2412.06293",
        "title": "Mastering Collaborative Multi-modal Data Selection: A Focus on Informativeness, Uniqueness, and Representativeness",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Instruction tuning fine-tunes pre-trained Multi-modal Large Language Models (MLLMs) to handle real-world tasks. However, the rapid expansion of visual instruction datasets introduces data redundancy, leading to excessive computational costs. We propose a collaborative framework, DataTailor, which leverages three key principles--informativeness, uniqueness, and representativeness--for effective data selection. We argue that a valuable sample should be informative of the task, non-redundant, and represent the sample distribution (i.e., not an outlier). We further propose practical ways to score against each principle, which automatically adapts to a given dataset without tedious hyperparameter tuning. Comprehensive experiments on various benchmarks demonstrate that DataTailor achieves 100.8% of the performance of full-data fine-tuning with only 15% of the data, significantly reducing computational costs while maintaining superior results. This exemplifies the \"Less is More\" philosophy in MLLM development.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "14 pages, 7 figures"
    },
    {
        "paper id": "2412.06306",
        "abstract url": "https://arxiv.org/abs/2412.06306",
        "title": "Self-Paced Learning Strategy with Easy Sample Prior Based on Confidence for the Flying Bird Object Detection Model Training",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In order to avoid the impact of hard samples on the training process of the Flying Bird Object Detection model (FBOD model, in our previous work, we designed the FBOD model according to the characteristics of flying bird objects in surveillance video), the Self-Paced Learning strategy with Easy Sample Prior Based on Confidence (SPL-ESP-BC), a new model training strategy, is proposed. Firstly, the loss-based Minimizer Function in Self-Paced Learning (SPL) is improved, and the confidence-based Minimizer Function is proposed, which makes it more suitable for one-class object detection tasks. Secondly, to give the model the ability to judge easy and hard samples at the early stage of training by using the SPL strategy, an SPL strategy with Easy Sample Prior (ESP) is proposed. The FBOD model is trained using the standard training strategy with easy samples first, then the SPL strategy with all samples is used to train it. Combining the strategy of the ESP and the Minimizer Function based on confidence, the SPL-ESP-BC model training strategy is proposed. Using this strategy to train the FBOD model can make it to learn the characteristics of the flying bird object in the surveillance video better, from easy to hard. The experimental results show that compared with the standard training strategy that does not distinguish between easy and hard samples, the AP50 of the FBOD model trained by the SPL-ESP-BC is increased by 2.1%, and compared with other loss-based SPL strategies, the FBOD model trained with SPL-ESP-BC strategy has the best comprehensive detection performance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06323",
        "abstract url": "https://arxiv.org/abs/2412.06323",
        "title": "HAIFAI: Human-AI Collaboration for Mental Face Reconstruction",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present HAIFAI - a novel collaborative human-AI system to tackle the challenging task of reconstructing a visual representation of a face that exists only in a person's mind. Users iteratively rank images presented by the AI system based on their resemblance to a mental image. These rankings, in turn, allow the system to extract relevant image features, fuse them into a unified feature vector, and use a generative model to reconstruct the mental image. We also propose an extension called HAIFAI-X that allows users to manually refine and further improve the reconstruction using an easy-to-use slider interface. To avoid the need for tedious human data collection for model training, we introduce a computational user model of human ranking behaviour. For this, we collected a small face ranking dataset through an online crowd-sourcing study containing data from 275 participants. We evaluate HAIFAI and HAIFAI-X in a 12-participant user study and show that HAIFAI outperforms the previous state of the art regarding reconstruction quality, usability, perceived workload, and reconstruction speed. HAIFAI-X achieves even better reconstruction quality at the cost of reduced usability, perceived workload, and increased reconstruction time. We further validate the reconstructions in a subsequent face ranking study with 18 participants and show that HAIFAI-X achieves a new state-of-the-art identification rate of 60.6%. These findings represent a significant advancement towards developing new collaborative intelligent systems capable of reliably and effortlessly reconstructing a user's mental image.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06341",
        "abstract url": "https://arxiv.org/abs/2412.06341",
        "title": "Elastic-DETR: Making Image Resolution Learnable with Content-Specific Network Prediction",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Multi-scale image resolution is a de facto standard approach in modern object detectors, such as DETR. This technique allows for the acquisition of various scale information from multiple image resolutions. However, manual hyperparameter selection of the resolution can restrict its flexibility, which is informed by prior knowledge, necessitating human intervention. This work introduces a novel strategy for learnable resolution, called Elastic-DETR, enabling elastic utilization of multiple image resolutions. Our network provides an adaptive scale factor based on the content of the image with a compact scale prediction module (< 2 GFLOPs). The key aspect of our method lies in how to determine the resolution without prior knowledge. We present two loss functions derived from identified key components for resolution optimization: scale loss, which increases adaptiveness according to the image, and distribution loss, which determines the overall degree of scaling based on network performance. By leveraging the resolution's flexibility, we can demonstrate various models that exhibit varying trade-offs between accuracy and computational complexity. We empirically show that our scheme can unleash the potential of a wide spectrum of image resolutions without constraining flexibility. Our models on MS COCO establish a maximum accuracy gain of 3.5%p or 26% decrease in computation than MS-trained DN-DETR.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06352",
        "abstract url": "https://arxiv.org/abs/2412.06352",
        "title": "SeFENet: Robust Deep Homography Estimation via Semantic-Driven Feature Enhancement",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Images captured in harsh environments often exhibit blurred details, reduced contrast, and color distortion, which hinder feature detection and matching, thereby affecting the accuracy and robustness of homography estimation. While visual enhancement can improve contrast and clarity, it may introduce visual-tolerant artifacts that obscure the structural integrity of images. Considering the resilience of semantic information against environmental interference, we propose a semantic-driven feature enhancement network for robust homography estimation, dubbed SeFENet. Concretely, we first introduce an innovative hierarchical scale-aware module to expand the receptive field by aggregating multi-scale information, thereby effectively extracting image features under diverse harsh conditions. Subsequently, we propose a semantic-guided constraint module combined with a high-level perceptual framework to achieve degradation-tolerant with semantic feature. A meta-learning-based training strategy is introduced to mitigate the disparity between semantic and structural features. By internal-external alternating optimization, the proposed network achieves implicit semantic-wise feature enhancement, thereby improving the robustness of homography estimation in adverse environments by strengthening the local feature comprehension and context information extraction. Experimental results under both normal and harsh conditions demonstrate that SeFENet significantly outperforms SOTA methods, reducing point match error by at least 41\\% on the large-scale datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06365",
        "abstract url": "https://arxiv.org/abs/2412.06365",
        "title": "Is Self-Supervision Enough? Benchmarking Foundation Models Against End-to-End Training for Mitotic Figure Classification",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Foundation models (FMs), i.e., models trained on a vast amount of typically unlabeled data, have become popular and available recently for the domain of histopathology. The key idea is to extract semantically rich vectors from any input patch, allowing for the use of simple subsequent classification networks potentially reducing the required amounts of labeled data, and increasing domain robustness. In this work, we investigate to which degree this also holds for mitotic figure classification. Utilizing two popular public mitotic figure datasets, we compared linear probing of five publicly available FMs against models trained on ImageNet and a simple ResNet50 end-to-end-trained baseline. We found that the end-to-end-trained baseline outperformed all FM-based classifiers, regardless of the amount of data provided. Additionally, we did not observe the FM-based classifiers to be more robust against domain shifts, rendering both of the above assumptions incorrect.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "6 pages, 2 figures"
    },
    {
        "paper id": "2412.06394",
        "abstract url": "https://arxiv.org/abs/2412.06394",
        "title": "GameArena: Evaluating LLM Reasoning through Live Computer Games",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Evaluating the reasoning abilities of large language models (LLMs) is challenging. Existing benchmarks often depend on static datasets, which are vulnerable to data contamination and may get saturated over time, or on binary live human feedback that conflates reasoning with other abilities. As the most prominent dynamic benchmark, Chatbot Arena evaluates open-ended questions in real-world settings, but lacks the granularity in assessing specific reasoning capabilities. We introduce GameArena, a dynamic benchmark designed to evaluate LLM reasoning capabilities through interactive gameplay with humans. GameArena consists of three games designed to test specific reasoning capabilities (e.g., deductive and inductive reasoning), while keeping participants entertained and engaged. We analyze the gaming data retrospectively to uncover the underlying reasoning processes of LLMs and measure their fine-grained reasoning capabilities. We collect over 2000 game sessions and provide detailed assessments of various reasoning capabilities for five state-of-the-art LLMs. Our user study with 100 participants suggests that GameArena improves user engagement compared to Chatbot Arena. For the first time, GameArena enables the collection of step-by-step LLM reasoning data in the wild.",
        "subjects": [
            "cs.AI",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06412",
        "abstract url": "https://arxiv.org/abs/2412.06412",
        "title": "StarWhisper Telescope: Agent-Based Observation Assistant System to Approach AI Astrophysicist",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "With the rapid advancements in Large Language Models (LLMs), LLM-based agents have introduced convenient and user-friendly methods for leveraging tools across various domains. In the field of astronomical observation, the construction of new telescopes has significantly increased astronomers' workload. Deploying LLM-powered agents can effectively alleviate this burden and reduce the costs associated with training personnel. Within the Nearby Galaxy Supernovae Survey (NGSS) project, which encompasses eight telescopes across three observation sites, aiming to find the transients from the galaxies in 50 mpc, we have developed the \\textbf{StarWhisper Telescope System} to manage the entire observation process. This system automates tasks such as generating observation lists, conducting observations, analyzing data, and providing feedback to the observer. Observation lists are customized for different sites and strategies to ensure comprehensive coverage of celestial objects. After manual verification, these lists are uploaded to the telescopes via the agents in the system, which initiates observations upon neutral language. The observed images are analyzed in real-time, and the transients are promptly communicated to the observer. The agent modifies them into a real-time follow-up observation proposal and send to the Xinglong observatory group chat, then add them to the next-day observation lists. Additionally, the integration of AI agents within the system provides online accessibility, saving astronomers' time and encouraging greater participation from amateur astronomers in the NGSS project.",
        "subjects": [
            "astro-ph.IM",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "21 pages, 18 figures"
    },
    {
        "paper id": "2412.06418",
        "abstract url": "https://arxiv.org/abs/2412.06418",
        "title": "Continual Learning for Segment Anything Model Adaptation",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Although the current different types of SAM adaptation methods have achieved promising performance for various downstream tasks, such as prompt-based ones and adapter-based ones, most of them belong to the one-step adaptation paradigm. In real-world scenarios, we are generally confronted with the dynamic scenario where the data comes in a streaming manner. Driven by the practical need, in this paper, we first propose a novel Continual SAM adaptation (CoSAM) benchmark with 8 different task domains and carefully analyze the limitations of the existing SAM one-step adaptation methods in the continual segmentation scenario. Then we propose a novel simple-yet-effective Mixture of Domain Adapters (MoDA) algorithm which utilizes the Global Feature Tokens (GFT) and Global Assistant Tokens (GAT) modules to help the SAM encoder extract well-separated features for different task domains, and then provide the accurate task-specific information for continual learning. Extensive experiments demonstrate that our proposed MoDA obviously surpasses the existing classic continual learning methods, as well as prompt-based and adapter-based approaches for continual segmentation. Moreover, after sequential learning on the CoSAM benchmark with diverse data distributions, our MoDA maintains highly competitive results in the natural image domain, approaching the zero-shot performance of the original SAM, demonstrating its superior capability in knowledge preservation. Notably, the proposed MoDA can be seamlessly integrated into various one-step adaptation methods of SAM, which can consistently bring obvious performance gains. Code is available at \\url{https://github.com/yangjl1215/CoSAM}",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Code is available at \\url{https://github.com/yangjl1215/CoSAM}"
    },
    {
        "paper id": "2412.06419",
        "abstract url": "https://arxiv.org/abs/2412.06419",
        "title": "LLM-BIP: Structured Pruning for Large Language Models with Block-Wise Forward Importance Propagation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance across various language tasks, but their widespread deployment is impeded by their large size and high computational costs. Structural pruning is a prevailing technique used to introduce sparsity into pre-trained models and facilitate direct hardware acceleration during inference by removing redundant connections (structurally-grouped parameters), such as channels and attention heads. Existing structural pruning approaches often employ either global or layer-wise pruning criteria; however, they are hindered by ineffectiveness stemming from inaccurate evaluation of connection importance. Global pruning methods typically assess component importance using near-zero and unreliable gradients, while layer-wise pruning approaches encounter significant pruning error accumulation issues. To this end, we propose a more accurate pruning metric based on the block-wise importance score propagation, termed LLM-BIP. Specifically, LLM-BIP precisely evaluates connection importance by gauging its influence on the respective transformer block output, which can be efficiently approximated in a single forward pass through an upper bound derived from the assumption of Lipschitz continuity. We evaluate the proposed method using LLaMA-7B, Vicuna-7B, and LLaMA-13B across common zero-shot tasks. The results demonstrate that our approach achieves an average of 3.26% increase in accuracy for common reasoning tasks compared to previous best baselines. It also reduces perplexity by 14.09 and 68.76 on average for the WikiText2 dataset and PTB dataset, respectively.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06432",
        "abstract url": "https://arxiv.org/abs/2412.06432",
        "title": "Integrating Expert Labels into LLM-based Emission Goal Detection: Example Selection vs Automatic Prompt Design",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "We address the detection of emission reduction goals in corporate reports, an important task for monitoring companies' progress in addressing climate change. Specifically, we focus on the issue of integrating expert feedback in the form of labeled example passages into LLM-based pipelines, and compare the two strategies of (1) a dynamic selection of few-shot examples and (2) the automatic optimization of the prompt by the LLM itself. Our findings on a public dataset of 769 climate-related passages from real-world business reports indicate that automatic prompt optimization is the superior approach, while combining both methods provides only limited benefit. Qualitative results indicate that optimized prompts do indeed capture many intricacies of the targeted emission goal extraction task.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06439",
        "abstract url": "https://arxiv.org/abs/2412.06439",
        "title": "Local Attention Transformers for High-Detail Optical Flow Upsampling",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Most recent works on optical flow use convex upsampling as the last step to obtain high-resolution flow. In this work, we show and discuss several issues and limitations of this currently widely adopted convex upsampling approach. We propose a series of changes, in an attempt to resolve current issues. First, we propose to decouple the weights for the final convex upsampler, making it easier to find the correct convex combination. For the same reason, we also provide extra contextual features to the convex upsampler. Then, we increase the convex mask size by using an attention-based alternative convex upsampler; Transformers for Convex Upsampling. This upsampler is based on the observation that convex upsampling can be reformulated as attention, and we propose to use local attention masks as a drop-in replacement for convex masks to increase the mask size. We provide empirical evidence that a larger mask size increases the likelihood of the existence of the convex combination. Lastly, we propose an alternative training scheme to remove bilinear interpolation artifacts from the model output. Our proposed ideas could theoretically be applied to almost every current state-of-the-art optical flow architecture. On the FlyingChairs + FlyingThings3D training setting we reduce the Sintel Clean training end-point-error of RAFT from 1.42 to 1.26, GMA from 1.31 to 1.18, and that of FlowFormer from 0.94 to 0.90, by solely adapting the convex upsampler.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Note; this work is an extension of my Master's thesis, available as \"Optical Flow Upsamplers Ignore Details: Neighborhood Attention Transformers for Convex Upsampling\""
    },
    {
        "paper id": "2412.06451",
        "abstract url": "https://arxiv.org/abs/2412.06451",
        "title": "How Certain are Uncertainty Estimates? Three Novel Earth Observation Datasets for Benchmarking Uncertainty Quantification in Machine Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "eess.IV"
            ]
        ],
        "abstract": "Uncertainty quantification (UQ) is essential for assessing the reliability of Earth observation (EO) products. However, the extensive use of machine learning models in EO introduces an additional layer of complexity, as those models themselves are inherently uncertain. While various UQ methods do exist for machine learning models, their performance on EO datasets remains largely unevaluated. A key challenge in the community is the absence of the ground truth for uncertainty, i.e. how certain the uncertainty estimates are, apart from the labels for the image/signal. This article fills this gap by introducing three benchmark datasets specifically designed for UQ in EO machine learning models. These datasets address three common problem types in EO: regression, image segmentation, and scene classification. They enable a transparent comparison of different UQ methods for EO machine learning models. We describe the creation and characteristics of each dataset, including data sources, preprocessing steps, and label generation, with a particular focus on calculating the reference uncertainty. We also showcase baseline performance of several machine learning models on each dataset, highlighting the utility of these benchmarks for model development and comparison. Overall, this article offers a valuable resource for researchers and practitioners working in artificial intelligence for EO, promoting a more accurate and reliable quality measure of the outputs of machine learning models. The dataset and code are accessible via https://gitlab.lrz.de/ai4eo/WG_Uncertainty.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "eess.IV"
        ],
        "comment": "Submitted to IEEE Geoscience and Remote Sensing Magazine"
    },
    {
        "paper id": "2412.06461",
        "abstract url": "https://arxiv.org/abs/2412.06461",
        "title": "Ranked from Within: Ranking Large Multimodal Models for Visual Question Answering Without Labels",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "As large multimodal models (LMMs) are increasingly deployed across diverse applications, the need for adaptable, real-world model ranking has become paramount. Traditional evaluation methods are largely dataset-centric, relying on fixed, labeled datasets and supervised metrics, which are resource-intensive and may lack generalizability to novel scenarios, highlighting the importance of unsupervised ranking. In this work, we explore unsupervised model ranking for LMMs by leveraging their uncertainty signals, such as softmax probabilities. We evaluate state-of-the-art LMMs (e.g., LLaVA) across visual question answering benchmarks, analyzing how uncertainty-based metrics can reflect model performance. Our findings show that uncertainty scores derived from softmax distributions provide a robust, consistent basis for ranking models across varied tasks. This finding enables the ranking of LMMs on real-world, unlabeled data for visual question answering, providing a practical approach for selecting models across diverse domains without requiring manual annotation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06484",
        "abstract url": "https://arxiv.org/abs/2412.06484",
        "title": "Small Languages, Big Models: A Study of Continual Training on Languages of Norway",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Training large language models requires vast amounts of data, posing a challenge for less widely spoken languages like Norwegian and even more so for truly low-resource languages like S\u00e1mi. To address this issue, we present a novel three-stage continual training approach. We also experiment with combining causal and masked language modeling to get more flexible models. Based on our findings, we train, evaluate, and openly release a new large generative language model for Norwegian Bokm\u00e5l, Nynorsk, and Northern S\u00e1mi with 11.4 billion parameters: NorMistral-11B.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "pre-print, under review"
    },
    {
        "paper id": "2412.06511",
        "abstract url": "https://arxiv.org/abs/2412.06511",
        "title": "Fitting Spherical Gaussians to Dynamic HDRI Sequences",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present a technique for fitting high dynamic range illumination (HDRI) sequences using anisotropic spherical Gaussians (ASGs) while preserving temporal consistency in the compressed HDRI maps. Our approach begins with an optimization network that iteratively minimizes a composite loss function, which includes both reconstruction and diffuse losses. This allows us to represent all-frequency signals with a small number of ASGs, optimizing their directions, sharpness, and intensity simultaneously for an individual HDRI. To extend this optimization into the temporal domain, we introduce a temporal consistency loss, ensuring a consistent approximation across the entire HDRI sequence.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "3 pages, 4 figures, SIGGRAPH Asia 2024 poster, https://www.eyelinestudios.com/research/hdri_sg_fit.html"
    },
    {
        "paper id": "2412.06512",
        "abstract url": "https://arxiv.org/abs/2412.06512",
        "title": "The Fusion of Large Language Models and Formal Methods for Trustworthy AI Agents: A Roadmap",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have emerged as a transformative AI paradigm, profoundly influencing daily life through their exceptional language understanding and contextual generation capabilities. Despite their remarkable performance, LLMs face a critical challenge: the propensity to produce unreliable outputs due to the inherent limitations of their learning-based nature. Formal methods (FMs), on the other hand, are a well-established computation paradigm that provides mathematically rigorous techniques for modeling, specifying, and verifying the correctness of systems. FMs have been extensively applied in mission-critical software engineering, embedded systems, and cybersecurity. However, the primary challenge impeding the deployment of FMs in real-world settings lies in their steep learning curves, the absence of user-friendly interfaces, and issues with efficiency and adaptability. This position paper outlines a roadmap for advancing the next generation of trustworthy AI systems by leveraging the mutual enhancement of LLMs and FMs. First, we illustrate how FMs, including reasoning and certification techniques, can help LLMs generate more reliable and formally certified outputs. Subsequently, we highlight how the advanced learning capabilities and adaptability of LLMs can significantly enhance the usability, efficiency, and scalability of existing FM tools. Finally, we show that unifying these two computation paradigms -- integrating the flexibility and intelligence of LLMs with the rigorous reasoning abilities of FMs -- has transformative potential for the development of trustworthy AI software systems. We acknowledge that this integration has the potential to enhance both the trustworthiness and efficiency of software engineering practices while fostering the development of intelligent FM tools capable of addressing complex yet real-world challenges.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.SE"
        ],
        "comment": "24 pages, 4 figures"
    },
    {
        "paper id": "2412.06534",
        "abstract url": "https://arxiv.org/abs/2412.06534",
        "title": "Inverting Visual Representations with Detection Transformers",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Understanding the mechanisms underlying deep neural networks in computer vision remains a fundamental challenge. While many prior approaches have focused on visualizing intermediate representations within deep neural networks, particularly convolutional neural networks, these techniques have yet to be thoroughly explored in transformer-based vision models. In this study, we apply the approach of training inverse models to reconstruct input images from intermediate layers within a Detection Transformer, showing that this approach is efficient and feasible for transformer-based vision models. Through qualitative and quantitative evaluations of reconstructed images across model stages, we demonstrate critical properties of Detection Transformers, including contextual shape preservation, inter-layer correlation, and robustness to color perturbations, illustrating how these characteristics emerge within the model's architecture. Our findings contribute to a deeper understanding of transformer-based vision models. The code for reproducing our experiments will be made available at github.com/wiskott-lab/inverse-detection-transformer.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06559",
        "abstract url": "https://arxiv.org/abs/2412.06559",
        "title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "As language models regularly make mistakes when solving math problems, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight. In this paper, we introduce ProcessBench for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. We conduct extensive evaluation on ProcessBench, involving two types of models: process reward models (PRMs) and critic models, where for the latter we prompt general language models to critique each solution step by step. We draw two main observations: (1) Existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models (i.e., prompted general language models) and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We hope ProcessBench can foster future research in reasoning process assessment, paving the way toward scalable oversight of language models.",
        "subjects": [
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06566",
        "abstract url": "https://arxiv.org/abs/2412.06566",
        "title": "DEX: Data Channel Extension for Efficient CNN Inference on Tiny AI Accelerators",
        "rating": "1",
        "keywords": [
            [
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Tiny machine learning (TinyML) aims to run ML models on small devices and is increasingly favored for its enhanced privacy, reduced latency, and low cost. Recently, the advent of tiny AI accelerators has revolutionized the TinyML field by significantly enhancing hardware processing power. These accelerators, equipped with multiple parallel processors and dedicated per-processor memory instances, offer substantial performance improvements over traditional microcontroller units (MCUs). However, their limited data memory often necessitates downsampling input images, resulting in accuracy degradation. To address this challenge, we propose Data channel EXtension (DEX), a novel approach for efficient CNN execution on tiny AI accelerators. DEX incorporates additional spatial information from original images into input images through patch-wise even sampling and channel-wise stacking, effectively extending data across input channels. By leveraging underutilized processors and data memory for channel extension, DEX facilitates parallel execution without increasing inference latency. Our evaluation with four models and four datasets on tiny AI accelerators demonstrates that this simple idea improves accuracy on average by 3.5%p while keeping the inference latency the same on the AI accelerator. The source code is available at https://github.com/Nokia-Bell-Labs/data-channel-extension.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Accepted to NeurIPS 2024"
    },
    {
        "paper id": "2412.06593",
        "abstract url": "https://arxiv.org/abs/2412.06593",
        "title": "Anchoring Bias in Large Language Models: An Experimental Study",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) like GPT-4 and Gemini have significantly advanced artificial intelligence by enabling machines to generate and comprehend human-like text. Despite their impressive capabilities, LLMs are not immune to limitations, including various biases. While much research has explored demographic biases, the cognitive biases in LLMs have not been equally scrutinized. This study delves into anchoring bias, a cognitive bias where initial information disproportionately influences judgment. Utilizing an experimental dataset, we examine how anchoring bias manifests in LLMs and verify the effectiveness of various mitigation strategies. Our findings highlight the sensitivity of LLM responses to biased hints. At the same time, our experiments show that, to mitigate anchoring bias, one needs to collect hints from comprehensive angles to prevent the LLMs from being anchored to individual pieces of information, while simple algorithms such as Chain-of-Thought, Thoughts of Principles, Ignoring Anchor Hints, and Reflection are not sufficient.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06619",
        "abstract url": "https://arxiv.org/abs/2412.06619",
        "title": "Copyright-Protected Language Generation via Adaptive Model Fusion",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "The risk of language models reproducing copyrighted material from their training data has led to the development of various protective measures. Among these, inference-time strategies that impose constraints via post-processing have shown promise in addressing the complexities of copyright regulation. However, they often incur prohibitive computational costs or suffer from performance trade-offs. To overcome these limitations, we introduce Copyright-Protecting Model Fusion (CP-Fuse), a novel approach that combines models trained on disjoint sets of copyrighted material during inference. In particular, CP-Fuse adaptively aggregates the model outputs to minimize the reproduction of copyrighted content, adhering to a crucial balancing property that prevents the regurgitation of memorized data. Through extensive experiments, we show that CP-Fuse significantly reduces the reproduction of protected material without compromising the quality of text and code generation. Moreover, its post-hoc nature allows seamless integration with other protective measures, further enhancing copyright safeguards. Lastly, we show that CP-Fuse is robust against common techniques for extracting training data.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "cs.CR"
        ],
        "comment": "47 pages, 21 Figures. arXiv admin note: substantial text overlap with arXiv:2407.20105"
    },
    {
        "paper id": "2412.06639",
        "abstract url": "https://arxiv.org/abs/2412.06639",
        "title": "Beyond Scalars: Concept-Based Alignment Analysis in Vision Transformers",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Vision transformers (ViTs) can be trained using various learning paradigms, from fully supervised to self-supervised. Diverse training protocols often result in significantly different feature spaces, which are usually compared through alignment analysis. However, current alignment measures quantify this relationship in terms of a single scalar value, obscuring the distinctions between common and unique features in pairs of representations that share the same scalar alignment. We address this limitation by combining alignment analysis with concept discovery, which enables a breakdown of alignment into single concepts encoded in feature space. This fine-grained comparison reveals both universal and unique concepts across different representations, as well as the internal structure of concepts within each of them. Our methodological contributions address two key prerequisites for concept-based alignment: 1) For a description of the representation in terms of concepts that faithfully capture the geometry of the feature space, we define concepts as the most general structure they can possibly form - arbitrary manifolds, allowing hidden features to be described by their proximity to these manifolds. 2) To measure distances between concept proximity scores of two representations, we use a generalized Rand index and partition it for alignment between pairs of concepts. We confirm the superiority of our novel concept definition for alignment analysis over existing linear baselines in a sanity check. The concept-based alignment analysis of representations from four different ViTs reveals that increased supervision correlates with a reduction in the semantic structure of learned representations.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "19 pages, 17 figures, code: https://github.com/jvielhaben/NLMCD-ALIGN"
    },
    {
        "paper id": "2412.06642",
        "abstract url": "https://arxiv.org/abs/2412.06642",
        "title": "Class Balance Matters to Active Class-Incremental Learning",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Few-Shot Class-Incremental Learning has shown remarkable efficacy in efficient learning new concepts with limited annotations. Nevertheless, the heuristic few-shot annotations may not always cover the most informative samples, which largely restricts the capability of incremental learner. We aim to start from a pool of large-scale unlabeled data and then annotate the most informative samples for incremental learning. Based on this premise, this paper introduces the Active Class-Incremental Learning (ACIL). The objective of ACIL is to select the most informative samples from the unlabeled pool to effectively train an incremental learner, aiming to maximize the performance of the resulting model. Note that vanilla active learning algorithms suffer from class-imbalanced distribution among annotated samples, which restricts the ability of incremental learning. To achieve both class balance and informativeness in chosen samples, we propose Class-Balanced Selection (CBS) strategy. Specifically, we first cluster the features of all unlabeled images into multiple groups. Then for each cluster, we employ greedy selection strategy to ensure that the Gaussian distribution of the sampled features closely matches the Gaussian distribution of all unlabeled features within the cluster. Our CBS can be plugged and played into those CIL methods which are based on pretrained models with prompts tunning technique. Extensive experiments under ACIL protocol across five diverse datasets demonstrate that CBS outperforms both random selection and other SOTA active learning approaches. Code is publicly available at https://github.com/1170300714/CBS.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "ACM MM 2024"
    },
    {
        "paper id": "2412.06654",
        "abstract url": "https://arxiv.org/abs/2412.06654",
        "title": "GEAR: A Simple GENERATE, EMBED, AVERAGE AND RANK Approach for Unsupervised Reverse Dictionary",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Reverse Dictionary (RD) is the task of obtaining the most relevant word or set of words given a textual description or dictionary definition. Effective RD methods have applications in accessibility, translation or writing support systems. Moreover, in NLP research we find RD to be used to benchmark text encoders at various granularities, as it often requires word, definition and sentence embeddings. In this paper, we propose a simple approach to RD that leverages LLMs in combination with embedding models. Despite its simplicity, this approach outperforms supervised baselines in well studied RD datasets, while also showing less over-fitting. We also conduct a number of experiments on different dictionaries and analyze how different styles, registers and target audiences impact the quality of RD systems. We conclude that, on average, untuned embeddings alone fare way below an LLM-only baseline (although they are competitive in highly technical dictionaries), but are crucial for boosting performance in combined methods.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "9 pages, Accepted at COLING 2025"
    },
    {
        "paper id": "2412.06673",
        "abstract url": "https://arxiv.org/abs/2412.06673",
        "title": "ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we introduce ILLUME, a unified multimodal large language model (MLLM) that seamlessly integrates multimodal understanding and generation capabilities within a single large language model through a unified next-token prediction formulation. To address the large dataset size typically required for image-text alignment, we propose to enhance data efficiency through the design of a vision tokenizer that incorporates semantic information and a progressive multi-stage training procedure. This approach reduces the dataset size to just 15M for pretraining -- over four times fewer than what is typically needed -- while achieving competitive or even superior performance with existing unified MLLMs, such as Janus. Additionally, to promote synergistic enhancement between understanding and generation capabilities, which is under-explored in previous works, we introduce a novel self-enhancing multimodal alignment scheme. This scheme supervises the MLLM to self-assess the consistency between text descriptions and self-generated images, facilitating the model to interpret images more accurately and avoid unrealistic and incorrect predictions caused by misalignment in image generation. Based on extensive experiments, our proposed ILLUME stands out and competes with state-of-the-art unified MLLMs and specialized models across various benchmarks for multimodal understanding, generation, and editing.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06693",
        "abstract url": "https://arxiv.org/abs/2412.06693",
        "title": "OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large Language Model and its Omni-Extensions",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "The rapid advancements in Large Language Models (LLMs) have significantly expanded their applications, ranging from multilingual support to domain-specific tasks and multimodal integration. In this paper, we present OmniEvalKit, a novel benchmarking toolbox designed to evaluate LLMs and their omni-extensions across multilingual, multidomain, and multimodal capabilities. Unlike existing benchmarks that often focus on a single aspect, OmniEvalKit provides a modular, lightweight, and automated evaluation system. It is structured with a modular architecture comprising a Static Builder and Dynamic Data Flow, promoting the seamless integration of new models and datasets. OmniEvalKit supports over 100 LLMs and 50 evaluation datasets, covering comprehensive evaluations across thousands of model-dataset combinations. OmniEvalKit is dedicated to creating an ultra-lightweight and fast-deployable evaluation framework, making downstream applications more convenient and versatile for the AI community.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06712",
        "abstract url": "https://arxiv.org/abs/2412.06712",
        "title": "How to Merge Your Multimodal Models Over Time?",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Model merging combines multiple expert models - finetuned from a base foundation model on diverse tasks and domains - into a single, more capable model. However, most existing model merging approaches assume that all experts are available simultaneously. In reality, new tasks and domains emerge progressively over time, requiring strategies to integrate the knowledge of expert models as they become available: a process we call temporal model merging. The temporal dimension introduces unique challenges not addressed in prior work, raising new questions such as: when training for a new task, should the expert model start from the merged past experts or from the original base model? Should we merge all models at each time step? Which merging techniques are best suited for temporal merging? Should different strategies be used to initialize the training and deploy the model? To answer these questions, we propose a unified framework called TIME - Temporal Integration of Model Expertise - which defines temporal model merging across three axes: (1) Initialization Phase, (2) Deployment Phase, and (3) Merging Technique. Using TIME, we study temporal model merging across model sizes, compute budgets, and learning horizons on the FoMo-in-Flux benchmark. Our comprehensive suite of experiments across TIME allows us to uncover key insights for temporal model merging, offering a better understanding of current challenges and best practices for effective temporal model merging.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "cs.CV"
        ],
        "comment": "Technical Report. Code at https://github.com/ExplainableML/fomo_in_flux"
    },
    {
        "paper id": "2412.06720",
        "abstract url": "https://arxiv.org/abs/2412.06720",
        "title": "VP-MEL: Visual Prompts Guided Multimodal Entity Linking",
        "rating": "1",
        "keywords": [
            [
                "cs.CV",
                "cs.CL"
            ]
        ],
        "abstract": "Multimodal Entity Linking (MEL) is extensively utilized in the domains of information retrieval. However, existing MEL methods typically utilize mention words as mentions for retrieval. This results in a significant dependence of MEL on mention words, thereby constraining its capacity to effectively leverage information from both images and text. In situations where mention words are absent, MEL methods struggle to leverage image-text pairs for entity linking. To solve these issues, we introduce a Visual Prompts guided Multimodal Entity Linking (VP-MEL) task. VP-MEL directly marks specific regions within the image. These markers are referred to as visual prompts in VP-MEL. Without mention words, VP-MEL aims to utilize marked image-text pairs to align visual prompts with specific entities in the knowledge bases. A new dataset for the VP-MEL task, VPWiki, is proposed in this paper. Moreover, we propose a framework named FBMEL, which enhances the significance of visual prompts and fully leverages the information in image-text pairs. Experimental results on the VPWiki dataset demonstrate that FBMEL outperforms baseline methods across multiple benchmarks for the VP-MEL task.",
        "subjects": [
            "cs.CV",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06724",
        "abstract url": "https://arxiv.org/abs/2412.06724",
        "title": "AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and Benchmark",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "We investigate the reasoning capabilities of large language models (LLMs) for automatically generating data-cleaning workflows. To evaluate LLMs' ability to complete data-cleaning tasks, we implemented a pipeline for LLM-based Auto Data Cleaning Workflow (AutoDCWorkflow), prompting LLMs on data cleaning operations to repair three types of data quality issues: duplicates, missing values, and inconsistent data formats. Given a dirty table and a purpose (expressed as a query), this pipeline generates a minimal, clean table sufficient to address the purpose and the data cleaning workflow used to produce the table. The planning process involves three main LLM-driven components: (1) Select Target Columns: Identifies a set of target columns related to the purpose. (2) Inspect Column Quality: Assesses the data quality for each target column and generates a Data Quality Report as operation objectives. (3) Generate Operation & Arguments: Predicts the next operation and arguments based on the data quality report results. Additionally, we propose a data cleaning benchmark to evaluate the capability of LLM agents to automatically generate workflows that address data cleaning purposes of varying difficulty levels. The benchmark comprises the annotated datasets as a collection of purpose, raw table, clean table, data cleaning workflow, and answer set. In our experiments, we evaluated three LLMs that auto-generate purpose-driven data cleaning workflows. The results indicate that LLMs perform well in planning and generating data-cleaning workflows without the need for fine-tuning.",
        "subjects": [
            "cs.DB",
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06738",
        "abstract url": "https://arxiv.org/abs/2412.06738",
        "title": "JAPAGEN: Efficient Few/Zero-shot Learning via Japanese Training Dataset Generation with LLM",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recently some studies have highlighted the potential of Large Language Models (LLMs) as effective generators of supervised training data, offering advantages such as enhanced inference efficiency and reduced costs associated with data collection. However, these studies have predominantly focused on English language tasks. In this paper, we address the fundamental research question: Can LLMs serve as proficient training data generators for other language tasks? Specifically, we leverage LLMs to synthesize supervised training data under few-shot and zero-shot learning scenarios across six diverse Japanese downstream tasks. Subsequently, we utilize this synthesized data to train compact models (e.g., BERT). This novel methodology is termed JAPAGEN. Our experimental findings underscore that JAPAGEN achieves robust performance in classification tasks that necessitate formal text inputs, demonstrating competitive results compared to conventional LLM prompting strategies.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "Accepted by PACLIC38 (2024)"
    },
    {
        "paper id": "2412.06748",
        "abstract url": "https://arxiv.org/abs/2412.06748",
        "title": "Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "A key component of building safe and reliable language models is enabling the models to appropriately refuse to follow certain instructions or answer certain questions. We may want models to output refusal messages for various categories of user queries, for example, ill-posed questions, instructions for committing illegal acts, or queries which require information past the model's knowledge horizon. Engineering models that refuse to answer such questions is complicated by the fact that an individual may want their model to exhibit varying levels of sensitivity for refusing queries of various categories, and different users may want different refusal rates. The current default approach involves training multiple models with varying proportions of refusal messages from each category to achieve the desired refusal rates, which is computationally expensive and may require training a new model to accommodate each user's desired preference over refusal rates. To address these challenges, we propose refusal tokens, one such token for each refusal category or a single refusal token, which are prepended to the model's responses during training. We then show how to increase or decrease the probability of generating the refusal token for each category during inference to steer the model's refusal behavior. Refusal tokens enable controlling a single model's refusal rates without the need of any further fine-tuning, but only by selectively intervening during generation.",
        "subjects": [
            "cs.LG",
            "cs.CL"
        ],
        "comment": "19 pages"
    },
    {
        "paper id": "2412.06769",
        "abstract url": "https://arxiv.org/abs/2412.06769",
        "title": "Training Large Language Models to Reason in a Continuous Latent Space",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) are restricted to reason in the \"language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed \"continuous thought\"). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06774",
        "abstract url": "https://arxiv.org/abs/2412.06774",
        "title": "Visual Lexicon: Rich Image Features in Language Space",
        "rating": "1",
        "keywords": [
            [
                "visual language",
                "VLMs"
            ],
            [
                "diffusion",
                "text-to-image"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We present Visual Lexicon, a novel visual language that encodes rich image information into the text space of vocabulary tokens while retaining intricate visual details that are often challenging to convey in natural language. Unlike traditional methods that prioritize either high-level semantics (e.g., CLIP) or pixel-level reconstruction (e.g., VAE), ViLex simultaneously captures rich semantic content and fine visual details, enabling high-quality image generation and comprehensive visual scene understanding. Through a self-supervised learning pipeline, ViLex generates tokens optimized for reconstructing input images using a frozen text-to-image (T2I) diffusion model, preserving the detailed information necessary for high-fidelity semantic-level reconstruction. As an image embedding in the language space, ViLex tokens leverage the compositionality of natural languages, allowing them to be used independently as \"text tokens\" or combined with natural language tokens to prompt pretrained T2I models with both visual and textual inputs, mirroring how we interact with vision-language models (VLMs). Experiments demonstrate that ViLex achieves higher fidelity in image reconstruction compared to text embeddings--even with a single ViLex token. Moreover, ViLex successfully performs various DreamBooth tasks in a zero-shot, unsupervised manner without fine-tuning T2I models. Additionally, ViLex serves as a powerful vision encoder, consistently improving vision-language model performance across 15 benchmarks relative to a strong SigLIP baseline.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Tech report. 16 pages, 10 figures"
    },
    {
        "paper id": "2412.06864",
        "abstract url": "https://arxiv.org/abs/2412.06864",
        "title": "Political-LLM: Large Language Models in Political Science",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "In recent years, large language models (LLMs) have been widely adopted in political science tasks such as election prediction, sentiment analysis, policy impact assessment, and misinformation detection. Meanwhile, the need to systematically understand how LLMs can further revolutionize the field also becomes urgent. In this work, we--a multidisciplinary team of researchers spanning computer science and political science--present the first principled framework termed Political-LLM to advance the comprehensive understanding of integrating LLMs into computational political science. Specifically, we first introduce a fundamental taxonomy classifying the existing explorations into two perspectives: political science and computational methodologies. In particular, from the political science perspective, we highlight the role of LLMs in automating predictive and generative tasks, simulating behavior dynamics, and improving causal inference through tools like counterfactual generation; from a computational perspective, we introduce advancements in data preparation, fine-tuning, and evaluation methods for LLMs that are tailored to political contexts. We identify key challenges and future directions, emphasizing the development of domain-specific datasets, addressing issues of bias and fairness, incorporating human expertise, and redefining evaluation criteria to align with the unique requirements of computational political science. Political-LLM seeks to serve as a guidebook for researchers to foster an informed, ethical, and impactful use of Artificial Intelligence in political science. Our online resource is available at: http://political-llm.org/.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "54 Pages, 9 Figures"
    },
    {
        "paper id": "2412.06868",
        "abstract url": "https://arxiv.org/abs/2412.06868",
        "title": "Compression for Better: A General and Stable Lossless Compression Framework",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "This work focus on how to stabilize and lossless model compression, aiming to reduce model complexity and enhance efficiency without sacrificing performance due to compression errors. A key challenge is effectively leveraging compression errors and defining the boundaries for lossless compression to minimize model loss. i.e., compression for better. Currently, there is no systematic approach to determining this error boundary or understanding its specific impact on model performance. We propose a general \\textbf{L}oss\\textbf{L}ess \\textbf{C}ompression theoretical framework (\\textbf{LLC}), which further delineates the compression neighborhood and higher-order analysis boundaries through the total differential, thereby specifying the error range within which a model can be compressed without loss. To verify the effectiveness of LLC, we apply various compression techniques, including quantization and decomposition. Specifically, for quantization, we reformulate the classic quantization search problem as a grouped knapsack problem within the lossless neighborhood, achieving lossless quantization while improving computational efficiency. For decomposition, LLC addresses the approximation problem under low-rank constraints, automatically determining the rank for each layer and producing lossless low-rank models. We conduct extensive experiments on multiple neural network architectures on different datasets. The results show that without fancy tricks, LLC can effectively achieve lossless model compression. Our code will be made publicly.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Under Review"
    },
    {
        "paper id": "2412.06877",
        "abstract url": "https://arxiv.org/abs/2412.06877",
        "title": "LLMs for Generalizable Language-Conditioned Policy Learning under Minimal Data Requirements",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "To develop autonomous agents capable of executing complex, multi-step decision-making tasks as specified by humans in natural language, existing reinforcement learning approaches typically require expensive labeled datasets or access to real-time experimentation. Moreover, conventional methods often face difficulties in generalizing to unseen goals and states, thereby limiting their practical applicability. This paper presents TEDUO, a novel training pipeline for offline language-conditioned policy learning. TEDUO operates on easy-to-obtain, unlabeled datasets and is suited for the so-called in-the-wild evaluation, wherein the agent encounters previously unseen goals and states. To address the challenges posed by such data and evaluation settings, our method leverages the prior knowledge and instruction-following capabilities of large language models (LLMs) to enhance the fidelity of pre-collected offline data and enable flexible generalization to new goals and states. Empirical results demonstrate that the dual role of LLMs in our framework-as data enhancers and generalizers-facilitates both effective and data-efficient learning of generalizable language-conditioned policies.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06878",
        "abstract url": "https://arxiv.org/abs/2412.06878",
        "title": "SafeWatch: An Efficient Safety-Policy Following Video Guardrail Model with Transparent Explanations",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "With the rise of generative AI and rapid growth of high-quality video generation, video guardrails have become more crucial than ever to ensure safety and security across platforms. Current video guardrails, however, are either overly simplistic, relying on pure classification models trained on simple policies with limited unsafe categories, which lack detailed explanations, or prompting multimodal large language models (MLLMs) with long safety guidelines, which are inefficient and impractical for guardrailing real-world content. To bridge this gap, we propose SafeWatch, an efficient MLLM-based video guardrail model designed to follow customized safety policies and provide multi-label video guardrail outputs with content-specific explanations in a zero-shot manner. In particular, unlike traditional MLLM-based guardrails that encode all safety policies autoregressively, causing inefficiency and bias, SafeWatch uniquely encodes each policy chunk in parallel and eliminates their position bias such that all policies are attended simultaneously with equal importance. In addition, to improve efficiency and accuracy, SafeWatch incorporates a policy-aware visual token pruning algorithm that adaptively selects the most relevant video tokens for each policy, discarding noisy or irrelevant information. This allows for more focused, policy-compliant guardrail with significantly reduced computational overhead. Considering the limitations of existing video guardrail benchmarks, we propose SafeWatch-Bench, a large-scale video guardrail benchmark comprising over 2M videos spanning six safety categories which covers over 30 tasks to ensure a comprehensive coverage of all potential safety scenarios. SafeWatch outperforms SOTA by 28.2% on SafeWatch-Bench, 13.6% on benchmarks, cuts costs by 10%, and delivers top-tier explanations validated by LLM and human reviews.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "43 pages, 20 figures"
    },
    {
        "paper id": "2412.06926",
        "abstract url": "https://arxiv.org/abs/2412.06926",
        "title": "When Every Token Counts: Optimal Segmentation for Low-Resource Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Traditional greedy tokenization methods have been a critical step in Natural Language Processing (NLP), influencing how text is converted into tokens and directly impacting model performance. While subword tokenizers like Byte-Pair Encoding (BPE) are widely used, questions remain about their optimality across model scales and languages. In this work, we demonstrate through extensive experiments that an optimal BPE configuration significantly reduces token count compared to greedy segmentation, yielding improvements in token-saving percentages and performance benefits, particularly for smaller models. We evaluate tokenization performance across various intrinsic and extrinsic tasks, including generation and classification. Our findings suggest that compression-optimized tokenization strategies could provide substantial advantages for multilingual and low-resource language applications, highlighting a promising direction for further research and inclusive NLP.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "LoResLM @ COLING 2025"
    },
    {
        "paper id": "2412.06951",
        "abstract url": "https://arxiv.org/abs/2412.06951",
        "title": "Analysing Public Transport User Sentiment on Low Resource Multilingual Data",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Public transport systems in many Sub-Saharan countries often receive less attention compared to other sectors, underscoring the need for innovative solutions to improve the Quality of Service (QoS) and overall user experience. This study explored commuter opinion mining to understand sentiments toward existing public transport systems in Kenya, Tanzania, and South Africa. We used a qualitative research design, analysing data from X (formerly Twitter) to assess sentiments across rail, mini-bus taxis, and buses. By leveraging Multilingual Opinion Mining techniques, we addressed the linguistic diversity and code-switching present in our dataset, thus demonstrating the application of Natural Language Processing (NLP) in extracting insights from under-resourced languages. We employed PLMs such as AfriBERTa, AfroXLMR, AfroLM, and PuoBERTa to conduct the sentiment analysis. The results revealed predominantly negative sentiments in South Africa and Kenya, while the Tanzanian dataset showed mainly positive sentiments due to the advertising nature of the tweets. Furthermore, feature extraction using the Word2Vec model and K-Means clustering illuminated semantic relationships and primary themes found within the different datasets. By prioritising the analysis of user experiences and sentiments, this research paves the way for developing more responsive, user-centered public transport systems in Sub-Saharan countries, contributing to the broader goal of improving urban mobility and sustainability.",
        "subjects": [
            "cs.CL",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06967",
        "abstract url": "https://arxiv.org/abs/2412.06967",
        "title": "Effective Text Adaptation for LLM-based ASR through Soft Prompt Fine-Tuning",
        "rating": "1",
        "keywords": [
            [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "The advent of Large Language Models (LLM) has reformed the Automatic Speech Recognition (ASR). Prompting LLM with audio embeddings to generate transcriptions becomes the new state-of-the-art ASR. Despite LLMs being trained with an extensive amount of text corpora, high-quality domain-specific text data can still significantly enhance ASR performance on domain adaptation tasks. Although LLM-based ASR can naturally incorporate more text corpora by fine-tuning the LLM decoder, fine-tuning such ASR on text-only data without paired prompts may diminish the effectiveness of domain-specific knowledge. To mitigate this issue, we propose a two-step soft prompt fine-tuning strategy that enhances domain-specific text adaptation. Experimental results show that text adaptation with our proposed method achieved a relative up to 9% Word Error Rate (WER) reduction and up to 18% Entity Error Rate (EER) reduction on the target domain compared to the baseline ASR. Combining this with domain-specific Language Model (LM) fusion can further improve the EER by a relative 2-5%",
        "subjects": [
            "cs.CL",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "accepted as SLT 2024 proceeding"
    },
    {
        "paper id": "2412.06975",
        "abstract url": "https://arxiv.org/abs/2412.06975",
        "title": "AutoReason: Automatic Few-Shot Reasoning Decomposition",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Chain of Thought (CoT) was introduced in recent research as a method for improving step-by-step reasoning in Large Language Models. However, CoT has limited applications such as its need for hand-crafted few-shot exemplar prompts and no capability to adjust itself to different queries. In this work, we propose a system to automatically generate rationales using CoT. Our method improves multi-step implicit reasoning capabilities by decomposing the implicit query into several explicit questions. This provides interpretability for the model, improving reasoning in weaker LLMs. We test our approach with two Q\\&A datasets: StrategyQA and HotpotQA. We show an increase in accuracy with both, especially on StrategyQA. To facilitate further research in this field, the complete source code for this study has been made publicly available on GitHub: https://github.com/miralab-ai/autoreason.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06978",
        "abstract url": "https://arxiv.org/abs/2412.06978",
        "title": "Edge-SD-SR: Low Latency and Parameter Efficient On-device Super-Resolution with Stable Diffusion via Bidirectional Conditioning",
        "rating": "1",
        "keywords": [
            [
                "Parameter Efficient"
            ],
            [
                "Diffusion",
                "Super-Resolution"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "There has been immense progress recently in the visual quality of Stable Diffusion-based Super Resolution (SD-SR). However, deploying large diffusion models on computationally restricted devices such as mobile phones remains impractical due to the large model size and high latency. This is compounded for SR as it often operates at high res (e.g. 4Kx3K). In this work, we introduce Edge-SD-SR, the first parameter efficient and low latency diffusion model for image super-resolution. Edge-SD-SR consists of ~169M parameters, including UNet, encoder and decoder, and has a complexity of only ~142 GFLOPs. To maintain a high visual quality on such low compute budget, we introduce a number of training strategies: (i) A novel conditioning mechanism on the low resolution input, coined bidirectional conditioning, which tailors the SD model for the SR task. (ii) Joint training of the UNet and encoder, while decoupling the encodings of the HR and LR images and using a dedicated schedule. (iii) Finetuning the decoder using the UNet's output to directly tailor the decoder to the latents obtained at inference time. Edge-SD-SR runs efficiently on device, e.g. it can upscale a 128x128 patch to 512x512 in 38 msec while running on a Samsung S24 DSP, and of a 512x512 to 2048x2048 (requiring 25 model evaluations) in just ~1.1 sec. Furthermore, we show that Edge-SD-SR matches or even outperforms state-of-the-art SR approaches on the most established SR benchmarks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07091",
        "abstract url": "https://arxiv.org/abs/2412.07091",
        "title": "Creative Portraiture: Exploring Creative Adversarial Networks and Conditional Creative Adversarial Networks",
        "rating": "1",
        "keywords": [
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Convolutional neural networks (CNNs) have been combined with generative adversarial networks (GANs) to create deep convolutional generative adversarial networks (DCGANs) with great success. DCGANs have been used for generating images and videos from creative domains such as fashion design and painting. A common critique of the use of DCGANs in creative applications is that they are limited in their ability to generate creative products because the generator simply learns to copy the training distribution. We explore an extension of DCGANs, creative adversarial networks (CANs). Using CANs, we generate novel, creative portraits, using the WikiArt dataset to train the network. Moreover, we introduce our extension of CANs, conditional creative adversarial networks (CCANs), and demonstrate their potential to generate creative portraits conditioned on a style label. We argue that generating products that are conditioned, or inspired, on a style label closely emulates real creative processes in which humans produce imaginative work that is still rooted in previous styles.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07096",
        "abstract url": "https://arxiv.org/abs/2412.07096",
        "title": "QAPyramid: Fine-grained Evaluation of Content Selection for Text Summarization",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "How to properly conduct human evaluations for text summarization is a longstanding challenge. The Pyramid human evaluation protocol, which assesses content selection by breaking the reference summary into sub-units and verifying their presence in the system summary, has been widely adopted. However, it suffers from a lack of systematicity in the definition and granularity of the sub-units. We address these problems by proposing QAPyramid, which decomposes each reference summary into finer-grained question-answer (QA) pairs according to the QA-SRL framework. We collect QA-SRL annotations for reference summaries from CNN/DM and evaluate 10 summarization systems, resulting in 8.9K QA-level annotations. We show that, compared to Pyramid, QAPyramid provides more systematic and fine-grained content selection evaluation while maintaining high inter-annotator agreement without needing expert annotations. Furthermore, we propose metrics that automate the evaluation pipeline and achieve higher correlations with QAPyramid than other widely adopted metrics, allowing future work to accurately and efficiently benchmark summarization systems.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "The first two authors contributed equally. Code: https://github.com/ZhangShiyue/QAPyramid"
    },
    {
        "paper id": "2412.07102",
        "abstract url": "https://arxiv.org/abs/2412.07102",
        "title": "Primary visual cortex contributes to color constancy by predicting rather than discounting the illuminant: evidence from a computational study",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Color constancy (CC) is an important ability of the human visual system to stably perceive the colors of objects despite considerable changes in the color of the light illuminating them. While increasing evidence from the field of neuroscience supports that multiple levels of the visual system contribute to the realization of CC, how the primary visual cortex (V1) plays role in CC is not fully resolved. In specific, double-opponent (DO) neurons in V1 have been thought to contribute to realizing a degree of CC, but the computational mechanism is not clear. We build an electrophysiologically based V1 neural model to learn the color of the light source from a natural image dataset with the ground truth illuminants as the labels. Based on the qualitative and quantitative analysis of the responsive properties of the learned model neurons, we found that both the spatial structures and color weights of the receptive fields of the learned model neurons are quite similar to those of the simple and DO neurons recorded in V1. Computationally, DO cells perform more robustly than the simple cells in V1 for illuminant prediction. Therefore, this work provides computational evidence supporting that V1 DO neurons serve to realize color constancy by encoding the illuminant,which is contradictory to the common hypothesis that V1 contributes to CC by discounting the illuminant using its DO cells. This evidence is expected to not only help resolve the visual mechanisms of CC, but also provide inspiration to develop more effective computer vision models.",
        "subjects": [
            "q-bio.NC",
            "cs.CV"
        ],
        "comment": "26 pages, 11 figures"
    },
    {
        "paper id": "2412.07108",
        "abstract url": "https://arxiv.org/abs/2412.07108",
        "title": "Improving the Natural Language Inference robustness to hard dataset by data augmentation and preprocessing",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Natural Language Inference (NLI) is the task of inferring whether the hypothesis can be justified by the given premise. Basically, we classify the hypothesis into three labels(entailment, neutrality and contradiction) given the premise. NLI was well studied by the previous researchers. A number of models, especially the transformer based ones, have achieved significant improvement on these tasks. However, it is reported that these models are suffering when they are dealing with hard datasets. Particularly, they perform much worse when dealing with unseen out-of-distribution premise and hypothesis. They may not understand the semantic content but learn the spurious correlations. In this work, we propose the data augmentation and preprocessing methods to solve the word overlap, numerical reasoning and length mismatch problems. These methods are general methods that do not rely on the distribution of the testing data and they help improve the robustness of the models.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07111",
        "abstract url": "https://arxiv.org/abs/2412.07111",
        "title": "Predictable Emergent Abilities of LLMs: Proxy Tasks Are All You Need",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "While scaling laws optimize training configurations for large language models (LLMs) through experiments on smaller or early-stage models, they fail to predict emergent abilities due to the absence of such capabilities in these models. To address this, we propose a method that predicts emergent abilities by leveraging proxy tasks. We begin by establishing relevance metrics between the target task and candidate tasks based on performance differences across multiple models. These candidate tasks are then validated for robustness with small model ensembles, leading to the selection of the most appropriate proxy tasks. The predicted performance on the target task is then derived by integrating the evaluation results of these proxies. In a case study on tool utilization capabilities, our method demonstrated a strong correlation between predicted and actual performance, confirming its effectiveness.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07113",
        "abstract url": "https://arxiv.org/abs/2412.07113",
        "title": "Exploring Coding Spot: Understanding Parametric Contributions to LLM Coding Performance",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated notable proficiency in both code generation and comprehension across multiple programming languages. However, the mechanisms underlying this proficiency remain underexplored, particularly with respect to whether distinct programming languages are processed independently or within a shared parametric region. Drawing an analogy to the specialized regions of the brain responsible for distinct cognitive functions, we introduce the concept of Coding Spot, a specialized parametric region within LLMs that facilitates coding capabilities. Our findings identify this Coding Spot and show that targeted modifications to this subset significantly affect performance on coding tasks, while largely preserving non-coding functionalities. This compartmentalization mirrors the functional specialization observed in cognitive neuroscience, where specific brain regions are dedicated to distinct tasks, suggesting that LLMs may similarly employ specialized parameter regions for different knowledge domains.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07114",
        "abstract url": "https://arxiv.org/abs/2412.07114",
        "title": "TT-MPD: Test Time Model Pruning and Distillation",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Pruning can be an effective method of compressing large pre-trained models for inference speed acceleration. Previous pruning approaches rely on access to the original training dataset for both pruning and subsequent fine-tuning. However, access to the training data can be limited due to concerns such as data privacy and commercial confidentiality. Furthermore, with covariate shift (disparities between test and training data distributions), pruning and finetuning with training datasets can hinder the generalization of the pruned model to test data. To address these issues, pruning and finetuning the model with test time samples becomes essential. However, test-time model pruning and fine-tuning incur additional computation costs and slow down the model's prediction speed, thus posing efficiency issues. Existing pruning methods are not efficient enough for test time model pruning setting, since finetuning the pruned model is needed to evaluate the importance of removable components. To address this, we propose two variables to approximate the fine-tuned accuracy. We then introduce an efficient pruning method that considers the approximated finetuned accuracy and potential inference latency saving. To enhance fine-tuning efficiency, we propose an efficient knowledge distillation method that only needs to generate pseudo labels for a small set of finetuning samples one time, thereby reducing the expensive pseudo-label generation cost. Experimental results demonstrate that our method achieves a comparable or superior tradeoff between test accuracy and inference latency, with a 32% relative reduction in pruning and finetuning time compared to the best existing method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07147",
        "abstract url": "https://arxiv.org/abs/2412.07147",
        "title": "MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Image Translation (IT) holds immense potential across diverse domains, enabling the translation of textual content within images into various languages. However, existing datasets often suffer from limitations in scale, diversity, and quality, hindering the development and evaluation of IT models. To address this issue, we introduce MIT-10M, a large-scale parallel corpus of multilingual image translation with over 10M image-text pairs derived from real-world data, which has undergone extensive data cleaning and multilingual translation validation. It contains 840K images in three sizes, 28 categories, tasks with three levels of difficulty and 14 languages image-text pairs, which is a considerable improvement on existing datasets. We conduct extensive experiments to evaluate and train models on MIT-10M. The experimental results clearly indicate that our dataset has higher adaptability when it comes to evaluating the performance of the models in tackling challenging and complex image translation tasks in the real world. Moreover, the performance of the model fine-tuned with MIT-10M has tripled compared to the baseline model, further confirming its superiority.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Accepted in COLING 2025"
    },
    {
        "paper id": "2412.07155",
        "abstract url": "https://arxiv.org/abs/2412.07155",
        "title": "Annotation Techniques for Judo Combat Phase Classification from Tournament Footage",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper presents a semi-supervised approach to extracting and analyzing combat phases in judo tournaments using live-streamed footage. The objective is to automate the annotation and summarization of live streamed judo matches. We train models that extract relevant entities and classify combat phases from fixed-perspective judo recordings. We employ semi-supervised methods to address limited labeled data in the domain. We build a model of combat phases via transfer learning from a fine-tuned object detector to classify the presence, activity, and standing state of the match. We evaluate our approach on a dataset of 19 thirty-second judo clips, achieving an F1 score on a $20\\%$ test hold-out of 0.66, 0.78, and 0.87 for the three classes, respectively. Our results show initial promise for automating more complex information retrieval tasks using rigorous methods with limited labeled data.",
        "subjects": [
            "cs.CV",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07161",
        "abstract url": "https://arxiv.org/abs/2412.07161",
        "title": "Compositional Zero-Shot Learning with Contextualized Cues and Adaptive Contrastive Training",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Compositional Zero-Shot Learning (CZSL) aims to recognize unseen combinations of seen attributes and objects. Current CLIP-based methods in CZSL, despite their advancements, often fail to effectively understand and link the attributes and objects due to inherent limitations in CLIP's pretraining mechanisms. To address these shortcomings, this paper introduces a novel framework, Understanding and Linking Attributes and Objects (ULAO) in CZSL, which comprises two innovative modules. The Understanding Attributes and Objects (UAO) module improves primitive understanding by sequential primitive prediction and leveraging recognized objects as contextual hints for attribute classification. Concurrently, the Linking Attributes and Objects (LAO) module improves the attribute-object linkage understanding through a new contrastive learning strategy that incorporates tailored hard negative generation and adaptive loss adjustments. We demonstrate our model's superiority by showcasing its state-of-the-art performance across three benchmark datasets in both Closed-World (CW) and Open-World (OW) scenarios.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07167",
        "abstract url": "https://arxiv.org/abs/2412.07167",
        "title": "Reinforcement Learning Policy as Macro Regulator Rather than Macro Placer",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "In modern chip design, placement aims at placing millions of circuit modules, which is an essential step that significantly influences power, performance, and area (PPA) metrics. Recently, reinforcement learning (RL) has emerged as a promising technique for improving placement quality, especially macro placement. However, current RL-based placement methods suffer from long training times, low generalization ability, and inability to guarantee PPA results. A key issue lies in the problem formulation, i.e., using RL to place from scratch, which results in limits useful information and inaccurate rewards during the training process. In this work, we propose an approach that utilizes RL for the refinement stage, which allows the RL policy to learn how to adjust existing placement layouts, thereby receiving sufficient information for the policy to act and obtain relatively dense and precise rewards. Additionally, we introduce the concept of regularity during training, which is considered an important metric in the chip design industry but is often overlooked in current RL placement methods. We evaluate our approach on the ISPD 2005 and ICCAD 2015 benchmark, comparing the global half-perimeter wirelength and regularity of our proposed method against several competitive approaches. Besides, we test the PPA performance using commercial software, showing that RL as a regulator can achieve significant PPA improvements. Our RL regulator can fine-tune placements from any method and enhance their quality. Our work opens up new possibilities for the application of RL in placement, providing a more effective and efficient approach to optimizing chip design. Our code is available at \\url{https://github.com/lamda-bbo/macro-regulator}.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "NeurIPS 2024"
    },
    {
        "paper id": "2412.07168",
        "abstract url": "https://arxiv.org/abs/2412.07168",
        "title": "3A-YOLO: New Real-Time Object Detectors with Triple Discriminative Awareness and Coordinated Representations",
        "rating": "1",
        "keywords": [
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent research on real-time object detectors (e.g., YOLO series) has demonstrated the effectiveness of attention mechanisms for elevating model performance. Nevertheless, existing methods neglect to unifiedly deploy hierarchical attention mechanisms to construct a more discriminative YOLO head which is enriched with more useful intermediate features. To tackle this gap, this work aims to leverage multiple attention mechanisms to hierarchically enhance the triple discriminative awareness of the YOLO detection head and complementarily learn the coordinated intermediate representations, resulting in a new series detectors denoted 3A-YOLO. Specifically, we first propose a new head denoted TDA-YOLO Module, which unifiedly enhance the representations learning of scale-awareness, spatial-awareness, and task-awareness. Secondly, we steer the intermediate features to coordinately learn the inter-channel relationships and precise positional information. Finally, we perform neck network improvements followed by introducing various tricks to boost the adaptability of 3A-YOLO. Extensive experiments across COCO and VOC benchmarks indicate the effectiveness of our detectors.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07171",
        "abstract url": "https://arxiv.org/abs/2412.07171",
        "title": "Breaking the Stage Barrier: A Novel Single-Stage Approach to Long Context Extension for Large Language Models",
        "rating": "1",
        "keywords": [
            [
                "cs.CL"
            ]
        ],
        "abstract": "Recently, Large language models (LLMs) have revolutionized Natural Language Processing (NLP). Pretrained LLMs, due to limited training context size, struggle with handling long token sequences, limiting their performance on various downstream tasks. Current solutions toward long context modeling often employ multi-stage continual pertaining, which progressively increases the effective context length through several continual pretraining stages. However, those approaches require extensive manual tuning and human expertise. In this paper, we introduce a novel single-stage continual pretraining method, Head-Adaptive Rotary Position Encoding (HARPE), to equip LLMs with long context modeling capabilities while simplifying the training process. Our HARPE leverages different Rotary Position Encoding (RoPE) base frequency values across different attention heads and directly trains LLMs on the target context length. Extensive experiments on 4 language modeling benchmarks, including the latest RULER benchmark, demonstrate that HARPE excels in understanding and integrating long-context tasks with single-stage training, matching and even outperforming existing multi-stage methods. Our results highlight that HARPE successfully breaks the stage barrier for training LLMs with long context modeling capabilities.",
        "subjects": [
            "cs.CL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07174",
        "abstract url": "https://arxiv.org/abs/2412.07174",
        "title": "Post-Training Statistical Calibration for Higher Activation Sparsity",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We present Statistical Calibrated Activation Pruning (SCAP), a post-training activation pruning framework that (1) generalizes sparsification by input activations of Fully-Connected layers for generic and flexible application across Transformers, and (2) features a simple Mode-Centering technique to pre-calibrate activation distributions for maximizing post-training sparsity. Our results demonstrate robust Pareto efficiency compared to prior methods, translating to a 1.5x additional LLM decoding speedup against CATS at iso model quality. SCAP effectiveness is empirically verified across a wide range of models, including recent Transformer Decoders, MoE, Mamba2, Encoding Transformer, and pre-quantized models, highlighting its practicality and scalability. The code is available at: https://github.com/IntelLabs/SCAP.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "ENLSP-IV NeurIPS Workshop 2024"
    },
    {
        "paper id": "2412.07186",
        "abstract url": "https://arxiv.org/abs/2412.07186",
        "title": "Monte Carlo Tree Search based Space Transfer for Black-box Optimization",
        "rating": "1",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Bayesian optimization (BO) is a popular method for computationally expensive black-box optimization. However, traditional BO methods need to solve new problems from scratch, leading to slow convergence. Recent studies try to extend BO to a transfer learning setup to speed up the optimization, where search space transfer is one of the most promising approaches and has shown impressive performance on many tasks. However, existing search space transfer methods either lack an adaptive mechanism or are not flexible enough, making it difficult to efficiently identify promising search space during the optimization process. In this paper, we propose a search space transfer learning method based on Monte Carlo tree search (MCTS), called MCTS-transfer, to iteratively divide, select, and optimize in a learned subspace. MCTS-transfer can not only provide a well-performing search space for warm-start but also adaptively identify and leverage the information of similar source tasks to reconstruct the search space during the optimization process. Experiments on synthetic functions, real-world problems, Design-Bench and hyper-parameter optimization show that MCTS-transfer can demonstrate superior performance compared to other search space transfer methods under different settings. Our code is available at \\url{https://github.com/lamda-bbo/mcts-transfer}.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "NeurIPS 2024 Spotlight"
    },
    {
        "paper id": "2412.06219",
        "abstract url": "https://arxiv.org/abs/2412.06219",
        "title": "Data Free Backdoor Attacks",
        "rating": "0.5",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.AI",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "Backdoor attacks aim to inject a backdoor into a classifier such that it predicts any input with an attacker-chosen backdoor trigger as an attacker-chosen target class. Existing backdoor attacks require either retraining the classifier with some clean data or modifying the model's architecture. As a result, they are 1) not applicable when clean data is unavailable, 2) less efficient when the model is large, and 3) less stealthy due to architecture changes. In this work, we propose DFBA, a novel retraining-free and data-free backdoor attack without changing the model architecture. Technically, our proposed method modifies a few parameters of a classifier to inject a backdoor. Through theoretical analysis, we verify that our injected backdoor is provably undetectable and unremovable by various state-of-the-art defenses under mild assumptions. Our evaluation on multiple datasets further demonstrates that our injected backdoor: 1) incurs negligible classification loss, 2) achieves 100% attack success rates, and 3) bypasses six existing state-of-the-art defenses. Moreover, our comparison with a state-of-the-art non-data-free backdoor attack shows our attack is more stealthy and effective against various defenses while achieving less classification accuracy loss.",
        "subjects": [
            "cs.CR",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "24 pages, 8 figures, accepted by NeurIPS 2024"
    },
    {
        "paper id": "2412.06233",
        "abstract url": "https://arxiv.org/abs/2412.06233",
        "title": "Representational Transfer Learning for Matrix Completion",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose to transfer representational knowledge from multiple sources to a target noisy matrix completion task by aggregating singular subspaces information. Under our representational similarity framework, we first integrate linear representation information by solving a two-way principal component analysis problem based on a properly debiased matrix-valued dataset. After acquiring better column and row representation estimators from the sources, the original high-dimensional target matrix completion problem is then transformed into a low-dimensional linear regression, of which the statistical efficiency is guaranteed. A variety of extensional arguments, including post-transfer statistical inference and robustness against negative transfer, are also discussed alongside. Finally, extensive simulation results and a number of real data cases are reported to support our claims.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06261",
        "abstract url": "https://arxiv.org/abs/2412.06261",
        "title": "Vulnerability Coordination Under the Cyber Resilience Act",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "A new Cyber Resilience Act (CRA) was recently agreed upon in the European Union (EU). It imposes many new cyber security requirements practically to all information technology products, whether hardware or software. The paper examines and elaborates the CRA's new requirements for vulnerability coordination, including vulnerability disclosure. Although these requirements are only a part of the CRA's obligations for vendors, also some new vulnerability coordination mandates are present, including particularly with respect to so-called actively exploited vulnerabilities. The CRA further alters the coordination practices on the side of public administrations. With the examination, elaboration, and associated discussion, the paper contributes to the study of cyber security regulations, providing also a few practical takeaways.",
        "subjects": [
            "cs.CR",
            "cs.CY",
            "cs.SE"
        ],
        "comment": "Submitted to IFIP SEC"
    },
    {
        "paper id": "2412.06303",
        "abstract url": "https://arxiv.org/abs/2412.06303",
        "title": "DSAI: Unbiased and Interpretable Latent Feature Extraction for Data-Centric AI",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Large language models (LLMs) often struggle to objectively identify latent characteristics in large datasets due to their reliance on pre-trained knowledge rather than actual data patterns. To address this data grounding issue, we propose Data Scientist AI (DSAI), a framework that enables unbiased and interpretable feature extraction through a multi-stage pipeline with quantifiable prominence metrics for evaluating extracted features. On synthetic datasets with known ground-truth features, DSAI demonstrates high recall in identifying expert-defined features while faithfully reflecting the underlying data. Applications on real-world datasets illustrate the framework's practical utility in uncovering meaningful patterns with minimal expert oversight, supporting use cases such as interpretable classification. The title of our paper is chosen from multiple candidates based on DSAI-generated criteria.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06328",
        "abstract url": "https://arxiv.org/abs/2412.06328",
        "title": "Towards Civic Digital Twins: Co-Design the Citizen-Centric Future of Bologna",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "We introduce Civic Digital Twin (CDT), an evolution of Urban Digital Twins designed to support a citizen-centric transformative approach to urban planning and governance. CDT is being developed in the scope of the Bologna Digital Twin initiative, launched one year ago by the city of Bologna, to fulfill the city's political and strategic goal of adopting innovative digital tools to support decision-making and civic engagement. The CDT, in addition to its capability of sensing the city through spatial, temporal, and social data, must be able to model and simulate social dynamics in a city: the behavior, attitude, and preference of citizens and collectives and how they impact city life and transform transformation processes. Another distinctive feature of CDT is that it must be able to engage citizens (individuals, collectives, and organized civil society) and other civic stakeholders (utilities, economic actors, third sector) interested in co-designing the future of the city. In this paper, we discuss the motivations that led to the definition of the CDT, define its modeling aspects and key research challenges, and illustrate its intended use with two use cases in urban mobility and urban development.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06333",
        "abstract url": "https://arxiv.org/abs/2412.06333",
        "title": "Augmenting the action space with conventions to improve multi-agent cooperation in Hanabi",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The card game Hanabi is considered a strong medium for the testing and development of multi-agent reinforcement learning (MARL) algorithms, due to its cooperative nature, hidden information, limited communication and remarkable complexity. Previous research efforts have explored the capabilities of MARL algorithms within Hanabi, focusing largely on advanced architecture design and algorithmic manipulations to achieve state-of-the-art performance for a various number of cooperators. However, this often leads to complex solution strategies with high computational cost and requiring large amounts of training data. For humans to solve the Hanabi game effectively, they require the use of conventions, which often allows for a means to implicitly convey ideas or knowledge based on a predefined, and mutually agreed upon, set of ``rules''. Multi-agent problems containing partial observability, especially when limited communication is present, can benefit greatly from the use of implicit knowledge sharing. In this paper, we propose a novel approach to augmenting the action space using conventions, which act as special cooperative actions that span over multiple time steps and multiple agents, requiring agents to actively opt in for it to reach fruition. These conventions are based on existing human conventions, and result in a significant improvement on the performance of existing techniques for self-play and cross-play across a various number of cooperators within Hanabi.",
        "subjects": [
            "cs.MA",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "This paper is under review at the journal of autonomous agents and multi-agent systems (JAAMAS)"
    },
    {
        "paper id": "2412.06342",
        "abstract url": "https://arxiv.org/abs/2412.06342",
        "title": "Tracking control of latent dynamic systems with application to spacecraft attitude control",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "When intelligent spacecraft or space robots perform tasks in a complex environment, the controllable variables are usually not directly available and have to be inferred from high-dimensional observable variables, such as outputs of neural networks or images. While the dynamics of these observations are highly complex, the mechanisms behind them may be simple, which makes it possible to regard them as latent dynamic systems. For control of latent dynamic systems, methods based on reinforcement learning suffer from sample inefficiency and generalization problems. In this work, we propose an asymptotic tracking controller for latent dynamic systems. The latent variables are related to the high-dimensional observations through an unknown nonlinear function. The dynamics are unknown but assumed to be affine nonlinear. To realize asymptotic tracking, an identifiable latent dynamic model is learned to recover the latents and estimate the dynamics. This training process does not depend on the goals or reference trajectories. Based on the learned model, we use a manually designed feedback linearization controller to ensure the asymptotic tracking property of the closed-loop system. After considering fully controllable systems, the results are extended to the case that uncontrollable environmental latents exist. As an application, simulation experiments on a latent spacecraft attitude dynamic model are conducted to verify the proposed methods, and the observation noise and control deviation are taken into consideration.",
        "subjects": [
            "eess.SY",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06356",
        "abstract url": "https://arxiv.org/abs/2412.06356",
        "title": "Reputation Management in the ChatGPT Era",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Generative AI systems often generate outputs about real people, even when not explicitly prompted to do so. This can lead to significant reputational and privacy harms, especially when sensitive, misleading, and outright false. This paper considers what legal tools currently exist to protect such individuals, with a particular focus on defamation and data protection law. We explore the potential of libel law, arguing that it is a potential but not an ideal remedy, due to lack of harmonization, and the focus on damages rather than systematic prevention of future libel. We then turn to data protection law, arguing that the data subject rights to erasure and rectification may offer some more meaningful protection, although the technical feasibility of compliance is a matter of ongoing research. We conclude by noting the limitations of these individualistic remedies and hint at the need for a more systemic, environmental approach to protecting the infosphere against generative AI.",
        "subjects": [
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06368",
        "abstract url": "https://arxiv.org/abs/2412.06368",
        "title": "Measuring Pre-training Data Quality without Labels for Time Series Foundation Models",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Recently, there has been a growing interest in time series foundation models that generalize across different downstream tasks. A key to strong foundation models is a diverse pre-training dataset, which is particularly challenging to collect for time series classification. In this work, we explore the performance of a contrastive-learning-based foundation model as a function of the data used for pre-training. We introduce contrastive accuracy, a new measure to evaluate the quality of the representation space learned by the foundation model. Our experiments reveal the positive correlation between the proposed measure and the accuracy of the model on a collection of downstream tasks. This suggests that the contrastive accuracy can serve as a criterion to search for time series datasets that can enhance the pre-training and improve thereby the foundation model's generalization.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06370",
        "abstract url": "https://arxiv.org/abs/2412.06370",
        "title": "Exploring Memorization and Copyright Violation in Frontier LLMs: A Study of the New York Times v. OpenAI 2023 Lawsuit",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Copyright infringement in frontier LLMs has received much attention recently due to the New York Times v. OpenAI lawsuit, filed in December 2023. The New York Times claims that GPT-4 has infringed its copyrights by reproducing articles for use in LLM training and by memorizing the inputs, thereby publicly displaying them in LLM outputs. Our work aims to measure the propensity of OpenAI's LLMs to exhibit verbatim memorization in its outputs relative to other LLMs, specifically focusing on news articles. We discover that both GPT and Claude models use refusal training and output filters to prevent verbatim output of the memorized articles. We apply a basic prompt template to bypass the refusal training and show that OpenAI models are currently less prone to memorization elicitation than models from Meta, Mistral, and Anthropic. We find that as models increase in size, especially beyond 100 billion parameters, they demonstrate significantly greater capacity for memorization. Our findings have practical implications for training: more attention must be placed on preventing verbatim memorization in very large models. Our findings also have legal significance: in assessing the relative memorization capacity of OpenAI's LLMs, we probe the strength of The New York Times's copyright infringement claims and OpenAI's legal defenses, while underscoring issues at the intersection of generative AI, law, and policy.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06380",
        "abstract url": "https://arxiv.org/abs/2412.06380",
        "title": "Low-Rank Matrix Factorizations with Volume-based Constraints and Regularizations",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Low-rank matrix factorizations are a class of linear models widely used in various fields such as machine learning, signal processing, and data analysis. These models approximate a matrix as the product of two smaller matrices, where the left matrix captures latent features while the right matrix linearly decomposes the data based on these features. There are many ways to define what makes a component \"important.\" Standard LRMFs, such as the truncated singular value decomposition, focus on minimizing the distance between the original matrix and its low-rank approximation. In this thesis, the notion of \"importance\" is closely linked to interpretability and uniqueness, which are key to obtaining reliable and meaningful results. This thesis thus focuses on volume-based constraints and regularizations designed to enhance interpretability and uniqueness. We first introduce two new volume-constrained LRMFs designed to enhance these properties. The first assumes that data points are naturally bounded (e.g., movie ratings between 1 and 5 stars) and can be explained by convex combinations of features within the same bounds, allowing them to be interpreted in the same way as the data. The second model is more general, constraining the factors to belong to convex polytopes. Then, two variants of volume-regularized LRMFs are proposed. The first minimizes the volume of the latent features, encouraging them to cluster closely together, while the second maximizes the volume of the decompositions, promoting sparse representations. Across all these models, uniqueness is achieved under the core principle that the factors must be \"sufficiently scattered\" within their respective feasible sets. Motivated by applications such as blind source separation and missing data imputation, this thesis also proposes efficient algorithms that make these models practical for real-world applications.",
        "subjects": [
            "cs.LG",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06381",
        "abstract url": "https://arxiv.org/abs/2412.06381",
        "title": "Gentle robustness implies Generalization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Robustness and generalization ability of machine learning models are of utmost importance in various application domains. There is a wide interest in efficient ways to analyze those properties. One important direction is to analyze connection between those two properties. Prior theories suggest that a robust learning algorithm can produce trained models with a high generalization ability. However, we show in this work that the existing error bounds are vacuous for the Bayes optimal classifier which is the best among all measurable classifiers for a classification problem with overlapping classes. Those bounds cannot converge to the true error of this ideal classifier. This is undesirable, surprizing, and never known before. We then present a class of novel bounds, which are model-dependent and provably tighter than the existing robustness-based ones. Unlike prior ones, our bounds are guaranteed to converge to the true error of the best classifier, as the number of samples increases. We further provide an extensive experiment and find that two of our bounds are often non-vacuous for a large class of deep neural networks, pretrained from ImageNet.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06390",
        "abstract url": "https://arxiv.org/abs/2412.06390",
        "title": "Edge Delayed Deep Deterministic Policy Gradient: efficient continuous control for edge scenarios",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Deep Reinforcement Learning is gaining increasing attention thanks to its capability to learn complex policies in high-dimensional settings. Recent advancements utilize a dual-network architecture to learn optimal policies through the Q-learning algorithm. However, this approach has notable drawbacks, such as an overestimation bias that can disrupt the learning process and degrade the performance of the resulting policy. To address this, novel algorithms have been developed that mitigate overestimation bias by employing multiple Q-functions. Edge scenarios, which prioritize privacy, have recently gained prominence. In these settings, limited computational resources pose a significant challenge for complex Machine Learning approaches, making the efficiency of algorithms crucial for their performance. In this work, we introduce a novel Reinforcement Learning algorithm tailored for edge scenarios, called Edge Delayed Deep Deterministic Policy Gradient (EdgeD3). EdgeD3 enhances the Deep Deterministic Policy Gradient (DDPG) algorithm, achieving significantly improved performance with $25\\%$ less Graphics Process Unit (GPU) time while maintaining the same memory usage. Additionally, EdgeD3 consistently matches or surpasses the performance of state-of-the-art methods across various benchmarks, all while using $30\\%$ fewer computational resources and requiring $30\\%$ less memory.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06410",
        "abstract url": "https://arxiv.org/abs/2412.06410",
        "title": "BatchTopK Sparse Autoencoders",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting language model activations by decomposing them into sparse, interpretable features. A popular approach is the TopK SAE, that uses a fixed number of the most active latents per sample to reconstruct the model activations. We introduce BatchTopK SAEs, a training method that improves upon TopK SAEs by relaxing the top-k constraint to the batch-level, allowing for a variable number of latents to be active per sample. As a result, BatchTopK adaptively allocates more or fewer latents depending on the sample, improving reconstruction without sacrificing average sparsity. We show that BatchTopK SAEs consistently outperform TopK SAEs in reconstructing activations from GPT-2 Small and Gemma 2 2B, and achieve comparable performance to state-of-the-art JumpReLU SAEs. However, an advantage of BatchTopK is that the average number of latents can be directly specified, rather than approximately tuned through a costly hyperparameter sweep. We provide code for training and evaluating BatchTopK SAEs at https://github.com/bartbussmann/BatchTopK",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06414",
        "abstract url": "https://arxiv.org/abs/2412.06414",
        "title": "Federated Split Learning with Model Pruning and Gradient Quantization in Wireless Networks",
        "rating": "0.5",
        "keywords": [
            [
                "efficient fine-tuning"
            ],
            [
                "federated learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "As a paradigm of distributed machine learning, federated learning typically requires all edge devices to train a complete model locally. However, with the increasing scale of artificial intelligence models, the limited resources on edge devices often become a bottleneck for efficient fine-tuning. To address this challenge, federated split learning (FedSL) implements collaborative training across the edge devices and the server through model splitting. In this paper, we propose a lightweight FedSL scheme, that further alleviates the training burden on resource-constrained edge devices by pruning the client-side model dynamicly and using quantized gradient updates to reduce computation overhead. Additionally, we apply random dropout to the activation values at the split layer to reduce communication overhead. We conduct theoretical analysis to quantify the convergence performance of the proposed scheme. Finally, simulation results verify the effectiveness and advantages of the proposed lightweight FedSL in wireless network environments.",
        "subjects": [
            "cs.LG",
            "cs.DC",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06435",
        "abstract url": "https://arxiv.org/abs/2412.06435",
        "title": "Simulating Human-like Daily Activities with Desire-driven Autonomy",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Existing task-oriented AI agents often depend on explicit instructions or external rewards, limiting their ability to be driven by intrinsic motivations like humans. In this paper, we present a desire-driven autonomy framework to guide a Large Language Model-based (LLM-based) agent to simulate human-like daily activities. In contrast to previous agents, our Desire-driven Autonomous Agent (D2A) operates on the principle of intrinsic desire, allowing it to propose and select tasks that fulfill its motivational framework autonomously. Inspired by the Theory of Needs, the motivational framework incorporates an understanding of human-like desires, such as the need for social interaction, personal fulfillment, and self-care. Utilizing a desire-driven task generation mechanism, the agent evaluates its current state and takes a sequence of activities aligned with its intrinsic motivations. Through simulations, we demonstrate that our Desire-driven Autonomous Agent (D2A) generates coherent, contextually relevant daily activities while exhibiting variability and adaptability similar to human behavior. A comparative analysis with other LLM-based frameworks demonstrates that our approach significantly enhances the rationality of the simulated activities.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06436",
        "abstract url": "https://arxiv.org/abs/2412.06436",
        "title": "An Adaptively Inexact Method for Bilevel Learning Using Primal-Dual Style Differentiation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We consider a bilevel learning framework for learning linear operators. In this framework, the learnable parameters are optimized via a loss function that also depends on the minimizer of a convex optimization problem (denoted lower-level problem). We utilize an iterative algorithm called `piggyback' to compute the gradient of the loss and minimizer of the lower-level problem. Given that the lower-level problem is solved numerically, the loss function and thus its gradient can only be computed inexactly. To estimate the accuracy of the computed hypergradient, we derive an a-posteriori error bound, which provides guides for setting the tolerance for the lower-level problem, as well as the piggyback algorithm. To efficiently solve the upper-level optimization, we also propose an adaptive method for choosing a suitable step-size. To illustrate the proposed method, we consider a few learned regularizer problems, such as training an input-convex neural network.",
        "subjects": [
            "math.OC",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06478",
        "abstract url": "https://arxiv.org/abs/2412.06478",
        "title": "An inferential measure of dependence between two systems using Bayesian model comparison",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose to quantify dependence between two systems $X$ and $Y$ in a dataset $D$ based on the Bayesian comparison of two models: one, $H_0$, of statistical independence and another one, $H_1$, of dependence. In this framework, dependence between $X$ and $Y$ in $D$, denoted $B(X,Y|D)$, is quantified as $P(H_1|D)$, the posterior probability for the model of dependence given $D$, or any strictly increasing function thereof. It is therefore a measure of the evidence for dependence between $X$ and $Y$ as modeled by $H_1$ and observed in $D$. We review several statistical models and reconsider standard results in the light of $B(X,Y|D)$ as a measure of dependence. Using simulations, we focus on two specific issues: the effect of noise and the behavior of $B(X,Y|D)$ when $H_1$ has a parameter coding for the intensity of dependence. We then derive some general properties of $B(X,Y|D)$, showing that it quantifies the information contained in $D$ in favor of $H_1$ versus $H_0$. While some of these properties are typical of what is expected from a valid measure of dependence, others are novel and naturally appear as desired features for specific measures of dependence, which we call inferential. We finally put these results in perspective; in particular, we discuss the consequences of using the Bayesian framework as well as the similarities and differences between $B(X,Y|D)$ and mutual information.",
        "subjects": [
            "stat.ML",
            "cs.LG",
            "q-bio.QM"
        ],
        "comment": "To be published in IEEE Transaction on Systems, Man, and Cybernetics: Systems"
    },
    {
        "paper id": "2412.06486",
        "abstract url": "https://arxiv.org/abs/2412.06486",
        "title": "SimuDICE: Offline Policy Optimization Through World Model Updates and DICE Estimation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In offline reinforcement learning, deriving an effective policy from a pre-collected set of experiences is challenging due to the distribution mismatch between the target policy and the behavioral policy used to collect the data, as well as the limited sample size. Model-based reinforcement learning improves sample efficiency by generating simulated experiences using a learned dynamic model of the environment. However, these synthetic experiences often suffer from the same distribution mismatch. To address these challenges, we introduce SimuDICE, a framework that iteratively refines the initial policy derived from offline data using synthetically generated experiences from the world model. SimuDICE enhances the quality of these simulated experiences by adjusting the sampling probabilities of state-action pairs based on stationary DIstribution Correction Estimation (DICE) and the estimated confidence in the model's predictions. This approach guides policy improvement by balancing experiences similar to those frequently encountered with ones that have a distribution mismatch. Our experiments show that SimuDICE achieves performance comparable to existing algorithms while requiring fewer pre-collected experiences and planning steps, and it remains robust across varying data collection policies.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Published at BNAIC/BeNeLearn 2024"
    },
    {
        "paper id": "2412.06531",
        "abstract url": "https://arxiv.org/abs/2412.06531",
        "title": "Unraveling the Complexity of Memory in RL Agents: an Approach for Classification and Evaluation",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The incorporation of memory into agents is essential for numerous tasks within the domain of Reinforcement Learning (RL). In particular, memory is paramount for tasks that require the utilization of past information, adaptation to novel environments, and improved sample efficiency. However, the term ``memory'' encompasses a wide range of concepts, which, coupled with the lack of a unified methodology for validating an agent's memory, leads to erroneous judgments about agents' memory capabilities and prevents objective comparison with other memory-enhanced agents. This paper aims to streamline the concept of memory in RL by providing practical precise definitions of agent memory types, such as long-term versus short-term memory and declarative versus procedural memory, inspired by cognitive science. Using these definitions, we categorize different classes of agent memory, propose a robust experimental methodology for evaluating the memory capabilities of RL agents, and standardize evaluations. Furthermore, we empirically demonstrate the importance of adhering to the proposed methodology when evaluating different types of agent memory by conducting experiments with different RL agents and what its violation leads to.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "18 pages, 6 figures"
    },
    {
        "paper id": "2412.06540",
        "abstract url": "https://arxiv.org/abs/2412.06540",
        "title": "Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Scaling laws for large language models (LLMs) predict model performance based on parameters like size and training data. However, differences in training configurations and data processing across model families lead to significant variations in benchmark performance, making it difficult for a single scaling law to generalize across all LLMs. On the other hand, training family-specific scaling laws requires training models of varying sizes for every family. In this work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a novel scaling law that leverages publicly available benchmark data and assumes LLM performance is driven by low-dimensional latent skills, such as reasoning and instruction following. These latent skills are influenced by computational resources like model size and training tokens but with varying efficiencies across model families. Sloth exploits correlations across benchmarks to provide more accurate and interpretable predictions while alleviating the need to train multiple LLMs per family. We present both theoretical results on parameter identification and empirical evaluations on 12 prominent benchmarks, from Open LLM Leaderboard v1/v2, demonstrating that Sloth predicts LLM performance efficiently and offers insights into scaling behaviors for downstream tasks such as coding and emotional intelligence applications.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06545",
        "abstract url": "https://arxiv.org/abs/2412.06545",
        "title": "On How Iterative Magnitude Pruning Discovers Local Receptive Fields in Fully Connected Neural Networks",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Since its use in the Lottery Ticket Hypothesis, iterative magnitude pruning (IMP) has become a popular method for extracting sparse subnetworks that can be trained to high performance. Despite this, the underlying nature of IMP's general success remains unclear. One possibility is that IMP is especially capable of extracting and maintaining strong inductive biases. In support of this, recent work has shown that applying IMP to fully connected neural networks (FCNs) leads to the emergence of local receptive fields (RFs), an architectural feature present in mammalian visual cortex and convolutional neural networks. The question of how IMP is able to do this remains unanswered. Inspired by results showing that training FCNs on synthetic images with highly non-Gaussian statistics (e.g., sharp edges) is sufficient to drive the formation of local RFs, we hypothesize that IMP iteratively maximizes the non-Gaussian statistics present in the representations of FCNs, creating a feedback loop that enhances localization. We develop a new method for measuring the effect of individual weights on the statistics of the FCN representations (\"cavity method\"), which allows us to find evidence in support of this hypothesis. Our work, which is the first to study the effect IMP has on the representations of neural networks, sheds parsimonious light one way in which IMP can drive the formation of strong inductive biases.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "10 pages, 5 figures, comments welcome!"
    },
    {
        "paper id": "2412.06597",
        "abstract url": "https://arxiv.org/abs/2412.06597",
        "title": "Self-Interested Agents in Collaborative Learning: An Incentivized Adaptive Data-Centric Framework",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We propose a framework for adaptive data-centric collaborative learning among self-interested agents, coordinated by an arbiter. Designed to handle the incremental nature of real-world data, the framework operates in an online manner: at each step, the arbiter collects a batch of data from agents, trains a machine learning model, and provides each agent with a distinct model reflecting its data contributions. This setup establishes a feedback loop where shared data influence model updates, and the resulting models guide future data-sharing strategies. Agents evaluate and partition their data, selecting a partition to share using a stochastic parameterized policy optimized via policy gradient methods to optimize the utility of the received model as defined by agent-specific evaluation functions. On the arbiter side, the expected loss function over the true data distribution is optimized, incorporating agent-specific weights to account for distributional differences arising from diverse sources and selective sharing. A bilevel optimization algorithm jointly learns the model parameters and agent-specific weights. Mean-zero noise, computed using a distortion function that adjusts these agent-specific weights, is introduced to generate distinct agent-specific models, promoting valuable data sharing without requiring separate training. Our framework is underpinned by non-asymptotic analyses, ensuring convergence of the agent-side policy optimization to an approximate stationary point of the evaluation functions and convergence of the arbiter-side optimization to an approximate stationary point of the expected loss function.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06604",
        "abstract url": "https://arxiv.org/abs/2412.06604",
        "title": "VOPy: A Framework for Black-box Vector Optimization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We introduce VOPy, an open-source Python library designed to address black-box vector optimization, where multiple objectives must be optimized simultaneously with respect to a partial order induced by a convex cone. VOPy extends beyond traditional multi-objective optimization (MOO) tools by enabling flexible, cone-based ordering of solutions; with an application scope that includes environments with observation noise, discrete or continuous design spaces, limited budgets, and batch observations. VOPy provides a modular architecture, facilitating the integration of existing methods and the development of novel algorithms. We detail VOPy's architecture, usage, and potential to advance research and application in the field of vector optimization. The source code for VOPy is available at https://github.com/Bilkent-CYBORG/VOPy.",
        "subjects": [
            "cs.LG",
            "stat.AP",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06606",
        "abstract url": "https://arxiv.org/abs/2412.06606",
        "title": "Vulnerability of Text-Matching in ML/AI Conference Reviewer Assignments to Collusions",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the peer review process of top-tier machine learning (ML) and artificial intelligence (AI) conferences, reviewers are assigned to papers through automated methods. These assignment algorithms consider two main factors: (1) reviewers' expressed interests indicated by their bids for papers, and (2) reviewers' domain expertise inferred from the similarity between the text of their previously published papers and the submitted manuscripts. A significant challenge these conferences face is the existence of collusion rings, where groups of researchers manipulate the assignment process to review each other's papers, providing positive evaluations regardless of their actual quality. Most efforts to combat collusion rings have focused on preventing bid manipulation, under the assumption that the text similarity component is secure. In this paper, we demonstrate that even in the absence of bidding, colluding reviewers and authors can exploit the machine learning based text-matching component of reviewer assignment used at top ML/AI venues to get assigned their target paper. We also highlight specific vulnerabilities within this system and offer suggestions to enhance its robustness.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "cs.DL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06651",
        "abstract url": "https://arxiv.org/abs/2412.06651",
        "title": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur automatischen Bewertung von Hausaufgaben",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "[Study in German language.] This study examines the AI-powered grading tool \"AI Grading Assistant\" by the German company Fobizz, designed to support teachers in evaluating and providing feedback on student assignments. Against the societal backdrop of an overburdened education system and rising expectations for artificial intelligence as a solution to these challenges, the investigation evaluates the tool's functional suitability through two test series. The results reveal significant shortcomings: The tool's numerical grades and qualitative feedback are often random and do not improve even when its suggestions are incorporated. The highest ratings are achievable only with texts generated by ChatGPT. False claims and nonsensical submissions frequently go undetected, while the implementation of some grading criteria is unreliable and opaque. Since these deficiencies stem from the inherent limitations of large language models (LLMs), fundamental improvements to this or similar tools are not immediately foreseeable. The study critiques the broader trend of adopting AI as a quick fix for systemic problems in education, concluding that Fobizz's marketing of the tool as an objective and time-saving solution is misleading and irresponsible. Finally, the study calls for systematic evaluation and subject-specific pedagogical scrutiny of the use of AI tools in educational contexts.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "32 pages, in German language"
    },
    {
        "paper id": "2412.06655",
        "abstract url": "https://arxiv.org/abs/2412.06655",
        "title": "Off-Policy Maximum Entropy RL with Future State and Action Visitation Measures",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We introduce a new maximum entropy reinforcement learning framework based on the distribution of states and actions visited by a policy. More precisely, an intrinsic reward function is added to the reward function of the Markov decision process that shall be controlled. For each state and action, this intrinsic reward is the relative entropy of the discounted distribution of states and actions (or features from these states and actions) visited during the next time steps. We first prove that an optimal exploration policy, which maximizes the expected discounted sum of intrinsic rewards, is also a policy that maximizes a lower bound on the state-action value function of the decision process under some assumptions. We also prove that the visitation distribution used in the intrinsic reward definition is the fixed point of a contraction operator. Following, we describe how to adapt existing algorithms to learn this fixed point and compute the intrinsic rewards to enhance exploration. A new practical off-policy maximum entropy reinforcement learning algorithm is finally introduced. Empirically, exploration policies have good state-action space coverage, and high-performing control policies are computed efficiently.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06681",
        "abstract url": "https://arxiv.org/abs/2412.06681",
        "title": "Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual Framework",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "In transportation system demand modeling and simulation, agent-based models and microsimulations are current state-of-the-art approaches. However, existing agent-based models still have some limitations on behavioral realism and resource demand that limit their applicability. In this study, leveraging the emerging technology of large language models (LLMs) and LLM-based agents, we propose a general LLM-agent-based modeling framework for transportation systems. We argue that LLM agents not only possess the essential capabilities to function as agents but also offer promising solutions to overcome some limitations of existing agent-based models. Our conceptual framework design closely replicates the decision-making and interaction processes and traits of human travelers within transportation networks, and we demonstrate that the proposed systems can meet critical behavioral criteria for decision-making and learning behaviors using related studies and a demonstrative example of LLM agents' learning and adjustment in the bottleneck setting. Although further refinement of the LLM-agent-based modeling framework is necessary, we believe that this approach has the potential to improve transportation system modeling and simulation.",
        "subjects": [
            "cs.AI",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06686",
        "abstract url": "https://arxiv.org/abs/2412.06686",
        "title": "Some Best Practices in Operator Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Hyperparameters searches are computationally expensive. This paper studies some general choices of hyperparameters and training methods specifically for operator learning. It considers the architectures DeepONets, Fourier neural operators and Koopman autoencoders for several differential equations to find robust trends. Some options considered are activation functions, dropout and stochastic weight averaging.",
        "subjects": [
            "cs.LG",
            "physics.comp-ph"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2412.04578"
    },
    {
        "paper id": "2412.06694",
        "abstract url": "https://arxiv.org/abs/2412.06694",
        "title": "Digital Transformation in the Water Distribution System based on the Digital Twins Concept",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "Digital Twins have emerged as a disruptive technology with great potential; they can enhance WDS by offering real-time monitoring, predictive maintenance, and optimization capabilities. This paper describes the development of a state-of-the-art DT platform for WDS, introducing advanced technologies such as the Internet of Things, Artificial Intelligence, and Machine Learning models. This paper provides insight into the architecture of the proposed platform-CAUCCES-that, informed by both historical and meteorological data, effectively deploys AI/ML models like LSTM networks, Prophet, LightGBM, and XGBoost in trying to predict water consumption patterns. Furthermore, we delve into how optimization in the maintenance of WDS can be achieved by formulating a Constraint Programming problem for scheduling, hence minimizing the operational cost efficiently with reduced environmental impacts. It also focuses on cybersecurity and protection to ensure the integrity and reliability of the DT platform. In this view, the system will contribute to improvements in decision-making capabilities, operational efficiency, and system reliability, with reassurance being drawn from the important role it can play toward sustainable management of water resources.",
        "subjects": [
            "cs.CY",
            "cs.AI"
        ],
        "comment": "78 pages, 18 figures"
    },
    {
        "paper id": "2412.06861",
        "abstract url": "https://arxiv.org/abs/2412.06861",
        "title": "Mining Limited Data Sufficiently: A BERT-inspired Approach for CSI Time Series Application in Wireless Communication and Sensing",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Channel State Information (CSI) is the cornerstone in both wireless communication and sensing systems. In wireless communication systems, CSI provides essential insights into channel conditions, enabling system optimizations like channel compensation and dynamic resource allocation. However, the high computational complexity of CSI estimation algorithms necessitates the development of fast deep learning methods for CSI prediction. In wireless sensing systems, CSI can be leveraged to infer environmental changes, facilitating various functions, including gesture recognition and people identification. Deep learning methods have demonstrated significant advantages over model-based approaches in these fine-grained CSI classification tasks, particularly when classes vary across different scenarios. However, a major challenge in training deep learning networks for wireless systems is the limited availability of data, further complicated by the diverse formats of many public datasets, which hinder integration. Additionally, collecting CSI data can be resource-intensive, requiring considerable time and manpower. To address these challenges, we propose CSI-BERT2 for CSI prediction and classification tasks, effectively utilizing limited data through a pre-training and fine-tuning approach. Building on CSI-BERT1, we enhance the model architecture by introducing an Adaptive Re-Weighting Layer (ARL) and a Multi-Layer Perceptron (MLP) to better capture sub-carrier and timestamp information, effectively addressing the permutation-invariance problem. Furthermore, we propose a Mask Prediction Model (MPM) fine-tuning method to improve the model's adaptability for CSI prediction tasks. Experimental results demonstrate that CSI-BERT2 achieves state-of-the-art performance across all tasks.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06865",
        "abstract url": "https://arxiv.org/abs/2412.06865",
        "title": "FP=xINT:A Low-Bit Series Expansion Algorithm for Post-Training Quantization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Post-Training Quantization (PTQ) converts pre-trained Full-Precision (FP) models into quantized versions without training. While existing methods reduce size and computational costs, they also significantly degrade performance and quantization efficiency at extremely low settings due to quantization noise. We introduce a deep model series expansion framework to address this issue, enabling rapid and accurate approximation of unquantized models without calibration sets or fine-tuning. This is the first use of series expansion for neural network quantization. Specifically, our method expands the FP model into multiple low-bit basis models. To ensure accurate quantization, we develop low-bit basis model expansions at different granularities (tensor, layer, model), and theoretically confirm their convergence to the dense model, thus restoring FP model accuracy. Additionally, we design AbelianAdd/Mul operations between isomorphic models in the low-bit expansion, forming an Abelian group to ensure operation parallelism and commutativity. The experiments show that our algorithm achieves state-of-the-art performance in low-bit settings; for example, 4-bit quantization of ResNet-50 surpasses the original accuracy, reaching 77.03%. The code will be made public.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": "Under Review"
    },
    {
        "paper id": "2412.06867",
        "abstract url": "https://arxiv.org/abs/2412.06867",
        "title": "Lossless Model Compression via Joint Low-Rank Factorization Optimization",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Low-rank factorization is a popular model compression technique that minimizes the error $\u03b4$ between approximated and original weight matrices. Despite achieving performances close to the original models when $\u03b4$ is optimized, a performance discrepancy remains due to the separate optimization processes for low-rank factorization and model performance, resulting in unavoidable losses. We address this issue by introducing a novel joint optimization strategy for lossless low-rank weight factorization, which, for the first time, enhances the model's performance beyond the original. Our approach begins with a theoretical analysis of the relationship between low-rank factorization and model optimization objectives, establishing a precise perturbation range for matrix factorization errors on model performance. This challenge is then reformulated as a numerical rank deficiency problem with inequality constraints and develop a joint objective that simultaneously addresses factorization error and model performance. Based on the above analysis, we propose two optimization algorithms: \\textbf{a lossless optimization algorithm} that maximizes model accuracy while ensuring compression, and \\textbf{a compact optimization algorithm} that minimizes model size while preserving performance. These algorithms do not require fine-tuning and can directly compress numerous deep models to achieve lossless results. Our methods demonstrate robust efficacy across various vision and language tasks. For example, the compressed model reduced by 70\\% on ResNext50 outperforms the original. Our code will be made public.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CC"
        ],
        "comment": "Under Review"
    },
    {
        "paper id": "2412.06870",
        "abstract url": "https://arxiv.org/abs/2412.06870",
        "title": "Variable Selection for Comparing High-dimensional Time-Series Data",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Given a pair of multivariate time-series data of the same length and dimensions, an approach is proposed to select variables and time intervals where the two series are significantly different. In applications where one time series is an output from a computationally expensive simulator, the approach may be used for validating the simulator against real data, for comparing the outputs of two simulators, and for validating a machine learning-based emulator against the simulator. With the proposed approach, the entire time interval is split into multiple subintervals, and on each subinterval, the two sample sets are compared to select variables that distinguish their distributions and a two-sample test is performed. The validity and limitations of the proposed approach are investigated in synthetic data experiments. Its usefulness is demonstrated in an application with a particle-based fluid simulator, where a deep neural network model is compared against the simulator, and in an application with a microscopic traffic simulator, where the effects of changing the simulator's parameters on traffic flows are analysed.",
        "subjects": [
            "stat.ME",
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06875",
        "abstract url": "https://arxiv.org/abs/2412.06875",
        "title": "VQ4ALL: Efficient Neural Network Representation via a Universal Codebook",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The rapid growth of the big neural network models puts forward new requirements for lightweight network representation methods. The traditional methods based on model compression have achieved great success, especially VQ technology which realizes the high compression ratio of models by sharing code words. However, because each layer of the network needs to build a code table, the traditional top-down compression technology lacks attention to the underlying commonalities, resulting in limited compression rate and frequent memory access. In this paper, we propose a bottom-up method to share the universal codebook among multiple neural networks, which not only effectively reduces the number of codebooks but also further reduces the memory access and chip area by storing static code tables in the built-in ROM. Specifically, we introduce VQ4ALL, a VQ-based method that utilizes codewords to enable the construction of various neural networks and achieve efficient representations. The core idea of our method is to adopt a kernel density estimation approach to extract a universal codebook and then progressively construct different low-bit networks by updating differentiable assignments. Experimental results demonstrate that VQ4ALL achieves compression rates exceeding 16 $\\times$ while preserving high accuracy across multiple network architectures, highlighting its effectiveness and versatility.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06908",
        "abstract url": "https://arxiv.org/abs/2412.06908",
        "title": "Framework to coordinate ubiquitous devices with SOA standards",
        "rating": "0.5",
        "keywords": [
            [
                "cs.CY"
            ]
        ],
        "abstract": "Context: Ubiquitous devices and pervasive environments are in permanent interaction in people's daily lives. In today's hyper-connected environments, it is necessary for these devices to interact with each other, transparently to the users. The problem is analyzed from the different perspectives that compose it: SOA, service composition, interaction, and the capabilities of ubiquitous devices. Problem: Currently, ubiquitous devices can interact in a limited way due to the proprietary mechanisms and protocols available on the market. The few proposals from academia have hardly achieved an impact in practice. This is not in harmony with the situation of the Internet environment and web services, which have standardized mechanisms for service composition. Aim: Apply the principles of SOA, currently standardized and tested in the information systems industry, for the connectivity of ubiquitous devices in pervasive environments. For this, a coordination framework based on these technologies is proposed. Methodology: We apply an adaptation of Design Science in our environment to allow the iterative construction and evaluation of prototypes. For this, a proof of concept is developed on which this methodology and its cycles are based. Results: We built and put into operation a coordination framework for ubiquitous devices based on WS-CDL, along with a proof of concept. In addition, we contribute to the WS-CDL language in order to support the characteristics of specific ubiquitous devices.",
        "subjects": [
            "cs.SE",
            "cs.CY",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06940",
        "abstract url": "https://arxiv.org/abs/2412.06940",
        "title": "Digital Twin-Empowered Voltage Control for Power Systems",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Emerging digital twin technology has the potential to revolutionize voltage control in power systems. However, the state-of-the-art digital twin method suffers from low computational and sampling efficiency, which hinders its applications. To address this issue, we propose a Gumbel-Consistency Digital Twin (GC-DT) method that enhances voltage control with improved computational and sampling efficiency. First, the proposed method incorporates a Gumbel-based strategy improvement that leverages the Gumbel-top trick to enhance non-repetitive sampling actions and reduce the reliance on Monte Carlo Tree Search simulations, thereby improving computational efficiency. Second, a consistency loss function aligns predicted hidden states with actual hidden states in the latent space, which increases both prediction accuracy and sampling efficiency. Experiments on IEEE 123-bus, 34-bus, and 13-bus systems demonstrate that the proposed GC-DT outperforms the state-of-the-art DT method in both computational and sampling efficiency.",
        "subjects": [
            "cs.LG",
            "eess.SP"
        ],
        "comment": "6 pages, 1 figure, conference paper"
    },
    {
        "paper id": "2412.06947",
        "abstract url": "https://arxiv.org/abs/2412.06947",
        "title": "PyraNet: A Multi-Layered Hierarchical Dataset for Verilog",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Recently, there has been a growing interest in leveraging Large Language Models for Verilog code generation. However, the current quality of the generated Verilog code remains suboptimal. This is largely due to the absence of well-defined, well-organized datasets with high-quality samples, as well as a lack of innovative fine-tuning methods and models specifically trained on Verilog. In this paper, we introduce a novel open-source dataset and a corresponding fine-tuning technique, which utilizes a multi-layered structure that we refer to as PyraNet. Our experiments demonstrate that employing the proposed dataset and fine-tuning approach leads to a more accurate fine-tuned model, producing syntactically and functionally correct Verilog code. The evaluation results show improvements by up-to $32.6\\%$ in comparison to the CodeLlama-7B baseline model and up-to $16.7\\%$ in comparison to the state-of-the-art models using VerilogEval evaluation platform.",
        "subjects": [
            "cs.AR",
            "cs.AI",
            "cs.LG",
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06989",
        "abstract url": "https://arxiv.org/abs/2412.06989",
        "title": "Learning about algorithm auditing in five steps: scaffolding how high school youth can systematically and critically evaluate machine learning applications",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "While there is widespread interest in supporting young people to critically evaluate machine learning-powered systems, there is little research on how we can support them in inquiring about how these systems work and what their limitations and implications may be. Outside of K-12 education, an effective strategy in evaluating black-boxed systems is algorithm auditing-a method for understanding algorithmic systems' opaque inner workings and external impacts from the outside in. In this paper, we review how expert researchers conduct algorithm audits and how end users engage in auditing practices to propose five steps that, when incorporated into learning activities, can support young people in auditing algorithms. We present a case study of a team of teenagers engaging with each step during an out-of-school workshop in which they audited peer-designed generative AI TikTok filters. We discuss the kind of scaffolds we provided to support youth in algorithm auditing and directions and challenges for integrating algorithm auditing into classroom activities. This paper contributes: (a) a conceptualization of five steps to scaffold algorithm auditing learning activities, and (b) examples of how youth engaged with each step during our pilot study.",
        "subjects": [
            "cs.HC",
            "cs.AI",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06990",
        "abstract url": "https://arxiv.org/abs/2412.06990",
        "title": "The Oracle Complexity of Simplex-based Matrix Games: Linear Separability and Nash Equilibria",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study the problem of solving matrix games of the form $\\max_{\\mathbf{w}\\in\\mathcal{W}}\\min_{\\mathbf{p}\\in\u0394}\\mathbf{p}^{\\top}A\\mathbf{w}$, where $A$ is some matrix and $\u0394$ is the probability simplex. This problem encapsulates canonical tasks such as finding a linear separator and computing Nash equilibria in zero-sum games. However, perhaps surprisingly, its inherent complexity (as formalized in the standard framework of oracle complexity [Nemirovski and Yudin, 1983]) is not well-understood. In this work, we first identify different oracle models which are implicitly used by prior algorithms, amounting to multiplying the matrix $A$ by a vector from either one or both sides. We then prove complexity lower bounds for algorithms under both access models, which in particular imply a separation between them. Specifically, we start by proving that algorithms for linear separability based on one-sided multiplications must require $\u03a9(\u03b3_A^{-2})$ iterations, where $\u03b3_A$ is the margin, as matched by the Perceptron algorithm. We then prove that accelerated algorithms for this task, which utilize multiplications from both sides, must require $\\tilde\u03a9(\u03b3_{A}^{-2/3})$ iterations, establishing the first oracle complexity barrier for such algorithms. Finally, by adapting our lower bound to $\\ell_1$ geometry, we prove that computing an $\u03b5$-approximate Nash equilibrium requires $\\tilde\u03a9(\u03b5^{-2/5})$ iterations, which is an exponential improvement over the previously best-known lower bound due to Hadiji et al. [2024].",
        "subjects": [
            "cs.GT",
            "cs.LG",
            "math.OC"
        ],
        "comment": "25 pages"
    },
    {
        "paper id": "2412.07000",
        "abstract url": "https://arxiv.org/abs/2412.07000",
        "title": "Extreme AutoML: Analysis of Classification, Regression, and NLP Performance",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Utilizing machine learning techniques has always required choosing hyperparameters. This is true whether one uses a classical technique such as a KNN or very modern neural networks such as Deep Learning. Though in many applications, hyperparameters are chosen by hand, automated methods have become increasingly more common. These automated methods have become collectively known as automated machine learning, or AutoML. Several automated selection algorithms have shown similar or improved performance over state-of-the-art methods. This breakthrough has led to the development of cloud-based services like Google AutoML, which is based on Deep Learning and is widely considered to be the industry leader in AutoML services. Extreme Learning Machines (ELMs) use a fundamentally different type of neural architecture, producing better results at a significantly discounted computational cost. We benchmark the Extreme AutoML technology against Google's AutoML using several popular classification data sets from the University of California at Irvine's (UCI) repository, and several other data sets, observing significant advantages for Extreme AutoML in accuracy, Jaccard Indices, the variance of Jaccard Indices across classes (i.e. class variance) and training times.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07003",
        "abstract url": "https://arxiv.org/abs/2412.07003",
        "title": "Understanding Gradient Descent through the Training Jacobian",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "We examine the geometry of neural network training using the Jacobian of trained network parameters with respect to their initial values. Our analysis reveals low-dimensional structure in the training process which is dependent on the input data but largely independent of the labels. We find that the singular value spectrum of the Jacobian matrix consists of three distinctive regions: a \"chaotic\" region of values orders of magnitude greater than one, a large \"bulk\" region of values extremely close to one, and a \"stable\" region of values less than one. Along each bulk direction, the left and right singular vectors are nearly identical, indicating that perturbations to the initialization are carried through training almost unchanged. These perturbations have virtually no effect on the network's output in-distribution, yet do have an effect far out-of-distribution. While the Jacobian applies only locally around a single initialization, we find substantial overlap in bulk subspaces for different random seeds.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07005",
        "abstract url": "https://arxiv.org/abs/2412.07005",
        "title": "In-Application Defense Against Evasive Web Scans through Behavioral Analysis",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Web traffic has evolved to include both human users and automated agents, ranging from benign web crawlers to adversarial scanners such as those capable of credential stuffing, command injection, and account hijacking at the web scale. The estimated financial costs of these adversarial activities are estimated to exceed tens of billions of dollars in 2023. In this work, we introduce WebGuard, a low-overhead in-application forensics engine, to enable robust identification and monitoring of automated web scanners, and help mitigate the associated security risks. WebGuard focuses on the following design criteria: (i) integration into web applications without any changes to the underlying software components or infrastructure, (ii) minimal communication overhead, (iii) capability for real-time detection, e.g., within hundreds of milliseconds, and (iv) attribution capability to identify new behavioral patterns and detect emerging agent categories. To this end, we have equipped WebGuard with multi-modal behavioral monitoring mechanisms, such as monitoring spatio-temporal data and browser events. We also design supervised and unsupervised learning architectures for real-time detection and offline attribution of human and automated agents, respectively. Information theoretic analysis and empirical evaluations are provided to show that multi-modal data analysis, as opposed to uni-modal analysis which relies solely on mouse movement dynamics, significantly improves time-to-detection and attribution accuracy. Various numerical evaluations using real-world data collected via WebGuard are provided achieving high accuracy in hundreds of milliseconds, with a communication overhead below 10 KB per second.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07021",
        "abstract url": "https://arxiv.org/abs/2412.07021",
        "title": "Sequential Compression Layers for Efficient Federated Learning in Foundational Models",
        "rating": "0.5",
        "keywords": [
            [
                "parameter efficient",
                "efficient fine-tuning"
            ],
            [
                "Federated Learning"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Federated Learning (FL) has gained popularity for fine-tuning large language models (LLMs) across multiple nodes, each with its own private data. While LoRA has been widely adopted for parameter efficient federated fine-tuning, recent theoretical and empirical studies highlight its suboptimal performance in the federated learning context. In response, we propose a novel, simple, and more effective parameter-efficient fine-tuning method that does not rely on LoRA. Our approach introduces a small multi-layer perceptron (MLP) layer between two existing MLP layers the up proj (the FFN projection layer following the self-attention module) and down proj within the feed forward network of the transformer block. This solution addresses the bottlenecks associated with LoRA in federated fine tuning and outperforms recent LoRA-based approaches, demonstrating superior performance for both language models and vision encoders.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07031",
        "abstract url": "https://arxiv.org/abs/2412.07031",
        "title": "Large Language Models: An Applied Econometric Framework",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Large language models (LLMs) are being used in economics research to form predictions, label text, simulate human responses, generate hypotheses, and even produce data for times and places where such data don't exist. While these uses are creative, are they valid? When can we abstract away from the inner workings of an LLM and simply rely on their outputs? We develop an econometric framework to answer this question. Our framework distinguishes between two types of empirical tasks. Using LLM outputs for prediction problems (including hypothesis generation) is valid under one condition: no \"leakage\" between the LLM's training dataset and the researcher's sample. Using LLM outputs for estimation problems to automate the measurement of some economic concept (expressed by some text or from human subjects) requires an additional assumption: LLM outputs must be as good as the gold standard measurements they replace. Otherwise estimates can be biased, even if LLM outputs are highly accurate but not perfectly so. We document the extent to which these conditions are violated and the implications for research findings in illustrative applications to finance and political economy. We also provide guidance to empirical researchers. The only way to ensure no training leakage is to use open-source LLMs with documented training data and published weights. The only way to deal with LLM measurement error is to collect validation data and model the error structure. A corollary is that if such conditions can't be met for a candidate LLM application, our strong advice is: don't.",
        "subjects": [
            "econ.EM",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07042",
        "abstract url": "https://arxiv.org/abs/2412.07042",
        "title": "Generative AI Impact on Labor Market: Analyzing ChatGPT's Demand in Job Advertisements",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "The rapid advancement of Generative AI (Gen AI) technologies, particularly tools like ChatGPT, is significantly impacting the labor market by reshaping job roles and skill requirements. This study examines the demand for ChatGPT-related skills in the U.S. labor market by analyzing job advertisements collected from major job platforms between May and December 2023. Using text mining and topic modeling techniques, we extracted and analyzed the Gen AI-related skills that employers are hiring for. Our analysis identified five distinct ChatGPT-related skill sets: general familiarity, creative content generation, marketing, advanced functionalities (such as prompt engineering), and product development. In addition, the study provides insights into job attributes such as occupation titles, degree requirements, salary ranges, and other relevant job characteristics. These findings highlight the increasing integration of Gen AI across various industries, emphasizing the growing need for both foundational knowledge and advanced technical skills. The study offers valuable insights into the evolving demands of the labor market, as employers seek candidates equipped to leverage generative AI tools to improve productivity, streamline processes, and drive innovation.",
        "subjects": [
            "cs.CY",
            "cs.AI",
            "econ.GN"
        ],
        "comment": "20 pages, 4 figures, 2 tables, submitted to International Journal of Information Management to be reviewed"
    },
    {
        "paper id": "2412.07057",
        "abstract url": "https://arxiv.org/abs/2412.07057",
        "title": "A Note on Sample Complexity of Interactive Imitation Learning with Log Loss",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Imitation learning (IL) is a general paradigm for learning from experts in sequential decision-making problems. Recent advancements in IL have shown that offline imitation learning, specifically Behavior Cloning (BC) with log loss, is minimax optimal. Meanwhile, its interactive counterpart, DAgger, is shown to suffer from suboptimal sample complexity. In this note, we focus on realizable deterministic expert and revisit interactive imitation learning, particularly DAgger with log loss. We demonstrate: 1. A one-sample-per-round DAgger variant that outperforms BC in state-wise annotation. 2. Without recoverability assumption, DAgger with first-step mixture policies matches the performance of BC. Along the analysis, we introduce a new notion of decoupled Hellinger distance that separates state and action sequences, which can be of independent interest.",
        "subjects": [
            "stat.ML",
            "cs.LG"
        ],
        "comment": "18 pages"
    },
    {
        "paper id": "2412.07066",
        "abstract url": "https://arxiv.org/abs/2412.07066",
        "title": "The Mirage of Artificial Intelligence Terms of Use Restrictions",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Artificial intelligence (AI) model creators commonly attach restrictive terms of use to both their models and their outputs. These terms typically prohibit activities ranging from creating competing AI models to spreading disinformation. Often taken at face value, these terms are positioned by companies as key enforceable tools for preventing misuse, particularly in policy dialogs. But are these terms truly meaningful? There are myriad examples where these broad terms are regularly and repeatedly violated. Yet except for some account suspensions on platforms, no model creator has actually tried to enforce these terms with monetary penalties or injunctive relief. This is likely for good reason: we think that the legal enforceability of these licenses is questionable. This Article systematically assesses of the enforceability of AI model terms of use and offers three contributions. First, we pinpoint a key problem: the artifacts that they protect, namely model weights and model outputs, are largely not copyrightable, making it unclear whether there is even anything to be licensed. Second, we examine the problems this creates for other enforcement. Recent doctrinal trends in copyright preemption may further undermine state-law claims, while other legal frameworks like the DMCA and CFAA offer limited recourse. Anti-competitive provisions likely fare even worse than responsible use provisions. Third, we provide recommendations to policymakers. There are compelling reasons for many provisions to be unenforceable: they chill good faith research, constrain competition, and create quasi-copyright ownership where none should exist. There are, of course, downsides: model creators have fewer tools to prevent harmful misuse. But we think the better approach is for statutory provisions, not private fiat, to distinguish between good and bad uses of AI, restricting the latter.",
        "subjects": [
            "cs.CY",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Forthcoming Indiana Law Journal"
    },
    {
        "paper id": "2412.07067",
        "abstract url": "https://arxiv.org/abs/2412.07067",
        "title": "MoE-CAP: Cost-Accuracy-Performance Benchmarking for Mixture-of-Experts Systems",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for scaling Large Language Models (LLMs) efficiently; however, MoE systems rely on heterogeneous compute and memory resources. These factors collectively influence the system's Cost, Accuracy, and Performance (CAP), creating a challenging trade-off. Current benchmarks often fail to provide precise estimates of these effects, complicating practical considerations for deploying MoE systems. To bridge this gap, we introduce MoE-CAP, a benchmark specifically designed to evaluate MoE systems. Our findings highlight the difficulty of achieving an optimal balance of cost, accuracy, and performance with existing hardware capabilities. MoE systems often necessitate compromises on one factor to optimize the other two, a dynamic we term the MoE-CAP trade-off. To identify the best trade-off, we propose novel performance evaluation metrics - Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS Utilization (S-MFU) - and develop cost models that account for the heterogeneous compute and memory hardware integral to MoE systems. This benchmark is publicly available on HuggingFace: https://huggingface.co/spaces/sparse-generative-ai/open-moe-llm-leaderboard.",
        "subjects": [
            "cs.LG",
            "cs.DC"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07069",
        "abstract url": "https://arxiv.org/abs/2412.07069",
        "title": "Enhancing radioisotope identification in gamma spectra with transfer learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Machine learning methods in gamma spectroscopy have the potential to provide accurate, real-time classification of unknown radioactive samples. However, obtaining sufficient experimental training data is often prohibitively expensive and time-consuming, and models trained solely on synthetic data can struggle to generalize to the unpredictable range of real-world operating scenarios. In this work, we pretrain a model using physically derived synthetic data and subsequently leverage transfer learning techniques to fine-tune the model for a specific target domain. This paradigm enables us to embed physical principles during the pretraining step, thus requiring less data from the target domain compared to classical machine learning methods. Results of this analysis indicate that fine-tuned models significantly outperform those trained exclusively on synthetic data or solely on target-domain data, particularly in the intermediate data regime (${\\approx} 10^4$ training samples). This conclusion is consistent across four different machine learning architectures (MLP, CNN, Transformer, and LSTM) considered in this study. This research serves as proof of concept for applying transfer learning techniques to application scenarios where access to experimental data is limited.",
        "subjects": [
            "cs.LG",
            "nucl-th"
        ],
        "comment": "11 pages and 4 figures"
    },
    {
        "paper id": "2412.07093",
        "abstract url": "https://arxiv.org/abs/2412.07093",
        "title": "Streaming Private Continual Counting via Binning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In differential privacy, $\\textit{continual observation}$ refers to problems in which we wish to continuously release a function of a dataset that is revealed one element at a time. The challenge is to maintain a good approximation while keeping the combined output over all time steps differentially private. In the special case of $\\textit{continual counting}$ we seek to approximate a sum of binary input elements. This problem has received considerable attention lately, in part due to its relevance in implementations of differentially private stochastic gradient descent. $\\textit{Factorization mechanisms}$ are the leading approach to continual counting, but the best such mechanisms do not work well in $\\textit{streaming}$ settings since they require space proportional to the size of the input. In this paper, we present a simple approach to approximating factorization mechanisms in low space via $\\textit{binning}$, where adjacent matrix entries with similar values are changed to be identical in such a way that a matrix-vector product can be maintained in sublinear space. Our approach has provable sublinear space guarantees for a class of lower triangular matrices whose entries are monotonically decreasing away from the diagonal. We show empirically that even with very low space usage we are able to closely match, and sometimes surpass, the performance of asymptotically optimal factorization mechanisms. Recently, and independently of our work, Dvijotham et al. have also suggested an approach to implementing factorization mechanisms in a streaming setting. Their work differs from ours in several respects: It only addresses factorization into $\\textit{Toeplitz}$ matrices, only considers $\\textit{maximum}$ error, and uses a different technique based on rational function approximation that seems less versatile than our binning approach.",
        "subjects": [
            "cs.LG",
            "cs.CR",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07094",
        "abstract url": "https://arxiv.org/abs/2412.07094",
        "title": "Access Point Deployment for Localizing Accuracy and User Rate in Cell-Free Systems",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Evolving next-generation mobile networks is designed to provide ubiquitous coverage and networked sensing. With utility of multi-view sensing and multi-node joint transmission, cell-free is a promising technique to realize this prospect. This paper aims to tackle the problem of access point (AP) deployment in cell-free systems to balance the sensing accuracy and user rate. By merging the D-optimality with Euclidean criterion, a novel integrated metric is proposed to be the objective function for both max-sum and max-min problems, which respectively guarantee the overall and lowest performance in multi-user communication and target tracking scenario. To solve the corresponding high dimensional non-convex multi-objective problem, the Soft actor-critic (SAC) is utilized to avoid risk of local optimal result. Numerical results demonstrate that proposed SAC-based APs deployment method achieves $20\\%$ of overall performance and $120\\%$ of lowest performance.",
        "subjects": [
            "cs.NI",
            "cs.AI"
        ],
        "comment": "Presented at MobiCom 2024"
    },
    {
        "paper id": "2412.07097",
        "abstract url": "https://arxiv.org/abs/2412.07097",
        "title": "On Evaluating the Durability of Safeguards for Open-Weight LLMs",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI"
            ]
        ],
        "abstract": "Stakeholders -- from model developers to policymakers -- seek to minimize the dual-use risks of large language models (LLMs). An open challenge to this goal is whether technical safeguards can impede the misuse of LLMs, even when models are customizable via fine-tuning or when model weights are fully open. In response, several recent studies have proposed methods to produce durable LLM safeguards for open-weight LLMs that can withstand adversarial modifications of the model's weights via fine-tuning. This holds the promise of raising adversaries' costs even under strong threat models where adversaries can directly fine-tune model weights. However, in this paper, we urge for more careful characterization of the limits of these approaches. Through several case studies, we demonstrate that even evaluating these defenses is exceedingly difficult and can easily mislead audiences into thinking that safeguards are more durable than they really are. We draw lessons from the evaluation pitfalls that we identify and suggest future research carefully cabin claims to more constrained, well-defined, and rigorously examined threat models, which can provide more useful and candid assessments to stakeholders.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07120",
        "abstract url": "https://arxiv.org/abs/2412.07120",
        "title": "Corrupted Learning Dynamics in Games",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Learning in games is the problem where multiple players interact in a shared environment, each aiming to minimize their own regret, and it is known that an approximate equilibrium can be obtained when all players employ no-regret algorithms. Notably, by adopting optimistic follow-the-regularized-leader (OFTRL), the regret of each player after $T$ rounds is constant in two-player zero-sum games, implying that an equilibrium can be computed at a faster rate of $O(1/T)$. However, this acceleration is limited to the honest regime, where all players fully adhere to the given algorithms. To address this limitation, this paper presents corrupted learning dynamics that adaptively find an equilibrium at a rate dependent on the degree of deviation by each player from the given algorithm's output. First, in two-player zero-sum games, we provide learning dynamics where the external regret of the x-player (and similarly for the y-player) in the corrupted regime is roughly bounded by $O(\\log (m_\\mathrm{x} m_\\mathrm{y}) + \\sqrt{C_\\mathrm{y}} + C_\\mathrm{x})$, which implies a convergence rate of $\\tilde{O}((\\sqrt{C_\\mathrm{y}} + C_\\mathrm{x})/T)$ to a Nash equilibrium. Here, $m_\\mathrm{x}$ and $m_\\mathrm{y}$ are the number of actions of the x- and y-players, respectively, and $C_\\mathrm{x}$ and $C_\\mathrm{y}$ are the cumulative deviations of the x- and y-players from their given algorithms. Furthermore, we extend our approach to multi-player general-sum games, showing that the swap regret of player $i$ in the corrupted regime is bounded by $O(\\log T + \\sqrt{\\sum_j C_j \\log T} + C_i)$, where $C_i$ is the cumulative deviations of player $i$ from the given algorithm. This implies a convergence rate of $O((\\log T + \\sqrt{\\sum_j C_j \\log T} + C_i)/T)$ to a correlated equilibrium. Our learning dynamics are agnostic to the corruption levels and are based on OFTRL with new adaptive learning rates.",
        "subjects": [
            "cs.GT",
            "cs.LG",
            "stat.ML"
        ],
        "comment": "28 pages"
    },
    {
        "paper id": "2412.07138",
        "abstract url": "https://arxiv.org/abs/2412.07138",
        "title": "Unlocking TriLevel Learning with Level-Wise Zeroth Order Constraints: Distributed Algorithms and Provable Non-Asymptotic Convergence",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "Trilevel learning (TLL) found diverse applications in numerous machine learning applications, ranging from robust hyperparameter optimization to domain adaptation. However, existing researches primarily focus on scenarios where TLL can be addressed with first order information available at each level, which is inadequate in many situations involving zeroth order constraints, such as when black-box models are employed. Moreover, in trilevel learning, data may be distributed across various nodes, necessitating strategies to address TLL problems without centralizing data on servers to uphold data privacy. To this end, an effective distributed trilevel zeroth order learning framework DTZO is proposed in this work to address the TLL problems with level-wise zeroth order constraints in a distributed manner. The proposed DTZO is versatile and can be adapted to a wide range of (grey-box) TLL problems with partial zeroth order constraints. In DTZO, the cascaded polynomial approximation can be constructed without relying on gradients or sub-gradients, leveraging a novel cut, i.e., zeroth order cut. Furthermore, we theoretically carry out the non-asymptotic convergence rate analysis for the proposed DTZO in achieving the $\u03b5$-stationary point. Extensive experiments have been conducted to demonstrate and validate the superior performance of the proposed DTZO, e.g., it approximately achieves up to a 40$\\%$ improvement in performance.",
        "subjects": [
            "cs.LG",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07160",
        "abstract url": "https://arxiv.org/abs/2412.07160",
        "title": "Motion-aware Contrastive Learning for Temporal Panoptic Scene Graph Generation",
        "rating": "0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.CV"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "To equip artificial intelligence with a comprehensive understanding towards a temporal world, video and 4D panoptic scene graph generation abstracts visual data into nodes to represent entities and edges to capture temporal relations. Existing methods encode entity masks tracked across temporal dimensions (mask tubes), then predict their relations with temporal pooling operation, which does not fully utilize the motion indicative of the entities' relation. To overcome this limitation, we introduce a contrastive representation learning framework that focuses on motion pattern for temporal scene graph generation. Firstly, our framework encourages the model to learn close representations for mask tubes of similar subject-relation-object triplets. Secondly, we seek to push apart mask tubes from their temporally shuffled versions. Moreover, we also learn distant representations for mask tubes belonging to the same video but different triplets. Extensive experiments show that our motion-aware contrastive framework significantly improves state-of-the-art methods on both video and 4D datasets.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted at AAAI 2025"
    },
    {
        "paper id": "2412.07165",
        "abstract url": "https://arxiv.org/abs/2412.07165",
        "title": "A Method for Evaluating Hyperparameter Sensitivity in Reinforcement Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "The performance of modern reinforcement learning algorithms critically relies on tuning ever-increasing numbers of hyperparameters. Often, small changes in a hyperparameter can lead to drastic changes in performance, and different environments require very different hyperparameter settings to achieve state-of-the-art performance reported in the literature. We currently lack a scalable and widely accepted approach to characterizing these complex interactions. This work proposes a new empirical methodology for studying, comparing, and quantifying the sensitivity of an algorithm's performance to hyperparameter tuning for a given set of environments. We then demonstrate the utility of this methodology by assessing the hyperparameter sensitivity of several commonly used normalization variants of PPO. The results suggest that several algorithmic performance improvements may, in fact, be a result of an increased reliance on hyperparameter tuning.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07177",
        "abstract url": "https://arxiv.org/abs/2412.07177",
        "title": "Effective Reward Specification in Deep Reinforcement Learning",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the last decade, Deep Reinforcement Learning has evolved into a powerful tool for complex sequential decision-making problems. It combines deep learning's proficiency in processing rich input signals with reinforcement learning's adaptability across diverse control tasks. At its core, an RL agent seeks to maximize its cumulative reward, enabling AI algorithms to uncover novel solutions previously unknown to experts. However, this focus on reward maximization also introduces a significant difficulty: improper reward specification can result in unexpected, misaligned agent behavior and inefficient learning. The complexity of accurately specifying the reward function is further amplified by the sequential nature of the task, the sparsity of learning signals, and the multifaceted aspects of the desired behavior. In this thesis, we survey the literature on effective reward specification strategies, identify core challenges relating to each of these approaches, and propose original contributions addressing the issue of sample efficiency and alignment in deep reinforcement learning. Reward specification represents one of the most challenging aspects of applying reinforcement learning in real-world domains. Our work underscores the absence of a universal solution to this complex and nuanced challenge; solving it requires selecting the most appropriate tools for the specific requirements of each unique application.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07184",
        "abstract url": "https://arxiv.org/abs/2412.07184",
        "title": "Automatic Doubly Robust Forests",
        "rating": "0.5",
        "keywords": [
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper proposes the automatic Doubly Robust Random Forest (DRRF) algorithm for estimating the conditional expectation of a moment functional in the presence of high-dimensional nuisance functions. DRRF combines the automatic debiasing framework using the Riesz representer (Chernozhukov et al., 2022) with non-parametric, forest-based estimation methods for the conditional moment (Athey et al., 2019; Oprescu et al., 2019). In contrast to existing methods, DRRF does not require prior knowledge of the form of the debiasing term nor impose restrictive parametric or semi-parametric assumptions on the target quantity. Additionally, it is computationally efficient for making predictions at multiple query points and significantly reduces runtime compared to methods such as Orthogonal Random Forest (Oprescu et al., 2019). We establish the consistency and asymptotic normality results of DRRF estimator under general assumptions, allowing for the construction of valid confidence intervals. Through extensive simulations in heterogeneous treatment effect (HTE) estimation, we demonstrate the superior performance of DRRF over benchmark approaches in terms of estimation accuracy, robustness, and computational efficiency.",
        "subjects": [
            "stat.ME",
            "cs.LG",
            "econ.EM",
            "math.ST"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06215",
        "abstract url": "https://arxiv.org/abs/2412.06215",
        "title": "A Real-Time Defense Against Object Vanishing Adversarial Patch Attacks for Object Detection in Autonomous Vehicles",
        "rating": "0",
        "keywords": [
            [
                "Attacks"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Autonomous vehicles (AVs) increasingly use DNN-based object detection models in vision-based perception. Correct detection and classification of obstacles is critical to ensure safe, trustworthy driving decisions. Adversarial patches aim to fool a DNN with intentionally generated patterns concentrated in a localized region of an image. In particular, object vanishing patch attacks can cause object detection models to fail to detect most or all objects in a scene, posing a significant practical threat to AVs. This work proposes ADAV (Adversarial Defense for Autonomous Vehicles), a novel defense methodology against object vanishing patch attacks specifically designed for autonomous vehicles. Unlike existing defense methods which have high latency or are designed for static images, ADAV runs in real-time and leverages contextual information from prior frames in an AV's video feed. ADAV checks if the object detector's output for the target frame is temporally consistent with the output from a previous reference frame to detect the presence of a patch. If the presence of a patch is detected, ADAV uses gradient-based attribution to localize adversarial pixels that break temporal consistency. This two stage procedure allows ADAV to efficiently process clean inputs, and both stages are optimized to be low latency. ADAV is evaluated using real-world driving data from the Berkeley Deep Drive BDD100K dataset, and demonstrates high adversarial and clean performance.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06227",
        "abstract url": "https://arxiv.org/abs/2412.06227",
        "title": "Attention-Enhanced Lightweight Hourglass Network for Human Pose Estimation",
        "rating": "0",
        "keywords": [
            [
                "robot"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Pose estimation is a critical task in computer vision with a wide range of applications from activity monitoring to human-robot interaction. However,most of the existing methods are computationally expensive or have complex architecture. Here we propose a lightweight attention based pose estimation network that utilizes depthwise separable convolution and Convolutional Block Attention Module on an hourglass backbone. The network significantly reduces the computational complexity (floating point operations) and the model size (number of parameters) containing only about 10% of parameters of original eight stack Hourglass network.Experiments were conducted on COCO and MPII datasets using a two stack hourglass backbone. The results showed that our model performs well in comparison to six other lightweight pose estimation models with an average precision of 72.07. The model achieves this performance with only 2.3M parameters and 3.7G FLOPs.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06234",
        "abstract url": "https://arxiv.org/abs/2412.06234",
        "title": "Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian splatting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Generalized feed-forward Gaussian models have achieved significant progress in sparse-view 3D reconstruction by leveraging prior knowledge from large multi-view datasets. However, these models often struggle to represent high-frequency details due to the limited number of Gaussians. While the densification strategy used in per-scene 3D Gaussian splatting (3D-GS) optimization can be adapted to the feed-forward models, it may not be ideally suited for generalized scenarios. In this paper, we propose Generative Densification, an efficient and generalizable method to densify Gaussians generated by feed-forward models. Unlike the 3D-GS densification strategy, which iteratively splits and clones raw Gaussian parameters, our method up-samples feature representations from the feed-forward models and generates their corresponding fine Gaussians in a single forward pass, leveraging the embedded prior knowledge for enhanced generalization. Experimental results on both object-level and scene-level reconstruction tasks demonstrate that our method outperforms state-of-the-art approaches with comparable or smaller model sizes, achieving notable improvements in representing fine details.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06235",
        "abstract url": "https://arxiv.org/abs/2412.06235",
        "title": "VariFace: Fair and Diverse Synthetic Dataset Generation for Face Recognition",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "The use of large-scale, web-scraped datasets to train face recognition models has raised significant privacy and bias concerns. Synthetic methods mitigate these concerns and provide scalable and controllable face generation to enable fair and accurate face recognition. However, existing synthetic datasets display limited intraclass and interclass diversity and do not match the face recognition performance obtained using real datasets. Here, we propose VariFace, a two-stage diffusion-based pipeline to create fair and diverse synthetic face datasets to train face recognition models. Specifically, we introduce three methods: Face Recognition Consistency to refine demographic labels, Face Vendi Score Guidance to improve interclass diversity, and Divergence Score Conditioning to balance the identity preservation-intraclass diversity trade-off. When constrained to the same dataset size, VariFace considerably outperforms previous synthetic datasets (0.9200 $\\rightarrow$ 0.9405) and achieves comparable performance to face recognition models trained with real data (Real Gap = -0.0065). In an unconstrained setting, VariFace not only consistently achieves better performance compared to previous synthetic methods across dataset sizes but also, for the first time, outperforms the real dataset (CASIA-WebFace) across six evaluation datasets. This sets a new state-of-the-art performance with an average face verification accuracy of 0.9567 (Real Gap = +0.0097) across LFW, CFP-FP, CPLFW, AgeDB, and CALFW datasets and 0.9366 (Real Gap = +0.0380) on the RFW dataset.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06243",
        "abstract url": "https://arxiv.org/abs/2412.06243",
        "title": "U-Know-DiffPAN: An Uncertainty-aware Knowledge Distillation Diffusion Framework with Details Enhancement for PAN-Sharpening",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Conventional methods for PAN-sharpening often struggle to restore fine details due to limitations in leveraging high-frequency information. Moreover, diffusion-based approaches lack sufficient conditioning to fully utilize Panchromatic (PAN) images and low-resolution multispectral (LRMS) inputs effectively. To address these challenges, we propose an uncertainty-aware knowledge distillation diffusion framework with details enhancement for PAN-sharpening, called U-Know-DiffPAN. The U-Know-DiffPAN incorporates uncertainty-aware knowledge distillation for effective transfer of feature details from our teacher model to a student one. The teacher model in our U-Know-DiffPAN captures frequency details through freqeuncy selective attention, facilitating accurate reverse process learning. By conditioning the encoder on compact vector representations of PAN and LRMS and the decoder on Wavelet transforms, we enable rich frequency utilization. So, the high-capacity teacher model distills frequency-rich features into a lightweight student model aided by an uncertainty map. From this, the teacher model can guide the student model to focus on difficult image regions for PAN-sharpening via the usage of the uncertainty map. Extensive experiments on diverse datasets demonstrate the robustness and superior performance of our U-Know-DiffPAN over very recent state-of-the-art PAN-sharpening methods.",
        "subjects": [
            "cs.CV",
            "eess.IV"
        ],
        "comment": "Please visit our project page at https://kaist-viclab.github.io/U-Know-DiffPAN-site/"
    },
    {
        "paper id": "2412.06250",
        "abstract url": "https://arxiv.org/abs/2412.06250",
        "title": "Splatter-360: Generalizable 360$^{\\circ}$ Gaussian Splatting for Wide-baseline Panoramic Images",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "depth",
                "NeRF"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Wide-baseline panoramic images are frequently used in applications like VR and simulations to minimize capturing labor costs and storage needs. However, synthesizing novel views from these panoramic images in real time remains a significant challenge, especially due to panoramic imagery's high resolution and inherent distortions. Although existing 3D Gaussian splatting (3DGS) methods can produce photo-realistic views under narrow baselines, they often overfit the training views when dealing with wide-baseline panoramic images due to the difficulty in learning precise geometry from sparse 360$^{\\circ}$ views. This paper presents \\textit{Splatter-360}, a novel end-to-end generalizable 3DGS framework designed to handle wide-baseline panoramic images. Unlike previous approaches, \\textit{Splatter-360} performs multi-view matching directly in the spherical domain by constructing a spherical cost volume through a spherical sweep algorithm, enhancing the network's depth perception and geometry estimation. Additionally, we introduce a 3D-aware bi-projection encoder to mitigate the distortions inherent in panoramic images and integrate cross-view attention to improve feature interactions across multiple viewpoints. This enables robust 3D-aware feature representations and real-time rendering capabilities. Experimental results on the HM3D~\\cite{hm3d} and Replica~\\cite{replica} demonstrate that \\textit{Splatter-360} significantly outperforms state-of-the-art NeRF and 3DGS methods (e.g., PanoGRF, MVSplat, DepthSplat, and HiSplat) in both synthesis quality and generalization performance for wide-baseline panoramic images. Code and trained models are available at \\url{https://3d-aigc.github.io/Splatter-360/}.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "Project page:https://3d-aigc.github.io/Splatter-360/. Code: https://github.com/thucz/splatter360"
    },
    {
        "paper id": "2412.06257",
        "abstract url": "https://arxiv.org/abs/2412.06257",
        "title": "Advancing Extended Reality with 3D Gaussian Splatting: Innovations and Prospects",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "3D Gaussian Splatting (3DGS) has attracted significant attention for its potential to revolutionize 3D representation, rendering, and interaction. Despite the rapid growth of 3DGS research, its direct application to Extended Reality (XR) remains underexplored. Although many studies recognize the potential of 3DGS for XR, few have explicitly focused on or demonstrated its effectiveness within XR environments. In this paper, we aim to synthesize innovations in 3DGS that show specific potential for advancing XR research and development. We conduct a comprehensive review of publicly available 3DGS papers, with a focus on those referencing XR-related concepts. Additionally, we perform an in-depth analysis of innovations explicitly relevant to XR and propose a taxonomy to highlight their significance. Building on these insights, we propose several prospective XR research areas where 3DGS can make promising contributions, yet remain rarely touched. By investigating the intersection of 3DGS and XR, this paper provides a roadmap to push the boundaries of XR using cutting-edge 3DGS techniques.",
        "subjects": [
            "cs.CV",
            "cs.GR",
            "cs.HC"
        ],
        "comment": "IEEE AIxVR 2025"
    },
    {
        "paper id": "2412.06268",
        "abstract url": "https://arxiv.org/abs/2412.06268",
        "title": "Open-Vocabulary High-Resolution 3D (OVHR3D) Data Segmentation and Annotation Framework",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "point cloud"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In the domain of the U.S. Army modeling and simulation, the availability of high quality annotated 3D data is pivotal to creating virtual environments for training and simulations. Traditional methodologies for 3D semantic and instance segmentation, such as KpConv, RandLA, Mask3D, etc., are designed to train on extensive labeled datasets to obtain satisfactory performance in practical tasks. This requirement presents a significant challenge, given the inherent scarcity of manually annotated 3D datasets, particularly for the military use cases. Recognizing this gap, our previous research leverages the One World Terrain data repository manually annotated databases, as showcased at IITSEC 2019 and 2021, to enrich the training dataset for deep learning models. However, collecting and annotating large scale 3D data for specific tasks remains costly and inefficient. To this end, the objective of this research is to design and develop a comprehensive and efficient framework for 3D segmentation tasks to assist in 3D data annotation. This framework integrates Grounding DINO and Segment anything Model, augmented by an enhancement in 2D image rendering via 3D mesh. Furthermore, the authors have also developed a user friendly interface that facilitates the 3D annotation process, offering intuitive visualization of rendered images and the 3D point cloud.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06292",
        "abstract url": "https://arxiv.org/abs/2412.06292",
        "title": "ZeroKey: Point-Level Reasoning and Zero-Shot 3D Keypoint Detection from Large Language Models",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We propose a novel zero-shot approach for keypoint detection on 3D shapes. Point-level reasoning on visual data is challenging as it requires precise localization capability, posing problems even for powerful models like DINO or CLIP. Traditional methods for 3D keypoint detection rely heavily on annotated 3D datasets and extensive supervised training, limiting their scalability and applicability to new categories or domains. In contrast, our method utilizes the rich knowledge embedded within Multi-Modal Large Language Models (MLLMs). Specifically, we demonstrate, for the first time, that pixel-level annotations used to train recent MLLMs can be exploited for both extracting and naming salient keypoints on 3D models without any ground truth labels or supervision. Experimental evaluations demonstrate that our approach achieves competitive performance on standard benchmarks compared to supervised methods, despite not requiring any 3D keypoint annotations during training. Our results highlight the potential of integrating language models for localized 3D shape understanding. This work opens new avenues for cross-modal learning and underscores the effectiveness of MLLMs in contributing to 3D computer vision challenges.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project website is accessible at https://sites.google.com/view/zerokey"
    },
    {
        "paper id": "2412.06295",
        "abstract url": "https://arxiv.org/abs/2412.06295",
        "title": "See Further When Clear: Curriculum Consistency Model",
        "rating": "0",
        "keywords": [
            [
                "diffusion",
                "text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Significant advances have been made in the sampling efficiency of diffusion models and flow matching models, driven by Consistency Distillation (CD), which trains a student model to mimic the output of a teacher model at a later timestep. However, we found that the learning complexity of the student model varies significantly across different timesteps, leading to suboptimal performance in CD.To address this issue, we propose the Curriculum Consistency Model (CCM), which stabilizes and balances the learning complexity across timesteps. Specifically, we regard the distillation process at each timestep as a curriculum and introduce a metric based on Peak Signal-to-Noise Ratio (PSNR) to quantify the learning complexity of this curriculum, then ensure that the curriculum maintains consistent learning complexity across different timesteps by having the teacher model iterate more steps when the noise intensity is low. Our method achieves competitive single-step sampling Fr\u00e9chet Inception Distance (FID) scores of 1.64 on CIFAR-10 and 2.18 on ImageNet 64x64.Moreover, we have extended our method to large-scale text-to-image models and confirmed that it generalizes well to both diffusion models (Stable Diffusion XL) and flow matching models (Stable Diffusion 3). The generated samples demonstrate improved image-text alignment and semantic structure, since CCM enlarges the distillation step at large timesteps and reduces the accumulated error.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06296",
        "abstract url": "https://arxiv.org/abs/2412.06296",
        "title": "VidMusician: Video-to-Music Generation with Semantic-Rhythmic Alignment via Hierarchical Visual Features",
        "rating": "0",
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "Music",
                "text-to-music"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Video-to-music generation presents significant potential in video production, requiring the generated music to be both semantically and rhythmically aligned with the video. Achieving this alignment demands advanced music generation capabilities, sophisticated video understanding, and an efficient mechanism to learn the correspondence between the two modalities. In this paper, we propose VidMusician, a parameter-efficient video-to-music generation framework built upon text-to-music models. VidMusician leverages hierarchical visual features to ensure semantic and rhythmic alignment between video and music. Specifically, our approach utilizes global visual features as semantic conditions and local visual features as rhythmic cues. These features are integrated into the generative backbone via cross-attention and in-attention mechanisms, respectively. Through a two-stage training process, we incrementally incorporate semantic and rhythmic features, utilizing zero initialization and identity initialization to maintain the inherent music-generative capabilities of the backbone. Additionally, we construct a diverse video-music dataset, DVMSet, encompassing various scenarios, such as promo videos, commercials, and compilations. Experiments demonstrate that VidMusician outperforms state-of-the-art methods across multiple evaluation metrics and exhibits robust performance on AI-generated videos. Samples are available at \\url{https://youtu.be/EPOSXwtl1jw}.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06299",
        "abstract url": "https://arxiv.org/abs/2412.06299",
        "title": "4D Gaussian Splatting with Scale-aware Residual Field and Adaptive Optimization for Real-time Rendering of Temporally Complex Dynamic Scenes",
        "rating": "0",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Reconstructing dynamic scenes from video sequences is a highly promising task in the multimedia domain. While previous methods have made progress, they often struggle with slow rendering and managing temporal complexities such as significant motion and object appearance/disappearance. In this paper, we propose SaRO-GS as a novel dynamic scene representation capable of achieving real-time rendering while effectively handling temporal complexities in dynamic scenes. To address the issue of slow rendering speed, we adopt a Gaussian primitive-based representation and optimize the Gaussians in 4D space, which facilitates real-time rendering with the assistance of 3D Gaussian Splatting. Additionally, to handle temporally complex dynamic scenes, we introduce a Scale-aware Residual Field. This field considers the size information of each Gaussian primitive while encoding its residual feature and aligns with the self-splitting behavior of Gaussian primitives. Furthermore, we propose an Adaptive Optimization Schedule, which assigns different optimization strategies to Gaussian primitives based on their distinct temporal properties, thereby expediting the reconstruction of dynamic regions. Through evaluations on monocular and multi-view datasets, our method has demonstrated state-of-the-art performance. Please see our project page at https://yjb6.github.io/SaRO-GS.github.io.",
        "subjects": [
            "cs.CV",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06324",
        "abstract url": "https://arxiv.org/abs/2412.06324",
        "title": "World knowledge-enhanced Reasoning Using Instruction-guided Interactor in Autonomous Driving",
        "rating": "0",
        "keywords": [
            [
                "Autonomous Driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The Multi-modal Large Language Models (MLLMs) with extensive world knowledge have revitalized autonomous driving, particularly in reasoning tasks within perceivable regions. However, when faced with perception-limited areas (dynamic or static occlusion regions), MLLMs struggle to effectively integrate perception ability with world knowledge for reasoning. These perception-limited regions can conceal crucial safety information, especially for vulnerable road users. In this paper, we propose a framework, which aims to improve autonomous driving performance under perceptionlimited conditions by enhancing the integration of perception capabilities and world knowledge. Specifically, we propose a plug-and-play instruction-guided interaction module that bridges modality gaps and significantly reduces the input sequence length, allowing it to adapt effectively to multi-view video inputs. Furthermore, to better integrate world knowledge with driving-related tasks, we have collected and refined a large-scale multi-modal dataset that includes 2 million natural language QA pairs, 1.7 million grounding task data. To evaluate the model's utilization of world knowledge, we introduce an object-level risk assessment dataset comprising 200K QA pairs, where the questions necessitate multi-step reasoning leveraging world knowledge for resolution. Extensive experiments validate the effectiveness of our proposed method.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "14 pages. Supplementary Material"
    },
    {
        "paper id": "2412.06329",
        "abstract url": "https://arxiv.org/abs/2412.06329",
        "title": "Normalizing Flows are Capable Generative Models",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Normalizing Flows (NFs) are likelihood-based models for continuous inputs. They have demonstrated promising results on both density estimation and generative modeling tasks, but have received relatively little attention in recent years. In this work, we demonstrate that NFs are more powerful than previously believed. We present TarFlow: a simple and scalable architecture that enables highly performant NF models. TarFlow can be thought of as a Transformer-based variant of Masked Autoregressive Flows (MAFs): it consists of a stack of autoregressive Transformer blocks on image patches, alternating the autoregression direction between layers. TarFlow is straightforward to train end-to-end, and capable of directly modeling and generating pixels. We also propose three key techniques to improve sample quality: Gaussian noise augmentation during training, a post training denoising procedure, and an effective guidance method for both class-conditional and unconditional settings. Putting these together, TarFlow sets new state-of-the-art results on likelihood estimation for images, beating the previous best methods by a large margin, and generates samples with quality and diversity comparable to diffusion models, for the first time with a stand-alone NF model. We make our code available at https://github.com/apple/ml-tarflow.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06340",
        "abstract url": "https://arxiv.org/abs/2412.06340",
        "title": "UniPaint: Unified Space-time Video Inpainting via Mixture-of-Experts",
        "rating": "0",
        "keywords": [
            [
                "Inpainting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper, we present UniPaint, a unified generative space-time video inpainting framework that enables spatial-temporal inpainting and interpolation. Different from existing methods that treat video inpainting and video interpolation as two distinct tasks, we leverage a unified inpainting framework to tackle them and observe that these two tasks can mutually enhance synthesis performance. Specifically, we first introduce a plug-and-play space-time video inpainting adapter, which can be employed in various personalized models. The key insight is to propose a Mixture of Experts (MoE) attention to cover various tasks. Then, we design a spatial-temporal masking strategy during the training stage to mutually enhance each other and improve performance. UniPaint produces high-quality and aesthetically pleasing results, achieving the best quantitative results across various tasks and scale setups. The code and checkpoints will be available soon.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06403",
        "abstract url": "https://arxiv.org/abs/2412.06403",
        "title": "Generative Lines Matching Models",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In this paper we identify the source of a singularity in the training loss of key denoising models, that causes the denoiser's predictions to collapse towards the mean of the source or target distributions. This degeneracy creates false basins of attraction, distorting the denoising trajectories and ultimately increasing the number of steps required to sample these models. We circumvent this artifact by leveraging the deterministic ODE-based samplers, offered by certain denoising diffusion and score-matching models, which establish a well-defined change-of-variables between the source and target distributions. Given this correspondence, we propose a new probability flow model, the Lines Matching Model (LMM), which matches globally straight lines interpolating the two distributions. We demonstrate that the flow fields produced by the LMM exhibit notable temporal consistency, resulting in trajectories with excellent straightness scores. Beyond its sampling efficiency, the LMM formulation allows us to enhance the fidelity of the generated samples by integrating domain-specific reconstruction and adversarial losses, and by optimizing its training for the sampling procedure used. Overall, the LMM achieves state-of-the-art FID scores with minimal NFEs on established benchmark datasets: 1.57/1.39 (NFE=1/2) on CIFAR-10, 1.47/1.17 on ImageNet 64x64, and 2.68/1.54 on AFHQ 64x64. Finally, we provide a theoretical analysis showing that the use of optimal transport to relate the two distributions suffers from a curse of dimensionality, where the pairing set size (mini-batch) must scale exponentially with the signal dimension.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06488",
        "abstract url": "https://arxiv.org/abs/2412.06488",
        "title": "An Efficient Scene Coordinate Encoding and Relocalization Method",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Scene Coordinate Regression (SCR) is a visual localization technique that utilizes deep neural networks (DNN) to directly regress 2D-3D correspondences for camera pose estimation. However, current SCR methods often face challenges in handling repetitive textures and meaningless areas due to their reliance on implicit triangulation. In this paper, we propose an efficient scene coordinate encoding and relocalization method. Compared with the existing SCR methods, we design a unified architecture for both scene encoding and salient keypoint detection, enabling our system to focus on encoding informative regions, thereby significantly enhancing efficiency. Additionally, we introduce a mechanism that leverages sequential information during both map encoding and relocalization, which strengthens implicit triangulation, particularly in repetitive texture environments. Comprehensive experiments conducted across indoor and outdoor datasets demonstrate that the proposed system outperforms other state-of-the-art (SOTA) SCR methods. Our single-frame relocalization mode improves the recall rate of our baseline by 6.4% and increases the running speed from 56Hz to 90Hz. Furthermore, our sequence-based mode increases the recall rate by 11% while maintaining the original efficiency.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": "8 pages, 6 figures"
    },
    {
        "paper id": "2412.06510",
        "abstract url": "https://arxiv.org/abs/2412.06510",
        "title": "AnomalyControl: Learning Cross-modal Semantic Features for Controllable Anomaly Synthesis",
        "rating": "0",
        "keywords": [
            [
                "text-to-image"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Anomaly synthesis is a crucial approach to augment abnormal data for advancing anomaly inspection. Based on the knowledge from the large-scale pre-training, existing text-to-image anomaly synthesis methods predominantly focus on textual information or coarse-aligned visual features to guide the entire generation process. However, these methods often lack sufficient descriptors to capture the complicated characteristics of realistic anomalies (e.g., the fine-grained visual pattern of anomalies), limiting the realism and generalization of the generation process. To this end, we propose a novel anomaly synthesis framework called AnomalyControl to learn cross-modal semantic features as guidance signals, which could encode the generalized anomaly cues from text-image reference prompts and improve the realism of synthesized abnormal samples. Specifically, AnomalyControl adopts a flexible and non-matching prompt pair (i.e., a text-image reference prompt and a targeted text prompt), where a Cross-modal Semantic Modeling (CSM) module is designed to extract cross-modal semantic features from the textual and visual descriptors. Then, an Anomaly-Semantic Enhanced Attention (ASEA) mechanism is formulated to allow CSM to focus on the specific visual patterns of the anomaly, thus enhancing the realism and contextual relevance of the generated anomaly features. Treating cross-modal semantic features as the prior, a Semantic Guided Adapter (SGA) is designed to encode effective guidance signals for the adequate and controllable synthesis process. Extensive experiments indicate that AnomalyControl can achieve state-of-the-art results in anomaly synthesis compared with existing methods while exhibiting superior performance for downstream tasks.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06538",
        "abstract url": "https://arxiv.org/abs/2412.06538",
        "title": "Understanding Factual Recall in Transformers via Associative Memories",
        "rating": "0",
        "keywords": [
            [
                "trajectory"
            ],
            [
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models have demonstrated an impressive ability to perform factual recall. Prior work has found that transformers trained on factual recall tasks can store information at a rate proportional to their parameter count. In our work, we show that shallow transformers can use a combination of associative memories to obtain such near optimal storage capacity. We begin by proving that the storage capacities of both linear and MLP associative memories scale linearly with parameter count. We next introduce a synthetic factual recall task, and prove that a transformer with a single layer of self-attention followed by an MLP can obtain 100% accuracy on the task whenever either the total number of self-attention parameters or MLP parameters scales (up to log factors) linearly with the number of facts. In particular, the transformer can trade off between using the value matrices or the MLP as an associative memory to store the dataset of facts. We complement these expressivity results with an analysis of the gradient flow trajectory of a simplified linear attention model trained on our factual recall task, where we show that the model exhibits sequential learning behavior.",
        "subjects": [
            "cs.LG",
            "cs.CL",
            "cs.IT",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06578",
        "abstract url": "https://arxiv.org/abs/2412.06578",
        "title": "MoViE: Mobile Diffusion for Video Editing",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "Video Editing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent progress in diffusion-based video editing has shown remarkable potential for practical applications. However, these methods remain prohibitively expensive and challenging to deploy on mobile devices. In this study, we introduce a series of optimizations that render mobile video editing feasible. Building upon the existing image editing model, we first optimize its architecture and incorporate a lightweight autoencoder. Subsequently, we extend classifier-free guidance distillation to multiple modalities, resulting in a threefold on-device speedup. Finally, we reduce the number of sampling steps to one by introducing a novel adversarial distillation scheme which preserves the controllability of the editing process. Collectively, these optimizations enable video editing at 12 frames per second on mobile devices, while maintaining high quality. Our results are available at https://qualcomm-ai-research.github.io/mobile-video-editing/",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages"
    },
    {
        "paper id": "2412.06613",
        "abstract url": "https://arxiv.org/abs/2412.06613",
        "title": "3D Spatial Understanding in MLLMs: Disambiguation and Evaluation",
        "rating": "0",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Multimodal Large Language Models (MLLMs) have made significant progress in tasks such as image captioning and question answering. However, while these models can generate realistic captions, they often struggle with providing precise instructions, particularly when it comes to localizing and disambiguating objects in complex 3D environments. This capability is critical as MLLMs become more integrated with collaborative robotic systems. In scenarios where a target object is surrounded by similar objects (distractors), robots must deliver clear, spatially-aware instructions to guide humans effectively. We refer to this challenge as contextual object localization and disambiguation, which imposes stricter constraints than conventional 3D dense captioning, especially regarding ensuring target exclusivity. In response, we propose simple yet effective techniques to enhance the model's ability to localize and disambiguate target objects. Our approach not only achieves state-of-the-art performance on conventional metrics that evaluate sentence similarity, but also demonstrates improved 3D spatial understanding through 3D visual grounding model.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06647",
        "abstract url": "https://arxiv.org/abs/2412.06647",
        "title": "Object Detection using Event Camera: A MoE Heat Conduction based Detector and A New Benchmark Dataset",
        "rating": "0",
        "keywords": [
            [
                "Event Camera"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Object detection in event streams has emerged as a cutting-edge research area, demonstrating superior performance in low-light conditions, scenarios with motion blur, and rapid movements. Current detectors leverage spiking neural networks, Transformers, or convolutional neural networks as their core architectures, each with its own set of limitations including restricted performance, high computational overhead, or limited local receptive fields. This paper introduces a novel MoE (Mixture of Experts) heat conduction-based object detection algorithm that strikingly balances accuracy and computational efficiency. Initially, we employ a stem network for event data embedding, followed by processing through our innovative MoE-HCO blocks. Each block integrates various expert modules to mimic heat conduction within event streams. Subsequently, an IoU-based query selection module is utilized for efficient token extraction, which is then channeled into a detection head for the final object detection process. Furthermore, we are pleased to introduce EvDET200K, a novel benchmark dataset for event-based object detection. Captured with a high-definition Prophesee EVK4-HD event camera, this dataset encompasses 10 distinct categories, 200,000 bounding boxes, and 10,054 samples, each spanning 2 to 5 seconds. We also provide comprehensive results from over 15 state-of-the-art detectors, offering a solid foundation for future research and comparison. The source code of this paper will be released on: https://github.com/Event-AHU/OpenEvDET",
        "subjects": [
            "cs.CV",
            "cs.NE"
        ],
        "comment": "In Peer Review"
    },
    {
        "paper id": "2412.06661",
        "abstract url": "https://arxiv.org/abs/2412.06661",
        "title": "Efficiency Meets Fidelity: A Novel Quantization Framework for Stable Diffusion",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "Text-to-image"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Text-to-image generation of Stable Diffusion models has achieved notable success due to its remarkable generation ability. However, the repetitive denoising process is computationally intensive during inference, which renders Diffusion models less suitable for real-world applications that require low latency and scalability. Recent studies have employed post-training quantization (PTQ) and quantization-aware training (QAT) methods to compress Diffusion models. Nevertheless, prior research has often neglected to examine the consistency between results generated by quantized models and those from floating-point models. This consistency is crucial in fields such as content creation, design, and edge deployment, as it can significantly enhance both efficiency and system stability for practitioners. To ensure that quantized models generate high-quality and consistent images, we propose an efficient quantization framework for Stable Diffusion models. Our approach features a Serial-to-Parallel calibration pipeline that addresses the consistency of both the calibration and inference processes, as well as ensuring training stability. Based on this pipeline, we further introduce a mix-precision quantization strategy, multi-timestep activation quantization, and time information precalculation techniques to ensure high-fidelity generation in comparison to floating-point models. Through extensive experiments with Stable Diffusion v1-4, v2-1, and XL 1.0, we have demonstrated that our method outperforms the current state-of-the-art techniques when tested on prompts from the COCO validation dataset and the Stable-Diffusion-Prompts dataset. Under W4A8 quantization settings, our approach enhances both distribution similarity and visual similarity by 45%-60%.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06674",
        "abstract url": "https://arxiv.org/abs/2412.06674",
        "title": "EMOv2: Pushing 5M Vision Model Frontier",
        "rating": "0",
        "keywords": [
            [
                "parameter-efficient"
            ],
            [
                "5G"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This work focuses on developing parameter-efficient and lightweight models for dense predictions while trading off parameters, FLOPs, and performance. Our goal is to set up the new frontier of the 5M magnitude lightweight model on various downstream tasks. Inverted Residual Block (IRB) serves as the infrastructure for lightweight CNNs, but no counterparts have been recognized by attention-based design. Our work rethinks the lightweight infrastructure of efficient IRB and practical components in Transformer from a unified perspective, extending CNN-based IRB to attention-based models and abstracting a one-residual Meta Mobile Block (MMBlock) for lightweight model design. Following neat but effective design criterion, we deduce a modern Improved Inverted Residual Mobile Block (i2RMB) and improve a hierarchical Efficient MOdel (EMOv2) with no elaborate complex structures. Considering the imperceptible latency for mobile users when downloading models under 4G/5G bandwidth and ensuring model performance, we investigate the performance upper limit of lightweight models with a magnitude of 5M. Extensive experiments on various vision recognition, dense prediction, and image generation tasks demonstrate the superiority of our EMOv2 over state-of-the-art methods, e.g., EMOv2-1M/2M/5M achieve 72.3, 75.8, and 79.4 Top-1 that surpass equal-order CNN-/Attention-based models significantly. At the same time, EMOv2-5M equipped RetinaNet achieves 41.5 mAP for object detection tasks that surpasses the previous EMO-5M by +2.6. When employing the more robust training recipe, our EMOv2-5M eventually achieves 82.9 Top-1 accuracy, which elevates the performance of 5M magnitude models to a new level. Code is available at https://github.com/zhangzjn/EMOv2.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06708",
        "abstract url": "https://arxiv.org/abs/2412.06708",
        "title": "FlexEvent: Event Camera Object Detection at Arbitrary Frequencies",
        "rating": "0",
        "keywords": [
            [
                "Event Camera"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Event cameras offer unparalleled advantages for real-time perception in dynamic environments, thanks to their microsecond-level temporal resolution and asynchronous operation. Existing event-based object detection methods, however, are limited by fixed-frequency paradigms and fail to fully exploit the high-temporal resolution and adaptability of event cameras. To address these limitations, we propose FlexEvent, a novel event camera object detection framework that enables detection at arbitrary frequencies. Our approach consists of two key components: FlexFuser, an adaptive event-frame fusion module that integrates high-frequency event data with rich semantic information from RGB frames, and FAL, a frequency-adaptive learning mechanism that generates frequency-adjusted labels to enhance model generalization across varying operational frequencies. This combination allows our method to detect objects with high accuracy in both fast-moving and static scenarios, while adapting to dynamic environments. Extensive experiments on large-scale event camera datasets demonstrate that our approach surpasses state-of-the-art methods, achieving significant improvements in both standard and high-frequency settings. Notably, our method maintains robust performance when scaling from 20 Hz to 90 Hz and delivers accurate detection up to 180 Hz, proving its effectiveness in extreme conditions. Our framework sets a new benchmark for event-based object detection and paves the way for more adaptable, real-time vision systems.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "Preprint; 29 pages, 12 figures, 9 tables; Project Page at https://flexevent.github.io/"
    },
    {
        "paper id": "2412.06742",
        "abstract url": "https://arxiv.org/abs/2412.06742",
        "title": "ContRail: A Framework for Realistic Railway Image Synthesis using ControlNet",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Deep Learning became an ubiquitous paradigm due to its extraordinary effectiveness and applicability in numerous domains. However, the approach suffers from the high demand of data required to achieve the potential of this type of model. An ever-increasing sub-field of Artificial Intelligence, Image Synthesis, aims to address this limitation through the design of intelligent models capable of creating original and realistic images, endeavour which could drastically reduce the need for real data. The Stable Diffusion generation paradigm recently propelled state-of-the-art approaches to exceed all previous benchmarks. In this work, we propose the ContRail framework based on the novel Stable Diffusion model ControlNet, which we empower through a multi-modal conditioning method. We experiment with the task of synthetic railway image generation, where we improve the performance in rail-specific tasks, such as rail semantic segmentation by enriching the dataset with realistic synthetic images.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "9 pages, 5 figures, 2 tables"
    },
    {
        "paper id": "2412.06770",
        "abstract url": "https://arxiv.org/abs/2412.06770",
        "title": "Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view Event Cameras",
        "rating": "0",
        "keywords": [
            [
                "NeRF",
                "Event Cameras"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Volumetric reconstruction of dynamic scenes is an important problem in computer vision. It is especially challenging in poor lighting and with fast motion. It is partly due to the limitations of RGB cameras: To capture fast motion without much blur, the framerate must be increased, which in turn requires more lighting. In contrast, event cameras, which record changes in pixel brightness asynchronously, are much less dependent on lighting, making them more suitable for recording fast motion. We hence propose the first method to spatiotemporally reconstruct a scene from sparse multi-view event streams and sparse RGB frames. We train a sequence of cross-faded time-conditioned NeRF models, one per short recording segment. The individual segments are supervised with a set of event- and RGB-based losses and sparse-view regularisation. We assemble a real-world multi-view camera rig with six static event cameras around the object and record a benchmark multi-view event stream dataset of challenging motions. Our work outperforms RGB-based baselines, producing state-of-the-art results, and opens up the topic of multi-view event-based reconstruction as a new path for fast scene capture beyond RGB cameras. The code and the data will be released soon at https://4dqv.mpi-inf.mpg.de/DynEventNeRF/",
        "subjects": [
            "cs.CV"
        ],
        "comment": "15 pages, 11 figures, 6 tables"
    },
    {
        "paper id": "2412.06781",
        "abstract url": "https://arxiv.org/abs/2412.06781",
        "title": "Around the World in 80 Timesteps: A Generative Approach to Global Visual Geolocation",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Global visual geolocation predicts where an image was captured on Earth. Since images vary in how precisely they can be localized, this task inherently involves a significant degree of ambiguity. However, existing approaches are deterministic and overlook this aspect. In this paper, we aim to close the gap between traditional geolocalization and modern generative methods. We propose the first generative geolocation approach based on diffusion and Riemannian flow matching, where the denoising process operates directly on the Earth's surface. Our model achieves state-of-the-art performance on three visual geolocation benchmarks: OpenStreetView-5M, YFCC-100M, and iNat21. In addition, we introduce the task of probabilistic visual geolocation, where the model predicts a probability distribution over all possible locations instead of a single point. We introduce new metrics and baselines for this task, demonstrating the advantages of our diffusion-based approach. Codes and models will be made available.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Project page: https://nicolas-dufour.github.io/plonk"
    },
    {
        "paper id": "2412.06784",
        "abstract url": "https://arxiv.org/abs/2412.06784",
        "title": "P3-PO: Prescriptive Point Priors for Visuo-Spatial Generalization of Robot Policies",
        "rating": "0",
        "keywords": [
            [
                "Robot"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Developing generalizable robot policies that can robustly handle varied environmental conditions and object instances remains a fundamental challenge in robot learning. While considerable efforts have focused on collecting large robot datasets and developing policy architectures to learn from such data, naively learning from visual inputs often results in brittle policies that fail to transfer beyond the training data. This work presents Prescriptive Point Priors for Policies or P3-PO, a novel framework that constructs a unique state representation of the environment leveraging recent advances in computer vision and robot learning to achieve improved out-of-distribution generalization for robot manipulation. This representation is obtained through two steps. First, a human annotator prescribes a set of semantically meaningful points on a single demonstration frame. These points are then propagated through the dataset using off-the-shelf vision models. The derived points serve as an input to state-of-the-art policy architectures for policy learning. Our experiments across four real-world tasks demonstrate an overall 43% absolute improvement over prior methods when evaluated in identical settings as training. Further, P3-PO exhibits 58% and 80% gains across tasks for new object instances and more cluttered environments respectively. Videos illustrating the robot's performance are best viewed at point-priors.github.io.",
        "subjects": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06786",
        "abstract url": "https://arxiv.org/abs/2412.06786",
        "title": "Retrieving Semantics from the Deep: an RAG Solution for Gesture Synthesis",
        "rating": "0",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Non-verbal communication often comprises of semantically rich gestures that help convey the meaning of an utterance. Producing such semantic co-speech gestures has been a major challenge for the existing neural systems that can generate rhythmic beat gestures, but struggle to produce semantically meaningful gestures. Therefore, we present RAG-Gesture, a diffusion-based gesture generation approach that leverages Retrieval Augmented Generation (RAG) to produce natural-looking and semantically rich gestures. Our neuro-explicit gesture generation approach is designed to produce semantic gestures grounded in interpretable linguistic knowledge. We achieve this by using explicit domain knowledge to retrieve exemplar motions from a database of co-speech gestures. Once retrieved, we then inject these semantic exemplar gestures into our diffusion-based gesture generation pipeline using DDIM inversion and retrieval guidance at the inference time without any need of training. Further, we propose a control paradigm for guidance, that allows the users to modulate the amount of influence each retrieval insertion has over the generated sequence. Our comparative evaluations demonstrate the validity of our approach against recent gesture generation approaches. The reader is urged to explore the results on our project page.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Preprint. Project page: https://vcai.mpi-inf.mpg.de/projects/RAG-Gesture/"
    },
    {
        "paper id": "2412.06787",
        "abstract url": "https://arxiv.org/abs/2412.06787",
        "title": "[MASK] is All You Need",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "In generative models, two paradigms have gained attraction in various applications: next-set prediction-based Masked Generative Models and next-noise prediction-based Non-Autoregressive Models, e.g., Diffusion Models. In this work, we propose using discrete-state models to connect them and explore their scalability in the vision domain. First, we conduct a step-by-step analysis in a unified design space across two types of models including timestep-independence, noise schedule, temperature, guidance strength, etc in a scalable manner. Second, we re-cast typical discriminative tasks, e.g., image segmentation, as an unmasking process from [MASK] tokens on a discrete-state model. This enables us to perform various sampling processes, including flexible conditional sampling by only training once to model the joint distribution. All aforementioned explorations lead to our framework named Discrete Interpolants, which enables us to achieve state-of-the-art or competitive performance compared to previous discrete-state based methods in various benchmarks, like ImageNet256, MS COCO, and video dataset FaceForensics. In summary, by leveraging [MASK] in discrete-state models, we can bridge Masked Generative and Non-autoregressive Diffusion models, as well as generative and discriminative tasks.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Technical Report (WIP), Project Page(code, model, dataset): https://compvis.github.io/mask/"
    },
    {
        "paper id": "2412.06965",
        "abstract url": "https://arxiv.org/abs/2412.06965",
        "title": "Improving Source Extraction with Diffusion and Consistency Models",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "In this work, we demonstrate the integration of a score-matching diffusion model into a deterministic architecture for time-domain musical source extraction, resulting in enhanced audio quality. To address the typically slow iterative sampling process of diffusion models, we apply consistency distillation and reduce the sampling process to a single step, achieving performance comparable to that of diffusion models, and with two or more steps, even surpassing them. Trained on the Slakh2100 dataset for four instruments (bass, drums, guitar, and piano), our model shows significant improvements across objective metrics compared to baseline methods. Sound examples are available at https://consistency-separation.github.io/.",
        "subjects": [
            "cs.SD",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06966",
        "abstract url": "https://arxiv.org/abs/2412.06966",
        "title": "Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy, Research, and Practice",
        "rating": "0",
        "keywords": [
            [
                "Unlearning"
            ],
            [
                "cs.LG",
                "cs.CY"
            ],
            [
                "ICML"
            ]
        ],
        "abstract": "We articulate fundamental mismatches between technical methods for machine unlearning in Generative AI, and documented aspirations for broader impact that these methods could have for law and policy. These aspirations are both numerous and varied, motivated by issues that pertain to privacy, copyright, safety, and more. For example, unlearning is often invoked as a solution for removing the effects of targeted information from a generative-AI model's parameters, e.g., a particular individual's personal data or in-copyright expression of Spiderman that was included in the model's training data. Unlearning is also proposed as a way to prevent a model from generating targeted types of information in its outputs, e.g., generations that closely resemble a particular individual's data or reflect the concept of \"Spiderman.\" Both of these goals--the targeted removal of information from a model and the targeted suppression of information from a model's outputs--present various technical and substantive challenges. We provide a framework for thinking rigorously about these challenges, which enables us to be clear about why unlearning is not a general-purpose solution for circumscribing generative-AI model behavior in service of broader positive impact. We aim for conceptual clarity and to encourage more thoughtful communication among machine learning (ML), law, and policy experts who seek to develop and apply technical methods for compliance with policy objectives.",
        "subjects": [
            "cs.LG",
            "cs.CY"
        ],
        "comment": "Presented at the 2nd Workshop on Generative AI and Law at ICML (July 2024)"
    },
    {
        "paper id": "2412.06968",
        "abstract url": "https://arxiv.org/abs/2412.06968",
        "title": "SphereUFormer: A U-Shaped Transformer for Spherical 360 Perception",
        "rating": "0",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "This paper proposes a novel method for omnidirectional 360$\\degree$ perception. Most common previous methods relied on equirectangular projection. This representation is easily applicable to 2D operation layers but introduces distortions into the image. Other methods attempted to remove the distortions by maintaining a sphere representation but relied on complicated convolution kernels that failed to show competitive results. In this work, we introduce a transformer-based architecture that, by incorporating a novel ``Spherical Local Self-Attention'' and other spherically-oriented modules, successfully operates in the spherical domain and outperforms the state-of-the-art in 360$\\degree$ perception benchmarks for depth estimation and semantic segmentation.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06974",
        "abstract url": "https://arxiv.org/abs/2412.06974",
        "title": "MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds",
        "rating": "0",
        "keywords": [
            [
                "Gaussian splatting"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Recent sparse multi-view scene reconstruction advances like DUSt3R and MASt3R no longer require camera calibration and camera pose estimation. However, they only process a pair of views at a time to infer pixel-aligned pointmaps. When dealing with more than two views, a combinatorial number of error prone pairwise reconstructions are usually followed by an expensive global optimization, which often fails to rectify the pairwise reconstruction errors. To handle more views, reduce errors, and improve inference time, we propose the fast single-stage feed-forward network MV-DUSt3R. At its core are multi-view decoder blocks which exchange information across any number of views while considering one reference view. To make our method robust to reference view selection, we further propose MV-DUSt3R+, which employs cross-reference-view blocks to fuse information across different reference view choices. To further enable novel view synthesis, we extend both by adding and jointly training Gaussian splatting heads. Experiments on multi-view stereo reconstruction, multi-view pose estimation, and novel view synthesis confirm that our methods improve significantly upon prior art. Code will be released.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07017",
        "abstract url": "https://arxiv.org/abs/2412.07017",
        "title": "Asynchronous LLM Function Calling",
        "rating": "0",
        "keywords": [
            [
                "flight"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "Large language models (LLMs) use function calls to interface with external tools and data source. However, the current approach to LLM function calling is inherently synchronous, where each call blocks LLM inference, limiting LLM operation and concurrent function execution. In this work, we propose AsyncLM, a system for asynchronous LLM function calling. AsyncLM improves LLM's operational efficiency by enabling LLMs to generate and execute function calls concurrently. Instead of waiting for each call's completion, AsyncLM introduces an interrupt mechanism to asynchronously notify the LLM in-flight when function calls return. We design an in-context protocol for function calls and interrupts, provide fine-tuning strategy to adapt LLMs to the interrupt semantics, and implement these mechanisms efficiently on LLM inference process. We demonstrate that AsyncLM can reduce end-to-end task completion latency from 1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks in the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss how interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM interactions.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07049",
        "abstract url": "https://arxiv.org/abs/2412.07049",
        "title": "Static Key Attention in Vision",
        "rating": "0",
        "keywords": [
            [
                "depth"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The success of vision transformers is widely attributed to the expressive power of their dynamically parameterized multi-head self-attention mechanism. We examine the impact of substituting the dynamic parameterized key with a static key within the standard attention mechanism in Vision Transformers. Our findings reveal that static key attention mechanisms can match or even exceed the performance of standard self-attention. Integrating static key attention modules into a Metaformer backbone, we find that it serves as a better intermediate stage in hierarchical hybrid architectures, balancing the strengths of depth-wise convolution and self-attention. Experiments on several vision tasks underscore the effectiveness of the static key mechanism, indicating that the typical two-step dynamic parameterization in attention can be streamlined to a single step without impacting performance under certain circumstances.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07078",
        "abstract url": "https://arxiv.org/abs/2412.07078",
        "title": "Defensive Dual Masking for Robust Adversarial Defense",
        "rating": "0",
        "keywords": [
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The field of textual adversarial defenses has gained considerable attention in recent years due to the increasing vulnerability of natural language processing (NLP) models to adversarial attacks, which exploit subtle perturbations in input text to deceive models. This paper introduces the Defensive Dual Masking (DDM) algorithm, a novel approach designed to enhance model robustness against such attacks. DDM utilizes a unique adversarial training strategy where [MASK] tokens are strategically inserted into training samples to prepare the model to handle adversarial perturbations more effectively. During inference, potentially adversarial tokens are dynamically replaced with [MASK] tokens to neutralize potential threats while preserving the core semantics of the input. The theoretical foundation of our approach is explored, demonstrating how the selective masking mechanism strengthens the model's ability to identify and mitigate adversarial manipulations. Our empirical evaluation across a diverse set of benchmark datasets and attack mechanisms consistently shows that DDM outperforms state-of-the-art defense techniques, improving model accuracy and robustness. Moreover, when applied to Large Language Models (LLMs), DDM also enhances their resilience to adversarial attacks, providing a scalable defense mechanism for large-scale NLP applications.",
        "subjects": [
            "cs.CL",
            "cs.AI"
        ],
        "comment": "First version"
    },
    {
        "paper id": "2412.07080",
        "abstract url": "https://arxiv.org/abs/2412.07080",
        "title": "EvRepSL: Event-Stream Representation via Self-Supervised Learning for Event-Based Vision",
        "rating": "0",
        "keywords": [
            [
                "event cameras"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Event-stream representation is the first step for many computer vision tasks using event cameras. It converts the asynchronous event-streams into a formatted structure so that conventional machine learning models can be applied easily. However, most of the state-of-the-art event-stream representations are manually designed and the quality of these representations cannot be guaranteed due to the noisy nature of event-streams. In this paper, we introduce a data-driven approach aiming at enhancing the quality of event-stream representations. Our approach commences with the introduction of a new event-stream representation based on spatial-temporal statistics, denoted as EvRep. Subsequently, we theoretically derive the intrinsic relationship between asynchronous event-streams and synchronous video frames. Building upon this theoretical relationship, we train a representation generator, RepGen, in a self-supervised learning manner accepting EvRep as input. Finally, the event-streams are converted to high-quality representations, termed as EvRepSL, by going through the learned RepGen (without the need of fine-tuning or retraining). Our methodology is rigorously validated through extensive evaluations on a variety of mainstream event-based classification and optical flow datasets (captured with various types of event cameras). The experimental results highlight not only our approach's superior performance over existing event-stream representations but also its versatility, being agnostic to different event cameras and tasks.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "comment": "Published on IEEE Transactions on Image Processing"
    },
    {
        "paper id": "2412.07105",
        "abstract url": "https://arxiv.org/abs/2412.07105",
        "title": "A Powered Prosthetic Hand with Vision System for Enhancing the Anthropopathic Grasp",
        "rating": "0",
        "keywords": [
            [
                "Trajectory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The anthropomorphism of grasping process significantly benefits the experience and grasping efficiency of prosthetic hand wearers. Currently, prosthetic hands controlled by signals such as brain-computer interfaces (BCI) and electromyography (EMG) face difficulties in precisely recognizing the amputees' grasping gestures and executing anthropomorphic grasp processes. Although prosthetic hands equipped with vision systems enables the objects' feature recognition, they lack perception of human grasping intention. Therefore, this paper explores the estimation of grasping gestures solely through visual data to accomplish anthropopathic grasping control and the determination of grasping intention within a multi-object environment. To address this, we propose the Spatial Geometry-based Gesture Mapping (SG-GM) method, which constructs gesture functions based on the geometric features of the human hand grasping processes. It's subsequently implemented on the prosthetic hand. Furthermore, we propose the Motion Trajectory Regression-based Grasping Intent Estimation (MTR-GIE) algorithm. This algorithm predicts pre-grasping object utilizing regression prediction and prior spatial segmentation estimation derived from the prosthetic hand's position and trajectory. The experiments were conducted to grasp 8 common daily objects including cup, fork, etc. The experimental results presented a similarity coefficient $R^{2}$ of grasping process of 0.911, a Root Mean Squared Error ($RMSE$) of 2.47\\degree, a success rate of grasping of 95.43$\\%$, and an average duration of grasping process of 3.07$\\pm$0.41 s. Furthermore, grasping experiments in a multi-object environment were conducted. The average accuracy of intent estimation reached 94.35$\\%$. Our methodologies offer a groundbreaking approach to enhance the prosthetic hand's functionality and provides valuable insights for future research.",
        "subjects": [
            "cs.RO",
            "cs.CV",
            "cs.HC",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07140",
        "abstract url": "https://arxiv.org/abs/2412.07140",
        "title": "FIRE: Robust Detection of Diffusion-Generated Images via Frequency-Guided Reconstruction Error",
        "rating": "0",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The rapid advancement of diffusion models has significantly improved high-quality image generation, making generated content increasingly challenging to distinguish from real images and raising concerns about potential misuse. In this paper, we observe that diffusion models struggle to accurately reconstruct mid-band frequency information in real images, suggesting the limitation could serve as a cue for detecting diffusion model generated images. Motivated by this observation, we propose a novel method called Frequency-guided Reconstruction Error (FIRE), which, to the best of our knowledge, is the first to investigate the influence of frequency decomposition on reconstruction error. FIRE assesses the variation in reconstruction error before and after the frequency decomposition, offering a robust method for identifying diffusion model generated images. Extensive experiments show that FIRE generalizes effectively to unseen diffusion models and maintains robustness against diverse perturbations.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "14 pages, 14 figures"
    },
    {
        "paper id": "2412.07149",
        "abstract url": "https://arxiv.org/abs/2412.07149",
        "title": "RAP-SR: RestorAtion Prior Enhancement in Diffusion Models for Realistic Image Super-Resolution",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "Super-Resolution"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Benefiting from their powerful generative capabilities, pretrained diffusion models have garnered significant attention for real-world image super-resolution (Real-SR). Existing diffusion-based SR approaches typically utilize semantic information from degraded images and restoration prompts to activate prior for producing realistic high-resolution images. However, general-purpose pretrained diffusion models, not designed for restoration tasks, often have suboptimal prior, and manually defined prompts may fail to fully exploit the generated potential. To address these limitations, we introduce RAP-SR, a novel restoration prior enhancement approach in pretrained diffusion models for Real-SR. First, we develop the High-Fidelity Aesthetic Image Dataset (HFAID), curated through a Quality-Driven Aesthetic Image Selection Pipeline (QDAISP). Our dataset not only surpasses existing ones in fidelity but also excels in aesthetic quality. Second, we propose the Restoration Priors Enhancement Framework, which includes Restoration Priors Refinement (RPR) and Restoration-Oriented Prompt Optimization (ROPO) modules. RPR refines the restoration prior using the HFAID, while ROPO optimizes the unique restoration identifier, improving the quality of the resulting images. RAP-SR effectively bridges the gap between general-purpose models and the demands of Real-SR by enhancing restoration prior. Leveraging the plug-and-play nature of RAP-SR, our approach can be seamlessly integrated into existing diffusion-based SR methods, boosting their performance. Extensive experiments demonstrate its broad applicability and state-of-the-art results. Codes and datasets will be available upon acceptance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "15 pages, 12 figures"
    },
    {
        "paper id": "2412.07152",
        "abstract url": "https://arxiv.org/abs/2412.07152",
        "title": "Hero-SR: One-Step Diffusion for Super-Resolution with Human Perception Priors",
        "rating": "0",
        "keywords": [
            [
                "Diffusion",
                "Super-Resolution"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Owing to the robust priors of diffusion models, recent approaches have shown promise in addressing real-world super-resolution (Real-SR). However, achieving semantic consistency and perceptual naturalness to meet human perception demands remains difficult, especially under conditions of heavy degradation and varied input complexities. To tackle this, we propose Hero-SR, a one-step diffusion-based SR framework explicitly designed with human perception priors. Hero-SR consists of two novel modules: the Dynamic Time-Step Module (DTSM), which adaptively selects optimal diffusion steps for flexibly meeting human perceptual standards, and the Open-World Multi-modality Supervision (OWMS), which integrates guidance from both image and text domains through CLIP to improve semantic consistency and perceptual naturalness. Through these modules, Hero-SR generates high-resolution images that not only preserve intricate details but also reflect human perceptual preferences. Extensive experiments validate that Hero-SR achieves state-of-the-art performance in Real-SR. The code will be publicly available upon paper acceptance.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "16 pages, 9 figures"
    },
    {
        "paper id": "2412.06239",
        "abstract url": "https://arxiv.org/abs/2412.06239",
        "title": "Unseen Attack Detection in Software-Defined Networking Using a BERT-Based Large Language Model",
        "rating": "-0.5",
        "keywords": [
            [
                "Attack"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Software defined networking (SDN) represents a transformative shift in network architecture by decoupling the control plane from the data plane, enabling centralized and flexible management of network resources. However, this architectural shift introduces significant security challenges, as SDN's centralized control becomes an attractive target for various types of attacks. While current research has yielded valuable insights into attack detection in SDN, critical gaps remain. Addressing challenges in feature selection, broadening the scope beyond DDoS attacks, strengthening attack decisions based on multi flow analysis, and building models capable of detecting unseen attacks that they have not been explicitly trained on are essential steps toward advancing security in SDN. In this paper, we introduce a novel approach that leverages Natural Language Processing (NLP) and the pre trained BERT base model to enhance attack detection in SDN. Our approach transforms network flow data into a format interpretable by language models, allowing BERT to capture intricate patterns and relationships within network traffic. By using Random Forest for feature selection, we optimize model performance and reduce computational overhead, ensuring accurate detection. Attack decisions are made based on several flows, providing stronger and more reliable detection of malicious traffic. Furthermore, our approach is specifically designed to detect previously unseen attacks, offering a solution for identifying threats that the model was not explicitly trained on. To rigorously evaluate our approach, we conducted experiments in two scenarios: one focused on detecting known attacks, achieving 99.96% accuracy, and another on detecting unseen attacks, where our model achieved 99.96% accuracy, demonstrating the robustness of our approach in detecting evolving threats to improve the security of SDN networks.",
        "subjects": [
            "cs.CR",
            "cs.AI"
        ],
        "comment": "Mohammed N. Swileh is first author. Shengli Zhang is corresponding author"
    },
    {
        "paper id": "2412.06322",
        "abstract url": "https://arxiv.org/abs/2412.06322",
        "title": "LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph Generation with Enhanced Spatial Relations",
        "rating": "-0.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "Graph"
            ],
            [
                "cs.CV"
            ],
            [
                "WACV"
            ]
        ],
        "abstract": "Scene Graph Generation (SGG) converts visual scenes into structured graph representations, providing deeper scene understanding for complex vision tasks. However, existing SGG models often overlook essential spatial relationships and struggle with generalization in open-vocabulary contexts. To address these limitations, we propose LLaVA-SpaceSGG, a multimodal large language model (MLLM) designed for open-vocabulary SGG with enhanced spatial relation modeling. To train it, we collect the SGG instruction-tuning dataset, named SpaceSGG. This dataset is constructed by combining publicly available datasets and synthesizing data using open-source models within our data construction pipeline. It combines object locations, object relations, and depth information, resulting in three data formats: spatial SGG description, question-answering, and conversation. To enhance the transfer of MLLMs' inherent capabilities to the SGG task, we introduce a two-stage training paradigm. Experiments show that LLaVA-SpaceSGG outperforms other open-vocabulary SGG methods, boosting recall by 8.6% and mean recall by 28.4% compared to the baseline. Our codebase, dataset, and trained models are publicly accessible on GitHub at the following URL: https://github.com/Endlinc/LLaVA-SpaceSGG.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted by the WACV 2025, including supplementary material"
    },
    {
        "paper id": "2412.06354",
        "abstract url": "https://arxiv.org/abs/2412.06354",
        "title": "GraphNeuralNetworks.jl: Deep Learning on Graphs with Julia",
        "rating": "-0.5",
        "keywords": [
            [
                "Graphs"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "GraphNeuralNetworks.jl is an open-source framework for deep learning on graphs, written in the Julia programming language. It supports multiple GPU backends, generic sparse or dense graph representations, and offers convenient interfaces for manipulating standard, heterogeneous, and temporal graphs with attributes at the node, edge, and graph levels. The framework allows users to define custom graph convolutional layers using gather/scatter message-passing primitives or optimized fused operations. It also includes several popular layers, enabling efficient experimentation with complex deep architectures. The package is available on GitHub: \\url{https://github.com/JuliaGraphs/GraphNeuralNetworks.jl}.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "Submitted to JMLR OSS"
    },
    {
        "paper id": "2412.06438",
        "abstract url": "https://arxiv.org/abs/2412.06438",
        "title": "Can foundation models actively gather information in interactive environments to test hypotheses?",
        "rating": "-0.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "While problem solving is a standard evaluation task for foundation models, a crucial component of problem solving -- actively and strategically gathering information to test hypotheses -- has not been closely investigated. To assess the information gathering abilities of foundation models in interactive environments, we introduce a framework in which a model must determine the factors influencing a hidden reward function by iteratively reasoning about its previously gathered information and proposing its next exploratory action to maximize information gain at each step. We implement this framework in both a text-based environment, which offers a tightly controlled setting and enables high-throughput parameter sweeps, and in an embodied 3D environment, which requires addressing complexities of multi-modal interaction more relevant to real-world applications. We further investigate whether approaches such as self-correction and increased inference time improve information gathering efficiency. In a relatively simple task that requires identifying a single rewarding feature, we find that LLM's information gathering capability is close to optimal. However, when the model must identify a conjunction of rewarding features, performance is suboptimal. The hit in performance is due partly to the model translating task description to a policy and partly to the model's effectiveness in using its in-context memory. Performance is comparable in both text and 3D embodied environments, although imperfect visual object recognition reduces its accuracy in drawing conclusions from gathered information in the 3D embodied case. For single-feature-based rewards, we find that smaller models curiously perform better; for conjunction-based rewards, incorporating self correction into the model improves performance.",
        "subjects": [
            "cs.LG",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06465",
        "abstract url": "https://arxiv.org/abs/2412.06465",
        "title": "Agent Journey Beyond RGB: Unveiling Hybrid Semantic-Spatial Environmental Representations for Vision-and-Language Navigation",
        "rating": "-0.5",
        "keywords": [
            [
                "Depth"
            ],
            [
                "Navigation"
            ],
            [
                "cs.CV"
            ],
            [
                "CVPR"
            ]
        ],
        "abstract": "Navigating unseen environments based on natural language instructions remains difficult for egocentric agents in Vision-and-Language Navigation (VLN). While recent advancements have yielded promising outcomes, they primarily rely on RGB images for environmental representation, often overlooking the underlying semantic knowledge and spatial cues. Intuitively, humans inherently ground textual semantics within the spatial layout during indoor navigation. Inspired by this, we propose a versatile Semantic Understanding and Spatial Awareness (SUSA) architecture to facilitate navigation. SUSA includes a Textual Semantic Understanding (TSU) module, which narrows the modality gap between instructions and environments by generating and associating the descriptions of environmental landmarks in the agent's immediate surroundings. Additionally, a Depth-based Spatial Perception (DSP) module incrementally constructs a depth exploration map, enabling a more nuanced comprehension of environmental layouts. Experimental results demonstrate that SUSA hybrid semantic-spatial representations effectively enhance navigation performance, setting new state-of-the-art performance across three VLN benchmarks (REVERIE, R2R, and SOON). The source code will be publicly available.",
        "subjects": [
            "cs.CV",
            "cs.MM"
        ],
        "comment": "underreview in CVPR 2025"
    },
    {
        "paper id": "2412.06555",
        "abstract url": "https://arxiv.org/abs/2412.06555",
        "title": "When Dimensionality Reduction Meets Graph (Drawing) Theory: Introducing a Common Framework, Challenges and Opportunities",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the vast landscape of visualization research, Dimensionality Reduction (DR) and graph analysis are two popular subfields, often essential to most visual data analytics setups. DR aims to create representations to support neighborhood and similarity analysis on complex, large datasets. Graph analysis focuses on identifying the salient topological properties and key actors within networked data, with specialized research on investigating how such features could be presented to the user to ease the comprehension of the underlying structure. Although these two disciplines are typically regarded as disjoint subfields, we argue that both fields share strong similarities and synergies that can potentially benefit both. Therefore, this paper discusses and introduces a unifying framework to help bridge the gap between DR and graph (drawing) theory. Our goal is to use the strongly math-grounded graph theory to improve the overall process of creating DR visual representations. We propose how to break the DR process into well-defined stages, discussing how to match some of the DR state-of-the-art techniques to this framework and presenting ideas on how graph drawing, topology features, and some popular algorithms and strategies used in graph analysis can be employed to improve DR topology extraction, embedding generation, and result validation. We also discuss the challenges and identify opportunities for implementing and using our framework, opening directions for future visualization research.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06568",
        "abstract url": "https://arxiv.org/abs/2412.06568",
        "title": "CONDEN-FI: Consistency and Diversity Learning-based Multi-View Unsupervised Feature and In-stance Co-Selection",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The objective of multi-view unsupervised feature and instance co-selection is to simultaneously iden-tify the most representative features and samples from multi-view unlabeled data, which aids in mit-igating the curse of dimensionality and reducing instance size to improve the performance of down-stream tasks. However, existing methods treat feature selection and instance selection as two separate processes, failing to leverage the potential interactions between the feature and instance spaces. Addi-tionally, previous co-selection methods for multi-view data require concatenating different views, which overlooks the consistent information among them. In this paper, we propose a CONsistency and DivErsity learNing-based multi-view unsupervised Feature and Instance co-selection (CONDEN-FI) to address the above-mentioned issues. Specifically, CONDEN-FI reconstructs mul-ti-view data from both the sample and feature spaces to learn representations that are consistent across views and specific to each view, enabling the simultaneous selection of the most important features and instances. Moreover, CONDEN-FI adaptively learns a view-consensus similarity graph to help select both dissimilar and similar samples in the reconstructed data space, leading to a more diverse selection of instances. An efficient algorithm is developed to solve the resultant optimization problem, and the comprehensive experimental results on real-world datasets demonstrate that CONDEN-FI is effective compared to state-of-the-art methods.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06617",
        "abstract url": "https://arxiv.org/abs/2412.06617",
        "title": "AI TrackMate: Finally, Someone Who Will Give Your Music More Than Just \"Sounds Great!\"",
        "rating": "-0.5",
        "keywords": [
            [
                "Music"
            ],
            [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "The rise of \"bedroom producers\" has democratized music creation, while challenging producers to objectively evaluate their work. To address this, we present AI TrackMate, an LLM-based music chatbot designed to provide constructive feedback on music productions. By combining LLMs' inherent musical knowledge with direct audio track analysis, AI TrackMate offers production-specific insights, distinguishing it from text-only approaches. Our framework integrates a Music Analysis Module, an LLM-Readable Music Report, and Music Production-Oriented Feedback Instruction, creating a plug-and-play, training-free system compatible with various LLMs and adaptable to future advancements. We demonstrate AI TrackMate's capabilities through an interactive web interface and present findings from a pilot study with a music producer. By bridging AI capabilities with the needs of independent producers, AI TrackMate offers on-demand analytical feedback, potentially supporting the creative process and skill development in music production. This system addresses the growing demand for objective self-assessment tools in the evolving landscape of independent music production.",
        "subjects": [
            "cs.SD",
            "cs.HC",
            "cs.LG",
            "cs.MM",
            "eess.AS"
        ],
        "comment": "Accepted for the NeurIPS 2024 Creative AI Track"
    },
    {
        "paper id": "2412.06689",
        "abstract url": "https://arxiv.org/abs/2412.06689",
        "title": "Impact of Privacy Parameters on Deep Learning Models for Image Classification",
        "rating": "-0.5",
        "keywords": [
            [
                "Support Vector Machine"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The project aims to develop differentially private deep learning models for image classification on CIFAR-10 datasets \\cite{cifar10} and analyze the impact of various privacy parameters on model accuracy. We have implemented five different deep learning models, namely ConvNet, ResNet18, EfficientNet, ViT, and DenseNet121 and three supervised classifiers namely K-Nearest Neighbors, Naive Bayes Classifier and Support Vector Machine. We evaluated the performance of these models under varying settings. Our best performing model to date is EfficientNet with test accuracy of $59.63\\%$ with the following parameters (Adam optimizer, batch size 256, epoch size 100, epsilon value 5.0, learning rate $1e-3$, clipping threshold 1.0, and noise multiplier 0.912).",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": "10 pages"
    },
    {
        "paper id": "2412.06711",
        "abstract url": "https://arxiv.org/abs/2412.06711",
        "title": "MISFEAT: Feature Selection for Subgroups with Systematic Missing Data",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We investigate the problem of selecting features for datasets that can be naturally partitioned into subgroups (e.g., according to socio-demographic groups and age), each with its own dominant set of features. Within this subgroup-oriented framework, we address the challenge of systematic missing data, a scenario in which some feature values are missing for all tuples of a subgroup, due to flawed data integration, regulatory constraints, or privacy concerns. Feature selection is governed by finding mutual Information, a popular quantification of correlation, between features and a target variable. Our goal is to identify top-K feature subsets of some fixed size with the highest joint mutual information with a target variable. In the presence of systematic missing data, the closed form of mutual information could not simply be applied. We argue that in such a setting, leveraging relationships between available feature mutual information within a subgroup or across subgroups can assist inferring missing mutual information values. We propose a generalizable model based on heterogeneous graph neural network to identify interdependencies between feature-subgroup-target variable connections by modeling it as a multiplex graph, and employing information propagation between its nodes. We address two distinct scalability challenges related to training and propose principled solutions to tackle them. Through an extensive empirical evaluation, we demonstrate the efficacy of the proposed solutions both qualitatively and running time wise.",
        "subjects": [
            "cs.LG",
            "cs.DB",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06785",
        "abstract url": "https://arxiv.org/abs/2412.06785",
        "title": "Tactile DreamFusion: Exploiting Tactile Sensing for 3D Generation",
        "rating": "-0.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "3D generation methods have shown visually compelling results powered by diffusion image priors. However, they often fail to produce realistic geometric details, resulting in overly smooth surfaces or geometric details inaccurately baked in albedo maps. To address this, we introduce a new method that incorporates touch as an additional modality to improve the geometric details of generated 3D assets. We design a lightweight 3D texture field to synthesize visual and tactile textures, guided by 2D diffusion model priors on both visual and tactile domains. We condition the visual texture generation on high-resolution tactile normals and guide the patch-based tactile texture refinement with a customized TextureDreambooth. We further present a multi-part generation pipeline that enables us to synthesize different textures across various regions. To our knowledge, we are the first to leverage high-resolution tactile sensing to enhance geometric details for 3D generation tasks. We evaluate our method in both text-to-3D and image-to-3D settings. Our experiments demonstrate that our method provides customized and realistic fine geometric textures while maintaining accurate alignment between two modalities of vision and touch.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "Accepted to NeurIPS 2024. Project webpage: https://ruihangao.github.io/TactileDreamFusion/ Code: https://github.com/RuihanGao/TactileDreamFusion"
    },
    {
        "paper id": "2412.06862",
        "abstract url": "https://arxiv.org/abs/2412.06862",
        "title": "Stock Type Prediction Model Based on Hierarchical Graph Neural Network",
        "rating": "-0.5",
        "keywords": [
            [
                "Graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "This paper introduces a novel approach to stock data analysis by employing a Hierarchical Graph Neural Network (HGNN) model that captures multi-level information and relational structures in the stock market. The HGNN model integrates stock relationship data and hierarchical attributes to predict stock types effectively. The paper discusses the construction of a stock industry relationship graph and the extraction of temporal information from historical price sequences. It also highlights the design of a graph convolution operation and a temporal attention aggregator to model the macro market state. The integration of these features results in a comprehensive stock prediction model that addresses the challenges of utilizing stock relationship data and modeling hierarchical attributes in the stock market.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06959",
        "abstract url": "https://arxiv.org/abs/2412.06959",
        "title": "Geological and Well prior assisted full waveform inversion using conditional diffusion models",
        "rating": "-0.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Full waveform inversion (FWI) often faces challenges due to inadequate seismic observations, resulting in band-limited and geologically inaccurate inversion results. Incorporating prior information from potential velocity distributions, well-log information, and our geological knowledge and expectations can significantly improve FWI convergence to a realistic model. While diffusion-regularized FWI has shown improved performance compared to conventional FWI by incorporating the velocity distribution prior, it can benefit even more by incorporating well-log information and other geological knowledge priors. To leverage this fact, we propose a geological class and well-information prior-assisted FWI using conditional diffusion models. This method seamlessly integrates multi-modal information into FWI, simultaneously achieving data fitting and universal geologic and geophysics prior matching, which is often not achieved with traditional regularization methods. Specifically, we propose to combine conditional diffusion models with FWI, where we integrate well-log data and geological class conditions into these conditional diffusion models using classifier-free guidance for multi-modal prior matching beyond the original velocity distribution prior. Numerical experiments on the OpenFWI datasets and field marine data demonstrate the effectiveness of our method compared to conventional FWI and the unconditional diffusion-regularized FWI.",
        "subjects": [
            "physics.geo-ph",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06981",
        "abstract url": "https://arxiv.org/abs/2412.06981",
        "title": "Diffusing Differentiable Representations",
        "rating": "-0.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "cs.LG",
                "cs.CV"
            ],
            [
                "NeurIPS"
            ]
        ],
        "abstract": "We introduce a novel, training-free method for sampling differentiable representations (diffreps) using pretrained diffusion models. Rather than merely mode-seeking, our method achieves sampling by \"pulling back\" the dynamics of the reverse-time process--from the image space to the diffrep parameter space--and updating the parameters according to this pulled-back process. We identify an implicit constraint on the samples induced by the diffrep and demonstrate that addressing this constraint significantly improves the consistency and detail of the generated objects. Our method yields diffreps with substantially improved quality and diversity for images, panoramas, and 3D NeRFs compared to existing techniques. Our approach is a general-purpose method for sampling diffreps, expanding the scope of problems that diffusion models can tackle.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": "Published at NeurIPS 2024"
    },
    {
        "paper id": "2412.07062",
        "abstract url": "https://arxiv.org/abs/2412.07062",
        "title": "Optimizing Personalized Federated Learning through Adaptive Layer-Wise Learning",
        "rating": "-0.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Real-life deployment of federated Learning (FL) often faces non-IID data, which leads to poor accuracy and slow convergence. Personalized FL (pFL) tackles these issues by tailoring local models to individual data sources and using weighted aggregation methods for client-specific learning. However, existing pFL methods often fail to provide each local model with global knowledge on demand while maintaining low computational overhead. Additionally, local models tend to over-personalize their data during the training process, potentially dropping previously acquired global information. We propose FLAYER, a novel layer-wise learning method for pFL that optimizes local model personalization performance. FLAYER considers the different roles and learning abilities of neural network layers of individual local models. It incorporates global information for each local model as needed to initialize the local model cost-effectively. It then dynamically adjusts learning rates for each layer during local training, optimizing the personalized learning process for each local model while preserving global knowledge. Additionally, to enhance global representation in pFL, FLAYER selectively uploads parameters for global aggregation in a layer-wise manner. We evaluate FLAYER on four representative datasets in computer vision and natural language processing domains. Compared to six state-of-the-art pFL methods, FLAYER improves the inference accuracy, on average, by 5.42% (up to 14.29%).",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07081",
        "abstract url": "https://arxiv.org/abs/2412.07081",
        "title": "Sequential Controlled Langevin Diffusions",
        "rating": "-0.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "An effective approach for sampling from unnormalized densities is based on the idea of gradually transporting samples from an easy prior to the complicated target distribution. Two popular methods are (1) Sequential Monte Carlo (SMC), where the transport is performed through successive annealed densities via prescribed Markov chains and resampling steps, and (2) recently developed diffusion-based sampling methods, where a learned dynamical transport is used. Despite the common goal, both approaches have different, often complementary, advantages and drawbacks. The resampling steps in SMC allow focusing on promising regions of the space, often leading to robust performance. While the algorithm enjoys asymptotic guarantees, the lack of flexible, learnable transitions can lead to slow convergence. On the other hand, diffusion-based samplers are learned and can potentially better adapt themselves to the target at hand, yet often suffer from training instabilities. In this work, we present a principled framework for combining SMC with diffusion-based samplers by viewing both methods in continuous time and considering measures on path space. This culminates in the new Sequential Controlled Langevin Diffusion (SCLD) sampling method, which is able to utilize the benefits of both methods and reaches improved performance on multiple benchmark problems, in many cases using only 10% of the training budget of previous diffusion-based samplers.",
        "subjects": [
            "stat.ML",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07106",
        "abstract url": "https://arxiv.org/abs/2412.07106",
        "title": "Covered Forest: Fine-grained generalization analysis of graph neural networks",
        "rating": "-0.5",
        "keywords": [
            [
                "graph"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The expressive power of message-passing graph neural networks (MPNNs) is reasonably well understood, primarily through combinatorial techniques from graph isomorphism testing. However, MPNNs' generalization abilities -- making meaningful predictions beyond the training set -- remain less explored. Current generalization analyses often overlook graph structure, limit the focus to specific aggregation functions, and assume the impractical, hard-to-optimize $0$-$1$ loss function. Here, we extend recent advances in graph similarity theory to assess the influence of graph structure, aggregation, and loss functions on MPNNs' generalization abilities. Our empirical study supports our theoretical insights, improving our understanding of MPNNs' generalization properties.",
        "subjects": [
            "cs.LG",
            "cs.DM",
            "cs.DS",
            "cs.NE",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07127",
        "abstract url": "https://arxiv.org/abs/2412.07127",
        "title": "Deep Learning-Enhanced Preconditioning for Efficient Conjugate Gradient Solvers in Large-Scale PDE Systems",
        "rating": "-0.5",
        "keywords": [
            [
                "GNN",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Preconditioning techniques are crucial for enhancing the efficiency of solving large-scale linear equation systems that arise from partial differential equation (PDE) discretization. These techniques, such as Incomplete Cholesky factorization (IC) and data-driven neural network methods, accelerate the convergence of iterative solvers like Conjugate Gradient (CG) by approximating the original matrices. This paper introduces a novel approach that integrates Graph Neural Network (GNN) with traditional IC, addressing the shortcomings of direct generation methods based on GNN and achieving significant improvements in computational efficiency and scalability. Experimental results demonstrate an average reduction in iteration counts by 24.8% compared to IC and a two-order-of-magnitude increase in training scale compared to previous methods. A three-dimensional static structural analysis utilizing finite element methods was validated on training sparse matrices of up to 5 million dimensions and inference scales of up to 10 million. Furthermore, the approach demon-strates robust generalization capabilities across scales, facilitating the effective acceleration of CG solvers for large-scale linear equations using small-scale data on modest hardware. The method's robustness and scalability make it a practical solution for computational science.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "math.NA"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07188",
        "abstract url": "https://arxiv.org/abs/2412.07188",
        "title": "Graph Neural Networks Are More Than Filters: Revisiting and Benchmarking from A Spectral Perspective",
        "rating": "-0.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success in various graph-based learning tasks. While their performance is often attributed to the powerful neighborhood aggregation mechanism, recent studies suggest that other components such as non-linear layers may also significantly affecting how GNNs process the input graph data in the spectral domain. Such evidence challenges the prevalent opinion that neighborhood aggregation mechanisms dominate the behavioral characteristics of GNNs in the spectral domain. To demystify such a conflict, this paper introduces a comprehensive benchmark to measure and evaluate GNNs' capability in capturing and leveraging the information encoded in different frequency components of the input graph data. Specifically, we first conduct an exploratory study demonstrating that GNNs can flexibly yield outputs with diverse frequency components even when certain frequencies are absent or filtered out from the input graph data. We then formulate a novel research problem of measuring and benchmarking the performance of GNNs from a spectral perspective. To take an initial step towards a comprehensive benchmark, we design an evaluation protocol supported by comprehensive theoretical analysis. Finally, we introduce a comprehensive benchmark on real-world datasets, revealing insights that challenge prevalent opinions from a spectral perspective. We believe that our findings will open new avenues for future advancements in this area. Our implementations can be found at: https://github.com/yushundong/Spectral-benchmark.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06248",
        "abstract url": "https://arxiv.org/abs/2412.06248",
        "title": "Rendering-Refined Stable Diffusion for Privacy Compliant Synthetic Data",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Growing privacy concerns and regulations like GDPR and CCPA necessitate pseudonymization techniques that protect identity in image datasets. However, retaining utility is also essential. Traditional methods like masking and blurring degrade quality and obscure critical context, especially in human-centric images. We introduce Rendering-Refined Stable Diffusion (RefSD), a pipeline that combines 3D-rendering with Stable Diffusion, enabling prompt-based control over human attributes while preserving posture. Unlike standard diffusion models that fail to retain posture or GANs that lack realism and flexible attribute control, RefSD balances posture preservation, realism, and customization. We also propose HumanGenAI, a framework for human perception and utility evaluation. Human perception assessments reveal attribute-specific strengths and weaknesses of RefSD. Our utility experiments show that models trained on RefSD pseudonymized data outperform those trained on real data in detection tasks, with further performance gains when combining RefSD with real data. For classification tasks, we consistently observe performance improvements when using RefSD data with real data, confirming the utility of our pseudonymized data.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06254",
        "abstract url": "https://arxiv.org/abs/2412.06254",
        "title": "Towards a Comprehensive Framework for Cyber-Incident Response Decision Support in Smart Grids",
        "rating": "-1",
        "keywords": [
            [
                "Attack"
            ]
        ],
        "abstract": "The modernization of power grid infrastructures necessitates the incorporation of decision support systems to effectively mitigate cybersecurity threats. This paper presents a comprehensive framework based on integrating Attack-Defense Trees and the Multi-Criteria Decision Making method to enhance smart grid cybersecurity. By analyzing risk attributes and optimizing defense strategies, this framework enables grid operators to prioritize critical security measures. Additionally, this paper incorporates findings on decision-making processes in intelligent power systems to present a comprehensive approach to grid cybersecurity. The proposed model aims to optimize the effectiveness and efficiency of grid cybersecurity efforts while offering insights into future grid management challenges.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06255",
        "abstract url": "https://arxiv.org/abs/2412.06255",
        "title": "Simulation of Multi-Stage Attack and Defense Mechanisms in Smart Grids",
        "rating": "-1",
        "keywords": [
            [
                "Attack"
            ]
        ],
        "abstract": "The power grid is a critical infrastructure essential for public safety and welfare. As its reliance on digital technologies grows, so do its vulnerabilities to sophisticated cyber threats, which could severely disrupt operations. Effective protective measures, such as intrusion detection and decision support systems, are essential to mitigate these risks. Machine learning offers significant potential in this field, yet its effectiveness is constrained by the limited availability of high-quality data due to confidentiality and access restrictions. To address this, we introduce a simulation environment that replicates the power grid's infrastructure and communication dynamics. This environment enables the modeling of complex, multi-stage cyber attacks and defensive responses, using attack trees to outline attacker strategies and game-theoretic approaches to model defender actions. The framework generates diverse, realistic attack data to train machine learning algorithms for detecting and mitigating cyber threats. It also provides a controlled, flexible platform to evaluate emerging security technologies, including advanced decision support systems. The environment is modular and scalable, facilitating the integration of new scenarios without dependence on external components. It supports scenario generation, data modeling, mapping, power flow simulation, and communication traffic analysis in a cohesive chain, capturing all relevant data for cyber security investigations under consistent conditions. Detailed modeling of communication protocols and grid operations offers insights into attack propagation, while datasets undergo validation in laboratory settings to ensure real-world applicability. These datasets are leveraged to train machine learning models for intrusion detection, focusing on their ability to identify complex attack patterns within power grid operations.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06259",
        "abstract url": "https://arxiv.org/abs/2412.06259",
        "title": "Leveraging Prompt Learning and Pause Encoding for Alzheimer's Disease Detection",
        "rating": "-1",
        "keywords": [
            [
                "Disease",
                "clinical"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Compared to other clinical screening techniques, speech-and-language-based automated Alzheimer's disease (AD) detection methods are characterized by their non-invasiveness, cost-effectiveness, and convenience. Previous studies have demonstrated the efficacy of fine-tuning pre-trained language models (PLMs) for AD detection. However, the objective of this traditional fine-tuning method, which involves inputting only transcripts, is inconsistent with the masked language modeling (MLM) task used during the pre-training phase of PLMs. In this paper, we investigate prompt-based fine-tuning of PLMs, converting the classification task into a MLM task by inserting prompt templates into the transcript inputs. We also explore the impact of incorporating pause information from forced alignment into manual transcripts. Additionally, we compare the performance of various automatic speech recognition (ASR) models and select the Whisper model to generate ASR-based transcripts for comparison with manual transcripts. Furthermore, majority voting and ensemble techniques are applied across different PLMs (BERT and RoBERTa) using different random seeds. Ultimately, we obtain maximum detection accuracy of 95.8% (with mean 87.9%, std 3.3%) using manual transcripts, achieving state-of-the-art performance for AD detection using only transcripts on the ADReSS test set.",
        "subjects": [
            "eess.AS",
            "cs.SD"
        ],
        "comment": "Accepted by ISCSLP 2024"
    },
    {
        "paper id": "2412.06262",
        "abstract url": "https://arxiv.org/abs/2412.06262",
        "title": "A Lightweight U-like Network Utilizing Neural Memory Ordinary Differential Equations for Slimming the Decoder",
        "rating": "-1",
        "keywords": [
            [
                "medical"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "In recent years, advanced U-like networks have demonstrated remarkable performance in medical image segmentation tasks. However, their drawbacks, including excessive parameters, high computational complexity, and slow inference speed, pose challenges for practical implementation in scenarios with limited computational resources. Existing lightweight U-like networks have alleviated some of these problems, but they often have pre-designed structures and consist of inseparable modules, limiting their application scenarios. In this paper, we propose three plug-and-play decoders by employing different discretization methods of the neural memory Ordinary Differential Equations (nmODEs). These decoders integrate features at various levels of abstraction by processing information from skip connections and performing numerical operations on upward path. Through experiments on the PH2, ISIC2017, and ISIC2018 datasets, we embed these decoders into different U-like networks, demonstrating their effectiveness in significantly reducing the number of parameters and FLOPs while maintaining performance. In summary, the proposed discretized nmODEs decoders are capable of reducing the number of parameters by about 20% ~ 50% and FLOPs by up to 74%, while possessing the potential to adapt to all U-like networks. Our code is available at https://github.com/nayutayuki/Lightweight-nmODE-Decoders-For-U-like-networks.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06275",
        "abstract url": "https://arxiv.org/abs/2412.06275",
        "title": "Performance Analysis and Code Design for Resistive Random-Access Memory Using Channel Decomposition Approach",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "A novel framework for performance analysis and code design is proposed to address the sneak path (SP) problem in resistive random-access memory (ReRAM) arrays. The main idea is to decompose the ReRAM channel, which is both non-ergodic and data-dependent, into multiple stationary memoryless channels. A finite-length performance bound is derived by analyzing the capacity and dispersion of these stationary memoryless channels. Furthermore, leveraging this channel decomposition, a practical sparse-graph code design is proposed using density evolution. The obtained channel codes are not only asymptotic capacity approaching but also close to the derived finite-length performance bound.",
        "subjects": [
            "cs.IT"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06279",
        "abstract url": "https://arxiv.org/abs/2412.06279",
        "title": "Reconfigurable Holographic Surface-aided Distributed MIMO Radar Systems",
        "rating": "-1",
        "keywords": [
            [
                "Radar"
            ]
        ],
        "abstract": "Distributed phased Multiple-Input Multiple-Output (phased-MIMO) radar systems have attracted wide attention in target detection and tracking. However, the phase-shifting circuits in phased subarrays contribute to high power consumption and hardware cost. To address this issue, an energy-efficient and cost-efficient metamaterial antenna array, i.e., reconfigurable holographic surface (RHS), has been developed. In this letter, we propose RHS-aided distributed MIMO radar systems to achieve more accurate multi-target detection under equivalent power consumption and hardware cost as that of distributed phased-MIMO radar systems. Different from phased arrays, the RHS achieves beam steering by regulating the radiation amplitude of its elements, and thus conventional beamforming schemes designed for phased arrays are no longer applicable. Aiming to maximize detection accuracy, we design an amplitude-controlled beamforming scheme for multiple RHS transceiver subarrays. The simulations validate the superiority of the proposed scheme over the distributed phased-MIMO radar scheme and reveal the optimal allocation of spatial diversity and coherent processing gain that leads to the best system performance when hardware resources are fixed.",
        "subjects": [
            "eess.SP",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06285",
        "abstract url": "https://arxiv.org/abs/2412.06285",
        "title": "Neural Garment Dynamic Super-Resolution",
        "rating": "-1",
        "keywords": [
            [
                "Super-Resolution"
            ],
            [
                "graph"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Achieving efficient, high-fidelity, high-resolution garment simulation is challenging due to its computational demands. Conversely, low-resolution garment simulation is more accessible and ideal for low-budget devices like smartphones. In this paper, we introduce a lightweight, learning-based method for garment dynamic super-resolution, designed to efficiently enhance high-resolution, high-frequency details in low-resolution garment simulations. Starting with low-resolution garment simulation and underlying body motion, we utilize a mesh-graph-net to compute super-resolution features based on coarse garment dynamics and garment-body interactions. These features are then used by a hyper-net to construct an implicit function of detailed wrinkle residuals for each coarse mesh triangle. Considering the influence of coarse garment shapes on detailed wrinkle performance, we correct the coarse garment shape and predict detailed wrinkle residuals using these implicit functions. Finally, we generate detailed high-resolution garment geometry by applying the detailed wrinkle residuals to the corrected coarse garment. Our method enables roll-out prediction by iteratively using its predictions as input for subsequent frames, producing fine-grained wrinkle details to enhance the low-resolution simulation. Despite training on a small dataset, our network robustly generalizes to different body shapes, motions, and garment types not present in the training data. We demonstrate significant improvements over state-of-the-art alternatives, particularly in enhancing the quality of high-frequency, fine-grained wrinkle details.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06287",
        "abstract url": "https://arxiv.org/abs/2412.06287",
        "title": "PediaBench: A Comprehensive Chinese Pediatric Dataset for Benchmarking Large Language Models",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "disease",
                "clinical"
            ],
            [
                "cs.CL"
            ]
        ],
        "abstract": "The emergence of Large Language Models (LLMs) in the medical domain has stressed a compelling need for standard datasets to evaluate their question-answering (QA) performance. Although there have been several benchmark datasets for medical QA, they either cover common knowledge across different departments or are specific to another department rather than pediatrics. Moreover, some of them are limited to objective questions and do not measure the generation capacity of LLMs. Therefore, they cannot comprehensively assess the QA ability of LLMs in pediatrics. To fill this gap, we construct PediaBench, the first Chinese pediatric dataset for LLM evaluation. Specifically, it contains 4,565 objective questions and 1,632 subjective questions spanning 12 pediatric disease groups. It adopts an integrated scoring criterion based on different difficulty levels to thoroughly assess the proficiency of an LLM in instruction following, knowledge understanding, clinical case analysis, etc. Finally, we validate the effectiveness of PediaBench with extensive experiments on 20 open-source and commercial LLMs. Through an in-depth analysis of experimental results, we offer insights into the ability of LLMs to answer pediatric questions in the Chinese context, highlighting their limitations for further improvements. Our code and data are published at https://github.com/ACMISLab/PediaBench.",
        "subjects": [
            "cs.CL"
        ],
        "comment": "21 pages, 12 figures"
    },
    {
        "paper id": "2412.06314",
        "abstract url": "https://arxiv.org/abs/2412.06314",
        "title": "CAD-Unet: A Capsule Network-Enhanced Unet Architecture for Accurate Segmentation of COVID-19 Lung Infections from CT Images",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "diagnosing",
                "CT",
                "clinical",
                "lesion"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Since the outbreak of the COVID-19 pandemic in 2019, medical imaging has emerged as a primary modality for diagnosing COVID-19 pneumonia. In clinical settings, the segmentation of lung infections from computed tomography images enables rapid and accurate quantification and diagnosis of COVID-19. Segmentation of COVID-19 infections in the lungs poses a formidable challenge, primarily due to the indistinct boundaries and limited contrast presented by ground glass opacity manifestations. Moreover, the confounding similarity between infiltrates, lung tissues, and lung walls further complicates this segmentation task. To address these challenges, this paper introduces a novel deep network architecture, called CAD-Unet, for segmenting COVID-19 lung infections. In this architecture, capsule networks are incorporated into the existing Unet framework. Capsule networks represent a novel network architecture that differs from traditional convolutional neural networks. They utilize vectors for information transfer among capsules, facilitating the extraction of intricate lesion spatial information. Additionally, we design a capsule encoder path and establish a coupling path between the unet encoder and the capsule encoder. This design maximizes the complementary advantages of both network structures while achieving efficient information fusion. \\noindent Finally, extensive experiments are conducted on four publicly available datasets, encompassing binary segmentation tasks and multi-class segmentation tasks. The experimental results demonstrate the superior segmentation performance of the proposed model. The code has been released at: https://github.com/AmanoTooko-jie/CAD-Unet.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06316",
        "abstract url": "https://arxiv.org/abs/2412.06316",
        "title": "Geometric spanners of bounded tree-width",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Given a point set $P$ in the Euclidean space, a geometric $t$-spanner $G$ is a graph on $P$ such that for every pair of points, the shortest path in $G$ between those points is at most a factor $t$ longer than the Euclidean distance between those points. The value $t\\geq 1$ is called the dilation of $G$. Commonly, the aim is to construct a $t$-spanner with additional desirable properties. In graph theory, a powerful tool to admit efficient algorithms is bounded tree-width. We therefore investigate the problem of computing geometric spanners with bounded tree-width and small dilation $t$. Let $d$ be a fixed integer and $P \\subset \\mathbb{R}^d$ be a point set with $n$ points. We give a first algorithm to compute an $\\mathcal{O}(n/k^{d/(d-1)})$-spanner on $P$ with tree-width at most $k$. The dilation obtained by the algorithm is asymptotically worst-case optimal for graphs with tree-width $k$: We show that there is a set of $n$ points such that every spanner of tree-width $k$ has dilation $\\mathcal{O}(n/k^{d/(d-1)})$. We further prove a tight dependency between tree-width and the number of edges in sparse connected planar graphs, which admits, for point sets in $\\mathbb{R}^2$, a plane spanner with tree-width at most $k$ and small maximum vertex degree. Finally, we show an almost tight bound on the minimum dilation of a spanning tree of $n$ equally spaced points on a circle, answering an open question asked in previous work.",
        "subjects": [
            "cs.CG"
        ],
        "comment": "23 pages, 4 figures"
    },
    {
        "paper id": "2412.06327",
        "abstract url": "https://arxiv.org/abs/2412.06327",
        "title": "Robust Output Tracking for an Uncertain and Nonlinear 3D PDE-ODE System: Preventing Induced Seismicity in Underground Reservoirs",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "This paper presents a robust control strategy for output tracking of a nonlinear 3D PDE-ODE system. The output feedback control was developed by bounding the solution and its time derivative for both the infinite-dimensional system and the nonlinear ODE, and leveraging these bounds to ensure the boundedness of the control coefficient and error dynamics perturbations. The mathematical framework demonstrates the controller's ability to manage two output types within the system, overcoming model uncertainties and heterogeneities using minimal system information and a continuous control signal. A case study addressing induced seismicity mitigation while ensuring energy production in the Groningen gas reservoir highlights the control's effectiveness. The strategy guarantees precise tracking of target seismicity rates and pressures across reservoir regions, even under parameter uncertainties. Numerical simulations validate the approach in two scenarios: gas extraction with minimal seismicity and the addition of CO$_2$ injections achieving net-zero environmental impact.",
        "subjects": [
            "eess.SY",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06332",
        "abstract url": "https://arxiv.org/abs/2412.06332",
        "title": "Not All Errors Are Equal: Investigation of Speech Recognition Errors in Alzheimer's Disease Detection",
        "rating": "-1",
        "keywords": [
            [
                "diagnosis",
                "Disease"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Automatic Speech Recognition (ASR) plays an important role in speech-based automatic detection of Alzheimer's disease (AD). However, recognition errors could propagate downstream, potentially impacting the detection decisions. Recent studies have revealed a non-linear relationship between word error rates (WER) and AD detection performance, where ASR transcriptions with notable errors could still yield AD detection accuracy equivalent to that based on manual transcriptions. This work presents a series of analyses to explore the effect of ASR transcription errors in BERT-based AD detection systems. Our investigation reveals that not all ASR errors contribute equally to detection performance. Certain words, such as stopwords, despite constituting a large proportion of errors, are shown to play a limited role in distinguishing AD. In contrast, the keywords related to diagnosis tasks exhibit significantly greater importance relative to other words. These findings provide insights into the interplay between ASR errors and the downstream detection model.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "q-bio.NC"
        ],
        "comment": "Accepted by IEEE ISCSLP 2024"
    },
    {
        "paper id": "2412.06334",
        "abstract url": "https://arxiv.org/abs/2412.06334",
        "title": "TriDi: Trilateral Diffusion of 3D Humans, Objects, and Interactions",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Modeling 3D human-object interaction (HOI) is a problem of great interest for computer vision and a key enabler for virtual and mixed-reality applications. Existing methods work in a one-way direction: some recover plausible human interactions conditioned on a 3D object; others recover the object pose conditioned on a human pose. Instead, we provide the first unified model - TriDi which works in any direction. Concretely, we generate Human, Object, and Interaction modalities simultaneously with a new three-way diffusion process, allowing to model seven distributions with one network. We implement TriDi as a transformer attending to the various modalities' tokens, thereby discovering conditional relations between them. The user can control the interaction either as a text description of HOI or a contact map. We embed these two representations into a shared latent space, combining the practicality of text descriptions with the expressiveness of contact maps. Using a single network, TriDi unifies all the special cases of prior work and extends to new ones, modeling a family of seven distributions. Remarkably, despite using a single model, TriDi generated samples surpass one-way specialized baselines on GRAB and BEHAVE in terms of both qualitative and quantitative metrics, and demonstrating better diversity. We show the applicability of TriDi to scene population, generating objects for human-contact datasets, and generalization to unseen object geometry. The project page is available at: https://virtualhumans.mpi-inf.mpg.de/tridi.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06359",
        "abstract url": "https://arxiv.org/abs/2412.06359",
        "title": "On-Device Self-Supervised Learning of Low-Latency Monocular Depth from Only Events",
        "rating": "-1",
        "keywords": [
            [
                "memory efficiency"
            ],
            [
                "Depth",
                "Event cameras"
            ],
            [
                "flight"
            ],
            [
                "robot"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Event cameras provide low-latency perception for only milliwatts of power. This makes them highly suitable for resource-restricted, agile robots such as small flying drones. Self-supervised learning based on contrast maximization holds great potential for event-based robot vision, as it foregoes the need to high-frequency ground truth and allows for online learning in the robot's operational environment. However, online, onboard learning raises the major challenge of achieving sufficient computational efficiency for real-time learning, while maintaining competitive visual perception performance. In this work, we improve the time and memory efficiency of the contrast maximization learning pipeline. Benchmarking experiments show that the proposed pipeline achieves competitive results with the state of the art on the task of depth estimation from events. Furthermore, we demonstrate the usability of the learned depth for obstacle avoidance through real-world flight experiments. Finally, we compare the performance of different combinations of pre-training and fine-tuning of the depth estimation networks, showing that on-board domain adaptation is feasible given a few minutes of flight.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06388",
        "abstract url": "https://arxiv.org/abs/2412.06388",
        "title": "Sparse Identification of Nonlinear Dynamics-based Model Predictive Control for Multirotor Collision Avoidance",
        "rating": "-1",
        "keywords": [
            [
                "trajectory"
            ]
        ],
        "abstract": "This paper proposes a data-driven model predictive control for multirotor collision avoidance considering uncertainty and an unknown model from a payload. To address this challenge, sparse identification of nonlinear dynamics (SINDy) is used to obtain the governing equation of the multirotor system. The SINDy can discover the equations of target systems with low data, assuming that few functions have the dominant characteristic of the system. Model predictive control (MPC) is utilized to obtain accurate trajectory tracking performance by considering state and control input constraints. To avoid a collision during operation, MPC optimization problem is again formulated using inequality constraints about an obstacle. In simulation, SINDy can discover a governing equation of multirotor system including mass parameter uncertainty and aerodynamic effects. In addition, the simulation results show that the proposed method has the capability to avoid an obstacle and track the desired trajectory accurately.",
        "subjects": [
            "cs.RO",
            "math.OC"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06405",
        "abstract url": "https://arxiv.org/abs/2412.06405",
        "title": "Parameter Adjustments in POMDP-Based Trajectory Planning for Unsignalized Intersections",
        "rating": "-1",
        "keywords": [
            [
                "Trajectory",
                "vehicle"
            ]
        ],
        "abstract": "This paper investigates the problem of trajectory planning for autonomous vehicles at unsignalized intersections, specifically focusing on scenarios where the vehicle lacks the right of way and yet must cross safely. To address this issue, we have employed a method based on the Partially Observable Markov Decision Processes (POMDPs) framework designed for planning under uncertainty. The method utilizes the Adaptive Belief Tree (ABT) algorithm as an approximate solver for the POMDPs. We outline the POMDP formulation, beginning with discretizing the intersection's topology. Additionally, we present a dynamics model for the prediction of the evolving states of vehicles, such as their position and velocity. Using an observation model, we also describe the connection of those states with the imperfect (noisy) available measurements. Our results confirmed that the method is able to plan collision-free trajectories in a series of simulations utilizing real-world traffic data from aerial footage of two distinct intersections. Furthermore, we studied the impact of parameter adjustments of the ABT algorithm on the method's performance. This provides guidance in determining reasonable parameter settings, which is valuable for future method applications.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06424",
        "abstract url": "https://arxiv.org/abs/2412.06424",
        "title": "Deblur4DGS: 4D Gaussian Splatting from Blurry Monocular Video",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "Gaussian Splatting",
                "NeRF"
            ],
            [
                "trajectory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent 4D reconstruction methods have yielded impressive results but rely on sharp videos as supervision. However, motion blur often occurs in videos due to camera shake and object movement, while existing methods render blurry results when using such videos for reconstructing 4D models. Although a few NeRF-based approaches attempted to address the problem, they struggled to produce high-quality results, due to the inaccuracy in estimating continuous dynamic representations within the exposure time. Encouraged by recent works in 3D motion trajectory modeling using 3D Gaussian Splatting (3DGS), we suggest taking 3DGS as the scene representation manner, and propose the first 4D Gaussian Splatting framework to reconstruct a high-quality 4D model from blurry monocular video, named Deblur4DGS. Specifically, we transform continuous dynamic representations estimation within an exposure time into the exposure time estimation. Moreover, we introduce exposure regularization to avoid trivial solutions, as well as multi-frame and multi-resolution consistency ones to alleviate artifacts. Furthermore, to better represent objects with large motion, we suggest blur-aware variable canonical Gaussians. Beyond novel-view synthesis, Deblur4DGS can be applied to improve blurry video from multiple perspectives, including deblurring, frame interpolation, and video stabilization. Extensive experiments on the above four tasks show that Deblur4DGS outperforms state-of-the-art 4D reconstruction methods. The codes are available at https://github.com/ZcsrenlongZ/Deblur4DGS.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "17 pages"
    },
    {
        "paper id": "2412.06425",
        "abstract url": "https://arxiv.org/abs/2412.06425",
        "title": "Foresee and Act Ahead: Task Prediction and Pre-Scheduling Enabled Efficient Robotic Warehousing",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "In warehousing systems, to enhance logistical efficiency amid surging demand volumes, much focus is placed on how to reasonably allocate tasks to robots. However, the robots labor is still inevitably wasted to some extent. In response to this, we propose a pre-scheduling enhanced warehousing framework that predicts task flow and acts in advance. It consists of task flow prediction and hybrid tasks allocation. For task prediction, we notice that it is possible to provide a spatio-temporal representation of task flow, so we introduce a periodicity-decoupled mechanism tailored for the generation patterns of aggregated orders, and then further extract spatial features of task distribution with novel combination of graph structures. In hybrid tasks allocation, we consider the known tasks and predicted future tasks simultaneously and optimize the allocation dynamically. In addition, we consider factors such as predicted task uncertainty and sector-level efficiency evaluation in warehousing to realize more balanced and rational allocations. We validate our task prediction model across actual datasets derived from real factories, achieving SOTA performance. Furthermore, we implement our compelte scheduling system in a real-world robotic warehouse for months of lifelong validation, demonstrating large improvements in key metrics of warehousing, such as empty running rate, by more than 50%.",
        "subjects": [
            "cs.RO",
            "cs.MA"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06430",
        "abstract url": "https://arxiv.org/abs/2412.06430",
        "title": "Subgraph-Oriented Testing for Deep Learning Libraries",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "Deep Learning (DL) libraries, such as PyTorch, are widely used for building and deploying DL models on various hardware platforms. Meanwhile, they are found to contain bugs that lead to incorrect calculation results and cause issues like non-convergence training and inaccurate prediction of DL models. Thus, many efforts have been made to test DL libraries and reveal bugs. However, existing DL library testing methods manifest limitations: model-level testing methods cause complexity in fault localization. Meanwhile, API-level testing methods often generate invalid inputs or primarily focus on extreme inputs that lead to crash failures; they also ignore testing realistic API interactions. These limitations may lead to missing detection of bugs, even in the frequently used APIs. To address these limitations, we propose SORT (Subgraph-Oriented Realistic Testing) to differential test DL libraries on different hardware platforms. SORT takes popular API interaction patterns, represented as frequent subgraphs of model computation graphs, as test subjects. In this way, it introduces realistic API interaction sequences while maintaining efficiency in locating faulty APIs for observed errors. Besides, SORT prepares test inputs by referring to extensive features of runtime inputs for each API in executing real-life benchmark data. The generated inputs are expected to better simulate such valid real inputs and reveal bugs more likely to happen in real-life usage. Evaluation on 728 frequent subgraphs of 49 popular PyTorch models demonstrates that SORT achieves a 100\\% valid input generation rate, detects more precision bugs than existing methods, and reveals interaction-related bugs missed by single-API testing. 18 precision bugs in PyTorch are identified.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06487",
        "abstract url": "https://arxiv.org/abs/2412.06487",
        "title": "Improving text-conditioned latent diffusion for cancer pathology",
        "rating": "-1",
        "keywords": [
            [
                "GPU memory"
            ],
            [
                "diffusion"
            ],
            [
                "cancer"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The development of generative models in the past decade has allowed for hyperrealistic data synthesis. While potentially beneficial, this synthetic data generation process has been relatively underexplored in cancer histopathology. One algorithm for synthesising a realistic image is diffusion; it iteratively converts an image to noise and learns the recovery process from this noise [Wang and Vastola, 2023]. While effective, it is highly computationally expensive for high-resolution images, rendering it infeasible for histopathology. The development of Variational Autoencoders (VAEs) has allowed us to learn the representation of complex high-resolution images in a latent space. A vital by-product of this is the ability to compress high-resolution images to space and recover them lossless. The marriage of diffusion and VAEs allows us to carry out diffusion in the latent space of an autoencoder, enabling us to leverage the realistic generative capabilities of diffusion while maintaining reasonable computational requirements. Rombach et al. [2021b] and Yellapragada et al. [2023] build foundational models for this task, paving the way to generate realistic histopathology images. In this paper, we discuss the pitfalls of current methods, namely [Yellapragada et al., 2023] and resolve critical errors while proposing improvements along the way. Our methods achieve an FID score of 21.11, beating its SOTA counterparts in [Yellapragada et al., 2023] by 1.2 FID, while presenting a train-time GPU memory usage reduction of 7%.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06490",
        "abstract url": "https://arxiv.org/abs/2412.06490",
        "title": "On Zarankiewicz's Problem for Intersection Hypergraphs of Geometric Objects",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "The hypergraph Zarankiewicz's problem, introduced by Erd\u0151s in 1964, asks for the maximum number of hyperedges in an $r$-partite hypergraph with $n$ vertices in each part that does not contain a copy of $K_{t,t,\\ldots,t}$. Erd\u0151s obtained a near optimal bound of $O(n^{r-1/t^{r-1}})$ for general hypergraphs. In recent years, several works obtained improved bounds under various algebraic assumptions -- e.g., if the hypergraph is semialgebraic. In this paper we study the problem in a geometric setting -- for $r$-partite intersection hypergraphs of families of geometric objects. Our main results are essentially sharp bounds for families of axis-parallel boxes in $\\mathbb{R}^d$ and families of pseudo-discs. For axis-parallel boxes, we obtain the sharp bound $O_{d,t}(n^{r-1}(\\frac{\\log n}{\\log \\log n})^{d-1})$. The best previous bound was larger by a factor of about $(\\log n)^{d(2^{r-1}-2)}$. For pseudo-discs, we obtain the bound $O_t(n^{r-1}(\\log n)^{r-2})$, which is sharp up to logarithmic factors. As this hypergraph has no algebraic structure, no improvement of Erd\u0151s' 60-year-old $O(n^{r-1/t^{r-1}})$ bound was known for this setting. Futhermore, even in the special case of discs for which the semialgebraic structure can be used, our result improves the best known result by a factor of $\\tilde\u03a9(n^{\\frac{2r-2}{3r-2}})$. To obtain our results, we use the recently improved results for the graph Zarankiewicz's problem in the corresponding settings, along with a variety of combinatorial and geometric techniques, including shallow cuttings, biclique covers, transversals, and planarity.",
        "subjects": [
            "math.CO",
            "cs.CG"
        ],
        "comment": "25 pages, 3 figures"
    },
    {
        "paper id": "2412.06499",
        "abstract url": "https://arxiv.org/abs/2412.06499",
        "title": "Hybrid Attention Network: An efficient approach for anatomy-free landmark detection",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "clinical"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Accurate anatomical landmark detection in medical images is crucial for clinical applications. Existing methods often struggle to balance global context with computational efficiency, particularly with high-resolution images. This paper introduces the Hybrid Attention Network(HAN), a novel hybrid architecture integrating CNNs and Transformers. Its core is the BiFormer module, utilizing Bi-Level Routing Attention (BRA) for efficient attention to relevant image regions. This, combined with Convolutional Attention Blocks (CAB) enhanced by CBAM, enables precise local feature refinement guided by the global context. A Feature Fusion Correction Module (FFCM) integrates multi-scale features, mitigating resolution loss. Deep supervision with MSE loss on multi-resolution heatmaps optimizes the model. Experiments on five diverse datasets demonstrate state-of-the-art performance, surpassing existing methods in accuracy, robustness, and efficiency. The HAN provides a promising solution for accurate and efficient anatomical landmark detection in complex medical images. Our codes and data will be released soon at: \\url{https://github.com/MIRACLE-Center/}.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06513",
        "abstract url": "https://arxiv.org/abs/2412.06513",
        "title": "EFX Allocations on Some Multi-graph Classes",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "The existence of EFX allocations is one of the most significant open questions in fair division. Recent work by Christodolou, Fiat, Koutsoupias, and Sgouritsa (\"Fair allocation in graphs\", EC 2023) establishes the existence of EFX allocations for graphical valuations, when agents are vertices in a graph, items are edges, and each item has zero value for all agents other than those at its end-points. Thus in this setting, each good has non-zero value for at most two agents, and there is at most one good valued by any pair of agents. This marks one of the few cases when an exact and complete EFX allocation is known to exist for arbitrary agents. In this work, we extend these results to multi-graphs, when each pair of vertices can have more than one edge between them. The existence of EFX allocations in multi-graphs is a natural open question given their existence in simple graphs. We show that EFX allocations exist, and can be computed in polynomial time, for agents with cancellable valuations in the following cases: (i) bipartite multi-graphs, (ii) multi-trees with monotone valuations, and (iii) multi-graphs with girth $(2t-1)$, where $t$ is the chromatic number of the multi-graph. The existence in multi-cycles follows from (i) and (iii).",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06518",
        "abstract url": "https://arxiv.org/abs/2412.06518",
        "title": "On the Bidirected Cut Relaxation for Steiner Forest",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "The Steiner Forest problem is an important generalization of the Steiner Tree problem. We are given an undirected graph with nonnegative edge costs and a collection of pairs of vertices. The task is to compute a cheapest forest with the property that the elements of each pair belong to the same connected component of the forest. The current best approximation factor for Steiner Forest is 2, which is achieved by the classical primal-dual algorithm; improving on this factor is a big open problem in the area. Motivated by this open problem, we study an LP relaxation for Steiner Forest that generalizes the well-studied Bidirected Cut Relaxation for Steiner Tree. We prove that this relaxation has several promising properties. Among them, it is possible to round any half-integral LP solution to a Steiner Forest instance while increasing the cost by at most a factor 16/9. To prove this result we introduce a novel recursive densest-subgraph contraction algorithm.",
        "subjects": [
            "cs.DS",
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06530",
        "abstract url": "https://arxiv.org/abs/2412.06530",
        "title": "HES-UNet: A U-Net for Hepatic Echinococcosis Lesion Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "CT",
                "disease",
                "Lesion"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Hepatic echinococcosis (HE) is a prevalent disease in economically underdeveloped pastoral areas, where adequate medical resources are usually lacking. Existing methods often ignore multi-scale feature fusion or focus only on feature fusion between adjacent levels, which may lead to insufficient feature fusion. To address these issues, we propose HES-UNet, an efficient and accurate model for HE lesion segmentation. This model combines convolutional layers and attention modules to capture local and global features. During downsampling, the multi-directional downsampling block (MDB) is employed to integrate high-frequency and low-frequency features, effectively extracting image details. The multi-scale aggregation block (MAB) aggregates multi-scale feature information. In contrast, the multi-scale upsampling Block (MUB) learns highly abstract features and supplies this information to the skip connection module to fuse multi-scale features. Due to the distinct regional characteristics of HE, there is currently no publicly available high-quality dataset for training our model. We collected CT slice data from 268 patients at a certain hospital to train and evaluate the model. The experimental results show that HES-UNet achieves state-of-the-art performance on our dataset, achieving an overall Dice Similarity Coefficient (DSC) of 89.21%, which is 1.09% higher than that of TransUNet. The project page is available at https://chenjiayan-qhu.github.io/HES-UNet-page.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "6 pages, 3 figures"
    },
    {
        "paper id": "2412.06543",
        "abstract url": "https://arxiv.org/abs/2412.06543",
        "title": "Challenges and Opportunities for Visual Analytics in Jurisprudence",
        "rating": "-1",
        "keywords": [
            [
                "navigation"
            ]
        ],
        "abstract": "Exploring, analyzing, and interpreting law can be tedious and challenging, even for legal scholars, since legal texts contain domain-specific language, require knowledge of tacit legal concepts, and are sometimes intentionally ambiguous. In related, text-based domains, Visual Analytics (VA) and large language models (LLMs) have become essential for working with documents as they support data navigation, knowledge representation, and analytical reasoning. However, legal scholars must simultaneously manage hierarchical information sources, leverage implicit domain knowledge, and document complex reasoning processes, which are neither adequately accessible through existing VA designs nor sufficiently supported by current LLMs. To address the needs of legal scholars, we identify previously unexamined challenges and opportunities when applying VA to jurisprudence. We conducted semi-structured interviews with nine experts from the legal domain and found that they lacked the ability to articulate their tacit domain knowledge as explicit, machine-interpretable knowledge. Hence, we propose leveraging interactive visualization for this articulation, teaching the machine relevant semantic relationships between legal documents. These relationships inform the predictions of VA and LLMs, facilitating the navigation between the hierarchies of legal document collections. The enhanced navigation can uncover additional relevant legal documents, reinforcing the legal reasoning process by generating legal insights that reflect internalized, tacit domain knowledge. In summary, we provide a human-is-the-loop VA workflow for jurisprudence that recognizes tacit domain knowledge as essential for deriving legal insights. More broadly, we compare this workflow with related text-based research practices, revealing research gaps and guiding visualization researchers in knowledge-assisted VA for law and beyond.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "13 pages, 2 figures, 1 table"
    },
    {
        "paper id": "2412.06549",
        "abstract url": "https://arxiv.org/abs/2412.06549",
        "title": "Prediction of Occluded Pedestrians in Road Scenes using Human-like Reasoning: Insights from the OccluRoads Dataset",
        "rating": "-1",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "Graph"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Pedestrian detection is a critical task in autonomous driving, aimed at enhancing safety and reducing risks on the road. Over recent years, significant advancements have been made in improving detection performance. However, these achievements still fall short of human perception, particularly in cases involving occluded pedestrians, especially entirely invisible ones. In this work, we present the Occlusion-Rich Road Scenes with Pedestrians (OccluRoads) dataset, which features a diverse collection of road scenes with partially and fully occluded pedestrians in both real and virtual environments. All scenes are meticulously labeled and enriched with contextual information that encapsulates human perception in such scenarios. Using this dataset, we developed a pipeline to predict the presence of occluded pedestrians, leveraging Knowledge Graph (KG), Knowledge Graph Embedding (KGE), and a Bayesian inference process. Our approach achieves a F1 score of 0.91, representing an improvement of up to 42% compared to traditional machine learning models.",
        "subjects": [
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06556",
        "abstract url": "https://arxiv.org/abs/2412.06556",
        "title": "Vulnerability, Where Art Thou? An Investigation of Vulnerability Management in Android Smartphone Chipsets",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Vulnerabilities in Android smartphone chipsets have severe consequences, as recent real-world attacks have demonstrated that adversaries can leverage vulnerabilities to execute arbitrary code or exfiltrate confidential information. Despite the far-reaching impact of such attacks, the lifecycle of chipset vulnerabilities has yet to be investigated, with existing papers primarily investigating vulnerabilities in the Android operating system. This paper provides a comprehensive and empirical study of the current state of smartphone chipset vulnerability management within the Android ecosystem. For the first time, we create a unified knowledge base of 3,676 chipset vulnerabilities affecting 437 chipset models from all four major chipset manufacturers, combined with 6,866 smartphone models. Our analysis revealed that the same vulnerabilities are often included in multiple generations of chipsets, providing novel empirical evidence that vulnerabilities are inherited through multiple chipset generations. Furthermore, we demonstrate that the commonly accepted 90-day responsible vulnerability disclosure period is seldom adhered to. We find that a single vulnerability often affects hundreds to thousands of different smartphone models, for which update availability is, as we show, often unclear or heavily delayed. Leveraging the new insights gained from our empirical analysis, we recommend several changes that chipset manufacturers can implement to improve the security posture of their products. At the same time, our knowledge base enables academic researchers to conduct more representative evaluations of smartphone chipsets, accurately assess the impact of vulnerabilities they discover, and identify avenues for future research.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Accepted by Network and Distributed System Security (NDSS) Symposium 2025"
    },
    {
        "paper id": "2412.06592",
        "abstract url": "https://arxiv.org/abs/2412.06592",
        "title": "PrEditor3D: Fast and Precise 3D Shape Editing",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "image editing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We propose a training-free approach to 3D editing that enables the editing of a single shape within a few minutes. The edited 3D mesh aligns well with the prompts, and remains identical for regions that are not intended to be altered. To this end, we first project the 3D object onto 4-view images and perform synchronized multi-view image editing along with user-guided text prompts and user-provided rough masks. However, the targeted regions to be edited are ambiguous due to projection from 3D to 2D. To ensure precise editing only in intended regions, we develop a 3D segmentation pipeline that detects edited areas in 3D space, followed by a merging algorithm to seamlessly integrate edited 3D regions with the original input. Extensive experiments demonstrate the superiority of our method over previous approaches, enabling fast, high-quality editing while preserving unintended regions.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "Project Page: https://ziyaerkoc.com/preditor3d/ Video: https://www.youtube.com/watch?v=Ty2xXaEuewI"
    },
    {
        "paper id": "2412.06596",
        "abstract url": "https://arxiv.org/abs/2412.06596",
        "title": "Augmented reality for upper limb rehabilitation: real-time kinematic feedback with HoloLens 2",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "Exoskeletons for rehabilitation can help enhance motor recovery in individuals suffering from neurological disorders. Precision in movement execution, especially in arm rehabilitation, is crucial to prevent maladaptive plasticity. However, current exoskeletons, while providing arm support, often lack the necessary 3D feedback capabilities to show how well rehabilitation exercises are being performed. This reduces therapist acceptance and patients' performance. Augmented Reality technologies offer promising solutions for feedback and gaming systems in rehabilitation. In this work, we leverage HoloLens 2 with its advanced hand-tracking system to develop an application for personalized rehabilitation. Our application generates custom holographic trajectories based on existing databases or therapists' demonstrations, represented as 3D tunnels. Such trajectories can be superimposed on the real training environment. They serve as a guide to the users and, thanks to colour-coded real-time feedback, indicate their performance. To assess the efficacy of the application in improving kinematic precision, we tested it with 15 healthy subjects. Comparing user tracking capabilities with and without the use of our feedback system in executing 4 different exercises, we observed significant differences, demonstrating that our application leads to improved kinematic performance. 12 clinicians tested our system and positively evaluated its usability (System Usability Scale score of 67.7) and acceptability (4.4 out of 5 in the 'Willingness to Use' category in the relative Technology Acceptance Model). The results from the tests on healthy participants and the feedback from clinicians encourage further exploration of our framework, to verify its potential in supporting arm rehabilitation for individuals with neurological disorders.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "25 pages, 9 figures, 3 tables, journal submission"
    },
    {
        "paper id": "2412.06601",
        "abstract url": "https://arxiv.org/abs/2412.06601",
        "title": "A switching Kalman filter approach to online mitigation and correction sensor corruption for inertial navigation",
        "rating": "-1",
        "keywords": [
            [
                "navigation"
            ]
        ],
        "abstract": "This paper introduces a novel approach to detect and address faulty or corrupted external sensors in the context of inertial navigation by leveraging a switching Kalman Filter combined with parameter augmentation. Instead of discarding the corrupted data, the proposed method retains and processes it, running multiple observation models simultaneously and evaluating their likelihoods to accurately identify the true state of the system. We demonstrate the effectiveness of this approach to both identify the moment that a sensor becomes faulty and to correct for the resulting sensor behavior to maintain accurate estimates. We demonstrate our approach on an application of balloon navigation in the atmosphere and shuttle reentry. The results show that our method can accurately recover the true system state even in the presence of significant sensor bias, thereby improving the robustness and reliability of state estimation systems under challenging conditions. We also provide a statistical analysis of problem settings to determine when and where our method is most accurate and where it fails.",
        "subjects": [
            "eess.SY",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06614",
        "abstract url": "https://arxiv.org/abs/2412.06614",
        "title": "MVReward: Better Aligning and Evaluating Multi-View Diffusion Models with Human Preferences",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent years have witnessed remarkable progress in 3D content generation. However, corresponding evaluation methods struggle to keep pace. Automatic approaches have proven challenging to align with human preferences, and the mixed comparison of text- and image-driven methods often leads to unfair evaluations. In this paper, we present a comprehensive framework to better align and evaluate multi-view diffusion models with human preferences. To begin with, we first collect and filter a standardized image prompt set from DALL$\\cdot$E and Objaverse, which we then use to generate multi-view assets with several multi-view diffusion models. Through a systematic ranking pipeline on these assets, we obtain a human annotation dataset with 16k expert pairwise comparisons and train a reward model, coined MVReward, to effectively encode human preferences. With MVReward, image-driven 3D methods can be evaluated against each other in a more fair and transparent manner. Building on this, we further propose Multi-View Preference Learning (MVP), a plug-and-play multi-view diffusion tuning strategy. Extensive experiments demonstrate that MVReward can serve as a reliable metric and MVP consistently enhances the alignment of multi-view diffusion models with human preferences.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06624",
        "abstract url": "https://arxiv.org/abs/2412.06624",
        "title": "Fundus Image-based Visual Acuity Assessment with PAC-Guarantees",
        "rating": "-1",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Timely detection and treatment are essential for maintaining eye health. Visual acuity (VA), which measures the clarity of vision at a distance, is a crucial metric for managing eye health. Machine learning (ML) techniques have been introduced to assist in VA measurement, potentially alleviating clinicians' workloads. However, the inherent uncertainties in ML models make relying solely on them for VA prediction less than ideal. The VA prediction task involves multiple sources of uncertainty, requiring more robust approaches. A promising method is to build prediction sets or intervals rather than point estimates, offering coverage guarantees through techniques like conformal prediction and Probably Approximately Correct (PAC) prediction sets. Despite the potential, to date, these approaches have not been applied to the VA prediction task.To address this, we propose a method for deriving prediction intervals for estimating visual acuity from fundus images with a PAC guarantee. Our experimental results demonstrate that the PAC guarantees are upheld, with performance comparable to or better than that of two prior works that do not provide such guarantees.",
        "subjects": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "comment": "To be published in ML4H 2024"
    },
    {
        "paper id": "2412.06636",
        "abstract url": "https://arxiv.org/abs/2412.06636",
        "title": "Neo-FREE: Policy Composition Through Thousand Brains And Free Energy Optimization",
        "rating": "-1",
        "keywords": [
            [
                "robot",
                "navigation"
            ]
        ],
        "abstract": "We consider the problem of optimally composing a set of primitives to tackle control tasks. To address this problem, we introduce Neo-FREE: a control architecture inspired by the Thousand Brains Theory and Free Energy Principle from cognitive sciences. In accordance with the neocortical (Neo) processes postulated by the Thousand Brains Theory, Neo-FREE consists of functional units returning control primitives. These are linearly combined by a gating mechanism that minimizes the variational free energy (FREE). The problem of finding the optimal primitives' weights is then recast as a finite-horizon optimal control problem, which is convex even when the cost is not and the environment is nonlinear, stochastic, non-stationary. The results yield an algorithm for primitives composition and the effectiveness of Neo-FREE is illustrated via in-silico and hardware experiments on an application involving robot navigation in an environment with obstacles.",
        "subjects": [
            "math.OC",
            "cs.RO",
            "eess.SY"
        ],
        "comment": "13 pages, 2 figures"
    },
    {
        "paper id": "2412.06660",
        "abstract url": "https://arxiv.org/abs/2412.06660",
        "title": "MuMu-LLaMA: Multi-modal Music Understanding and Generation via Large Language Models",
        "rating": "-1",
        "keywords": [
            [
                "Music",
                "text-to-music"
            ],
            [
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Research on large language models has advanced significantly across text, speech, images, and videos. However, multi-modal music understanding and generation remain underexplored due to the lack of well-annotated datasets. To address this, we introduce a dataset with 167.69 hours of multi-modal data, including text, images, videos, and music annotations. Based on this dataset, we propose MuMu-LLaMA, a model that leverages pre-trained encoders for music, images, and videos. For music generation, we integrate AudioLDM 2 and MusicGen. Our evaluation across four tasks--music understanding, text-to-music generation, prompt-based music editing, and multi-modal music generation--demonstrates that MuMu-LLaMA outperforms state-of-the-art models, showing its potential for multi-modal music applications.",
        "subjects": [
            "cs.SD",
            "cs.MM",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06664",
        "abstract url": "https://arxiv.org/abs/2412.06664",
        "title": "Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing Image Segmentation",
        "rating": "-1",
        "keywords": [
            [
                "Remote Sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Fine-grained remote sensing image segmentation is essential for accurately identifying detailed objects in remote sensing images. Recently, vision transformer models (VTM) pretrained on large-scale datasets have shown strong zero-shot generalization, indicating that they have learned the general knowledge of object understanding. We introduce a novel end-to-end learning paradigm combining knowledge guidance with domain refinement to enhance performance. We present two key components: the Feature Alignment Module (FAM) and the Feature Modulation Module (FMM). FAM aligns features from a CNN-based backbone with those from the pretrained VTM's encoder using channel transformation and spatial interpolation, and transfers knowledge via KL divergence and L2 normalization constraint. FMM further adapts the knowledge to the specific domain to address domain shift. We also introduce a fine-grained grass segmentation dataset and demonstrate, through experiments on two datasets, that our method achieves a significant improvement of 2.57 mIoU on the grass dataset and 3.73 mIoU on the cloud dataset. The results highlight the potential of combining knowledge transfer and domain adaptation to overcome domain-related challenges and data limitations. The project page is available at https://xavierjiezou.github.io/KTDA/.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "6 pages, 4 figures, 6 tables"
    },
    {
        "paper id": "2412.06671",
        "abstract url": "https://arxiv.org/abs/2412.06671",
        "title": "Mathematical Formulations And Results Regarding Two Echelon Electric Vehicle Routing Problems",
        "rating": "-1",
        "keywords": [
            [
                "Vehicle"
            ]
        ],
        "abstract": "The growing need for sustainable logistics solutions has led to the evolution of vehicle routing problems (VRPs) into more complex variants that address modern challenges. Among these, the Two-Echelon Electric Vehicle Routing Problem (2E-EVRP) has emerged as a significant problem variant, integrating electric vehicles (EVs) into a multi-echelon distribution system. This problem considers environmental and operational constraints such as limited battery range, charging infrastructure, and urban logistics complexities. In this report, we present a comprehensive mathematical formulation for the 2E-EVRP and its variants, which include constraints like time windows, simultaneous pickup and delivery, and partial deliveries. These formulations aim to provide a robust framework for optimizing multi-tiered distribution networks using sustainable practices. Computational experiments demonstrate the effectiveness of the proposed methods.",
        "subjects": [
            "cs.DM"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06698",
        "abstract url": "https://arxiv.org/abs/2412.06698",
        "title": "Gen-3Diffusion: Realistic Image-to-3D Generation via 2D & 3D Diffusion Synergy",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "avatar"
            ],
            [
                "Diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Creating realistic 3D objects and clothed avatars from a single RGB image is an attractive yet challenging problem. Due to its ill-posed nature, recent works leverage powerful prior from 2D diffusion models pretrained on large datasets. Although 2D diffusion models demonstrate strong generalization capability, they cannot guarantee the generated multi-view images are 3D consistent. In this paper, we propose Gen-3Diffusion: Realistic Image-to-3D Generation via 2D & 3D Diffusion Synergy. We leverage a pre-trained 2D diffusion model and a 3D diffusion model via our elegantly designed process that synchronizes two diffusion models at both training and sampling time. The synergy between the 2D and 3D diffusion models brings two major advantages: 1) 2D helps 3D in generalization: the pretrained 2D model has strong generalization ability to unseen images, providing strong shape priors for the 3D diffusion model; 2) 3D helps 2D in multi-view consistency: the 3D diffusion model enhances the 3D consistency of 2D multi-view sampling process, resulting in more accurate multi-view generation. We validate our idea through extensive experiments in image-based objects and clothed avatar generation tasks. Results show that our method generates realistic 3D objects and avatars with high-fidelity geometry and texture. Extensive ablations also validate our design choices and demonstrate the strong generalization ability to diverse clothing and compositional shapes. Our code and pretrained models will be publicly released on https://yuxuan-xue.com/gen-3diffusion.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://yuxuan-xue.com/gen-3diffusion"
    },
    {
        "paper id": "2412.06699",
        "abstract url": "https://arxiv.org/abs/2412.06699",
        "title": "You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent 3D generation models typically rely on limited-scale 3D `gold-labels' or 2D diffusion priors for 3D content creation. However, their performance is upper-bounded by constrained 3D priors due to the lack of scalable learning paradigms. In this work, we present See3D, a visual-conditional multi-view diffusion model trained on large-scale Internet videos for open-world 3D creation. The model aims to Get 3D knowledge by solely Seeing the visual contents from the vast and rapidly growing video data -- You See it, You Got it. To achieve this, we first scale up the training data using a proposed data curation pipeline that automatically filters out multi-view inconsistencies and insufficient observations from source videos. This results in a high-quality, richly diverse, large-scale dataset of multi-view images, termed WebVi3D, containing 320M frames from 16M video clips. Nevertheless, learning generic 3D priors from videos without explicit 3D geometry or camera pose annotations is nontrivial, and annotating poses for web-scale videos is prohibitively expensive. To eliminate the need for pose conditions, we introduce an innovative visual-condition - a purely 2D-inductive visual signal generated by adding time-dependent noise to the masked video data. Finally, we introduce a novel visual-conditional 3D generation framework by integrating See3D into a warping-based pipeline for high-fidelity 3D generation. Our numerical and visual comparisons on single and sparse reconstruction benchmarks show that See3D, trained on cost-effective and scalable video data, achieves notable zero-shot and open-world generation capabilities, markedly outperforming models trained on costly and constrained 3D datasets. Please refer to our project page at: https://vision.baai.ac.cn/see3d",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://vision.baai.ac.cn/see3d"
    },
    {
        "paper id": "2412.06709",
        "abstract url": "https://arxiv.org/abs/2412.06709",
        "title": "Parkinson's Disease Diagnosis Through Deep Learning: A Novel LSTM-Based Approach for Freezing of Gait Detection",
        "rating": "-1",
        "keywords": [
            [
                "healthcare",
                "Diagnosis",
                "Disease",
                "clinical"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Deep learning holds tremendous potential in healthcare for uncovering hidden patterns within extensive clinical datasets, aiding in the diagnosis of various diseases. Parkinson's disease (PD) is a neurodegenerative condition characterized by the deterioration of brain function. In the initial stages of PD, automatic diagnosis poses a challenge due to the similarity in behavior between individuals with PD and those who are healthy. Our objective is to propose an effective model that can aid in the early detection of Parkinson's disease. We employed the VGRF gait signal dataset sourced from Physionet for distinguishing between healthy individuals and those diagnosed with Parkinson's disease. This paper introduces a novel deep learning architecture based on the LSTM network for automatically detecting freezing of gait episodes in Parkinson's disease patients. In contrast to conventional machine learning algorithms, this method eliminates manual feature engineering and proficiently captures prolonged temporal dependencies in gait patterns, thereby improving the diagnosis of Parkinson's disease. The LSTM network resolves the issue of vanishing gradients by employing memory blocks in place of self-connected hidden units, allowing for optimal information assimilation. To prevent overfitting, dropout and L2 regularization techniques have been employed. Additionally, the stochastic gradient-based optimizer Adam is used for the optimization process. The results indicate that our proposed approach surpasses current state-of-the-art models in FOG episode detection, achieving an accuracy of 97.71%, sensitivity of 99%, precision of 98%, and specificity of 96%. This demonstrates its potential as a superior classification method for Parkinson's disease detection.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06716",
        "abstract url": "https://arxiv.org/abs/2412.06716",
        "title": "On Pooling-Based Track Fusion Strategies : Harmonic Mean Density",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "In a distributed sensor fusion architecture, using standard Kalman filter (naive fusion) can lead to degraded results as track correlations are ignored and conservative fusion strategies are employed as a sub-optimal alternative to the problem. Since, Gaussian mixtures provide a flexible means of modeling any density, therefore fusion strategies suitable for use with Gaussian mixtures are needed. While the generalized covariance intersection (CI) provides a means to fuse Gaussian mixtures, the procedure is cumbersome and requires evaluating a non-integer power of the mixture density. In this paper, we develop a pooling-based fusion strategy using the harmonic mean density (HMD) interpolation of local densities and show that the proposed method can handle both Gaussian and mixture densities without much changes to the framework. Mathematical properties of the proposed fusion strategy are studied and simulated on 2D and 3D maneuvering target tracking scenarios. The simulations suggest that the proposed HMD fusion performs better than other conservative strategies in terms of root-mean-squared error while being consistent.",
        "subjects": [
            "eess.SP",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06717",
        "abstract url": "https://arxiv.org/abs/2412.06717",
        "title": "Toward Non-Invasive Diagnosis of Bankart Lesions with Deep Learning",
        "rating": "-1",
        "keywords": [
            [
                "Diagnosis",
                "MRI",
                "lesion"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Bankart lesions, or anterior-inferior glenoid labral tears, are diagnostically challenging on standard MRIs due to their subtle imaging features-often necessitating invasive MRI arthrograms (MRAs). This study develops deep learning (DL) models to detect Bankart lesions on both standard MRIs and MRAs, aiming to improve diagnostic accuracy and reduce reliance on MRAs. We curated a dataset of 586 shoulder MRIs (335 standard, 251 MRAs) from 558 patients who underwent arthroscopy. Ground truth labels were derived from intraoperative findings, the gold standard for Bankart lesion diagnosis. Separate DL models for MRAs and standard MRIs were trained using the Swin Transformer architecture, pre-trained on a public knee MRI dataset. Predictions from sagittal, axial, and coronal views were ensembled to optimize performance. The models were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71 standard MRIs). Bankart lesions were identified in 31.9% of MRAs and 8.6% of standard MRIs. The models achieved AUCs of 0.87 (86% accuracy, 83% sensitivity, 86% specificity) and 0.90 (85% accuracy, 82% sensitivity, 86% specificity) on standard MRIs and MRAs, respectively. These results match or surpass radiologist performance on our dataset and reported literature metrics. Notably, our model's performance on non-invasive standard MRIs matched or surpassed the radiologists interpreting MRAs. This study demonstrates the feasibility of using DL to address the diagnostic challenges posed by subtle pathologies like Bankart lesions. Our models demonstrate potential to improve diagnostic confidence, reduce reliance on invasive imaging, and enhance accessibility to care.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Accepted for presentation at SPIE Medical Imaging 2025: Computer-Aided Diagnosis. The manuscript is expected to appear in the conference proceedings"
    },
    {
        "paper id": "2412.06726",
        "abstract url": "https://arxiv.org/abs/2412.06726",
        "title": "ICtoken: An NFT for Hardware IP Protection",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Protecting integrated circuits (ICs) from piracy and theft throughout their lifecycle is a persistent and complex challenge. In order to safeguard against illicit piracy attacks, this work proposes a novel framework utilizing Non-Fungible Tokens (NFTs) called ICtokens, uniquely linked to their corresponding physical ICs. Each ICtoken contains comprehensive information, including authentication data, supply chain stage and status, ownership details, and other IC metadata, while also making provision for the secure integration of a logic-locking key. Designed to be publicly logged, ICtokens securely obscure metering information without compromising functionality. In addition, the ICtracker, a distributed ledger technology powered by a swift and energy-efficient consortium blockchain, is used to register and manage ICtokens and their respective owners, tracking all associated interactions. This robust ledger guarantees the traceability and auditing of ICtokens while simultaneously developing a product-level NFT at every transaction point within the supply chain. Consequently, a scalable framework is established, creating unique, immutable digital twins for ICs and IC-embedded products in the form of ICtokens and their transactions. This provides a robust and reliable supply chain trail back to the original IP owner, while also offering unprecedented assurance to consumers of IC-embedded products. The rich information contained within ICtokens facilitates more detailed audits than previous proposals for IC supply chain monitoring. A proof-of-concept, implemented as an open-source solution, ensures the ease of adoption of the proposed framework.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Keywords: IC Piracy, Supply Chain, Blockchain, Non-Fungible Token, PUF, Logic Locking"
    },
    {
        "paper id": "2412.06740",
        "abstract url": "https://arxiv.org/abs/2412.06740",
        "title": "Convolution goes higher-order: a biologically inspired mechanism empowers image classification",
        "rating": "-1",
        "keywords": [
            [
                "biologically"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "We propose a novel approach to image classification inspired by complex nonlinear biological visual processing, whereby classical convolutional neural networks (CNNs) are equipped with learnable higher-order convolutions. Our model incorporates a Volterra-like expansion of the convolution operator, capturing multiplicative interactions akin to those observed in early and advanced stages of biological visual processing. We evaluated this approach on synthetic datasets by measuring sensitivity to testing higher-order correlations and performance in standard benchmarks (MNIST, FashionMNIST, CIFAR10, CIFAR100 and Imagenette). Our architecture outperforms traditional CNN baselines, and achieves optimal performance with expansions up to 3rd/4th order, aligning remarkably well with the distribution of pixel intensities in natural images. Through systematic perturbation analysis, we validate this alignment by isolating the contributions of specific image statistics to model performance, demonstrating how different orders of convolution process distinct aspects of visual information. Furthermore, Representational Similarity Analysis reveals distinct geometries across network layers, indicating qualitatively different modes of visual information processing. Our work bridges neuroscience and deep learning, offering a path towards more effective, biologically inspired computer vision models. It provides insights into visual information processing and lays the groundwork for neural networks that better capture complex visual patterns, particularly in resource-constrained scenarios.",
        "subjects": [
            "cs.CV",
            "cs.LG",
            "q-bio.NC"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06767",
        "abstract url": "https://arxiv.org/abs/2412.06767",
        "title": "MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "depth"
            ],
            [
                "robotics"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "We present a novel appearance model that simultaneously realizes explicit high-quality 3D surface mesh recovery and photorealistic novel view synthesis from sparse view samples. Our key idea is to model the underlying scene geometry Mesh as an Atlas of Charts which we render with 2D Gaussian surfels (MAtCha Gaussians). MAtCha distills high-frequency scene surface details from an off-the-shelf monocular depth estimator and refines it through Gaussian surfel rendering. The Gaussian surfels are attached to the charts on the fly, satisfying photorealism of neural volumetric rendering and crisp geometry of a mesh model, i.e., two seemingly contradicting goals in a single model. At the core of MAtCha lies a novel neural deformation model and a structure loss that preserve the fine surface details distilled from learned monocular depths while addressing their fundamental scale ambiguities. Results of extensive experimental validation demonstrate MAtCha's state-of-the-art quality of surface reconstruction and photorealism on-par with top contenders but with dramatic reduction in the number of input views and computational time. We believe MAtCha will serve as a foundational tool for any visual application in vision, graphics, and robotics that require explicit geometry in addition to photorealism. Our project page is the following: https://anttwo.github.io/matcha/",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "Project Webpage: https://anttwo.github.io/matcha/"
    },
    {
        "paper id": "2412.06771",
        "abstract url": "https://arxiv.org/abs/2412.06771",
        "title": "Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty",
        "rating": "-1",
        "keywords": [
            [
                "Text-to-Image"
            ],
            [
                "graph"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "User prompts for generative AI models are often underspecified, leading to sub-optimal responses. This problem is particularly evident in text-to-image (T2I) generation, where users commonly struggle to articulate their precise intent. This disconnect between the user's vision and the model's interpretation often forces users to painstakingly and repeatedly refine their prompts. To address this, we propose a design for proactive T2I agents equipped with an interface to (1) actively ask clarification questions when uncertain, and (2) present their understanding of user intent as an understandable belief graph that a user can edit. We build simple prototypes for such agents and verify their effectiveness through both human studies and automated evaluation. We observed that at least 90% of human subjects found these agents and their belief graphs helpful for their T2I workflow. Moreover, we develop a scalable automated evaluation approach using two agents, one with a ground truth image and the other tries to ask as few questions as possible to align with the ground truth. On DesignBench, a benchmark we created for artists and designers, the COCO dataset (Lin et al., 2014), and ImageInWords (Garg et al., 2024), we observed that these T2I agents were able to ask informative questions and elicit crucial information to achieve successful alignment with at least 2 times higher VQAScore (Lin et al., 2024) than the standard single-turn T2I generation. Demo: https://github.com/google-deepmind/proactive_t2i_agents.",
        "subjects": [
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06776",
        "abstract url": "https://arxiv.org/abs/2412.06776",
        "title": "Enhancing Robotic System Robustness via Lyapunov Exponent-Based Optimization",
        "rating": "-1",
        "keywords": [
            [
                "robotics",
                "robot"
            ]
        ],
        "abstract": "We present a novel approach to quantifying and optimizing stability in robotic systems based on the Lyapunov exponents addressing an open challenge in the field of robot analysis, design, and optimization. Our method leverages differentiable simulation over extended time horizons. The proposed metric offers several properties, including a natural extension to limit cycles commonly encountered in robotics tasks and locomotion. We showcase, with an ad-hoc JAX gradient-based optimization framework, remarkable power, and flexi-bility in tackling the robustness challenge. The effectiveness of our approach is tested through diverse scenarios of varying complexity, encompassing high-degree-of-freedom systems and contact-rich environments. The positive outcomes across these cases highlight the potential of our method in enhancing system robustness.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "7 pages, 11 figures"
    },
    {
        "paper id": "2412.06777",
        "abstract url": "https://arxiv.org/abs/2412.06777",
        "title": "Driv3R: Learning Dense 4D Reconstruction for Autonomous Driving",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "depth"
            ],
            [
                "Autonomous Driving"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Realtime 4D reconstruction for dynamic scenes remains a crucial challenge for autonomous driving perception. Most existing methods rely on depth estimation through self-supervision or multi-modality sensor fusion. In this paper, we propose Driv3R, a DUSt3R-based framework that directly regresses per-frame point maps from multi-view image sequences. To achieve streaming dense reconstruction, we maintain a memory pool to reason both spatial relationships across sensors and dynamic temporal contexts to enhance multi-view 3D consistency and temporal integration. Furthermore, we employ a 4D flow predictor to identify moving objects within the scene to direct our network focus more on reconstructing these dynamic regions. Finally, we align all per-frame pointmaps consistently to the world coordinate system in an optimization-free manner. We conduct extensive experiments on the large-scale nuScenes dataset to evaluate the effectiveness of our method. Driv3R outperforms previous frameworks in 4D dynamic scene reconstruction, achieving 15x faster inference speed compared to methods requiring global alignment. Code: https://github.com/Barrybarry-Smith/Driv3R.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "comment": "Code is available at: https://github.com/Barrybarry-Smith/Driv3R"
    },
    {
        "paper id": "2412.06780",
        "abstract url": "https://arxiv.org/abs/2412.06780",
        "title": "Diverse Score Distillation",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ],
            [
                "diffusion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Score distillation of 2D diffusion models has proven to be a powerful mechanism to guide 3D optimization, for example enabling text-based 3D generation or single-view reconstruction. A common limitation of existing score distillation formulations, however, is that the outputs of the (mode-seeking) optimization are limited in diversity despite the underlying diffusion model being capable of generating diverse samples. In this work, inspired by the sampling process in denoising diffusion, we propose a score formulation that guides the optimization to follow generation paths defined by random initial seeds, thus ensuring diversity. We then present an approximation to adopt this formulation for scenarios where the optimization may not precisely follow the generation paths (e.g. a 3D representation whose renderings evolve in a co-dependent manner). We showcase the applications of our `Diverse Score Distillation' (DSD) formulation across tasks such as 2D optimization, text-based 3D inference, and single-view reconstruction. We also empirically validate DSD against prior score distillation formulations and show that it significantly improves sample diversity while preserving fidelity.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project Page: https://billyxyb.github.io/Diverse-Score-Distillation/"
    },
    {
        "paper id": "2412.06782",
        "abstract url": "https://arxiv.org/abs/2412.06782",
        "title": "CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction",
        "rating": "-1",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "trajectory"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "In robotic visuomotor policy learning, diffusion-based models have achieved significant success in improving the accuracy of action trajectory generation compared to traditional autoregressive models. However, they suffer from inefficiency due to multiple denoising steps and limited flexibility from complex constraints. In this paper, we introduce Coarse-to-Fine AutoRegressive Policy (CARP), a novel paradigm for visuomotor policy learning that redefines the autoregressive action generation process as a coarse-to-fine, next-scale approach. CARP decouples action generation into two stages: first, an action autoencoder learns multi-scale representations of the entire action sequence; then, a GPT-style transformer refines the sequence prediction through a coarse-to-fine autoregressive process. This straightforward and intuitive approach produces highly accurate and smooth actions, matching or even surpassing the performance of diffusion-based policies while maintaining efficiency on par with autoregressive policies. We conduct extensive evaluations across diverse settings, including single-task and multi-task scenarios on state-based and image-based simulation benchmarks, as well as real-world tasks. CARP achieves competitive success rates, with up to a 10% improvement, and delivers 10x faster inference compared to state-of-the-art policies, establishing a high-performance, efficient, and flexible paradigm for action generation in robotic tasks.",
        "subjects": [
            "cs.RO",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06869",
        "abstract url": "https://arxiv.org/abs/2412.06869",
        "title": "Safety Monitoring of Machine Learning Perception Functions: a Survey",
        "rating": "-1",
        "keywords": [
            [
                "surgical"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Machine Learning (ML) models, such as deep neural networks, are widely applied in autonomous systems to perform complex perception tasks. New dependability challenges arise when ML predictions are used in safety-critical applications, like autonomous cars and surgical robots. Thus, the use of fault tolerance mechanisms, such as safety monitors, is essential to ensure the safe behavior of the system despite the occurrence of faults. This paper presents an extensive literature review on safety monitoring of perception functions using ML in a safety-critical context. In this review, we structure the existing literature to highlight key factors to consider when designing such monitors: threat identification, requirements elicitation, detection of failure, reaction, and evaluation. We also highlight the ongoing challenges associated with safety monitoring and suggest directions for future research.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.SE"
        ],
        "comment": "25 pages, 2 figures"
    },
    {
        "paper id": "2412.06874",
        "abstract url": "https://arxiv.org/abs/2412.06874",
        "title": "Real-Time Performance Optimization of Travel Reservation Systems Using AI and Microservices",
        "rating": "-1",
        "keywords": [
            [
                "forecast"
            ],
            [
                "cs.AI",
                "cs.CL"
            ]
        ],
        "abstract": "The rapid growth of the travel industry has increased the need for real-time optimization in reservation systems that could take care of huge data and transaction volumes. This study proposes a hybrid framework that ut folds an Artificial Intelligence and a Microservices approach for the performance optimization of the system. The AI algorithms forecast demand patterns, optimize the allocation of resources, and enhance decision-making driven by Microservices architecture, hence decentralizing system components for scalability, fault tolerance, and reduced downtime. The model provided focuses on major problems associated with the travel reservation systems such as latency of systems, load balancing and data consistency. It endows the systems with predictive models based on AI improved ability to forecast user demands. Microservices would also take care of different scales during uneven traffic patterns. Hence, both aspects ensure better handling of peak loads and spikes while minimizing delays and ensuring high service quality. A comparison was made between traditional reservation models, which are monolithic and the new model of AI-Microservices. Comparatively, the analysis results state that there is a drastic improvement in processing times where the system uptime and resource utilization proved the capability of AI and the microservices in transforming the travel industry in terms of reservation. This research work focused on AI and Microservices towards real-time optimization, providing critical insight into how to move forward with practical recommendations for upgrading travel reservation systems with this technology.",
        "subjects": [
            "cs.SE",
            "cs.AI",
            "cs.CE",
            "cs.CL",
            "cs.PL"
        ],
        "comment": "19 pages, 12 figures"
    },
    {
        "paper id": "2412.06952",
        "abstract url": "https://arxiv.org/abs/2412.06952",
        "title": "Massively Parallel Algorithms for Approximate Shortest Paths",
        "rating": "-1",
        "keywords": [
            [
                "graphs"
            ]
        ],
        "abstract": "We present fast algorithms for approximate shortest paths in the massively parallel computation (MPC) model. We provide randomized algorithms that take $poly(\\log{\\log{n}})$ rounds in the near-linear memory MPC model. Our results are for unweighted undirected graphs with $n$ vertices and $m$ edges. Our first contribution is a $(1+\u03b5)$-approximation algorithm for Single-Source Shortest Paths (SSSP) that takes $poly(\\log{\\log{n}})$ rounds in the near-linear MPC model, where the memory per machine is $\\tilde{O}(n)$ and the total memory is $\\tilde{O}(mn^\u03c1)$, where $\u03c1$ is a small constant. Our second contribution is a distance oracle that allows to approximate the distance between any pair of vertices. The distance oracle is constructed in $poly(\\log{\\log{n}})$ rounds and allows to query a $(1+\u03b5)(2k-1)$-approximate distance between any pair of vertices $u$ and $v$ in $O(1)$ additional rounds. The algorithm is for the near-linear memory MPC model with total memory of size $\\tilde{O}((m+n^{1+\u03c1})n^{1/k})$, where $\u03c1$ is a small constant. While our algorithms are for the near-linear MPC model, in fact they only use one machine with $\\tilde{O}(n)$ memory, where the rest of machines can have sublinear memory of size $O(n^\u03b3)$ for a small constant $\u03b3< 1$. All previous algorithms for approximate shortest paths in the near-linear MPC model either required $\u03a9(\\log{n})$ rounds or had an $\u03a9(\\log{n})$ approximation. Our approach is based on fast construction of near-additive emulators, limited-scale hopsets and limited-scale distance sketches that are tailored for the MPC model. While our end-results are for the near-linear MPC model, many of the tools we construct such as hopsets and emulators are constructed in the more restricted sublinear MPC model.",
        "subjects": [
            "cs.DS",
            "cs.DC"
        ],
        "comment": "50 pages, 4 figures"
    },
    {
        "paper id": "2412.06961",
        "abstract url": "https://arxiv.org/abs/2412.06961",
        "title": "Analysis of Conducted and Radiated Emission on a Self-oscillating Capacitive Touch Sensing Circuit",
        "rating": "-1",
        "keywords": [
            [
                "vehicle"
            ]
        ],
        "abstract": "With the advent of smartphones, there has been a recent increase in the use of capacitive touch sensing for various Human Machine Interfaces (HMI). Capacitive-based touch sensing provides higher flexibility and cost-effectiveness than, methodologies such as resistive-based touch sensing. However, Capacitive-based touch sensing is more prone to disturbances such as Electromagnetic interference (EMI) and noise due to temperature variation. This effect becomes more dominating as the sensing excitation frequency increases. Traditional capacitance to digital circuits, such as sigma-delta capacitive sensing, requires multiple clock cycles to measure sensing capacitance, thus necessitating higher frequency operation. In turn, this produces challenges in Electromagnetic Emission while also increasing its susceptibility to EMI, such as false or ghost touch due to exposure of the sensing electrodes to various frequency electric fields. This paper discusses the conducted electromagnetic emission behavior of an external excitation-frequency independent self-oscillating capacitance-to-time converter, where sensing is done with a single clock cycle, and discusses radiated Electromagnetic emission of the touch sensing electrode. The proposed approach is suitable for touch-sensing applications, mainly when used in a noisy EMI environment, such as inside a vehicle within the Automotive industry.",
        "subjects": [
            "cs.ET"
        ],
        "comment": "4 pages, 8 figures"
    },
    {
        "paper id": "2412.06983",
        "abstract url": "https://arxiv.org/abs/2412.06983",
        "title": "Collision-inclusive Manipulation Planning for Occluded Object Grasping via Compliant Robot Motions",
        "rating": "-1",
        "keywords": [
            [
                "Robot",
                "robotic manipulation"
            ]
        ],
        "abstract": "Traditional robotic manipulation mostly focuses on collision-free tasks. In practice, however, many manipulation tasks (e.g., occluded object grasping) require the robot to intentionally collide with the environment to reach a desired task configuration. By enabling compliant robot motions, collisions between the robot and the environment are allowed and can thus be exploited, but more physical uncertainties are introduced. To address collision-rich problems such as occluded object grasping while handling the involved uncertainties, we propose a collision-inclusive planning framework that can transition the robot to a desired task configuration via roughly modeled collisions absorbed by Cartesian impedance control. By strategically exploiting the environmental constraints and exploring inside a manipulation funnel formed by task repetitions, our framework can effectively reduce physical and perception uncertainties. With real-world evaluations on both single-arm and dual-arm setups, we show that our framework is able to efficiently address various realistic occluded grasping problems where a feasible grasp does not initially exist.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication"
    },
    {
        "paper id": "2412.07012",
        "abstract url": "https://arxiv.org/abs/2412.07012",
        "title": "ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models",
        "rating": "-1",
        "keywords": [
            [
                "3D",
                "depth"
            ],
            [
                "graphs"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "With the rise of multimodal applications, instruction data has become critical for training multimodal language models capable of understanding complex image-based queries. Existing practices rely on powerful but costly large language models (LLMs) or multimodal language models (MLMs) to produce instruction data. These are often prone to hallucinations, licensing issues and the generation process is often hard to scale and interpret. In this work, we present a programmatic approach that employs scene graphs as symbolic representations of images and human-written programs to systematically synthesize vision-centric instruction data. Our approach ensures the interpretability and controllability of the data generation process and scales efficiently while maintaining factual accuracy. By implementing a suite of 24 single-image, 14 multi-image instruction generators, and a scene graph generation pipeline, we build a scalable, cost-effective system: ProVision which produces diverse question-answer pairs concerning objects, attributes, relations, depth, etc., for any given image. Applied to Visual Genome and DataComp datasets, we generate over 10 million instruction data points, ProVision-10M, and leverage them in both pretraining and instruction tuning stages of MLMs. When adopted in the instruction tuning stage, our single-image instruction data yields up to a 7% improvement on the 2D split and 8% on the 3D split of CVBench, along with a 3% increase in performance on QBench2, RealWorldQA, and MMMU. Our multi-image instruction data leads to an 8% improvement on Mantis-Eval. Incorporation of our data in both pre-training and fine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6% across 11 benchmarks.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "code: https://github.com/JieyuZ2/ProVision"
    },
    {
        "paper id": "2412.07019",
        "abstract url": "https://arxiv.org/abs/2412.07019",
        "title": "Assessing the Impact of Conspiracy Theories Using Large Language Models",
        "rating": "-1",
        "keywords": [
            [
                "CT",
                "psychological"
            ],
            [
                "cs.CY",
                "cs.CL"
            ]
        ],
        "abstract": "Measuring the relative impact of CTs is important for prioritizing responses and allocating resources effectively, especially during crises. However, assessing the actual impact of CTs on the public poses unique challenges. It requires not only the collection of CT-specific knowledge but also diverse information from social, psychological, and cultural dimensions. Recent advancements in large language models (LLMs) suggest their potential utility in this context, not only due to their extensive knowledge from large training corpora but also because they can be harnessed for complex reasoning. In this work, we develop datasets of popular CTs with human-annotated impacts. Borrowing insights from human impact assessment processes, we then design tailored strategies to leverage LLMs for performing human-like CT impact assessments. Through rigorous experiments, we textit{discover that an impact assessment mode using multi-step reasoning to analyze more CT-related evidence critically produces accurate results; and most LLMs demonstrate strong bias, such as assigning higher impacts to CTs presented earlier in the prompt, while generating less accurate impact assessments for emotionally charged and verbose CTs.",
        "subjects": [
            "cs.CL",
            "cs.CY"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07022",
        "abstract url": "https://arxiv.org/abs/2412.07022",
        "title": "Dense Cross-Connected Ensemble Convolutional Neural Networks for Enhanced Model Robustness",
        "rating": "-1",
        "keywords": [
            [
                "depth"
            ],
            [
                "attacks"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The resilience of convolutional neural networks against input variations and adversarial attacks remains a significant challenge in image recognition tasks. Motivated by the need for more robust and reliable image recognition systems, we propose the Dense Cross-Connected Ensemble Convolutional Neural Network (DCC-ECNN). This novel architecture integrates the dense connectivity principle of DenseNet with the ensemble learning strategy, incorporating intermediate cross-connections between different DenseNet paths to facilitate extensive feature sharing and integration. The DCC-ECNN architecture leverages DenseNet's efficient parameter usage and depth while benefiting from the robustness of ensemble learning, ensuring a richer and more resilient feature representation.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "6 pages, 1 figure"
    },
    {
        "paper id": "2412.07036",
        "abstract url": "https://arxiv.org/abs/2412.07036",
        "title": "Visualizing Distributed Traces in Aggregate",
        "rating": "-1",
        "keywords": [
            [
                "graph"
            ]
        ],
        "abstract": "Distributed systems are comprised of many components that communicate together to form an application. Distributed tracing gives us visibility into these complex interactions, but it can be difficult to reason about the system's behavior, even with traces. Systems collect large amounts of tracing data even with low sampling rates. Even when there are patterns in the system, it is often difficult to detect similarities in traces since current tools mainly allow developers to visualize individual traces. Debugging and system optimization is difficult for developers without an understanding of the whole trace dataset. In order to help present these similarities, this paper proposes a method to aggregate traces in a way that groups together and visualizes similar traces. We do so by assigning a few traces that are representative of each set. We suggest that traces can be grouped based on how many services they share, how many levels the graph has, how structurally similar they are, or how close their latencies are. We also develop an aggregate trace data structure as a way to comprehensively visualize these groups and a method for filtering out incomplete traces if a more complete version of the trace exists. The unique traces of each group are especially useful to developers for troubleshooting. Overall, our approach allows for a more efficient method of analyzing system behavior.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "10 pages, 12figures"
    },
    {
        "paper id": "2412.07079",
        "abstract url": "https://arxiv.org/abs/2412.07079",
        "title": "Light Field Image Quality Assessment With Auxiliary Learning Based on Depthwise and Anglewise Separable Convolutions",
        "rating": "-1",
        "keywords": [
            [
                "Quality Assessment"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "In multimedia broadcasting, no-reference image quality assessment (NR-IQA) is used to indicate the user-perceived quality of experience (QoE) and to support intelligent data transmission while optimizing user experience. This paper proposes an improved no-reference light field image quality assessment (NR-LFIQA) metric for future immersive media broadcasting services. First, we extend the concept of depthwise separable convolution (DSC) to the spatial domain of light field image (LFI) and introduce \"light field depthwise separable convolution (LF-DSC)\", which can extract the LFI's spatial features efficiently. Second, we further theoretically extend the LF-DSC to the angular space of LFI and introduce the novel concept of \"light field anglewise separable convolution (LF-ASC)\", which is capable of extracting both the spatial and angular features for comprehensive quality assessment with low complexity. Third, we define the spatial and angular feature estimations as auxiliary tasks in aiding the primary NR-LFIQA task by providing spatial and angular quality features as hints. To the best of our knowledge, this work is the first exploration of deep auxiliary learning with spatial-angular hints on NR-LFIQA. Experiments were conducted in mainstream LFI datasets such as Win5-LID and SMART with comparisons to the mainstream full reference IQA metrics as well as the state-of-the-art NR-LFIQA methods. The experimental results show that the proposed metric yields overall 42.86% and 45.95% smaller prediction errors than the second-best benchmarking metric in Win5-LID and SMART, respectively. In some challenging cases with particular distortion types, the proposed metric can reduce the errors significantly by more than 60%.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07119",
        "abstract url": "https://arxiv.org/abs/2412.07119",
        "title": "DiffCLIP: Few-shot Language-driven Multimodal Classifier",
        "rating": "-1",
        "keywords": [
            [
                "Visual language"
            ],
            [
                "diffusion"
            ],
            [
                "remote sensing"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Visual language models like Contrastive Language-Image Pretraining (CLIP) have shown impressive performance in analyzing natural images with language information. However, these models often encounter challenges when applied to specialized domains such as remote sensing due to the limited availability of image-text pairs for training. To tackle this issue, we introduce DiffCLIP, a novel framework that extends CLIP to effectively convey comprehensive language-driven semantic information for accurate classification of high-dimensional multimodal remote sensing images. DiffCLIP is a few-shot learning method that leverages unlabeled images for pretraining. It employs unsupervised mask diffusion learning to capture the distribution of diverse modalities without requiring labels. The modality-shared image encoder maps multimodal data into a unified subspace, extracting shared features with consistent parameters across modalities. A well-trained image encoder further enhances learning by aligning visual representations with class-label text information from CLIP. By integrating these approaches, DiffCLIP significantly boosts CLIP performance using a minimal number of image-text pairs. We evaluate DiffCLIP on widely used high-dimensional multimodal datasets, demonstrating its effectiveness in addressing few-shot annotated classification tasks. DiffCLIP achieves an overall accuracy improvement of 10.65% across three remote sensing datasets compared with CLIP, while utilizing only 2-shot image-text pairs. The code has been released at https://github.com/icey-zhang/DiffCLIP.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07135",
        "abstract url": "https://arxiv.org/abs/2412.07135",
        "title": "Oreo: Protecting ASLR Against Microarchitectural Attacks (Extended Version)",
        "rating": "-1",
        "keywords": [
            [
                "Attacks"
            ]
        ],
        "abstract": "Address Space Layout Randomization (ASLR) is one of the most prominently deployed mitigations against memory corruption attacks. ASLR randomly shuffles program virtual addresses to prevent attackers from knowing the location of program contents in memory. Microarchitectural side channels have been shown to defeat ASLR through various hardware mechanisms. We systematically analyze existing microarchitectural attacks and identify multiple leakage paths. Given the vast attack surface exposed by ASLR, it is challenging to effectively prevent leaking the ASLR secret against microarchitectural attacks. Motivated by this, we present Oreo, a software-hardware co-design mitigation that strengthens ASLR against these attacks. Oreo uses a new memory mapping interface to remove secret randomized bits in virtual addresses before translating them to their corresponding physical addresses. This extra step hides randomized virtual addresses from microarchitecture structures, preventing side channels from leaking ASLR secrets. Oreo is transparent to user programs and incurs low overhead. We prototyped and evaluated our design on Linux using the hardware simulator gem5.",
        "subjects": [
            "cs.CR",
            "cs.AR"
        ],
        "comment": "This paper has been accepted to NDSS 2025"
    },
    {
        "paper id": "2412.07136",
        "abstract url": "https://arxiv.org/abs/2412.07136",
        "title": "A multimodal ensemble approach for clear cell renal cell carcinoma treatment outcome prediction",
        "rating": "-1",
        "keywords": [
            [
                "survival",
                "whole slide",
                "cancer",
                "disease",
                "clinical",
                "DNA"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Purpose: A reliable cancer prognosis model for clear cell renal cell carcinoma (ccRCC) can enhance personalized treatment. We developed a multi-modal ensemble model (MMEM) that integrates pretreatment clinical data, multi-omics data, and histopathology whole slide image (WSI) data to predict overall survival (OS) and disease-free survival (DFS) for ccRCC patients. Methods: We analyzed 226 patients from The Cancer Genome Atlas Kidney Renal Clear Cell Carcinoma (TCGA-KIRC) dataset, which includes OS, DFS follow-up data, and five data modalities: clinical data, WSIs, and three multi-omics datasets (mRNA, miRNA, and DNA methylation). Separate survival models were built for OS and DFS. Cox-proportional hazards (CPH) model with forward feature selection is used for clinical and multi-omics data. Features from WSIs were extracted using ResNet and three general-purpose foundation models. A deep learning-based CPH model predicted survival using encoded WSI features. Risk scores from all models were combined based on training performance. Results: Performance was assessed using concordance index (C-index) and AUROC. The clinical feature-based CPH model received the highest weight for both OS and DFS tasks. Among WSI-based models, the general-purpose foundation model (UNI) achieved the best performance. The final MMEM model surpassed single-modality models, achieving C-indices of 0.820 (OS) and 0.833 (DFS), and AUROC values of 0.831 (3-year patient death) and 0.862 (cancer recurrence). Using predicted risk medians to stratify high- and low-risk groups, log-rank tests showed improved performance in both OS and DFS compared to single-modality models. Conclusion: MMEM is the first multi-modal model for ccRCC patients, integrating five data modalities. It outperformed single-modality models in prognostic ability and has the potential to assist in ccRCC patient management if independently validated.",
        "subjects": [
            "cs.CV",
            "q-bio.QM"
        ],
        "comment": "10 pages, 3 figures, 4 tables"
    },
    {
        "paper id": "2412.07141",
        "abstract url": "https://arxiv.org/abs/2412.07141",
        "title": "Integrating MedCLIP and Cross-Modal Fusion for Automatic Radiology Report Generation",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "clinical",
                "Radiology"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Automating radiology report generation can significantly reduce the workload of radiologists and enhance the accuracy, consistency, and efficiency of clinical documentation.We propose a novel cross-modal framework that uses MedCLIP as both a vision extractor and a retrieval mechanism to improve the process of medical report generation.By extracting retrieved report features and image features through an attention-based extract module, and integrating them with a fusion module, our method improves the coherence and clinical relevance of generated reports.Experimental results on the widely used IU-Xray dataset demonstrate the effectiveness of our approach, showing improvements over commonly used methods in both report quality and relevance.Additionally, ablation studies provide further validation of the framework, highlighting the importance of accurate report retrieval and feature integration in generating comprehensive medical reports.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Accepted in IEEE Big Data 2024"
    },
    {
        "paper id": "2412.07151",
        "abstract url": "https://arxiv.org/abs/2412.07151",
        "title": "dSTAR: Straggler Tolerant and Byzantine Resilient Distributed SGD",
        "rating": "-1",
        "keywords": [
            [
                "attacks"
            ]
        ],
        "abstract": "Distributed model training needs to be adapted to challenges such as the straggler effect and Byzantine attacks. When coordinating the training process with multiple computing nodes, ensuring timely and reliable gradient aggregation amidst network and system malfunctions is essential. To tackle these issues, we propose \\textit{dSTAR}, a lightweight and efficient approach for distributed stochastic gradient descent (SGD) that enhances robustness and convergence. \\textit{dSTAR} selectively aggregates gradients by collecting updates from the first \\(k\\) workers to respond, filtering them based on deviations calculated using an ensemble median. This method not only mitigates the impact of stragglers but also fortifies the model against Byzantine adversaries. We theoretically establish that \\textit{dSTAR} is (\\(\u03b1, f\\))-Byzantine resilient and achieves a linear convergence rate. Empirical evaluations across various scenarios demonstrate that \\textit{dSTAR} consistently maintains high accuracy, outperforming other Byzantine-resilient methods that often suffer up to a 40-50\\% accuracy drop under attack. Our results highlight \\textit{dSTAR} as a robust solution for training models in distributed environments prone to both straggler delays and Byzantine faults.",
        "subjects": [
            "cs.DC"
        ],
        "comment": "15 pages"
    },
    {
        "paper id": "2412.07154",
        "abstract url": "https://arxiv.org/abs/2412.07154",
        "title": "Unified Vertex Motion Estimation for Integrated Video Stabilization and Stitching in Tractor-Trailer Wheeled Robots",
        "rating": "-1",
        "keywords": [
            [
                "robot"
            ]
        ],
        "abstract": "Tractor-trailer wheeled robots need to perform comprehensive perception tasks to enhance their operations in areas such as logistics parks and long-haul transportation. The perception of these robots face three major challenges: the relative pose change between the tractor and trailer, the asynchronous vibrations between the tractor and trailer, and the significant camera parallax caused by the large size. In this paper, we propose a novel Unified Vertex Motion Video Stabilization and Stitching framework designed for unknown environments. To establish the relationship between stabilization and stitching, the proposed Unified Vertex Motion framework comprises the Stitching Motion Field, which addresses relative positional change, and the Stabilization Motion Field, which tackles asynchronous vibrations. Then, recognizing the heterogeneity of optimization functions required for stabilization and stitching, a weighted cost function approach is proposed to address the problem of camera parallax. Furthermore, this framework has been successfully implemented in real tractor-trailer wheeled robots. The proposed Unified Vertex Motion Video Stabilization and Stitching method has been thoroughly tested in various challenging scenarios, demonstrating its accuracy and practicality in real-world robot tasks.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07169",
        "abstract url": "https://arxiv.org/abs/2412.07169",
        "title": "Rate-In: Information-Driven Adaptive Dropout Rates for Improved Inference-Time Uncertainty Estimation",
        "rating": "-1",
        "keywords": [
            [
                "medical",
                "diagnosis"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Accurate uncertainty estimation is crucial for deploying neural networks in risk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a widely used technique for approximating predictive uncertainty by performing stochastic forward passes with dropout during inference. However, using static dropout rates across all layers and inputs can lead to suboptimal uncertainty estimates, as it fails to adapt to the varying characteristics of individual inputs and network layers. Existing approaches optimize dropout rates during training using labeled data, resulting in fixed inference-time parameters that cannot adjust to new data distributions, compromising uncertainty estimates in Monte Carlo simulations. In this paper, we propose Rate-In, an algorithm that dynamically adjusts dropout rates during inference by quantifying the information loss induced by dropout in each layer's feature maps. By treating dropout as controlled noise injection and leveraging information-theoretic principles, Rate-In adapts dropout rates per layer and per input instance without requiring ground truth labels. By quantifying the functional information loss in feature maps, we adaptively tune dropout rates to maintain perceptual quality across diverse medical imaging tasks and architectural configurations. Our extensive empirical study on synthetic data and real-world medical imaging tasks demonstrates that Rate-In improves calibration and sharpens uncertainty estimates compared to fixed or heuristic dropout rates without compromising predictive performance. Rate-In offers a practical, unsupervised, inference-time approach to optimizing dropout for more reliable predictive uncertainty estimation in critical applications.",
        "subjects": [
            "cs.LG",
            "cs.CV",
            "stat.ML"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07180",
        "abstract url": "https://arxiv.org/abs/2412.07180",
        "title": "Digital Twin Assisted Beamforming Design for Integrated Sensing and Communication Systems",
        "rating": "-1",
        "keywords": [
            [
                "3D"
            ]
        ],
        "abstract": "This paper explores a novel research direction where a digital twin is leveraged to assist the beamforming design for an integrated sensing and communication (ISAC) system. In this setup, a base station designs joint communication and sensing beamforming to serve the communication user and detect the sensing target concurrently. Utilizing the electromagnetic (EM) 3D model of the environment and ray tracing, the digital twin can provide various information, e.g., propagation path parameters and wireless channels, to aid communication and sensing systems. More specifically, our digital twin-based beamforming design first leverages the environment EM 3D model and ray tracing to (i) predict the directions of the line-of-sight (LoS) and non-line-of-sight (NLoS) sensing channel paths and (ii) identify the dominant one among these sensing channel paths. Then, to optimize the joint sensing and communication beam, we maximize the sensing signal-to-noise ratio (SNR) on the dominant sensing channel component while satisfying a minimum communication signal-to-interference-plus-noise ratio (SINR) requirement. Simulation results show that the proposed digital twin-assisted beamforming design achieves near-optimal target sensing SNR in both LoS and NLoS dominant areas, while ensuring the required SINR for the communication user. This highlights the potential of leveraging digital twins to assist ISAC systems.",
        "subjects": [
            "eess.SP",
            "cs.IT"
        ],
        "comment": "Presented at Asilomar 2024. Code and dataset will be available on the paper page: https://www.wi-lab.net/research/digital-twin-assisted-beamforming-design-for-integrated-sensing-and-communication-systems/"
    },
    {
        "paper id": "2412.07182",
        "abstract url": "https://arxiv.org/abs/2412.07182",
        "title": "An Enhancement of CNN Algorithm for Rice Leaf Disease Image Classification in Mobile Applications",
        "rating": "-1",
        "keywords": [
            [
                "Disease"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "This study focuses on enhancing rice leaf disease image classification algorithms, which have traditionally relied on Convolutional Neural Network (CNN) models. We employed transfer learning with MobileViTV2_050 using ImageNet-1k weights, a lightweight model that integrates CNN's local feature extraction with Vision Transformers' global context learning through a separable self-attention mechanism. Our approach resulted in a significant 15.66% improvement in classification accuracy for MobileViTV2_050-A, our first enhanced model trained on the baseline dataset, achieving 93.14%. Furthermore, MobileViTV2_050-B, our second enhanced model trained on a broader rice leaf dataset, demonstrated a 22.12% improvement, reaching 99.6% test accuracy. Additionally, MobileViTV2-A attained an F1-score of 93% across four rice labels and a Receiver Operating Characteristic (ROC) curve ranging from 87% to 97%. In terms of resource consumption, our enhanced models reduced the total parameters of the baseline CNN model by up to 92.50%, from 14 million to 1.1 million. These results indicate that MobileViTV2_050 not only improves computational efficiency through its separable self-attention mechanism but also enhances global context learning. Consequently, it offers a lightweight and robust solution suitable for mobile deployment, advancing the interpretability and practicality of models in precision agriculture.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Presented at 46th World Conference on Applied Science, Engineering & Technology (WCASET) from Institute for Educational Research and Publication (IFERP)"
    },
    {
        "paper id": "2412.07183",
        "abstract url": "https://arxiv.org/abs/2412.07183",
        "title": "Exploring What Why and How: A Multifaceted Benchmark for Causation Understanding of Video Anomaly",
        "rating": "-1",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Recent advancements in video anomaly understanding (VAU) have opened the door to groundbreaking applications in various fields, such as traffic monitoring and industrial automation. While the current benchmarks in VAU predominantly emphasize the detection and localization of anomalies. Here, we endeavor to delve deeper into the practical aspects of VAU by addressing the essential questions: \"what anomaly occurred?\", \"why did it happen?\", and \"how severe is this abnormal event?\". In pursuit of these answers, we introduce a comprehensive benchmark for Exploring the Causation of Video Anomalies (ECVA). Our benchmark is meticulously designed, with each video accompanied by detailed human annotations. Specifically, each instance of our ECVA involves three sets of human annotations to indicate \"what\", \"why\" and \"how\" of an anomaly, including 1) anomaly type, start and end times, and event descriptions, 2) natural language explanations for the cause of an anomaly, and 3) free text reflecting the effect of the abnormality. Building upon this foundation, we propose a novel prompt-based methodology that serves as a baseline for tackling the intricate challenges posed by ECVA. We utilize \"hard prompt\" to guide the model to focus on the critical parts related to video anomaly segments, and \"soft prompt\" to establish temporal and spatial relationships within these anomaly segments. Furthermore, we propose AnomEval, a specialized evaluation metric crafted to align closely with human judgment criteria for ECVA. This metric leverages the unique features of the ECVA dataset to provide a more comprehensive and reliable assessment of various video large language models. We demonstrate the efficacy of our approach through rigorous experimental analysis and delineate possible avenues for further investigation into the comprehension of video anomaly causation.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence. arXiv admin note: substantial text overlap with arXiv:2405.00181"
    },
    {
        "paper id": "2412.07187",
        "abstract url": "https://arxiv.org/abs/2412.07187",
        "title": "A New Federated Learning Framework Against Gradient Inversion Attacks",
        "rating": "-1",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "Attacks"
            ],
            [
                "cs.LG"
            ],
            [
                "AAAI"
            ]
        ],
        "abstract": "Federated Learning (FL) aims to protect data privacy by enabling clients to collectively train machine learning models without sharing their raw data. However, recent studies demonstrate that information exchanged during FL is subject to Gradient Inversion Attacks (GIA) and, consequently, a variety of privacy-preserving methods have been integrated into FL to thwart such attacks, such as Secure Multi-party Computing (SMC), Homomorphic Encryption (HE), and Differential Privacy (DP). Despite their ability to protect data privacy, these approaches inherently involve substantial privacy-utility trade-offs. By revisiting the key to privacy exposure in FL under GIA, which lies in the frequent sharing of model gradients that contain private data, we take a new perspective by designing a novel privacy preserve FL framework that effectively ``breaks the direct connection'' between the shared parameters and the local private data to defend against GIA. Specifically, we propose a Hypernetwork Federated Learning (HyperFL) framework that utilizes hypernetworks to generate the parameters of the local model and only the hypernetwork parameters are uploaded to the server for aggregation. Theoretical analyses demonstrate the convergence rate of the proposed HyperFL, while extensive experimental results show the privacy-preserving capability and comparable performance of HyperFL. Code is available at https://github.com/Pengxin-Guo/HyperFL.",
        "subjects": [
            "cs.LG",
            "cs.CR"
        ],
        "comment": "Accepted at AAAI 2025"
    },
    {
        "paper id": "2412.07189",
        "abstract url": "https://arxiv.org/abs/2412.07189",
        "title": "When Graph Meets Retrieval Augmented Generation for Wireless Networks: A Tutorial and Case Study",
        "rating": "-1",
        "keywords": [
            [
                "Graph"
            ]
        ],
        "abstract": "The rapid development of next-generation networking technologies underscores their transformative role in revolutionizing modern communication systems, enabling faster, more reliable, and highly interconnected solutions. However, such development has also brought challenges to network optimizations. Thanks to the emergence of Large Language Models (LLMs) in recent years, tools including Retrieval Augmented Generation (RAG) have been developed and applied in various fields including networking, and have shown their effectiveness. Taking one step further, the integration of knowledge graphs into RAG frameworks further enhanced the performance of RAG in networking applications such as Intent-Driven Networks (IDNs) and spectrum knowledge maps by providing more contextually relevant responses through more accurate retrieval of related network information. This paper introduces the RAG framework that integrates knowledge graphs in its database and explores such framework's application in networking. We begin by exploring RAG's applications in networking and the limitations of conventional RAG and present the advantages that knowledge graphs' structured knowledge representation brings to the retrieval and generation processes. Next, we propose a detailed GraphRAG-based framework for networking, including a step-by-step tutorial on its construction. Our evaluation through a case study on channel gain prediction demonstrates GraphRAG's enhanced capability in generating accurate, contextually rich responses, surpassing traditional RAG models. Finally, we discuss key future directions for applying knowledge-graphs-empowered RAG frameworks in networking, including robust updates, mitigation of hallucination, and enhanced security measures for networking applications.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "9 pages, 4 figures"
    },
    {
        "paper id": "2412.06205",
        "abstract url": "https://arxiv.org/abs/2412.06205",
        "title": "Applying Machine Learning Tools for Urban Resilience Against Floods",
        "rating": "-1.5",
        "keywords": [
            [
                "Health"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Floods are among the most prevalent and destructive natural disasters, often leading to severe social and economic impacts in urban areas due to the high concentration of assets and population density. In Iran, particularly in Tehran, recurring flood events underscore the urgent need for robust urban resilience strategies. This paper explores flood resilience models to identify the most effective approach for District 6 in Tehran. Through an extensive literature review, various resilience models were analyzed, with the Climate Disaster Resilience Index (CDRI) emerging as the most suitable model for this district due to its comprehensive resilience dimensions: Physical, Social, Economic, Organizational, and Natural Health resilience. Although the CDRI model provides a structured approach to resilience measurement, it remains a static model focused on spatial characteristics and lacks temporal adaptability. An extensive literature review enhances the CDRI model by integrating data from 2013 to 2022 in three-year intervals and applying machine learning techniques to predict resilience dimensions for 2025. This integration enables a dynamic resilience model that can accommodate temporal changes, providing a more adaptable and data driven foundation for urban flood resilience planning. By employing artificial intelligence to reflect evolving urban conditions, this model offers valuable insights for policymakers and urban planners to enhance flood resilience in Tehrans critical District 6.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "IEEE Fifth International Conference on Advances in Electrical, Computing, Communications and Sustainable Technologies"
    },
    {
        "paper id": "2412.06216",
        "abstract url": "https://arxiv.org/abs/2412.06216",
        "title": "Top-r Influential Community Search in Bipartite Graphs",
        "rating": "-1.5",
        "keywords": [
            [
                "depth"
            ],
            [
                "Graphs"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Community search over bipartite graphs is a fundamental problem, and finding influential communities has attracted significant attention. However, all existing studies have used the minimum weight of vertices as the influence of communities. This leads to an inaccurate assessment of real influence in graphs where there are only a few vertices with low weights. In this paper, we propose a new cohesive subgraph model named ($\u03b1$,$\u03b2$)-influential community that considers the average weight of vertices from two layers on bipartite graphs, thereby providing a more comprehensive reflection of community influence. Based on this community model, we present a recursive algorithm that traverses the entire bipartite graph to find top-$r$ ($\u03b1$,$\u03b2$)-influential communities. To further expedite the search for influential communities, we propose a slim tree structure to reduce the search width and introduce several effective upper bounds to reduce the search depth. Since we have proven that this problem is NP-hard, using exact algorithms to find top-$r$ ($\u03b1$,$\u03b2$)-communities accurately is very time-consuming. Therefore, we propose an approximate algorithm using a greedy approach to find top-$r$ ($\u03b1$,$\u03b2$)-communities as quickly as possible. It only takes $O((n+m)+m\\log_{}{n})$ time. Additionally, we introduce a new pruning algorithm to improve the efficiency of the search. Extensive experiments on 10 real-world graphs validate both the effectiveness and the efficiency of our algorithms.",
        "subjects": [
            "cs.SI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06231",
        "abstract url": "https://arxiv.org/abs/2412.06231",
        "title": "A Scalable Decentralized Reinforcement Learning Framework for UAV Target Localization Using Recurrent PPO",
        "rating": "-1.5",
        "keywords": [
            [
                "UAV",
                "drone",
                "agricultural"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The rapid advancements in unmanned aerial vehicles (UAVs) have unlocked numerous applications, including environmental monitoring, disaster response, and agricultural surveying. Enhancing the collective behavior of multiple decentralized UAVs can significantly improve these applications through more efficient and coordinated operations. In this study, we explore a Recurrent PPO model for target localization in perceptually degraded environments like places without GNSS/GPS signals. We first developed a single-drone approach for target identification, followed by a decentralized two-drone model. Our approach can utilize two types of sensors on the UAVs, a detection sensor and a target signal sensor. The single-drone model achieved an accuracy of 93%, while the two-drone model achieved an accuracy of 86%, with the latter requiring fewer average steps to locate the target. This demonstrates the potential of our method in UAV swarms, offering efficient and effective localization of radiant targets in complex environmental conditions.",
        "subjects": [
            "cs.RO",
            "cs.LG"
        ],
        "comment": "Submitted to TENCON 2024"
    },
    {
        "paper id": "2412.06237",
        "abstract url": "https://arxiv.org/abs/2412.06237",
        "title": "In Silico Pharmacokinetic and Molecular Docking Studies of Natural Plants against Essential Protein KRAS for Treatment of Pancreatic Cancer",
        "rating": "-1.5",
        "keywords": [
            [
                "bioactivity",
                "Cancer"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "A kind of pancreatic cancer called Pancreatic Ductal Adenocarcinoma (PDAC) is anticipated to be one of the main causes of mortality during past years. Evidence from several researches supported the concept that the oncogenic KRAS (Ki-ras2 Kirsten rat sarcoma viral oncogene) mutation is the major cause of pancreatic cancer. KRAS acts as an on-off switch that promotes cell growth. But when the KRAS gene is mutated, it will be in one position, allowing the cell growth uncontrollably. This uncontrollable multiplication of cells causes cancer growth. Therefore, KRAS was selected as the target protein in the study. Fifty plant-derived compounds are selected for the study. To determine whether the examined drugs could bind to the KRAS complex's binding pocket, molecular docking was performed. Computational analyses were used to assess the possible ability of tested substances to pass the Blood Brain Barrier (BBB). To predict the bioactivity of ligands a machine learning model was created. Five machine learning models were created and have chosen the best one among them for analyzing the bioactivity of each ligand. From the fifty plant-derived compounds the compounds with the least binding energies are selected. Then bioactivity of these six compounds is analyzed using Random Forest Regression model. Adsorption, Distribution, Metabolism, Excretion (ADME) properties of compounds are analyzed. The results showed that borneol has powerful effects and acts as a promising agent for the treatment of pancreatic cancer. This suggests that borneol found in plants like mint, ginger, rosemary, etc., is a successful compound for the treatment of pancreatic cancer.",
        "subjects": [
            "q-bio.BM",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06264",
        "abstract url": "https://arxiv.org/abs/2412.06264",
        "title": "Flow Matching Guide and Code",
        "rating": "-1.5",
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Flow Matching (FM) is a recent framework for generative modeling that has achieved state-of-the-art performance across various domains, including image, video, audio, speech, and biological structures. This guide offers a comprehensive and self-contained review of FM, covering its mathematical foundations, design choices, and extensions. By also providing a PyTorch package featuring relevant examples (e.g., image and text generation), this work aims to serve as a resource for both novice and experienced researchers interested in understanding, applying and further developing FM.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06265",
        "abstract url": "https://arxiv.org/abs/2412.06265",
        "title": "Table2Image: Interpretable Tabular data Classification with Realistic Image Transformations",
        "rating": "-1.5",
        "keywords": [
            [
                "Tabular"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent advancements in deep learning for tabular data have demonstrated promising performance, yet interpretable models remain limited, with many relying on complex and large-scale architectures. This paper introduces Table2Image, an interpretable framework that transforms tabular data into realistic image representations for classification, achieving competitive performance with relatively lightweight models. Additionally, we propose variance inflation factor (VIF) initialization, which reflects the statistical properties of the data, and a novel interpretability framework that integrates insights from both the original tabular data and its image transformations. By leveraging Shapley additive explanations (SHAP) with methods to minimize distributional discrepancies, our approach combines tabular and image-based representations. Experiments on benchmark datasets showcase competitive classification accuracy, area under the curve (AUC), and improved interpretability, offering a scalable and reliable solution. Our code is available at https://github.com/duneag2/table2image.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06288",
        "abstract url": "https://arxiv.org/abs/2412.06288",
        "title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "rating": "-1.5",
        "keywords": [
            [
                "Health"
            ],
            [
                "cs.CY"
            ]
        ],
        "abstract": "The surging demand for AI has led to a rapid expansion of energy-intensive data centers, impacting the environment through escalating carbon emissions and water consumption. While significant attention has been paid to AI's growing environmental footprint, the public health burden, a hidden toll of AI, has been largely overlooked. Specifically, AI's lifecycle, from chip manufacturing to data center operation, significantly degrades air quality through emissions of criteria air pollutants such as fine particulate matter, substantially impacting public health. This paper introduces a methodology to model pollutant emissions across AI's lifecycle, quantifying the public health impacts. Our findings reveal that training an AI model of the Llama3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. The total public health burden of U.S. data centers in 2030 is valued at up to more than $20 billion per year, double that of U.S. coal-based steelmaking and comparable to that of on-road emissions of California. Further, the public health costs unevenly impact economically disadvantaged communities, where the per-household health burden could be 200x more than that in less-impacted communities. We recommend adopting a standard reporting protocol for criteria air pollutants and the public health costs of AI, paying attention to all impacted communities, and implementing health-informed AI to mitigate adverse effects while promoting public health equity.",
        "subjects": [
            "cs.CY"
        ],
        "comment": "29 pages"
    },
    {
        "paper id": "2412.06308",
        "abstract url": "https://arxiv.org/abs/2412.06308",
        "title": "PRECISE: Pre-training Sequential Recommenders with Collaborative and Semantic Information",
        "rating": "-1.5",
        "keywords": [
            [
                "industrial",
                "recommendation"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Real-world recommendation systems commonly offer diverse content scenarios for users to interact with. Considering the enormous number of users in industrial platforms, it is infeasible to utilize a single unified recommendation model to meet the requirements of all scenarios. Usually, separate recommendation pipelines are established for each distinct scenario. This practice leads to challenges in comprehensively grasping users' interests. Recent research endeavors have been made to tackle this problem by pre-training models to encapsulate the overall interests of users. Traditional pre-trained recommendation models mainly capture user interests by leveraging collaborative signals. Nevertheless, a prevalent drawback of these systems is their incapacity to handle long-tail items and cold-start scenarios. With the recent advent of large language models, there has been a significant increase in research efforts focused on exploiting LLMs to extract semantic information for users and items. However, text-based recommendations highly rely on elaborate feature engineering and frequently fail to capture collaborative similarities. To overcome these limitations, we propose a novel pre-training framework for sequential recommendation, termed PRECISE. This framework combines collaborative signals with semantic information. Moreover, PRECISE employs a learning framework that initially models users' comprehensive interests across all recommendation scenarios and subsequently concentrates on the specific interests of target-scene behaviors. We demonstrate that PRECISE precisely captures the entire range of user interests and effectively transfers them to the target interests. Empirical findings reveal that the PRECISE framework attains outstanding performance on both public and industrial datasets.",
        "subjects": [
            "cs.IR",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06312",
        "abstract url": "https://arxiv.org/abs/2412.06312",
        "title": "Towards High-Level Modelling in Automated Planning",
        "rating": "-1.5",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Planning is a fundamental activity, arising frequently in many contexts, from daily tasks to industrial processes. The planning task consists of selecting a sequence of actions to achieve a specified goal from specified initial conditions. The Planning Domain Definition Language (PDDL) is the leading language used in the field of automated planning to model planning problems. Previous work has highlighted the limitations of PDDL, particularly in terms of its expressivity. Our interest lies in facilitating the handling of complex problems and enhancing the overall capability of automated planning systems. Unified-Planning is a Python library offering high-level API to specify planning problems and to invoke automated planners. In this paper, we present an extension of the UP library aimed at enhancing its expressivity for high-level problem modelling. In particular, we have added an array type, an expression to count booleans, and the allowance for integer parameters in actions. We show how these facilities enable natural high-level models of three classical planning problems.",
        "subjects": [
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06382",
        "abstract url": "https://arxiv.org/abs/2412.06382",
        "title": "PyPulse: A Python Library for Biosignal Imputation",
        "rating": "-1.5",
        "keywords": [
            [
                "Biosignal",
                "clinical"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We introduce PyPulse, a Python package for imputation of biosignals in both clinical and wearable sensor settings. Missingness is commonplace in these settings and can arise from multiple causes, such as insecure sensor attachment or data transmission loss. PyPulse's framework provides a modular and extendable framework with high ease-of-use for a broad userbase, including non-machine-learning bioresearchers. Specifically, its new capabilities include using pre-trained imputation methods out-of-the-box on custom datasets, running the full workflow of training or testing a baseline method with a single line of code, and comparing baseline methods in an interactive visualization tool. We released PyPulse under the MIT License on Github and PyPI. The source code can be found at: https://github.com/rehg-lab/pulseimpute.",
        "subjects": [
            "cs.LG",
            "cs.SE"
        ],
        "comment": "7 pages, 3 figures. Implementation and documentation are available at https://github.com/rehg-lab/pulseimpute"
    },
    {
        "paper id": "2412.06472",
        "abstract url": "https://arxiv.org/abs/2412.06472",
        "title": "Food for thought: How can machine learning help better predict and understand changes in food prices?",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In this work, we address a lack of systematic understanding of fluctuations in food affordability in Canada. Canada's Food Price Report (CPFR) is an annual publication that predicts food inflation over the next calendar year. The published predictions are a collaborative effort between forecasting teams that each employ their own approach at Canadian Universities: Dalhousie University, the University of British Columbia, the University of Saskatchewan, and the University of Guelph/Vector Institute. While the University of Guelph/Vector Institute forecasting team has leveraged machine learning (ML) in previous reports, the most recent editions (2024--2025) have also included a human-in-the-loop approach. For the 2025 report, this focus was expanded to evaluate several different data-centric approaches to improve forecast accuracy. In this study, we evaluate how different types of forecasting models perform when estimating food price fluctuations. We also examine the sensitivity of models that curate time series data representing key factors in food pricing.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06507",
        "abstract url": "https://arxiv.org/abs/2412.06507",
        "title": "BATseg: Boundary-aware Multiclass Spinal Cord Tumor Segmentation on 3D MRI Scans",
        "rating": "-1.5",
        "keywords": [
            [
                "3D"
            ],
            [
                "medical",
                "MRI",
                "Tumor"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ],
            [
                "ECCV"
            ]
        ],
        "abstract": "Spinal cord tumors significantly contribute to neurological morbidity and mortality. Precise morphometric quantification, encompassing the size, location, and type of such tumors, holds promise for optimizing treatment planning strategies. Although recent methods have demonstrated excellent performance in medical image segmentation, they primarily focus on discerning shapes with relatively large morphology such as brain tumors, ignoring the challenging problem of identifying spinal cord tumors which tend to have tiny sizes, diverse locations, and shapes. To tackle this hard problem of multiclass spinal cord tumor segmentation, we propose a new method, called BATseg, to learn a tumor surface distance field by applying our new multiclass boundary-aware loss function. To verify the effectiveness of our approach, we also introduce the first and large-scale spinal cord tumor dataset. It comprises gadolinium-enhanced T1-weighted 3D MRI scans from 653 patients and contains the four most common spinal cord tumor types: astrocytomas, ependymomas, hemangioblastomas, and spinal meningiomas. Extensive experiments on our dataset and another public kidney tumor segmentation dataset show that our proposed method achieves superior performance for multiclass tumor segmentation.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "ECCV 2024 Workshop on BioImage Computing. Code and data are available at: https://github.com/vLAR-group/BATseg"
    },
    {
        "paper id": "2412.06629",
        "abstract url": "https://arxiv.org/abs/2412.06629",
        "title": "PolytopeWalk: Sparse MCMC Sampling over Polytopes",
        "rating": "-1.5",
        "keywords": [
            [
                "biology",
                "facial"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "High dimensional sampling is an important computational tool in statistics and other computational disciplines, with applications ranging from Bayesian statistical uncertainty quantification, metabolic modeling in systems biology to volume computation. We present $\\textsf{PolytopeWalk}$, a new scalable Python library designed for uniform sampling over polytopes. The library provides an end-to-end solution, which includes preprocessing algorithms such as facial reduction and initialization methods. Six state-of-the-art MCMC algorithms on polytopes are implemented, including the Dikin, Vaidya, and John Walk. Additionally, we introduce novel sparse constrained formulations of these algorithms, enabling efficient sampling from sparse polytopes of the form $K_2 = \\{x \\in \\mathbb{R}^d \\ | \\ Ax = b, x \\succeq_k 0\\}$. This implementation maintains sparsity in $A$, ensuring scalability to high dimensional settings $(d > 10^5)$. We demonstrate the improved sampling efficiency and per-iteration cost on both Netlib datasets and structured polytopes. $\\textsf{PolytopeWalk}$ is available at github.com/ethz-randomwalk/polytopewalk with documentation at polytopewalk.readthedocs.io .",
        "subjects": [
            "stat.CO",
            "cs.LG",
            "stat.ML"
        ],
        "comment": "17 pages"
    },
    {
        "paper id": "2412.06649",
        "abstract url": "https://arxiv.org/abs/2412.06649",
        "title": "Semantic Search and Recommendation Algorithm",
        "rating": "-1.5",
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "This paper introduces a new semantic search algorithm that uses Word2Vec and Annoy Index to improve the efficiency of information retrieval from large datasets. The proposed approach addresses the limitations of traditional search methods by offering enhanced speed, accuracy, and scalability. Testing on datasets up to 100GB demonstrates the method's effectiveness in processing vast amounts of data while maintaining high precision and performance.",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.DB",
            "cs.LG"
        ],
        "comment": "6 pages, 5 Figures"
    },
    {
        "paper id": "2412.06684",
        "abstract url": "https://arxiv.org/abs/2412.06684",
        "title": "Exploring Critical Testing Scenarios for Decision-Making Policies: An LLM Approach",
        "rating": "-1.5",
        "keywords": [
            [
                "autonomous driving"
            ],
            [
                "robotics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Recent years have witnessed surprising achievements of decision-making policies across various fields, such as autonomous driving and robotics. Testing for decision-making policies is crucial with the existence of critical scenarios that may threaten their reliability. Numerous research efforts have been dedicated to testing these policies. However, there are still significant challenges, such as low testing efficiency and diversity due to the complexity of the policies and environments under test. Inspired by the remarkable capabilities of large language models (LLMs), in this paper, we propose an LLM-driven online testing framework for efficiently testing decision-making policies. The main idea is to employ an LLM-based test scenario generator to intelligently generate challenging test cases through contemplation and reasoning. Specifically, we first design a \"generate-test-feedback\" pipeline and apply templated prompt engineering to fully leverage the knowledge and reasoning abilities of LLMs. Then, we introduce a multi-scale scenario generation strategy to address the inherent challenges LLMs face in making fine adjustments, further enhancing testing efficiency. Finally, we evaluate the LLM-driven approach on five widely used benchmarks. The experimental results demonstrate that our method significantly outperforms baseline approaches in uncovering both critical and diverse scenarios.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "16 pages, 13 figures"
    },
    {
        "paper id": "2412.06685",
        "abstract url": "https://arxiv.org/abs/2412.06685",
        "title": "Policy Agnostic RL: Offline RL and Online RL Fine-Tuning of Any Class and Backbone",
        "rating": "-1.5",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "robot"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Recent advances in learning decision-making policies can largely be attributed to training expressive policy models, largely via imitation learning. While imitation learning discards non-expert data, reinforcement learning (RL) can still learn from suboptimal data. However, instantiating RL training of a new policy class often presents a different challenge: most deep RL machinery is co-developed with assumptions on the policy class and backbone, resulting in poor performance when the policy class changes. For instance, SAC utilizes a low-variance reparameterization policy gradient for Gaussian policies, but this is unstable for diffusion policies and intractable for autoregressive categorical policies. To address this issue, we develop an offline RL and online fine-tuning approach called policy-agnostic RL (PA-RL) that can effectively train multiple policy classes, with varying architectures and sizes. We build off the basic idea that a universal supervised learning loss can replace the policy improvement step in RL, as long as it is applied on \"optimized\" actions. To obtain these optimized actions, we first sample multiple actions from a base policy, and run global optimization (i.e., re-ranking multiple action samples using the Q-function) and local optimization (i.e., running gradient steps on an action sample) to maximize the critic on these candidates. PA-RL enables fine-tuning diffusion and transformer policies with either autoregressive tokens or continuous action outputs, at different sizes, entirely via actor-critic RL. Moreover, PA-RL improves the performance and sample-efficiency by up to 2 times compared to existing offline RL and online fine-tuning methods. We show the first result that successfully fine-tunes OpenVLA, a 7B generalist robot policy, autonomously with Cal-QL, an online RL fine-tuning algorithm, improving from 40% to 70% in the real world in 40 minutes.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06759",
        "abstract url": "https://arxiv.org/abs/2412.06759",
        "title": "XRZoo: A Large-Scale and Versatile Dataset of Extended Reality (XR) Applications",
        "rating": "-1.5",
        "keywords": [
            [
                "healthcare"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "The rapid advancement of Extended Reality (XR, encompassing AR, MR, and VR) and spatial computing technologies forms a foundational layer for the emerging Metaverse, enabling innovative applications across healthcare, education, manufacturing, and entertainment. However, research in this area is often limited by the lack of large, representative, and highquality application datasets that can support empirical studies and the development of new approaches benefiting XR software processes. In this paper, we introduce XRZoo, a comprehensive and curated dataset of XR applications designed to bridge this gap. XRZoo contains 12,528 free XR applications, spanning nine app stores, across all XR techniques (i.e., AR, MR, and VR) and use cases, with detailed metadata on key aspects such as application descriptions, application categories, release dates, user review numbers, and hardware specifications, etc. By making XRZoo publicly available, we aim to foster reproducible XR software engineering and security research, enable cross-disciplinary investigations, and also support the development of advanced XR systems by providing examples to developers. Our dataset serves as a valuable resource for researchers and practitioners interested in improving the scalability, usability, and effectiveness of XR applications. XRZoo will be released and actively maintained.",
        "subjects": [
            "cs.SE",
            "cs.AI",
            "cs.CR",
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06779",
        "abstract url": "https://arxiv.org/abs/2412.06779",
        "title": "AnyBimanual: Transferring Unimanual Policy for General Bimanual Manipulation",
        "rating": "-1.5",
        "keywords": [
            [
                "industrial"
            ],
            [
                "cs.AI"
            ]
        ],
        "abstract": "Performing general language-conditioned bimanual manipulation tasks is of great importance for many applications ranging from household service to industrial assembly. However, collecting bimanual manipulation data is expensive due to the high-dimensional action space, which poses challenges for conventional methods to handle general bimanual manipulation tasks. In contrast, unimanual policy has recently demonstrated impressive generalizability across a wide range of tasks because of scaled model parameters and training data, which can provide sharable manipulation knowledge for bimanual systems. To this end, we propose a plug-and-play method named AnyBimanual, which transfers pre-trained unimanual policy to general bimanual manipulation policy with few bimanual demonstrations. Specifically, we first introduce a skill manager to dynamically schedule the skill representations discovered from pre-trained unimanual policy for bimanual manipulation tasks, which linearly combines skill primitives with task-oriented compensation to represent the bimanual manipulation instruction. To mitigate the observation discrepancy between unimanual and bimanual systems, we present a visual aligner to generate soft masks for visual embedding of the workspace, which aims to align visual input of unimanual policy model for each arm with those during pretraining stage. AnyBimanual shows superiority on 12 simulated tasks from RLBench2 with a sizable 12.67% improvement in success rate over previous methods. Experiments on 9 real-world tasks further verify its practicality with an average success rate of 84.62%.",
        "subjects": [
            "cs.RO",
            "cs.AI"
        ],
        "comment": "Project page: https://anybimanual.github.io/"
    },
    {
        "paper id": "2412.06871",
        "abstract url": "https://arxiv.org/abs/2412.06871",
        "title": "Predicting Subway Passenger Flows under Incident Situation with Causality",
        "rating": "-1.5",
        "keywords": [
            [
                "forecast"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "In the context of rail transit operations, real-time passenger flow prediction is essential; however, most models primarily focus on normal conditions, with limited research addressing incident situations. There are several intrinsic challenges associated with prediction during incidents, such as a lack of interpretability and data scarcity. To address these challenges, we propose a two-stage method that separates predictions under normal conditions and the causal effects of incidents. First, a normal prediction model is trained using data from normal situations. Next, the synthetic control method is employed to identify the causal effects of incidents, combined with placebo tests to determine significant levels of these effects. The significant effects are then utilized to train a causal effect prediction model, which can forecast the impact of incidents based on features of the incidents and passenger flows. During the prediction phase, the results from both the normal situation model and the causal effect prediction model are integrated to generate final passenger flow predictions during incidents. Our approach is validated using real-world data, demonstrating improved accuracy. Furthermore, the two-stage methodology enhances interpretability. By analyzing the causal effect prediction model, we can identify key influencing factors related to the effects of incidents and gain insights into their underlying mechanisms. Our work can assist subway system managers in estimating passenger flow affected by incidents and enable them to take proactive measures. Additionally, it can deepen researchers' understanding of the impact of incidents on subway passenger flows.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06924",
        "abstract url": "https://arxiv.org/abs/2412.06924",
        "title": "Efficient user history modeling with amortized inference for deep learning recommendation models",
        "rating": "-1.5",
        "keywords": [
            [
                "recommendation"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We study user history modeling via Transformer encoders in deep learning recommendation models (DLRM). Such architectures can significantly improve recommendation quality, but usually incur high latency cost necessitating infrastructure upgrades or very small Transformer models. An important part of user history modeling is early fusion of the candidate item and various methods have been studied. We revisit early fusion and compare concatenation of the candidate to each history item against appending it to the end of the list as a separate item. Using the latter method, allows us to reformulate the recently proposed amortized history inference algorithm M-FALCON \\cite{zhai2024actions} for the case of DLRM models. We show via experimental results that appending with cross-attention performs on par with concatenation and that amortization significantly reduces inference costs. We conclude with results from deploying this model on the LinkedIn Feed and Ads surfaces, where amortization reduces latency by 30\\% compared to non-amortized inference.",
        "subjects": [
            "cs.LG",
            "cs.IR"
        ],
        "comment": "5 pages, 3 figures, WWW 2025"
    },
    {
        "paper id": "2412.06935",
        "abstract url": "https://arxiv.org/abs/2412.06935",
        "title": "Hypermodularity and community detection in higher-order networks",
        "rating": "-1.5",
        "keywords": [
            [
                "biological"
            ],
            [
                "cs.SI"
            ]
        ],
        "abstract": "Numerous networked systems feature a structure of non-trivial communities, which often correspond to their functional modules. Such communities have been detected in real-world biological, social and technological systems, as well as in synthetic models thereof. While much effort has been devoted to develop methods for community detection in traditional networks, the study of community structure in networks with higher-order interactions is still relatively unexplored. In this article, we introduce a formalism for the hypermodularity of higher-order networks that allows us to use spectral methods to detect community structures in hypergraphs. We apply this approach to synthetic random networks as well as to real-world data, showing that it produces results that reflect the nature and the dynamics of the interactions modelled, thereby constituting a valuable tool for the extraction of hidden information from complex higher-order data sets.",
        "subjects": [
            "physics.soc-ph",
            "cond-mat.dis-nn",
            "cs.SI"
        ],
        "comment": "13 pages, 3 figures"
    },
    {
        "paper id": "2412.06936",
        "abstract url": "https://arxiv.org/abs/2412.06936",
        "title": "Creating a Cooperative AI Policymaking Platform through Open Source Collaboration",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CY"
            ]
        ],
        "abstract": "Advances in artificial intelligence (AI) present significant risks and opportunities, requiring improved governance to mitigate societal harms and promote equitable benefits. Current incentive structures and regulatory delays may hinder responsible AI development and deployment, particularly in light of the transformative potential of large language models (LLMs). To address these challenges, we propose developing the following three contributions: (1) a large multimodal text and economic-timeseries foundation model that integrates economic and natural language policy data for enhanced forecasting and decision-making, (2) algorithmic mechanisms for eliciting diverse and representative perspectives, enabling the creation of data-driven public policy recommendations, and (3) an AI-driven web platform for supporting transparent, inclusive, and data-driven policymaking.",
        "subjects": [
            "cs.CY",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06949",
        "abstract url": "https://arxiv.org/abs/2412.06949",
        "title": "Bridging Conversational and Collaborative Signals for Conversational Recommendation",
        "rating": "-1.5",
        "keywords": [
            [
                "Recommendation"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Conversational recommendation systems (CRS) leverage contextual information from conversations to generate recommendations but often struggle due to a lack of collaborative filtering (CF) signals, which capture user-item interaction patterns essential for accurate recommendations. We introduce Reddit-ML32M, a dataset that links reddit conversations with interactions on MovieLens 32M, to enrich item representations by leveraging collaborative knowledge and addressing interaction sparsity in conversational datasets. We propose an LLM-based framework that uses Reddit-ML32M to align LLM-generated recommendations with CF embeddings, refining rankings for better performance. We evaluate our framework against three sets of baselines: CF-based recommenders using only interactions from CRS tasks, traditional CRS models, and LLM-based methods relying on conversational context without item representations. Our approach achieves consistent improvements, including a 12.32% increase in Hit Rate and a 9.9% improvement in NDCG, outperforming the best-performing baseline that relies on conversational context but lacks collaborative item representations.",
        "subjects": [
            "cs.IR",
            "cs.AI",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06993",
        "abstract url": "https://arxiv.org/abs/2412.06993",
        "title": "Toward AI-Driven Digital Organism: Multiscale Foundation Models for Predicting, Simulating and Programming Biology at All Levels",
        "rating": "-1.5",
        "keywords": [
            [
                "Biology",
                "health"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "We present an approach of using AI to model and simulate biology and life. Why is it important? Because at the core of medicine, pharmacy, public health, longevity, agriculture and food security, environmental protection, and clean energy, it is biology at work. Biology in the physical world is too complex to manipulate and always expensive and risky to tamper with. In this perspective, we layout an engineering viable approach to address this challenge by constructing an AI-Driven Digital Organism (AIDO), a system of integrated multiscale foundation models, in a modular, connectable, and holistic fashion to reflect biological scales, connectedness, and complexities. An AIDO opens up a safe, affordable and high-throughput alternative platform for predicting, simulating and programming biology at all levels from molecules to cells to individuals. We envision that an AIDO is poised to trigger a new wave of better-guided wet-lab experimentation and better-informed first-principle reasoning, which can eventually help us better decode and improve life.",
        "subjects": [
            "cs.AI",
            "cs.LG",
            "q-bio.QM"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07010",
        "abstract url": "https://arxiv.org/abs/2412.07010",
        "title": "TAE: A Model-Constrained Tikhonov Autoencoder Approach for Forward and Inverse Problems",
        "rating": "-1.5",
        "keywords": [
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Efficient real-time solvers for forward and inverse problems are essential in engineering and science applications. Machine learning surrogate models have emerged as promising alternatives to traditional methods, offering substantially reduced computational time. Nevertheless, these models typically demand extensive training datasets to achieve robust generalization across diverse scenarios. While physics-based approaches can partially mitigate this data dependency and ensure physics-interpretable solutions, addressing scarce data regimes remains a challenge. Both purely data-driven and physics-based machine learning approaches demonstrate severe overfitting issues when trained with insufficient data. We propose a novel Tikhonov autoencoder model-constrained framework, called TAE, capable of learning both forward and inverse surrogate models using a single arbitrary observation sample. We develop comprehensive theoretical foundations including forward and inverse inference error bounds for the proposed approach for linear cases. For comparative analysis, we derive equivalent formulations for pure data-driven and model-constrained approach counterparts. At the heart of our approach is a data randomization strategy, which functions as a generative mechanism for exploring the training data space, enabling effective training of both forward and inverse surrogate models from a single observation, while regularizing the learning process. We validate our approach through extensive numerical experiments on two challenging inverse problems: 2D heat conductivity inversion and initial condition reconstruction for time-dependent 2D Navier-Stokes equations. Results demonstrate that TAE achieves accuracy comparable to traditional Tikhonov solvers and numerical forward solvers for both inverse and forward problems, respectively, while delivering orders of magnitude computational speedups.",
        "subjects": [
            "cs.LG",
            "physics.comp-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07026",
        "abstract url": "https://arxiv.org/abs/2412.07026",
        "title": "GenAI4UQ: A Software for Inverse Uncertainty Quantification Using Conditional Generative Models",
        "rating": "-1.5",
        "keywords": [
            [
                "forecasting"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "We introduce GenAI4UQ, a software package for inverse uncertainty quantification in model calibration, parameter estimation, and ensemble forecasting in scientific applications. GenAI4UQ leverages a generative artificial intelligence (AI) based conditional modeling framework to address the limitations of traditional inverse modeling techniques, such as Markov Chain Monte Carlo methods. By replacing computationally intensive iterative processes with a direct, learned mapping, GenAI4UQ enables efficient calibration of model input parameters and generation of output predictions directly from observations. The software's design allows for rapid ensemble forecasting with robust uncertainty quantification, while maintaining high computational and storage efficiency. GenAI4UQ simplifies the model training process through built-in auto-tuning of hyperparameters, making it accessible to users with varying levels of expertise. Its conditional generative framework ensures versatility, enabling applicability across a wide range of scientific domains. At its core, GenAI4UQ transforms the paradigm of inverse modeling by providing a fast, reliable, and user-friendly solution. It empowers researchers and practitioners to quickly estimate parameter distributions and generate model predictions for new observations, facilitating efficient decision-making and advancing the state of uncertainty quantification in computational modeling. (The code and data are available at https://github.com/patrickfan/GenAI4UQ).",
        "subjects": [
            "cs.LG",
            "physics.geo-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07039",
        "abstract url": "https://arxiv.org/abs/2412.07039",
        "title": "Data Augmentation with Variational Autoencoder for Imbalanced Dataset",
        "rating": "-1.5",
        "keywords": [
            [
                "tabular"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Learning from an imbalanced distribution presents a major challenge in predictive modeling, as it generally leads to a reduction in the performance of standard algorithms. Various approaches exist to address this issue, but many of them concern classification problems, with a limited focus on regression. In this paper, we introduce a novel method aimed at enhancing learning on tabular data in the Imbalanced Regression (IR) framework, which remains a significant problem. We propose to use variational autoencoders (VAE) which are known as a powerful tool for synthetic data generation, offering an interesting approach to modeling and capturing latent representations of complex distributions. However, VAEs can be inefficient when dealing with IR. Therefore, we develop a novel approach for generating data, combining VAE with a smoothed bootstrap, specifically designed to address the challenges of IR. We numerically investigate the scope of this method by comparing it against its competitors on simulations and datasets known for IR.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07051",
        "abstract url": "https://arxiv.org/abs/2412.07051",
        "title": "A Misclassification Network-Based Method for Comparative Genomic Analysis",
        "rating": "-1.5",
        "keywords": [
            [
                "health"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Classifying genome sequences based on metadata has been an active area of research in comparative genomics for decades with many important applications across the life sciences. Established methods for classifying genomes can be broadly grouped into sequence alignment-based and alignment-free models. Conventional alignment-based models rely on genome similarity measures calculated based on local sequence alignments or consistent ordering among sequences. However, such methods are computationally expensive when dealing with large ensembles of even moderately sized genomes. In contrast, alignment-free (AF) approaches measure genome similarity based on summary statistics in an unsupervised setting and are efficient enough to analyze large datasets. However, both alignment-based and AF methods typically assume fixed scoring rubrics that lack the flexibility to assign varying importance to different parts of the sequences based on prior knowledge. In this study, we integrate AI and network science approaches to develop a comparative genomic analysis framework that addresses these limitations. Our approach, termed the Genome Misclassification Network Analysis (GMNA), simultaneously leverages misclassified instances, a learned scoring rubric, and label information to classify genomes based on associated metadata and better understand potential drivers of misclassification. We evaluate the utility of the GMNA using Naive Bayes and convolutional neural network models, supplemented by additional experiments with transformer-based models, to construct SARS-CoV-2 sampling location classifiers using over 500,000 viral genome sequences and study the resulting network of misclassifications. We demonstrate the global health potential of the GMNA by leveraging the SARS-CoV-2 genome misclassification networks to investigate the role human mobility played in structuring geographic clustering of SARS-CoV-2.",
        "subjects": [
            "q-bio.GN",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06271",
        "abstract url": "https://arxiv.org/abs/2412.06271",
        "title": "EchoSim4D: A Proof-of-Concept Gamified XR Echocardiography Training Simulator for Neonates using 4D Ultrasound Volume",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "6-DOF"
            ],
            [
                "medical"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "Neonatal echocardiography is vital for early detection of heart anomalies in newborns, enabling timely, non-invasive interventions where 4D ultrasound, adds the dimension of time to 3D imaging, enhances diagnostic capabilities by visualizing real-time heart dynamics. However, training for 4D neonatal echocardiography is limited by the lack of simulators that support 4D Ultrasound volume visualization within gamified environments. This paper introduces EchoSim4D, an XR-based simulator leveraging novel pipeline for visualizing 4D volume data in Unity, incorporating real-time volume reconstruction, and a preloaded version optimized for low-end systems. EchoSim4D integrates a sensor-equipped manikin and a custom 3D-printed transducer with a 6-DOF sensor, replicating the precise probe maneuvers necessary for neonatal echocardiography. In a validation study with postgraduate medical students (0-5 years of experience), supervised by a domain expert, EchoSim4D demonstrated high visual fidelity and training efficacy. Findings suggest that 4D visualization techniques hold significant potential for advancing medical training in neonatal echocardiography.",
        "subjects": [
            "cs.HC",
            "eess.IV"
        ],
        "comment": "12 pages, 10 figures, work in progress"
    },
    {
        "paper id": "2412.06273",
        "abstract url": "https://arxiv.org/abs/2412.06273",
        "title": "Omni-Scene: Omni-Gaussian Representation for Ego-Centric Sparse-View Scene Reconstruction",
        "rating": "-2",
        "keywords": [
            [
                "3D",
                "depth"
            ],
            [
                "diffusion"
            ],
            [
                "autonomous driving"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Prior works employing pixel-based Gaussian representation have demonstrated efficacy in feed-forward sparse-view reconstruction. However, such representation necessitates cross-view overlap for accurate depth estimation, and is challenged by object occlusions and frustum truncations. As a result, these methods require scene-centric data acquisition to maintain cross-view overlap and complete scene visibility to circumvent occlusions and truncations, which limits their applicability to scene-centric reconstruction. In contrast, in autonomous driving scenarios, a more practical paradigm is ego-centric reconstruction, which is characterized by minimal cross-view overlap and frequent occlusions and truncations. The limitations of pixel-based representation thus hinder the utility of prior works in this task. In light of this, this paper conducts an in-depth analysis of different representations, and introduces Omni-Gaussian representation with tailored network design to complement their strengths and mitigate their drawbacks. Experiments show that our method significantly surpasses state-of-the-art methods, pixelSplat and MVSplat, in ego-centric reconstruction, and achieves comparable performance to prior works in scene-centric reconstruction. Furthermore, we extend our method with diffusion models, pioneering feed-forward multi-modal generation of 3D driving scenes.",
        "subjects": [
            "cs.CV",
            "cs.GR"
        ],
        "comment": "Under Review"
    },
    {
        "paper id": "2412.06307",
        "abstract url": "https://arxiv.org/abs/2412.06307",
        "title": "Industrial Code Quality Benchmarks: Toward Gamification of Software Maintainability",
        "rating": "-2",
        "keywords": [
            [
                "Industrial"
            ]
        ],
        "abstract": "Software maintainability is essential for long-term success in the software industry. Despite widespread evidence of the high costs associated with poor maintainability, market pressure drives many organizations to prioritize short-term releases. This focus leads to accumulating technical debt worldwide. In this preliminary work, we propose maintainability gamification through anonymous leaderboards to encourage organizations to maintain a sustained focus on code quality. Our approach envisions benchmarking to foster motivation and urgency across companies by highlighting thresholds for leaders and laggards. To initiate this concept, we analyze a sample of over 1,000 proprietary projects using CodeHealth scores. By examining the distribution of these scores across various dimensions, we assess the feasibility of creating effective leaderboards. Findings from this study offer valuable insights for future design activities in maintainability gamification.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Accepted for the 4th Workshop on Gamification in Software Development, Verification, and Validation (Gamify2025)"
    },
    {
        "paper id": "2412.06330",
        "abstract url": "https://arxiv.org/abs/2412.06330",
        "title": "A reconfigurable calibration-free digital-to-time converter based on a high-speed transceiver",
        "rating": "-2",
        "keywords": [
            [
                "FPGA"
            ]
        ],
        "abstract": "This paper proposes a high-speed transceiver-based method for implementing a digital-to-time converter (DTC). A real-time decoding technique is introduced to inject time information into high-speed pattern data. The stability of the high-speed clock ensures the high precision of the synthesized timing signal without the need for calibration. The reconfigurability of the clock resources provides the DTC with variable resolution and enhanced flexibility for various applications. Based on this approach, a multifunctional DTC is designed to offer both timing sequence and random timing signal functionalities, catering to a wide range of application scenarios. The timing sequence function generates a continuously variable timing signal stream, while the random timing signal function produces random signals with uniformly distributed time intervals. Experimental results, using a Xilinx Kintex-7 FPGA, validate the effectiveness of the proposed methodology. The system achieves a resolution of 100 ps, a dynamic range from 1 ns to 40 \u03bcs, a DNL of -0.02/0.02 LSB, an INL of -0.04/0.03 LSB across the entire range. This approach can be readily adapted to various high-precision timing signal applications.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06335",
        "abstract url": "https://arxiv.org/abs/2412.06335",
        "title": "StructRide: A Framework to Exploit the Structure Information of Shareability Graph in Ridesharing",
        "rating": "-2",
        "keywords": [
            [
                "vehicle"
            ],
            [
                "Graph"
            ]
        ],
        "abstract": "Ridesharing services play an essential role in modern transportation, which significantly reduces traffic congestion and exhaust pollution. In the ridesharing problem, improving the sharing rate between riders can not only save the travel cost of drivers but also utilize vehicle resources more efficiently. The existing online-based and batch-based methods for the ridesharing problem lack the analysis of the sharing relationship among riders, leading to a compromise between efficiency and accuracy. In addition, the graph is a powerful tool to analyze the structure information between nodes. Therefore, in this paper, we propose a framework, namely StructRide, to utilize the structure information to improve the results for ridesharing problems. Specifically, we extract the sharing relationships between riders to construct a shareability graph. Then, we define a novel measurement shareability loss for vehicles to select groups of requests such that the unselected requests still have high probabilities of sharing. Our SARD algorithm can efficiently solve dynamic ridesharing problems to achieve dramatically improved results. Through extensive experiments, we demonstrate the efficiency and effectiveness of our SARD algorithm on two real datasets. Our SARD can run up to 72.68 times faster and serve up to 50% more requests than the state-of-the-art algorithms.",
        "subjects": [
            "cs.DB"
        ],
        "comment": "ICDE 2025"
    },
    {
        "paper id": "2412.06395",
        "abstract url": "https://arxiv.org/abs/2412.06395",
        "title": "Query Complexity with Unknowns",
        "rating": "-2",
        "keywords": [
            [
                "quantum"
            ]
        ],
        "abstract": "We initiate the study of a new model of query complexity of Boolean functions where, in addition to 0 and 1, the oracle can answer queries with ``unknown''. The query algorithm is expected to output the function value if it can be conclusively determined by the partial information gathered, and it must output ``unknown'' if not. We formalize this model by using Kleene's strong logic of indeterminacy on three variables to capture unknowns. We call this model the `u-query model'. We relate the query complexity of functions in the new u-query model with their analogs in the standard query model. We show an explicit function that is exponentially harder in the u-query model than in the usual query model. We give sufficient conditions for a function to have u-query complexity asymptotically the same as its query complexity. Using u-query analogs of the combinatorial measures of sensitivity, block sensitivity, and certificate complexity, we show that deterministic, randomized, and quantum u-query complexities of all total Boolean functions are polynomially related to each other, just as in the usual query models.",
        "subjects": [
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06413",
        "abstract url": "https://arxiv.org/abs/2412.06413",
        "title": "World-Consistent Data Generation for Vision-and-Language Navigation",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "trajectory"
            ],
            [
                "Navigation"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Vision-and-Language Navigation (VLN) is a challenging task that requires an agent to navigate through photorealistic environments following natural-language instructions. One main obstacle existing in VLN is data scarcity, leading to poor generalization performance over unseen environments. Tough data argumentation is a promising way for scaling up the dataset, how to generate VLN data both diverse and world-consistent remains problematic. To cope with this issue, we propose the world-consistent data generation (WCGEN), an efficacious data-augmentation framework satisfying both diversity and world-consistency, targeting at enhancing the generalizations of agents to novel environments. Roughly, our framework consists of two stages, the trajectory stage which leverages a point-cloud based technique to ensure spatial coherency among viewpoints, and the viewpoint stage which adopts a novel angle synthesis method to guarantee spatial and wraparound consistency within the entire observation. By accurately predicting viewpoint changes with 3D knowledge, our approach maintains the world-consistency during the generation procedure. Experiments on a wide range of datasets verify the effectiveness of our method, demonstrating that our data augmentation strategy enables agents to achieve new state-of-the-art results on all navigation tasks, and is capable of enhancing the VLN agents' generalization ability to unseen environments.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06445",
        "abstract url": "https://arxiv.org/abs/2412.06445",
        "title": "Echocardiography to Cardiac MRI View Transformation for Real-Time Blind Restoration",
        "rating": "-2",
        "keywords": [
            [
                "GAN"
            ],
            [
                "Medical",
                "diagnosis",
                "MRI",
                "clinical",
                "Cardiac"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Echocardiography is the most widely used imaging to monitor cardiac functions, serving as the first line in early detection of myocardial ischemia and infarction. However, echocardiography often suffers from several artifacts including sensor noise, lack of contrast, severe saturation, and missing myocardial segments which severely limit its usage in clinical diagnosis. In recent years, several machine learning methods have been proposed to improve echocardiography views. Yet, these methods usually address only a specific problem (e.g. denoising) and thus cannot provide a robust and reliable restoration in general. On the other hand, cardiac MRI provides a clean view of the heart without suffering such severe issues. However, due to its significantly higher cost, it is often only afforded by a few major hospitals, hence hindering its use and accessibility. In this pilot study, we propose a novel approach to transform echocardiography into the cardiac MRI view. For this purpose, Echo2MRI dataset, consisting of echocardiography and real cardiac MRI image pairs, is composed and will be shared publicly. A dedicated Cycle-consistent Generative Adversarial Network (Cycle-GAN) is trained to learn the transformation from echocardiography frames to cardiac MRI views. An extensive set of qualitative evaluations shows that the proposed transformer can synthesize high-quality artifact-free synthetic cardiac MRI views from a given sequence of echocardiography frames. Medical evaluations performed by a group of cardiologists further demonstrate that synthetic MRI views are indistinguishable from their original counterparts and are preferred over their initial sequence of echocardiography frames for diagnosis in 78.9% of the cases.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "18 pages, 42 figures"
    },
    {
        "paper id": "2412.06454",
        "abstract url": "https://arxiv.org/abs/2412.06454",
        "title": "Adaptive Graph Learning from Spatial Information for Surgical Workflow Anticipation",
        "rating": "-2",
        "keywords": [
            [
                "Graph"
            ],
            [
                "Surgical",
                "Surgery"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Surgical workflow anticipation is the task of predicting the timing of relevant surgical events from live video data, which is critical in Robotic-Assisted Surgery (RAS). Accurate predictions require the use of spatial information to model surgical interactions. However, current methods focus solely on surgical instruments, assume static interactions between instruments, and only anticipate surgical events within a fixed time horizon. To address these challenges, we propose an adaptive graph learning framework for surgical workflow anticipation based on a novel spatial representation, featuring three key innovations. First, we introduce a new representation of spatial information based on bounding boxes of surgical instruments and targets, including their detection confidence levels. These are trained on additional annotations we provide for two benchmark datasets. Second, we design an adaptive graph learning method to capture dynamic interactions. Third, we develop a multi-horizon objective that balances learning objectives for different time horizons, allowing for unconstrained predictions. Evaluations on two benchmarks reveal superior performance in short-to-mid-term anticipation, with an error reduction of approximately 3% for surgical phase anticipation and 9% for remaining surgical duration anticipation. These performance improvements demonstrate the effectiveness of our method and highlight its potential for enhancing preparation and coordination within the RAS team. This can improve surgical safety and the efficiency of operating room usage.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": "Accepted by IEEE Transactions on Medical Robotics and Bionics, the direct link to the IEEE page will be updated upon publication"
    },
    {
        "paper id": "2412.06456",
        "abstract url": "https://arxiv.org/abs/2412.06456",
        "title": "UAV Virtual Antenna Array Deployment for Uplink Interference Mitigation in Data Collection Networks",
        "rating": "-2",
        "keywords": [
            [
                "UAV"
            ]
        ],
        "abstract": "Unmanned aerial vehicles (UAVs) have gained considerable attention as a platform for establishing aerial wireless networks and communications. However, the line-of-sight dominance in air-to-ground communications often leads to significant interference with terrestrial networks, reducing communication efficiency among terrestrial terminals. This paper explores a novel uplink interference mitigation approach based on the collaborative beamforming (CB) method in multi-UAV network systems. Specifically, the UAV swarm forms a UAV-enabled virtual antenna array (VAA) to achieve the transmissions of gathered data to multiple base stations (BSs) for data backup and distributed processing. However, there is a trade-off between the effectiveness of CB-based interference mitigation and the energy conservation of UAVs. Thus, by jointly optimizing the excitation current weights and hover position of UAVs as well as the sequence of data transmission to various BSs, we formulate an uplink interference mitigation multi-objective optimization problem (MOOP) to decrease interference affection, enhance transmission efficiency, and improve energy efficiency, simultaneously. In response to the computational demands of the formulated problem, we introduce an evolutionary computation method, namely chaotic non-dominated sorting genetic algorithm II (CNSGA-II) with multiple improved operators. The proposed CNSGA-II efficiently addresses the formulated MOOP, outperforming several other comparative algorithms, as evidenced by the outcomes of the simulations. Moreover, the proposed CB-based uplink interference mitigation approach can significantly reduce the interference caused by UAVs to non-receiving BSs.",
        "subjects": [
            "cs.NE",
            "cs.NI"
        ],
        "comment": "This paper has been accepted by IEEE Internet of Things Journal"
    },
    {
        "paper id": "2412.06469",
        "abstract url": "https://arxiv.org/abs/2412.06469",
        "title": "Jess+: designing embodied AI for interactive music-making",
        "rating": "-2",
        "keywords": [
            [
                "music"
            ]
        ],
        "abstract": "In this paper, we discuss the conceptualisation and design of embodied AI within an inclusive music-making project. The central case study is Jess+ an intelligent digital score system for shared creativity with a mixed ensemble of non-disabled and disabled musicians. The overarching aim is that the digital score enables disabled musicians to thrive in a live music conversation with other musicians regardless of the potential barriers of disability and music-making. After defining what we mean by embodied AI and how this approach supports the aims of the Jess+ project, we outline the main design features of the system. This includes several novel approaches such as its modular design, an AI Factory based on an embodied musicking dataset, and an embedded belief system. Our findings showed that the implemented design decisions and embodied-AI approach led to rich experiences for the musicians which in turn transformed their practice as an inclusive ensemble.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06497",
        "abstract url": "https://arxiv.org/abs/2412.06497",
        "title": "New Channel Coding Lower Bounds for Noisy Permutation Channels",
        "rating": "-2",
        "keywords": [
            [
                "biological"
            ]
        ],
        "abstract": "Motivated by the application of point-to-point communication networks and biological storage, we investigate new channel coding bounds for noisy permutation channels with strictly positive and full-rank square matrices. Our new achievability bounds use $\u03b5$-packing with Kullback-Leibler divergence as a metric to bound the distance between distributions and are tighter than existing bounds. Additionally, Gaussian approximations of achievability bounds are derived, and the numerical evaluation shows the precision of the approximation.",
        "subjects": [
            "cs.IT"
        ],
        "comment": "11 pages"
    },
    {
        "paper id": "2412.06541",
        "abstract url": "https://arxiv.org/abs/2412.06541",
        "title": "Numerical Estimation of Spatial Distributions under Differential Privacy",
        "rating": "-2",
        "keywords": [
            [
                "forecasting"
            ]
        ],
        "abstract": "Estimating spatial distributions is important in data analysis, such as traffic flow forecasting and epidemic prevention. To achieve accurate spatial distribution estimation, the analysis needs to collect sufficient user data. However, collecting data directly from individuals could compromise their privacy. Most previous works focused on private distribution estimation for one-dimensional data, which does not consider spatial data relation and leads to poor accuracy for spatial distribution estimation. In this paper, we address the problem of private spatial distribution estimation, where we collect spatial data from individuals and aim to minimize the distance between the actual distribution and estimated one under Local Differential Privacy (LDP). To leverage the numerical nature of the domain, we project spatial data and its relationships onto a one-dimensional distribution. We then use this projection to estimate the overall spatial distribution. Specifically, we propose a reporting mechanism called Disk Area Mechanism (DAM), which projects the spatial domain onto a line and optimizes the estimation using the sliced Wasserstein distance. Through extensive experiments, we show the effectiveness of our DAM approach on both real and synthetic data sets, compared with the state-of-the-art methods, such as Multi-dimensional Square Wave Mechanism (MDSW) and Subset Exponential Mechanism with Geo-I (SEM-Geo-I). Our results show that our DAM always performs better than MDSW and is better than SEM-Geo-I when the data granularity is fine enough.",
        "subjects": [
            "cs.DB"
        ],
        "comment": "ICDE 2025"
    },
    {
        "paper id": "2412.06581",
        "abstract url": "https://arxiv.org/abs/2412.06581",
        "title": "EmoSpeech: A Corpus of Emotionally Rich and Contextually Detailed Speech Annotations",
        "rating": "-2",
        "keywords": [
            [
                "depth"
            ],
            [
                "text-to-speech"
            ],
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Advances in text-to-speech (TTS) technology have significantly improved the quality of generated speech, closely matching the timbre and intonation of the target speaker. However, due to the inherent complexity of human emotional expression, the development of TTS systems capable of controlling subtle emotional differences remains a formidable challenge. Existing emotional speech databases often suffer from overly simplistic labelling schemes that fail to capture a wide range of emotional states, thus limiting the effectiveness of emotion synthesis in TTS applications. To this end, recent efforts have focussed on building databases that use natural language annotations to describe speech emotions. However, these approaches are costly and require more emotional depth to train robust systems. In this paper, we propose a novel process aimed at building databases by systematically extracting emotion-rich speech segments and annotating them with detailed natural language descriptions through a generative model. This approach enhances the emotional granularity of the database and significantly reduces the reliance on costly manual annotations by automatically augmenting the data with high-level language models. The resulting rich database provides a scalable and economically viable solution for developing a more nuanced and dynamic basis for developing emotionally controlled TTS systems.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "eess.AS"
        ],
        "comment": "4 pages, 1 figure. To appear in the Proceedings of the International Symposium on Chinese Spoken Language Processing, 7-10 November 2024, Beijing, China"
    },
    {
        "paper id": "2412.06643",
        "abstract url": "https://arxiv.org/abs/2412.06643",
        "title": "Detecting Facial Image Manipulations with Multi-Layer CNN Models",
        "rating": "-2",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "Facial"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "The rapid evolution of digital image manipulation techniques poses significant challenges for content verification, with models such as stable diffusion and mid-journey producing highly realistic, yet synthetic, images that can deceive human perception. This research develops and evaluates convolutional neural networks (CNNs) specifically tailored for the detection of these manipulated images. The study implements a comparative analysis of three progressively complex CNN architectures, assessing their ability to classify and localize manipulations across various facial image modifications. Regularization and optimization techniques were systematically incorporated to improve feature extraction and performance. The results indicate that the proposed models achieve an accuracy of up to 76\\% in distinguishing manipulated images from genuine ones, surpassing traditional approaches. This research not only highlights the potential of CNNs in enhancing the robustness of digital media verification tools, but also provides insights into effective architectural adaptations and training strategies for low-computation environments. Future work will build on these findings by extending the architectures to handle more diverse manipulation techniques and integrating multi-modal data for improved detection capabilities.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06666",
        "abstract url": "https://arxiv.org/abs/2412.06666",
        "title": "Diff5T: Benchmarking Human Brain Diffusion MRI with an Extensive 5.0 Tesla K-Space and Spatial Dataset",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "medical",
                "MRI"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Diffusion magnetic resonance imaging (dMRI) provides critical insights into the microstructural and connectional organization of the human brain. However, the availability of high-field, open-access datasets that include raw k-space data for advanced research remains limited. To address this gap, we introduce Diff5T, a first comprehensive 5.0 Tesla diffusion MRI dataset focusing on the human brain. This dataset includes raw k-space data and reconstructed diffusion images, acquired using a variety of imaging protocols. Diff5T is designed to support the development and benchmarking of innovative methods in artifact correction, image reconstruction, image preprocessing, diffusion modelling and tractography. The dataset features a wide range of diffusion parameters, including multiple b-values and gradient directions, allowing extensive research applications in studying human brain microstructure and connectivity. With its emphasis on open accessibility and detailed benchmarks, Diff5T serves as a valuable resource for advancing human brain mapping research using diffusion MRI, fostering reproducibility, and enabling collaboration across the neuroscience and medical imaging communities.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "physics.med-ph"
        ],
        "comment": "19 pages, 4 figures, 1 table"
    },
    {
        "paper id": "2412.06690",
        "abstract url": "https://arxiv.org/abs/2412.06690",
        "title": "FedSynthCT-Brain: A Federated Learning Framework for Multi-Institutional Brain MRI-to-CT Synthesis",
        "rating": "-2",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "MRI",
                "CT",
                "clinical"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "The generation of Synthetic Computed Tomography (sCT) images has become a pivotal methodology in modern clinical practice, particularly in the context of Radiotherapy (RT) treatment planning. The use of sCT enables the calculation of doses, pushing towards Magnetic Resonance Imaging (MRI) guided radiotherapy treatments. Moreover, with the introduction of MRI-Positron Emission Tomography (PET) hybrid scanners, the derivation of sCT from MRI can improve the attenuation correction of PET images. Deep learning methods for MRI-to-sCT have shown promising results, but their reliance on single-centre training dataset limits generalisation capabilities to diverse clinical settings. Moreover, creating centralised multicentre datasets may pose privacy concerns. To solve the issues, this study introduces FedSynthCT-Brain, a framework based on the Federated Learning (FL) paradigm for MRI-to-sCT in brain imaging. We reproduced a federation across four European and American centres using a U-Net-based model. The approach was implemented using data from centres belonging the federation and it was tested on an unseen dataset from a centre outside the federation. In the case of the unseen centre, the federated model achieved a median Mean Absolute Error (MAE) of 102.0 HU across 23 patients, with an interquartile range of 96.7-110.5 HU. The median (interquartile range) for the Structural Similarity Index (SSIM) and the Peak Signal to Noise Ratio (PNSR) were 0.89 (0.86-0.89) and 26.58 (25.52-27.42), respectively. The analysis of the results showed acceptable performances of the federated approach, thus highlighting the potential of FL to enhance MRI-to-sCT to improve generalisability and advancing safe and equitable clinical applications while fostering collaboration and preserving data privacy.",
        "subjects": [
            "eess.IV",
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06695",
        "abstract url": "https://arxiv.org/abs/2412.06695",
        "title": "DEEPER: Dense Electroencephalography Passage Retrieval",
        "rating": "-2",
        "keywords": [
            [
                "EEG"
            ]
        ],
        "abstract": "Information retrieval systems have historically relied on explicit query formulation, requiring users to translate their information needs into text. This process is particularly disruptive during reading tasks, where users must interrupt their natural flow to formulate queries. We present DEEPER (Dense Electroencephalography Passage Retrieval), a novel framework that enables direct retrieval of relevant passages from users' neural signals during naturalistic reading without intermediate text translation. Building on dense retrieval architectures, DEEPER employs a dual-encoder approach with specialised components for processing neural data, mapping EEG signals and text passages into a shared semantic space. Through careful architecture design and cross-modal negative sampling strategies, our model learns to align neural patterns with their corresponding textual content. Experimental results on the ZuCo dataset demonstrate that direct brain-to-passage retrieval significantly outperforms current EEG-to-text baselines, achieving a 571% improvement in Precision@1. Our ablation studies reveal that the model successfully learns aligned representations between EEG and text modalities (0.29 cosine similarity), while our hard negative sampling strategy contributes to overall performance increases.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06703",
        "abstract url": "https://arxiv.org/abs/2412.06703",
        "title": "Source Separation & Automatic Transcription for Music",
        "rating": "-2",
        "keywords": [
            [
                "speech enhancement"
            ],
            [
                "Music"
            ],
            [
                "cs.AI",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Source separation is the process of isolating individual sounds in an auditory mixture of multiple sounds [1], and has a variety of applications ranging from speech enhancement and lyric transcription [2] to digital audio production for music. Furthermore, Automatic Music Transcription (AMT) is the process of converting raw music audio into sheet music that musicians can read [3]. Historically, these tasks have faced challenges such as significant audio noise, long training times, and lack of free-use data due to copyright restrictions. However, recent developments in deep learning have brought new promising approaches to building low-distortion stems and generating sheet music from audio signals [4]. Using spectrogram masking, deep neural networks, and the MuseScore API, we attempt to create an end-to-end pipeline that allows for an initial music audio mixture (e.g...wav file) to be separated into instrument stems, converted into MIDI files, and transcribed into sheet music for each component instrument.",
        "subjects": [
            "cs.SD",
            "cs.AI",
            "eess.AS"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06728",
        "abstract url": "https://arxiv.org/abs/2412.06728",
        "title": "Byzantine-Eavesdropper Alliance: How to Achieve Symmetric Privacy in Quantum $X$-Secure $B$-Byzantine $E$-Eavesdropped $U$-Unresponsive $T$-Colluding PIR?",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "We consider the quantum \\emph{symmetric} private information retrieval (QSPIR) problem in a system with $N$ databases and $K$ messages, with $U$ unresponsive servers, $T$-colluding servers, and $X$-security parameter, under several fundamental threat models. In the first model, there are $\\mathcal{E}_1$ eavesdropped links in the uplink direction (the direction from the user to the $N$ servers), $\\mathcal{E}_2$ eavesdropped links in the downlink direction (the direction from the servers to the user), where $|\\mathcal{E}_1|, |\\mathcal{E}_2| \\leq E$; we coin this eavesdropper setting as \\emph{dynamic} eavesdroppers. We show that super-dense coding gain can be achieved for some regimes. In the second model, we consider the case with Byzantine servers, i.e., servers that can coordinate to devise a plan to harm the privacy and security of the system together with static eavesdroppers, by listening to the same links in both uplink and downlink directions. It is important to note the considerable difference between the two threat models, since the eavesdroppers can take huge advantage of the presence of the Byzantine servers. Unlike the previous works in SPIR with Byzantine servers, that assume that the Byzantine servers can send only random symbols independent of the stored messages, we follow the definition of Byzantine servers in \\cite{byzantine_tpir}, where the Byzantine servers can send symbols that can be functions of the storage, queries, as well as the random symbols in a way that can produce worse harm to the system. In the third and the most novel threat model, we consider the presence of Byzantine servers and dynamic eavesdroppers together. We show that having dynamic eavesdroppers along with Byzantine servers in the same system model creates more threats to the system than having static eavesdroppers with Byzantine servers.",
        "subjects": [
            "cs.IT",
            "cs.CR",
            "cs.NI",
            "eess.SP",
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06732",
        "abstract url": "https://arxiv.org/abs/2412.06732",
        "title": "Analyzing the Scalability of Bi-static Backscatter Networks for Large Scale Applications",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "Backscatter radio is a promising technology for low-cost and low-power Internet-of-Things (IoT) networks. The conventional monostatic backscatter radio is constrained by its limited communication range, which restricts its utility in wide-area applications. An alternative bi-static backscatter radio architecture, characterized by a dis-aggregated illuminator and receiver, can provide enhanced coverage and, thus, can support wide-area applications. In this paper, we analyze the scalability of the bi-static backscatter radio for large-scale wide-area IoT networks consisting of a large number of unsynchronized, receiver-less tags. We introduce the Tag Drop Rate (TDR) as a measure of reliability and develop a theoretical framework to estimate TDR in terms of the network parameters. We show that under certain approximations, a small-scale prototype can emulate a large-scale network. We then use the measurements from experimental prototypes of bi-static backscatter networks (BNs) to refine the theoretical model. Finally, based on the insights derived from the theoretical model and the experimental measurements, we describe a systematic methodology for tuning the network parameters and identifying the physical layer design requirements for the reliable operation of large-scale bi-static BNs. Our analysis shows that even with a modest physical layer requirement of bit error rate (BER) 0.2, 1000 receiver-less tags can be supported with 99.9% reliability. This demonstrates the feasibility of bi-static BNs for large-scale wide-area IoT applications.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "To be published in IEEE Journal of Radio Frequency Identification"
    },
    {
        "paper id": "2412.06760",
        "abstract url": "https://arxiv.org/abs/2412.06760",
        "title": "Ranking-aware adapter for text-driven image ordering with CLIP",
        "rating": "-2",
        "keywords": [
            [
                "vision-language",
                "VLMs"
            ],
            [
                "facial"
            ],
            [
                "quality assessment"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Recent advances in vision-language models (VLMs) have made significant progress in downstream tasks that require quantitative concepts such as facial age estimation and image quality assessment, enabling VLMs to explore applications like image ranking and retrieval. However, existing studies typically focus on the reasoning based on a single image and heavily depend on text prompting, limiting their ability to learn comprehensive understanding from multiple images. To address this, we propose an effective yet efficient approach that reframes the CLIP model into a learning-to-rank task and introduces a lightweight adapter to augment CLIP for text-guided image ranking. Specifically, our approach incorporates learnable prompts to adapt to new instructions for ranking purposes and an auxiliary branch with ranking-aware attention, leveraging text-conditioned visual differences for additional supervision in image ranking. Our ranking-aware adapter consistently outperforms fine-tuned CLIPs on various tasks and achieves competitive results compared to state-of-the-art models designed for specific tasks like facial age estimation and image quality assessment. Overall, our approach primarily focuses on ranking images with a single instruction, which provides a natural and generalized way of learning from visual differences across images, bypassing the need for extensive text prompts tailored to individual tasks. Code is available: https://github.com/uynaes/RankingAwareCLIP.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "github link: https://github.com/uynaes/RankingAwareCLIP"
    },
    {
        "paper id": "2412.06927",
        "abstract url": "https://arxiv.org/abs/2412.06927",
        "title": "Gradient-based facial encoding for key generation to encrypt and decrypt multimedia data",
        "rating": "-2",
        "keywords": [
            [
                "attacks"
            ],
            [
                "biometric-based",
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Password-based security is prone to forgetting, guessing, and hacking. Similarly, standalone biometric-based security is susceptible to template spoofing and replay attacks. This paper proposes a biocryptosystem based on face recognition technique to bridge this gap such that it can encrypt and decrypt any kind of file using the Advanced Encryption Standard (AES). The biocryptosystem uses a combination of biometric identification and cryptographic methods to protect sensitive information in a secure and effective manner. To verify a user's identity, our proposed system first captures an image of their face and extracts facial traits. The Histogram of Oriented Gradients (HOG) detects all the unique facial traits because HOG effectively captures edge-based features even in dim lighting. Every data type, including text, audio, and video files, can be encrypted and decrypted using this system. Biometric evidence is inherently tied to an individual, so it is almost impossible for attackers to access the user's data. This method also offers a high level of security by employing biometric data as an element in the 2-factor authentication process. The precision, efficiency, and security of this biocryptosystem are experimentally proven by different metrics like entropy and avalanche effect. Applications for the proposed system include safe file sharing, online transactions, and data archiving. Hence, it offers a strong and dependable option for safeguarding sensitive data.",
        "subjects": [
            "cs.CR",
            "cs.CV"
        ],
        "comment": "12 pages, 2 figures, submitted to \"Journal of Cryptology\""
    },
    {
        "paper id": "2412.06945",
        "abstract url": "https://arxiv.org/abs/2412.06945",
        "title": "\"It's Always a Losing Game\": How Workers Understand and Resist Surveillance Technologies on the Job",
        "rating": "-2",
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "With the rise of remote work, a range of surveillance technologies are increasingly being used by business owners to track and monitor employees, raising concerns about worker rights and privacy. Through analysis of Reddit posts and in-depth semi-structured interviews, this paper seeks to understand how workers across a range of sectors make sense of and respond to layered forms of surveillance. While workers express concern about risks to their health, safety, and privacy, they also face a lack of transparency and autonomy around the use of these systems. In response, workers take up tactics of everyday resistance, such as commiserating with other workers or employing technological hacks. Although these tactics demonstrate workers' ingenuity, they also show the limitations of existing approaches to protect workers against intrusive workplace monitoring. We argue that there is an opportunity for CSCW researchers to support these countermeasures through worker-led design and policy.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06954",
        "abstract url": "https://arxiv.org/abs/2412.06954",
        "title": "CURE: Clinical Understanding & Retrieval Evaluation",
        "rating": "-2",
        "keywords": [
            [
                "medical",
                "healthcare",
                "Clinical"
            ]
        ],
        "abstract": "Given the dominance of dense retrievers that do not generalize well beyond their training dataset distributions, domain-specific test sets are essential in evaluating retrieval. There are few test datasets for retrieval systems intended for use by healthcare providers in a point-of-care setting. To fill this gap we have collaborated with medical professionals to create CURE, an ad-hoc retrieval test dataset for passage ranking with 2000 queries spanning 10 medical domains with a monolingual (English) and two cross-lingual (French/Spanish -> English) conditions. In this paper, we describe how CURE was constructed and provide baseline results to showcase its effectiveness as an evaluation tool. CURE is published with a Creative Commons Attribution Non Commercial 4.0 license and can be accessed on Hugging Face.",
        "subjects": [
            "cs.IR"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06956",
        "abstract url": "https://arxiv.org/abs/2412.06956",
        "title": "Microcontroller-Driven MPPT System for Enhanced Photovoltaic Efficiency: An Experimental Approach in Nepal",
        "rating": "-2",
        "keywords": [
            [
                "IoT"
            ]
        ],
        "abstract": "Solar energy utilization in places like Nepal, is often obstructed by unpredicted environmental factors and existing technological barriers. The challenges encountered often result in fluctuating energy outputs, hindering the transition to greener energy solutions. To tackle these issues, this study introduces a custom-designed Maximum Power Point Tracking (MPPT) controller, seamlessly incorporated into a microcontroller-based battery charging system. This approach seeks to enhance the efficiency of photovoltaic (PV) systems, aligning with the global shift towards renewables. The research's primary objective is to enhance PV module power yield employing MPPT techniques, thereby reducing dependency on non-renewable energy sources. Key goals include real-time MPP tracking for optimal power extraction from PV modules and the integration of a real-time monitoring mechanism for PV and battery states. Leveraging a coordinated interplay of sensors measuring temperature, voltage, and current, vital metrics are fed to the microcontroller. This, in turn, generates a precise Pulse Width Modulation (PWM) signal, fine-tuning the voltage regulation of the buck-boost converter Metal Oxide Semiconductor Field Effect Transistor (MOSFET) for optimal operation. The adopted approach emphasizes monitoring environmental metrics, overseeing power outputs, and generating PWM signals to adeptly manage the buck-boost converter MOSFET voltage. Concurrently, data is transmitted hourly to a cloud platform, facilitating real-time monitoring capabilities showcasing the IoT application. As a result of these integrations, an efficiency improvement of approximately 37.28% was observed. In essence, this research underscores the profound impact of merging advanced technologies within the renewable energy sector, offering a robust blueprint for enhancing energy stability and productivity.",
        "subjects": [
            "eess.SY"
        ],
        "comment": "Experimental analysis"
    },
    {
        "paper id": "2412.06960",
        "abstract url": "https://arxiv.org/abs/2412.06960",
        "title": "Simplications: Why and how we should rethink data of/by/for the people in smart homes and its privacy implications",
        "rating": "-2",
        "keywords": [
            [
                "health"
            ]
        ],
        "abstract": "More and more smart devices enter our homes. Often these devices come with a variety of sensors, mostly simple sensors, e.g., for light, temperature, humidity or motion. And they all collect data. While it is data of the home environment it is also data of domestic life in the home. Thus it is data of the people and by the people in the home capturing their presence, arrival and departure, typical domestic activities, bad habits, health status etc. Based on previous as well as ongoing research we know that people are actually able to make sense of simple sensor data and that they will make use of it for their own purposes. Simple sensors, when critically reflected, are often only \"simple\" in a technical sense. The unreflected design and use of these sensors can easily lead to unintended implications, i.e. for privacy. However, it may not even need a Big Brother or data experts or AI to make the data of these sensors sensitive, e.g., if used for lateral surveillance within families. Often unintended but wicked implications emerge despite good intentions, such as improving efficiency or energy saving through collecting sensor data. Thus sensor data from the home is actually data of/by/for the people in the home. First, we explain how this might have relevance across scales of community of people - not only for the domain of the home but also in broader meaning. Second, we relate our previous as well as ongoing research in the domain of smart homes to this topic.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "In Workshop \"Data of/by/for the People: Designing Participatory Approaches to Data Governance\" at DIS24 - Designing Interactive Systems. July 1, 2024, Copenhagen, Denmark. 4 pages"
    },
    {
        "paper id": "2412.06977",
        "abstract url": "https://arxiv.org/abs/2412.06977",
        "title": "Implementing a Quantum Finite Automaton in IBMQ using Custom Control Pulses",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum finite automata can be used for pattern recognition. Present implementations on actual quantum devices face decoherence issues, which compromise the quality of long strings computation. In this work, we focus on the Measure Once 1-way Quantum Finite Automata (MO1QFA) model for addressing the MOD^p problem, investigating how quantum errors may affect the quality of the computation in this model when implemented in IBM-Q superconducting environment. To improve the performance of the implementation, we use pulse-level programming for calibrating a fast single-qubit gate designed specifically for the automaton implementation. The demonstrations conducted on the Jakarta quantum computer show that using custom pulses significantly reduces errors during extended word computations. While realizing improvements in error variations and predictability -- with a fourfold reduction in circuit latency -- the proposed solution demonstrates a substantial increase in the supported computation length of the automaton. When considering thresholds of 10% and 20% in absolute errors of acceptance probabilities, the solution has the potential to increase the maximum word length by 12 and 7+ times, respectively, compared to the default Qiskit gate.",
        "subjects": [
            "quant-ph",
            "cs.FL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06980",
        "abstract url": "https://arxiv.org/abs/2412.06980",
        "title": "Diff-GO$^\\text{n}$: Enhancing Diffusion Models for Goal-Oriented Communications",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "IoT"
            ],
            [
                "eess.IV"
            ]
        ],
        "abstract": "The rapid expansion of edge devices and Internet-of-Things (IoT) continues to heighten the demand for data transport under limited spectrum resources. The goal-oriented communications (GO-COM), unlike traditional communication systems designed for bit-level accuracy, prioritizes more critical information for specific application goals at the receiver. To improve the efficiency of generative learning models for GO-COM, this work introduces a novel noise-restricted diffusion-based GO-COM (Diff-GO$^\\text{n}$) framework for reducing bandwidth overhead while preserving the media quality at the receiver. Specifically, we propose an innovative Noise-Restricted Forward Diffusion (NR-FD) framework to accelerate model training and reduce the computation burden for diffusion-based GO-COMs by leveraging a pre-sampled pseudo-random noise bank (NB). Moreover, we design an early stopping criterion for improving computational efficiency and convergence speed, allowing high-quality generation in fewer training steps. Our experimental results demonstrate superior perceptual quality of data transmission at a reduced bandwidth usage and lower computation, making Diff-GO$^\\text{n}$ well-suited for real-time communications and downstream applications.",
        "subjects": [
            "eess.IV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06985",
        "abstract url": "https://arxiv.org/abs/2412.06985",
        "title": "Ground Perturbation Detection via Lower-Limb Kinematic States During Locomotion",
        "rating": "-2",
        "keywords": [
            [
                "biomechanics",
                "physiological"
            ]
        ],
        "abstract": "Falls during daily ambulation activities are a leading cause of injury in older adults due to delayed physiological responses to disturbances of balance. Lower-limb exoskeletons have the potential to mitigate fall incidents by detecting and reacting to perturbations before the user. Although commonly used, the standard metric for perturbation detection, whole-body angular momentum, is poorly suited for exoskeleton applications due to computational delays and additional tunings. To address this, we developed a novel ground perturbation detector using lower-limb kinematic states during locomotion. To identify perturbations, we tracked deviations in the kinematic states from their nominal steady-state trajectories. Using a data-driven approach, we further optimized our detector with an open-source ground perturbation biomechanics dataset. A pilot experimental validation with five able-bodied subjects demonstrated that our model detected ground perturbations with 97.8% accuracy and only a delay of 23.1% within the gait cycle, outperforming the benchmark by 46.8% in detection accuracy. The results of our study offer exciting promise for our detector and its potential utility to enhance the controllability of robotic assistive exoskeletons.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07029",
        "abstract url": "https://arxiv.org/abs/2412.07029",
        "title": "Key Focus Areas and Enabling Technologies for 6G",
        "rating": "-2",
        "keywords": [
            [
                "5G",
                "6G"
            ]
        ],
        "abstract": "We provide a taxonomy of a dozen enabling network architectures, protocols, and technologies that will define the evolution from 5G to 6G. These technologies span the network protocol stack, different target deployment environments, and various perceived levels of technical maturity. We outline four areas of societal focus that will be impacted by these technologies, and overview several research directions that hold the potential to address the problems in these important focus areas.",
        "subjects": [
            "cs.NI"
        ],
        "comment": "This paper has been accepted for publication in the IEEE Communications Magazine. Portions were released online as a report 6G Roadmap: A Global Taxonomy in November 2023"
    },
    {
        "paper id": "2412.07041",
        "abstract url": "https://arxiv.org/abs/2412.07041",
        "title": "Generalized Least Squares Kernelized Tensor Factorization",
        "rating": "-2",
        "keywords": [
            [
                "inpainting"
            ],
            [
                "MRI"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "Real-world datasets often contain missing or corrupted values. Completing multidimensional tensor-structured data with missing entries is essential for numerous applications. Smoothness-constrained low-rank factorization models have shown superior performance with reduced computational costs. While effective at capturing global and long-range correlations, these models struggle to reproduce short-scale, high-frequency variations in the data. In this paper, we introduce the \\Generalized Least Squares Kernelized Tensor Factorization (GLSKF) framework for tensor completion. GLSKF integrates smoothness-constrained low-rank factorization with a locally correlated residual process; the resulting additive structure can effectively characterize both global dependencies and local variations. In particular, we define the covariance norm to enforce the smoothness of factor matrices in the global low-rank factorization, and use structured covariance/kernel functions to model the local processes. For model estimation, we develop an alternating least squares (ALS) procedure with closed-form solutions for each subproblem. To efficiently handle missing data, GLSKF utilizes projection matrices that preserve the Kronecker structure of covariances, facilitating fast computations through conjugate gradient (CG) and preconditioned conjugate gradient (PCG) algorithms. The proposed framework is evaluated on four real-world datasets across diverse tasks: traffic speed imputation, color image inpainting, video completion, and MRI image reconstruction. Experimental results confirm that GLSKF delivers superior effectiveness and scalability, establishing it as a robust solution for multidimensional tensor completion.",
        "subjects": [
            "stat.ML",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07082",
        "abstract url": "https://arxiv.org/abs/2412.07082",
        "title": "User Authentication and Vital Signs Extraction from Low-Frame-Rate and Monochrome No-contact Fingerprint Captures",
        "rating": "-2",
        "keywords": [
            [
                "infrared"
            ],
            [
                "biometric",
                "healthcare"
            ],
            [
                "cs.LG",
                "eess.IV"
            ]
        ],
        "abstract": "We present our work on leveraging low-frame-rate monochrome (blue light) videos of fingertips, captured with an off-the-shelf fingerprint capture device, to extract vital signs and identify users. These videos utilize photoplethysmography (PPG), commonly used to measure vital signs like heart rate. While prior research predominantly utilizes high-frame-rate, multi-wavelength PPG sensors (e.g., infrared, red, or RGB), our preliminary findings demonstrate that both user identification and vital sign extraction are achievable with the low-frame-rate data we collected. Preliminary results are promising, with low error rates for both heart rate estimation and user authentication. These results indicate promise for effective biometric systems. We anticipate further optimization will enhance accuracy and advance healthcare and security.",
        "subjects": [
            "eess.IV",
            "cs.CR",
            "cs.LG"
        ],
        "comment": "Accepted at the 2024 Asilomar Conference on Signals, Systems, and Computers. 5 pages, 5 figures, 2 tables"
    },
    {
        "paper id": "2412.07116",
        "abstract url": "https://arxiv.org/abs/2412.07116",
        "title": "A Review of Human Emotion Synthesis Based on Generative Technology",
        "rating": "-2",
        "keywords": [
            [
                "Diffusion"
            ],
            [
                "facial"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL"
            ]
        ],
        "abstract": "Human emotion synthesis is a crucial aspect of affective computing. It involves using computational methods to mimic and convey human emotions through various modalities, with the goal of enabling more natural and effective human-computer interactions. Recent advancements in generative models, such as Autoencoders, Generative Adversarial Networks, Diffusion Models, Large Language Models, and Sequence-to-Sequence Models, have significantly contributed to the development of this field. However, there is a notable lack of comprehensive reviews in this field. To address this problem, this paper aims to address this gap by providing a thorough and systematic overview of recent advancements in human emotion synthesis based on generative models. Specifically, this review will first present the review methodology, the emotion models involved, the mathematical principles of generative models, and the datasets used. Then, the review covers the application of different generative models to emotion synthesis based on a variety of modalities, including facial images, speech, and text. It also examines mainstream evaluation metrics. Additionally, the review presents some major findings and suggests future research directions, providing a comprehensive understanding of the role of generative technology in the nuanced domain of emotion synthesis.",
        "subjects": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "comment": "25 pages, 10 figures"
    },
    {
        "paper id": "2412.07129",
        "abstract url": "https://arxiv.org/abs/2412.07129",
        "title": "StyleMark: A Robust Watermarking Method for Art Style Images Against Black-Box Arbitrary Style Transfer",
        "rating": "-2",
        "keywords": [
            [
                "attacks"
            ],
            [
                "Watermarking"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Arbitrary Style Transfer (AST) achieves the rendering of real natural images into the painting styles of arbitrary art style images, promoting art communication. However, misuse of unauthorized art style images for AST may infringe on artists' copyrights. One countermeasure is robust watermarking, which tracks image propagation by embedding copyright watermarks into carriers. Unfortunately, AST-generated images lose the structural and semantic information of the original style image, hindering end-to-end robust tracking by watermarks. To fill this gap, we propose StyleMark, the first robust watermarking method for black-box AST, which can be seamlessly applied to art style images achieving precise attribution of artistic styles after AST. Specifically, we propose a new style watermark network that adjusts the mean activations of style features through multi-scale watermark embedding, thereby planting watermark traces into the shared style feature space of style images. Furthermore, we design a distribution squeeze loss, which constrain content statistical feature distortion, forcing the reconstruction network to focus on integrating style features with watermarks, thus optimizing the intrinsic watermark distribution. Finally, based on solid end-to-end training, StyleMark mitigates the optimization conflict between robustness and watermark invisibility through decoder fine-tuning under random noise. Experimental results demonstrate that StyleMark exhibits significant robustness against black-box AST and common pixel-level distortions, while also securely defending against malicious adaptive attacks.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07132",
        "abstract url": "https://arxiv.org/abs/2412.07132",
        "title": "Revisiting Lesion Tracking in 3D Total Body Photography",
        "rating": "-2",
        "keywords": [
            [
                "3D"
            ],
            [
                "cancer",
                "skin lesions",
                "Lesion"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Melanoma is the most deadly form of skin cancer. Tracking the evolution of nevi and detecting new lesions across the body is essential for the early detection of melanoma. Despite prior work on longitudinal tracking of skin lesions in 3D total body photography, there are still several challenges, including 1) low accuracy for finding correct lesion pairs across scans, 2) sensitivity to noisy lesion detection, and 3) lack of large-scale datasets with numerous annotated lesion pairs. We propose a framework that takes in a pair of 3D textured meshes, matches lesions in the context of total body photography, and identifies unmatchable lesions. We start by computing correspondence maps bringing the source and target meshes to a template mesh. Using these maps to define source/target signals over the template domain, we construct a flow field aligning the mapped signals. The initial correspondence maps are then refined by advecting forward/backward along the vector field. Finally, lesion assignment is performed using the refined correspondence maps. We propose the first large-scale dataset for skin lesion tracking with 25K lesion pairs across 198 subjects. The proposed method achieves a success rate of 89.9% (at 10 mm criterion) for all pairs of annotated lesions and a matching accuracy of 98.2% for subjects with more than 200 lesions.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07156",
        "abstract url": "https://arxiv.org/abs/2412.07156",
        "title": "QCResUNet: Joint Subject-level and Voxel-level Segmentation Quality Prediction",
        "rating": "-2",
        "keywords": [
            [
                "Voxel"
            ],
            [
                "Diagnosis",
                "MRI",
                "clinical",
                "cardiac",
                "tumor"
            ],
            [
                "cs.LG",
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Deep learning has made significant strides in automated brain tumor segmentation from magnetic resonance imaging (MRI) scans in recent years. However, the reliability of these tools is hampered by the presence of poor-quality segmentation outliers, particularly in out-of-distribution samples, making their implementation in clinical practice difficult. Therefore, there is a need for quality control (QC) to screen the quality of the segmentation results. Although numerous automatic QC methods have been developed for segmentation quality screening, most were designed for cardiac MRI segmentation, which involves a single modality and a single tissue type. Furthermore, most prior works only provided subject-level predictions of segmentation quality and did not identify erroneous parts segmentation that may require refinement. To address these limitations, we proposed a novel multi-task deep learning architecture, termed QCResUNet, which produces subject-level segmentation-quality measures as well as voxel-level segmentation error maps for each available tissue class. To validate the effectiveness of the proposed method, we conducted experiments on assessing its performance on evaluating the quality of two distinct segmentation tasks. First, we aimed to assess the quality of brain tumor segmentation results. For this task, we performed experiments on one internal and two external datasets. Second, we aimed to evaluate the segmentation quality of cardiac Magnetic Resonance Imaging (MRI) data from the Automated Cardiac Diagnosis Challenge. The proposed method achieved high performance in predicting subject-level segmentation-quality metrics and accurately identifying segmentation errors on a voxel basis. This has the potential to be used to guide human-in-the-loop feedback to improve segmentations in clinical settings.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07175",
        "abstract url": "https://arxiv.org/abs/2412.07175",
        "title": "Robust Feature Engineering Techniques for Designing Efficient Motor Imagery-Based BCI-Systems",
        "rating": "-2",
        "keywords": [
            [
                "SVM",
                "Support Vector Machine"
            ],
            [
                "EEG"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "A multitude of individuals across the globe grapple with motor disabilities. Neural prosthetics utilizing Brain-Computer Interface (BCI) technology exhibit promise for improving motor rehabilitation outcomes. The intricate nature of EEG data poses a significant hurdle for current BCI systems. Recently, a qualitative repository of EEG signals tied to both upper and lower limb execution of motor and motor imagery tasks has been unveiled. Despite this, the productivity of the Machine Learning (ML) Models that were trained on this dataset was alarmingly deficient, and the evaluation framework seemed insufficient. To enhance outcomes, robust feature engineering (signal processing) methodologies are implemented. A collection of time domain, frequency domain, and wavelet-derived features was obtained from 16-channel EEG signals, and the Maximum Relevance Minimum Redundancy (MRMR) approach was employed to identify the four most significant features. For classification K Nearest Neighbors (KNN), Support Vector Machine (SVM), Decision Tree (DT), and Na\u00efve Bayes (NB) models were implemented with these selected features, evaluating their effectiveness through metrics such as testing accuracy, precision, recall, and F1 Score. By leveraging SVM with a Gaussian Kernel, a remarkable maximum testing accuracy of 92.50% for motor activities and 95.48% for imagery activities is achieved. These results are notably more dependable and gratifying compared to the previous study, where the peak accuracy was recorded at 74.36%. This research work provides an in-depth analysis of the MI Limb EEG dataset and it will help in designing and developing simple, cost-effective and reliable BCI systems for neuro-rehabilitation.",
        "subjects": [
            "eess.SP",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "26 pages"
    },
    {
        "paper id": "2412.07181",
        "abstract url": "https://arxiv.org/abs/2412.07181",
        "title": "Modeling and Simulating Rydberg Atom Quantum Computers for Hardware-Software Co-design with PachinQo",
        "rating": "-2",
        "keywords": [
            [
                "Quantum"
            ]
        ],
        "abstract": "Quantum computing has the potential to accelerate various domains: scientific computation, machine learning, and optimization. Recently, Rydberg atom quantum computing has emerged as a promising quantum computing technology, especially with the demonstration of the zonal addressing architecture. However, this demonstration is only compatible with one type of quantum algorithm, and extending it to compile and execute general quantum algorithms is a challenge. To address it, we propose PachinQo, a framework to co-design the architecture and compilation for zonal addressing systems for any given quantum algorithm. PachinQo's evaluation demonstrates its ability to improve a quantum algorithm's estimated probability of success by 45% on average in error-prone quantum environments.",
        "subjects": [
            "quant-ph",
            "cs.ET"
        ],
        "comment": "25 pages, 20 figures, ACM SIGMETRICS 2025"
    },
    {
        "paper id": "2412.06210",
        "abstract url": "https://arxiv.org/abs/2412.06210",
        "title": "H-FedSN: Personalized Sparse Networks for Efficient and Accurate Hierarchical Federated Learning for IoT Applications",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated Learning"
            ],
            [
                "IoT"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The proliferation of Internet of Things (IoT) has increased interest in federated learning (FL) for privacy-preserving distributed data utilization. However, traditional two-tier FL architectures inadequately adapt to multi-tier IoT environments. While Hierarchical Federated Learning (HFL) improves practicality in multi-tier IoT environments by multi-layer aggregation, it still faces challenges in communication efficiency and accuracy due to high data transfer volumes, data heterogeneity, and imbalanced device distribution, struggling to meet the low-latency and high-accuracy model training requirements of practical IoT scenarios. To overcome these limitations, we propose H-FedSN, an innovative approach for practical IoT environments. H-FedSN introduces a binary mask mechanism with shared and personalized layers to reduce communication overhead by creating a sparse network while keeping original weights frozen. To address data heterogeneity and imbalanced device distribution, we integrate personalized layers for local data adaptation and apply Bayesian aggregation with cumulative Beta distribution updates at edge and cloud levels, effectively balancing contributions from diverse client groups. Evaluations on three real-world IoT datasets and MNIST under non-IID settings demonstrate that H-FedSN significantly reduces communication costs by 58 to 238 times compared to HierFAVG while achieving high accuracy, making it highly effective for practical IoT applications in hierarchical federated learning scenarios.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06212",
        "abstract url": "https://arxiv.org/abs/2412.06212",
        "title": "A Self-guided Multimodal Approach to Enhancing Graph Representation Learning for Alzheimer's Diseases",
        "rating": "-2.5",
        "keywords": [
            [
                "GNNs",
                "Graph"
            ],
            [
                "diagnosis",
                "Disease"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Graph neural networks (GNNs) are powerful machine learning models designed to handle irregularly structured data. However, their generic design often proves inadequate for analyzing brain connectomes in Alzheimer's Disease (AD), highlighting the need to incorporate domain knowledge for optimal performance. Infusing AD-related knowledge into GNNs is a complicated task. Existing methods typically rely on collaboration between computer scientists and domain experts, which can be both time-intensive and resource-demanding. To address these limitations, this paper presents a novel self-guided, knowledge-infused multimodal GNN that autonomously incorporates domain knowledge into the model development process. Our approach conceptualizes domain knowledge as natural language and introduces a specialized multimodal GNN capable of leveraging this uncurated knowledge to guide the learning process of the GNN, such that it can improve the model performance and strengthen the interpretability of the predictions. To evaluate our framework, we curated a comprehensive dataset of recent peer-reviewed papers on AD and integrated it with multiple real-world AD datasets. Experimental results demonstrate the ability of our method to extract relevant domain knowledge, provide graph-based explanations for AD diagnosis, and improve the overall performance of the GNN. This approach provides a more scalable and efficient alternative to inject domain knowledge for AD compared with the manual design from the domain expert, advancing both prediction accuracy and interpretability in AD diagnosis.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06494",
        "abstract url": "https://arxiv.org/abs/2412.06494",
        "title": "A cautionary tale on the cost-effectiveness of collaborative AI in real-world medical applications",
        "rating": "-2.5",
        "keywords": [
            [
                "Federated learning"
            ],
            [
                "medical",
                "healthcare"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Background. Federated learning (FL) has gained wide popularity as a collaborative learning paradigm enabling collaborative AI in sensitive healthcare applications. Nevertheless, the practical implementation of FL presents technical and organizational challenges, as it generally requires complex communication infrastructures. In this context, consensus-based learning (CBL) may represent a promising collaborative learning alternative, thanks to the ability of combining local knowledge into a federated decision system, while potentially reducing deployment overhead. Methods. In this work we propose an extensive benchmark of the accuracy and cost-effectiveness of a panel of FL and CBL methods in a wide range of collaborative medical data analysis scenarios. The benchmark includes 7 different medical datasets, encompassing 3 machine learning tasks, 8 different data modalities, and multi-centric settings involving 3 to 23 clients. Findings. Our results reveal that CBL is a cost-effective alternative to FL. When compared across the panel of medical dataset in the considered benchmark, CBL methods provide equivalent accuracy to the one achieved by FL.Nonetheless, CBL significantly reduces training time and communication cost (resp. 15 fold and 60 fold decrease) (p < 0.05). Interpretation. This study opens a novel perspective on the deployment of collaborative AI in real-world applications, whereas the adoption of cost-effective methods is instrumental to achieve sustainability and democratisation of AI by alleviating the need for extensive computational resources.",
        "subjects": [
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06958",
        "abstract url": "https://arxiv.org/abs/2412.06958",
        "title": "Enhancing operational wind downscaling capabilities over Canada: Application of a Conditional Wasserstein GAN methodology",
        "rating": "-2.5",
        "keywords": [
            [
                "GAN"
            ],
            [
                "forecast"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Wind downscaling is essential for improving the spatial resolution of weather forecasts, particularly in operational Numerical Weather Prediction (NWP). This study advances wind downscaling by extending the DownGAN framework introduced by Annau et al.,to operational datasets from the Global Deterministic Prediction System (GDPS) and High-Resolution Deterministic Prediction System (HRDPS), covering the entire Canadian domain. We enhance the model by incorporating high-resolution static covariates, such as HRDPS-derived topography, into a Conditional Wasserstein Generative Adversarial Network with Gradient Penalty, implemented using a UNET-based generator. Following the DownGAN framework, our methodology integrates low-resolution GDPS forecasts (15 km, 10-day horizon) and high-resolution HRDPS forecasts (2.5 km, 48-hour horizon) with Frequency Separation techniques adapted from computer vision. Through robust training and inference over the Canadian region, we demonstrate the operational scalability of our approach, achieving significant improvements in wind downscaling accuracy. Statistical validation highlights reductions in root mean square error (RMSE) and log spectral distance (LSD) metrics compared to the original DownGAN. High-resolution conditioning covariates and Frequency Separation strategies prove instrumental in enhancing model performance. This work underscores the potential for extending high-resolution wind forecasts beyond the 48-hour horizon, bridging the gap to the 10-day low resolution global forecast window.",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06211",
        "abstract url": "https://arxiv.org/abs/2412.06211",
        "title": "MSCrackMamba: Leveraging Vision Mamba for Crack Detection in Fused Multispectral Imagery",
        "rating": "-3",
        "keywords": [
            [
                "super-resolution"
            ],
            [
                "infrared"
            ],
            [
                "health"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Crack detection is a critical task in structural health monitoring, aimed at assessing the structural integrity of bridges, buildings, and roads to prevent potential failures. Vision-based crack detection has become the mainstream approach due to its ease of implementation and effectiveness. Fusing infrared (IR) channels with red, green and blue (RGB) channels can enhance feature representation and thus improve crack detection. However, IR and RGB channels often differ in resolution. To align them, higher-resolution RGB images typically need to be downsampled to match the IR image resolution, which leads to the loss of fine details. Moreover, crack detection performance is restricted by the limited receptive fields and high computational complexity of traditional image segmentation networks. Inspired by the recently proposed Mamba neural architecture, this study introduces a two-stage paradigm called MSCrackMamba, which leverages Vision Mamba along with a super-resolution network to address these challenges. Specifically, to align IR and RGB channels, we first apply super-resolution to IR channels to match the resolution of RGB channels for data fusion. Vision Mamba is then adopted as the backbone network, while UperNet is employed as the decoder for crack detection. Our approach is validated on the large-scale Crack Detection dataset Crack900, demonstrating an improvement of 3.55% in mIoU compared to the best-performing baseline methods.",
        "subjects": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06313",
        "abstract url": "https://arxiv.org/abs/2412.06313",
        "title": "Vision-Based Deep Reinforcement Learning of UAV Autonomous Navigation Using Privileged Information",
        "rating": "-3",
        "keywords": [
            [
                "flight"
            ],
            [
                "Navigation"
            ],
            [
                "UAV",
                "agricultural"
            ],
            [
                "cs.LG",
                "cs.CV"
            ]
        ],
        "abstract": "The capability of UAVs for efficient autonomous navigation and obstacle avoidance in complex and unknown environments is critical for applications in agricultural irrigation, disaster relief and logistics. In this paper, we propose the DPRL (Distributed Privileged Reinforcement Learning) navigation algorithm, an end-to-end policy designed to address the challenge of high-speed autonomous UAV navigation under partially observable environmental conditions. Our approach combines deep reinforcement learning with privileged learning to overcome the impact of observation data corruption caused by partial observability. We leverage an asymmetric Actor-Critic architecture to provide the agent with privileged information during training, which enhances the model's perceptual capabilities. Additionally, we present a multi-agent exploration strategy across diverse environments to accelerate experience collection, which in turn expedites model convergence. We conducted extensive simulations across various scenarios, benchmarking our DPRL algorithm against the state-of-the-art navigation algorithms. The results consistently demonstrate the superior performance of our algorithm in terms of flight efficiency, robustness and overall success rate.",
        "subjects": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "comment": "19 pages, 9 figures"
    },
    {
        "paper id": "2412.06325",
        "abstract url": "https://arxiv.org/abs/2412.06325",
        "title": "Q-PnV: A Quantum Consensus Mechanism for Security Consortium Blockchains",
        "rating": "-3",
        "keywords": [
            [
                "attacks"
            ],
            [
                "Quantum"
            ]
        ],
        "abstract": "Due to the rapid development of quantum computing, many classical blockchain technologies are now considered insecure. The emergence of quantum blockchain holds promise for addressing this issue. Various quantum consensus algorithms have been proposed so far, but there has not yet been a quantum consensus algorithm tailored specifically for consortium blockchain scenarios. In this paper, we propose a novel quantum consensus mechanism, named Q-PnV. This consensus mechanism is based on the classical Proof of Vote (PoV), integrating quantum voting, quantum digital signature and quantum random number generators (QRNGs). By combining Q-PnV with a quantum blockchain using weighted hypergraph states, we propose a comprehensive quantum blockchain solution for consortium blockchain scenarios. Compared to the classical method, the quantum blockchain based on Q-PnV can resist quantum attacks and shows significant improvements in security and fairness, making it better suit-ed for the future quantum era.",
        "subjects": [
            "cs.CR",
            "quant-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06336",
        "abstract url": "https://arxiv.org/abs/2412.06336",
        "title": "A Combined Channel Approach for Decoding Intracranial EEG Signals: Enhancing Accuracy through Spatial Information Integration",
        "rating": "-3",
        "keywords": [
            [
                "Audio Visual"
            ],
            [
                "EEG",
                "physiological"
            ],
            [
                "Music"
            ]
        ],
        "abstract": "Intracranial EEG (iEEG) recording, characterized by high spatial and temporal resolution and superior signal-to-noise ratio (SNR), enables the development of precise brain-computer interface (BCI) systems for neural decoding. However, the invasive nature of the procedure significantly limits the availability of iEEG datasets in terms of both the number of participants and the duration of recorded sessions. To address this limitation, we propose a single-participant machine learning model optimized for decoding iEEG signals. The model employs 18 key features and operates in two modes: best channel and combined channel. The combined channel mode integrates spatial information from multiple brain regions, leading to superior classification performance. Evaluations across three datasets -- Music Reconstruction, Audio Visual, and AJILE12 -- demonstrate that the combined channel mode consistently outperforms the best channel mode across all classifiers. In the best-performing cases, Random Forest achieved an F1 score of 0.81 +/- 0.05 in the Music Reconstruction dataset and 0.82 +/- 0.10 in the Audio Visual dataset, while XGBoost achieved an F1 score of 0.84 +/- 0.08 in the AJILE12 dataset. Furthermore, the analysis of brain region contributions in the combined channel mode revealed that the model identifies relevant brain regions aligned with physiological expectations for each task and effectively combines data from electrodes in these regions to achieve high performance. These findings highlight the potential of integrating spatial information across brain regions to improve task decoding, offering new avenues for advancing BCI systems and neurotechnological applications.",
        "subjects": [
            "cs.HC",
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06353",
        "abstract url": "https://arxiv.org/abs/2412.06353",
        "title": "Cram\u00e9r-Rao Bound Analysis and Beamforming Design for 3D Extended Target in ISAC: From Optimization to Learning Approaches",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "radar"
            ],
            [
                "graph"
            ]
        ],
        "abstract": "This paper considers an integrated sensing and communication (ISAC) system where a multi-antenna base station transmits a common signal for joint multi-user communication and extended target (ET) sensing. We first propose a second-order truncated Fourier series surface model for an arbitrarily-shaped three-dimensional ET. Utilizing this model, we derive novel closed-form Cram{\u00e9}r-Rao bounds (CRBs) for the ET kinematic parameter estimation, such as the center range, azimuth angle, elevation angle, and orientation. Further, we formulate and solve two transmit beamforming design problems with optimization algorithms. The first one, named the CRB minimization problem, minimizes the CRB under constraints of communication signal-to-interference-plus-noise ratio (SINR) requirement, transmit power, and ET-specific beam coverage requirement, which is solved using the semidefinite relaxation technique. The second one, named the weighted-ISAC-metric problem, targets an weighted objective function combining the communication sum rate and sensing CRB, and is solved using the successive convex approximation technique. Additionally, by exploiting the penalty method, we introduce an unsupervised learning-based approach and propose a unique ISAC graph neural network (ISACGNN), composed of separate communication, sensing, and integration modules, to address both problems. Numerical results reveal the diverse CRB characteristics for different radar targets. The proposed beamforming designs are superior to existing baselines with better trade-off between communication and sensing performance, and a more appropriate beampattern for sensing the 3D ET. Besides, our proposed ISACGNN can effectively mimic the dynamic structures of the SINR, sum rate, and CRB, demonstrating remarkable scalability.",
        "subjects": [
            "cs.IT",
            "eess.SP"
        ],
        "comment": "14 pages, 10 figures, partially published in IEEE Global Communications Conference 2024"
    },
    {
        "paper id": "2412.06491",
        "abstract url": "https://arxiv.org/abs/2412.06491",
        "title": "PPT: Pre-Training with Pseudo-Labeled Trajectories for Motion Forecasting",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "autonomous driving"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Motion forecasting (MF) for autonomous driving aims at anticipating trajectories of surrounding agents in complex urban scenarios. In this work, we investigate a mixed strategy in MF training that first pre-train motion forecasters on pseudo-labeled data, then fine-tune them on annotated data. To obtain pseudo-labeled trajectories, we propose a simple pipeline that leverages off-the-shelf single-frame 3D object detectors and non-learning trackers. The whole pre-training strategy including pseudo-labeling is coined as PPT. Our extensive experiments demonstrate that: (1) combining PPT with supervised fine-tuning on annotated data achieves superior performance on diverse testbeds, especially under annotation-efficient regimes, (2) scaling up to multiple datasets improves the previous state-of-the-art and (3) PPT helps enhance cross-dataset generalization. Our findings showcase PPT as a promising pre-training solution for robust motion forecasting in diverse autonomous driving contexts.",
        "subjects": [
            "cs.CV",
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06599",
        "abstract url": "https://arxiv.org/abs/2412.06599",
        "title": "A No-Reference Medical Image Quality Assessment Method Based on Automated Distortion Recognition Technology: Application to Preprocessing in MRI-guided Radiotherapy",
        "rating": "-3",
        "keywords": [
            [
                "Medical",
                "MRI",
                "clinical",
                "Tumor"
            ],
            [
                "Quality Assessment"
            ],
            [
                "cs.CV",
                "eess.IV"
            ]
        ],
        "abstract": "Objective:To develop a no-reference image quality assessment method using automated distortion recognition to boost MRI-guided radiotherapy precision.Methods:We analyzed 106,000 MR images from 10 patients with liver metastasis,captured with the Elekta Unity MR-LINAC.Our No-Reference Quality Assessment Model includes:1)image preprocessing to enhance visibility of key diagnostic features;2)feature extraction and directional analysis using MSCN coefficients across four directions to capture textural attributes and gradients,vital for identifying image features and potential distortions;3)integrative Quality Index(QI)calculation,which integrates features via AGGD parameter estimation and K-means clustering.The QI,based on a weighted MAD computation of directional scores,provides a comprehensive image quality measure,robust against outliers.LOO-CV assessed model generalizability and performance.Tumor tracking algorithm performance was compared with and without preprocessing to verify tracking accuracy enhancements.Results:Preprocessing significantly improved image quality,with the QI showing substantial positive changes and surpassing other metrics.After normalization,the QI's average value was 79.6 times higher than CNR,indicating improved image definition and contrast.It also showed higher sensitivity in detail recognition with average values 6.5 times and 1.7 times higher than Tenengrad gradient and entropy.The tumor tracking algorithm confirmed significant tracking accuracy improvements with preprocessed images,validating preprocessing effectiveness.Conclusions:This study introduces a novel no-reference image quality evaluation method based on automated distortion recognition,offering a new quality control tool for MRIgRT tumor tracking.It enhances clinical application accuracy and facilitates medical image quality assessment standardization, with significant clinical and research value.",
        "subjects": [
            "eess.IV",
            "cs.CV",
            "physics.med-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06700",
        "abstract url": "https://arxiv.org/abs/2412.06700",
        "title": "Facade: High-Precision Insider Threat Detection Using Deep Contextual Anomaly Detection",
        "rating": "-3",
        "keywords": [
            [
                "Anomaly Detection"
            ],
            [
                "SQL"
            ]
        ],
        "abstract": "We present Facade (Fast and Accurate Contextual Anomaly DEtection): a high-precision deep-learning-based anomaly detection system deployed at Google (a large technology company) as the last line of defense against insider threats since 2018. Facade is an innovative unsupervised action-context system that detects suspicious actions by considering the context surrounding each action, including relevant facts about the user and other entities involved. It is built around a new multi-modal model that is trained on corporate document access, SQL query, and HTTP/RPC request logs. To overcome the scarcity of incident data, Facade harnesses a novel contrastive learning strategy that relies solely on benign data. Its use of history and implicit social network featurization efficiently handles the frequent out-of-distribution events that occur in a rapidly changing corporate environment, and sustains Facade's high precision performance for a full year after training. Beyond the core model, Facade contributes an innovative clustering approach based on user and action embeddings to improve detection robustness and achieve high precision, multi-scale detection. Functionally what sets Facade apart from existing anomaly detection systems is its high precision. It detects insider attackers with an extremely low false positive rate, lower than 0.01%. For single rogue actions, such as the illegitimate access to a sensitive document, the false positive rate is as low as 0.0003%. To the best of our knowledge, Facade is the only published insider risk anomaly detection system that helps secure such a large corporate environment.",
        "subjects": [
            "cs.CR"
        ],
        "comment": "Under review"
    },
    {
        "paper id": "2412.06727",
        "abstract url": "https://arxiv.org/abs/2412.06727",
        "title": "Take Fake as Real: Realistic-like Robust Black-box Adversarial Attack to Evade AIGC Detection",
        "rating": "-3",
        "keywords": [
            [
                "diffusion",
                "GAN"
            ],
            [
                "Attack"
            ],
            [
                "facial"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "The security of AI-generated content (AIGC) detection based on GANs and diffusion models is closely related to the credibility of multimedia content. Malicious adversarial attacks can evade these developing AIGC detection. However, most existing adversarial attacks focus only on GAN-generated facial images detection, struggle to be effective on multi-class natural images and diffusion-based detectors, and exhibit poor invisibility. To fill this gap, we first conduct an in-depth analysis of the vulnerability of AIGC detectors and discover the feature that detectors vary in vulnerability to different post-processing. Then, considering the uncertainty of detectors in real-world scenarios, and based on the discovery, we propose a Realistic-like Robust Black-box Adversarial attack (R$^2$BA) with post-processing fusion optimization. Unlike typical perturbations, R$^2$BA uses real-world post-processing, i.e., Gaussian blur, JPEG compression, Gaussian noise and light spot to generate adversarial examples. Specifically, we use a stochastic particle swarm algorithm with inertia decay to optimize post-processing fusion intensity and explore the detector's decision boundary. Guided by the detector's fake probability, R$^2$BA enhances/weakens the detector-vulnerable/detector-robust post-processing intensity to strike a balance between adversariality and invisibility. Extensive experiments on popular/commercial AIGC detectors and datasets demonstrate that R$^2$BA exhibits impressive anti-detection performance, excellent invisibility, and strong robustness in GAN-based and diffusion-based cases. Compared to state-of-the-art white-box and black-box attacks, R$^2$BA shows significant improvements of 15% and 21% in anti-detection performance under the original and robust scenario respectively, offering valuable insights for the security of AIGC detection in real-world applications.",
        "subjects": [
            "cs.CV"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06743",
        "abstract url": "https://arxiv.org/abs/2412.06743",
        "title": "3D Graph Attention Networks for High Fidelity Pediatric Glioma Segmentation",
        "rating": "-3",
        "keywords": [
            [
                "3D"
            ],
            [
                "Graph"
            ],
            [
                "diagnosis",
                "MRI",
                "cancer",
                "clinical",
                "tumor"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Pediatric brain tumors, particularly gliomas, represent a significant cause of cancer related mortality in children with complex infiltrative growth patterns that complicate treatment. Early, accurate segmentation of these tumors in neuroimaging data is crucial for effective diagnosis and intervention planning. This study presents a novel 3D UNet architecture with a spatial attention mechanism tailored for automated segmentation of pediatric gliomas. Using the BraTS pediatric glioma dataset with multiparametric MRI data, the proposed model captures multi-scale features and selectively attends to tumor relevant regions, enhancing segmentation precision and reducing interference from surrounding tissue. The model's performance is quantitatively evaluated using the Dice similarity coefficient and HD95, demonstrating improved delineation of complex glioma structured. This approach offers a promising advancement in automating pediatric glioma segmentation, with the potential to improve clinical decision making and outcomes.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "8 pages, 9 figures"
    },
    {
        "paper id": "2412.06931",
        "abstract url": "https://arxiv.org/abs/2412.06931",
        "title": "Non-Prehensile Tool-Object Manipulation by Integrating LLM-Based Planning and Manoeuvrability-Driven Controls",
        "rating": "-3",
        "keywords": [
            [
                "robot"
            ],
            [
                "biological"
            ]
        ],
        "abstract": "The ability to wield tools was once considered exclusive to human intelligence, but it's now known that many other animals, like crows, possess this capability. Yet, robotic systems still fall short of matching biological dexterity. In this paper, we investigate the use of Large Language Models (LLMs), tool affordances, and object manoeuvrability for non-prehensile tool-based manipulation tasks. Our novel method leverages LLMs based on scene information and natural language instructions to enable symbolic task planning for tool-object manipulation. This approach allows the system to convert the human language sentence into a sequence of feasible motion functions. We have developed a novel manoeuvrability-driven controller using a new tool affordance model derived from visual feedback. This controller helps guide the robot's tool utilization and manipulation actions, even within confined areas, using a stepping incremental approach. The proposed methodology is evaluated with experiments to prove its effectiveness under various manipulation scenarios.",
        "subjects": [
            "cs.RO"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07163",
        "abstract url": "https://arxiv.org/abs/2412.07163",
        "title": "Fast Occupancy Network",
        "rating": "-3",
        "keywords": [
            [
                "3D",
                "voxel"
            ],
            [
                "autonomous driving",
                "vehicle"
            ],
            [
                "BEV"
            ],
            [
                "cs.AI",
                "cs.CV"
            ]
        ],
        "abstract": "Occupancy Network has recently attracted much attention in autonomous driving. Instead of monocular 3D detection and recent bird's eye view(BEV) models predicting 3D bounding box of obstacles, Occupancy Network predicts the category of voxel in specified 3D space around the ego vehicle via transforming 3D detection task into 3D voxel segmentation task, which has much superiority in tackling category outlier obstacles and providing fine-grained 3D representation. However, existing methods usually require huge computation resources than previous methods, which hinder the Occupancy Network solution applying in intelligent driving systems. To address this problem, we make an analysis of the bottleneck of Occupancy Network inference cost, and present a simple and fast Occupancy Network model, which adopts a deformable 2D convolutional layer to lift BEV feature to 3D voxel feature and presents an efficient voxel feature pyramid network (FPN) module to improve performance with few computational cost. Further, we present a cost-free 2D segmentation branch in perspective view after feature extractors for Occupancy Network during inference phase to improve accuracy. Experimental results demonstrate that our method consistently outperforms existing methods in both accuracy and inference speed, which surpasses recent state-of-the-art (SOTA) OCCNet by 1.7% with ResNet50 backbone with about 3X inference speedup. Furthermore, our method can be easily applied to existing BEV models to transform them into Occupancy Network models.",
        "subjects": [
            "cs.CV",
            "cs.AI"
        ],
        "comment": "10 pages, 5 figures,"
    },
    {
        "paper id": "2412.06389",
        "abstract url": "https://arxiv.org/abs/2412.06389",
        "title": "Exploring the Impact of Synthetic Data on Human Gesture Recognition Tasks Using GANs",
        "rating": "-3.5",
        "keywords": [
            [
                "healthcare",
                "disease"
            ],
            [
                "IoT"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "In the evolving domain of Human Activity Recognition (HAR) using Internet of Things (IoT) devices, there is an emerging interest in employing Deep Generative Models (DGMs) to address data scarcity, enhance data quality, and improve classification metrics scores. Among these types of models, Generative Adversarial Networks (GANs) have arisen as a powerful tool for generating synthetic data that mimic real-world scenarios with high fidelity. However, Human Gesture Recognition (HGR), a subset of HAR, particularly in healthcare applications, using time series data such as allergic gestures, remains highly unexplored. In this paper, we examine and evaluate the performance of two GANs in the generation of synthetic gesture motion data that compose a part of an open-source benchmark dataset. The data is related to the disease identification domain and healthcare, specifically to allergic rhinitis. We also focus on these AI models' performance in terms of fidelity, diversity, and privacy. Furthermore, we examine the scenario if the synthetic data can substitute real data, in training scenarios and how well models trained on synthetic data can be generalized for the allergic rhinitis gestures. In our work, these gestures are related to 6-axes accelerometer and gyroscope data, serving as multi-variate time series instances, and retrieved from smart wearable devices. To the best of our knowledge, this study is the first to explore the feasibility of synthesizing motion gestures for allergic rhinitis from wearable IoT device data using Generative Adversarial Networks (GANs) and testing their impact on the generalization of gesture recognition systems. It is worth noting that, even if our method has been applied to a specific category of gestures, it is designed to be generalized and can be deployed also to other motion data in the HGR domain.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "8 pages, 5 figures, 20th International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT), 2024"
    },
    {
        "paper id": "2412.06600",
        "abstract url": "https://arxiv.org/abs/2412.06600",
        "title": "Advancing Music Therapy: Integrating Eastern Five-Element Music Theory and Western Techniques with AI in the Novel Five-Element Harmony System",
        "rating": "-3.5",
        "keywords": [
            [
                "medical",
                "psychological",
                "physiological"
            ],
            [
                "Music"
            ],
            [
                "cs.AI",
                "cs.CY"
            ]
        ],
        "abstract": "In traditional medical practices, music therapy has proven effective in treating various psychological and physiological ailments. Particularly in Eastern traditions, the Five Elements Music Therapy (FEMT), rooted in traditional Chinese medicine, possesses profound cultural significance and unique therapeutic philosophies. With the rapid advancement of Information Technology and Artificial Intelligence, applying these modern technologies to FEMT could enhance the personalization and cultural relevance of the therapy and potentially improve therapeutic outcomes. In this article, we developed a music therapy system for the first time by applying the theory of the five elements in music therapy to practice. This innovative approach integrates advanced Information Technology and Artificial Intelligence with Five-Element Music Therapy (FEMT) to enhance personalized music therapy practices. As traditional music therapy predominantly follows Western methodologies, the unique aspects of Eastern practices, specifically the Five-Element theory from traditional Chinese medicine, should be considered. This system aims to bridge this gap by utilizing computational technologies to provide a more personalized, culturally relevant, and therapeutically effective music therapy experience.",
        "subjects": [
            "cs.HC",
            "cs.AI",
            "cs.CY"
        ],
        "comment": "5 pages, 1 figure. Accepted for Publication in the International Symposium on Chinese Spoken Language Processing"
    },
    {
        "paper id": "2412.06866",
        "abstract url": "https://arxiv.org/abs/2412.06866",
        "title": "LMS-AutoTSF: Learnable Multi-Scale Decomposition and Integrated Autocorrelation for Time Series Forecasting",
        "rating": "-3.5",
        "keywords": [
            [
                "industrial"
            ],
            [
                "Forecasting"
            ],
            [
                "cs.AI",
                "cs.LG"
            ]
        ],
        "abstract": "Time series forecasting is an important challenge with significant applications in areas such as weather prediction, stock market analysis, scientific simulations and industrial process analysis. In this work, we introduce LMS-AutoTSF, a novel time series forecasting architecture that incorporates autocorrelation while leveraging dual encoders operating at multiple scales. Unlike models that rely on predefined trend and seasonal components, LMS-AutoTSF employs two separate encoders per scale: one focusing on low-pass filtering to capture trends and the other utilizing high-pass filtering to model seasonal variations. These filters are learnable, allowing the model to dynamically adapt and isolate trend and seasonal components directly in the frequency domain. A key innovation in our approach is the integration of autocorrelation, achieved by computing lagged differences in time steps, which enables the model to capture dependencies across time more effectively. Each encoder processes the input through fully connected layers to handle temporal and channel interactions. By combining frequency-domain filtering, autocorrelation-based temporal modeling, and channel-wise transformations, LMS-AutoTSF not only accurately captures long-term dependencies and fine-grained patterns but also operates more efficiently compared to other state-of-the-art methods. Its lightweight design ensures faster processing while maintaining high precision in forecasting across diverse time horizons. The source code is publicly available at \\url{http://github.com/mribrahim/LMS-TSF}",
        "subjects": [
            "cs.LG",
            "cs.AI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06946",
        "abstract url": "https://arxiv.org/abs/2412.06946",
        "title": "NRSurNN3dq4: A Deep Learning Powered Numerical Relativity Surrogate for Binary Black Hole Waveforms",
        "rating": "-3.5",
        "keywords": [
            [
                "astronomy"
            ],
            [
                "physics"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "Gravitational wave approximants are widely used tools in gravitational-wave astronomy. They allow for dense coverage of the parameter space of binary black hole (BBH) mergers for purposes of parameter inference, or, more generally, match filtering tasks, while avoiding the computationally expensive full evolution of numerical relativity simulations. However, this comes at a slight cost in terms of accuracy when compared to numerical relativity waveforms, depending on the approach. One way to minimize this is by constructing so-called~\\textit{surrogate models} which, instead of using approximate physics or phenomenological formulae, rather interpolate within the space of numerical relativity waveforms. In this work, we introduce~\\texttt{NRSurNN3dq4}, a surrogate model for non-precessing BBH merger waveforms powered by neural networks. By relying on the power of deep learning, this approximant is remarkably fast and competitively accurate, as it can generate millions of waveforms in a tenth of a second, while mismatches with numerical relativity waveforms are restrained below $10^{-3}$. We implement this approximant within the~\\textsc{bilby} framework for gravitational-wave parameter inference, and show that it it is suitable for parameter estimation tasks.",
        "subjects": [
            "gr-qc",
            "astro-ph.HE",
            "astro-ph.IM",
            "cs.LG"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07050",
        "abstract url": "https://arxiv.org/abs/2412.07050",
        "title": "Advancing clinical trial outcomes using deep learning and predictive modelling: bridging precision medicine and patient-centered care",
        "rating": "-3.5",
        "keywords": [
            [
                "healthcare",
                "survival",
                "clinical"
            ],
            [
                "forecast"
            ],
            [
                "cs.LG"
            ]
        ],
        "abstract": "The integration of artificial intelligence [AI] into clinical trials has revolutionized the process of drug development and personalized medicine. Among these advancements, deep learning and predictive modelling have emerged as transformative tools for optimizing clinical trial design, patient recruitment, and real-time monitoring. This study explores the application of deep learning techniques, such as convolutional neural networks [CNNs] and transformerbased models, to stratify patients, forecast adverse events, and personalize treatment plans. Furthermore, predictive modelling approaches, including survival analysis and time-series forecasting, are employed to predict trial outcomes, enhancing efficiency and reducing trial failure rates. To address challenges in analysing unstructured clinical data, such as patient notes and trial protocols, natural language processing [NLP] techniques are utilized for extracting actionable insights. A custom dataset comprising structured patient demographics, genomic data, and unstructured text is curated for training and validating these models. Key metrics, including precision, recall, and F1 scores, are used to evaluate model performance, while trade-offs between accuracy and computational efficiency are examined to identify the optimal model for clinical deployment. This research underscores the potential of AI-driven methods to streamline clinical trial workflows, improve patient-centric outcomes, and reduce costs associated with trial inefficiencies. The findings provide a robust framework for integrating predictive analytics into precision medicine, paving the way for more adaptive and efficient clinical trials. By bridging the gap between technological innovation and real-world applications, this study contributes to advancing the role of AI in healthcare, particularly in fostering personalized care and improving overall trial success rates.",
        "subjects": [
            "cs.LG"
        ],
        "comment": "22 pages excluding references, 11 figures, 6 tables"
    },
    {
        "paper id": "2412.06282",
        "abstract url": "https://arxiv.org/abs/2412.06282",
        "title": "Recent development of optical electric current transformer and its obstacles",
        "rating": "-4",
        "keywords": [
            [
                "thermal"
            ],
            [
                "forecasting"
            ]
        ],
        "abstract": "Conventional electromagnetic induction-based current transformers suffer from issues such as bulky and complex structures, slow response times, and low safety levels. Consequently, researchers have explored combining various sensing technologies with optical fibers to develop optical current transformers that could become the primary choice for power systems in the future. With the maturation of optoelectronic technology, optical current transformers have emerged. They offer outstanding advantages, including high sensitivity, integration, stability, and the ability to operate in complex environments. This review categorizes optical current transformers based on different principles, including all-fiber current transformers, those based on magnetostrictive effects, magneto-optic effects, and thermal effects. It also discusses their principles, structures, manufacturing techniques, and signal processing, while forecasting their future development trends.",
        "subjects": [
            "physics.optics",
            "eess.SP",
            "eess.SY",
            "physics.app-ph"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06355",
        "abstract url": "https://arxiv.org/abs/2412.06355",
        "title": "Flexible and Scalable Deep Dendritic Spiking Neural Networks with Multiple Nonlinear Branching",
        "rating": "-4",
        "keywords": [
            [
                "depth"
            ],
            [
                "attacks"
            ],
            [
                "bio-plausible"
            ]
        ],
        "abstract": "Recent advances in spiking neural networks (SNNs) have a predominant focus on network architectures, while relatively little attention has been paid to the underlying neuron model. The point neuron models, a cornerstone of deep SNNs, pose a bottleneck on the network-level expressivity since they depict somatic dynamics only. In contrast, the multi-compartment models in neuroscience offer remarkable expressivity by introducing dendritic morphology and dynamics, but remain underexplored in deep learning due to their unaffordable computational cost and inflexibility. To combine the advantages of both sides for a flexible, efficient yet more powerful model, we propose the dendritic spiking neuron (DendSN) incorporating multiple dendritic branches with nonlinear dynamics. Compared to the point spiking neurons, DendSN exhibits significantly higher expressivity. DendSN's flexibility enables its seamless integration into diverse deep SNN architectures. To accelerate dendritic SNNs (DendSNNs), we parallelize dendritic state updates across time steps, and develop Triton kernels for GPU-level acceleration. As a result, we can construct large-scale DendSNNs with depth comparable to their point SNN counterparts. Next, we comprehensively evaluate DendSNNs' performance on various demanding tasks. By modulating dendritic branch strengths using a context signal, catastrophic forgetting of DendSNNs is substantially mitigated. Moreover, DendSNNs demonstrate enhanced robustness against noise and adversarial attacks compared to point SNNs, and excel in few-shot learning settings. Our work firstly demonstrates the possibility of training bio-plausible dendritic SNNs with depths and scales comparable to traditional point SNNs, and reveals superior expressivity and robustness of reduced dendritic neuron models in deep learning, thereby offering a fresh perspective on advancing neural network design.",
        "subjects": [
            "cs.NE"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06602",
        "abstract url": "https://arxiv.org/abs/2412.06602",
        "title": "Towards Controllable Speech Synthesis in the Era of Large Language Models: A Survey",
        "rating": "-4",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "industrial"
            ],
            [
                "Text-to-speech"
            ],
            [
                "cs.AI",
                "cs.LG",
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        ],
        "abstract": "Text-to-speech (TTS), also known as speech synthesis, is a prominent research area that aims to generate natural-sounding human speech from text. Recently, with the increasing industrial demand, TTS technologies have evolved beyond synthesizing human-like speech to enabling controllable speech generation. This includes fine-grained control over various attributes of synthesized speech such as emotion, prosody, timbre, and duration. Besides, advancements in deep learning, such as diffusion and large language models, have significantly enhanced controllable TTS over the past several years. In this paper, we conduct a comprehensive survey of controllable TTS, covering approaches ranging from basic control techniques to methods utilizing natural language prompts, aiming to provide a clear understanding of the current state of research. We examine the general controllable TTS pipeline, challenges, model architectures, and control strategies, offering a comprehensive and clear taxonomy of existing methods. Additionally, we provide a detailed summary of datasets and evaluation metrics and shed some light on the applications and future directions of controllable TTS. To the best of our knowledge, this survey paper provides the first comprehensive review of emerging controllable TTS methods, which can serve as a beneficial resource for both academic researchers and industry practitioners.",
        "subjects": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "cs.MM",
            "cs.SD",
            "eess.AS"
        ],
        "comment": "A comprehensive survey on controllable TTS, 23 pages, 6 tables, 4 figures, 280 references"
    },
    {
        "paper id": "2412.06753",
        "abstract url": "https://arxiv.org/abs/2412.06753",
        "title": "InstantRestore: Single-Step Personalized Face Restoration with Shared-Image Attention",
        "rating": "-4",
        "keywords": [
            [
                "diffusion"
            ],
            [
                "facial"
            ],
            [
                "image restoration"
            ],
            [
                "cs.CV"
            ]
        ],
        "abstract": "Face image restoration aims to enhance degraded facial images while addressing challenges such as diverse degradation types, real-time processing demands, and, most crucially, the preservation of identity-specific features. Existing methods often struggle with slow processing times and suboptimal restoration, especially under severe degradation, failing to accurately reconstruct finer-level identity details. To address these issues, we introduce InstantRestore, a novel framework that leverages a single-step image diffusion model and an attention-sharing mechanism for fast and personalized face restoration. Additionally, InstantRestore incorporates a novel landmark attention loss, aligning key facial landmarks to refine the attention maps, enhancing identity preservation. At inference time, given a degraded input and a small (~4) set of reference images, InstantRestore performs a single forward pass through the network to achieve near real-time performance. Unlike prior approaches that rely on full diffusion processes or per-identity model tuning, InstantRestore offers a scalable solution suitable for large-scale applications. Extensive experiments demonstrate that InstantRestore outperforms existing methods in quality and speed, making it an appealing choice for identity-preserving face restoration.",
        "subjects": [
            "cs.CV"
        ],
        "comment": "Project page: https://snap-research.github.io/InstantRestore/"
    },
    {
        "paper id": "2412.07058",
        "abstract url": "https://arxiv.org/abs/2412.07058",
        "title": "Random regular graph states are complex at almost any depth",
        "rating": "-4",
        "keywords": [
            [
                "depth"
            ],
            [
                "graph"
            ],
            [
                "quantum"
            ]
        ],
        "abstract": "Graph states are fundamental objects in the theory of quantum information due to their simple classical description and rich entanglement structure. They are also intimately related to IQP circuits, which have applications in quantum pseudorandomness and quantum advantage. For us, they are a toy model to understand the relation between circuit connectivity, entanglement structure and computational complexity. In the worst case, a strict dichotomy in the computational universality of such graph states appears as a function of the degree $d$ of a regular graph state [GDH+23]. In this paper, we initiate the study of the average-case complexity of simulating random graph states of varying degree when measured in random product bases and give distinct evidence that a similar complexity-theoretic dichotomy exists in the average case. Specifically, we consider random $d$-regular graph states and prove three distinct results: First, we exhibit two families of IQP circuits of depth $d$ and show that they anticoncentrate for any $2 < d = o(n)$ when measured in a random $X$-$Y$-plane product basis. This implies anticoncentration for random constant-regular graph states. Second, in the regime $d = \u0398(n^c)$ with $c \\in (0,1)$, we prove that random $d$-regular graph states contain polynomially large grid graphs as induced subgraphs with high probability. This implies that they are universal resource states for measurement-based computation. Third, in the regime of high degree ($d\\sim n/2$), we show that random graph states are not sufficiently entangled to be trivially classically simulable, unlike Haar random states. Proving the three results requires different techniques--the analysis of a classical statistical-mechanics model using Krawtchouck polynomials, graph theoretic analysis using the switching method, and analysis of the ranks of submatrices of random adjacency matrices, respectively.",
        "subjects": [
            "quant-ph",
            "cs.CC",
            "math.CO"
        ],
        "comment": "many pages, lots of counting, lovely figures"
    },
    {
        "paper id": "2412.07166",
        "abstract url": "https://arxiv.org/abs/2412.07166",
        "title": "equilibrium-c: A Lightweight Modern Equilibrium Chemistry Calculator for Hypersonic Flow Applications",
        "rating": "-4",
        "keywords": [
            [
                "Chemistry",
                "chemical"
            ],
            [
                "physics"
            ]
        ],
        "abstract": "equilibrium-c (eqc) is a program for computing the composition of gas mixtures in chemical equilibrium. In typical usage, the program is given a known thermodynamic state, such as fixed temperature and pressure, as well as an initial composition of gaseous species, and computes the final composition in the limit of a large amount of time relative to the reaction speeds. eqc includes a database of thermodynamic properties taken from the literature, a set of core routines written the C programming language to solve the equilibrium problems, and a Python wrapper layer to organise the solution process and interface with user code. Dependencies are extremely minimal, and the API is designed to be easily embedded in multi-physics codes that solve problems in fluid dynamics, combustion, and chemical processing. In this paper, I first introduce the equations of chemical equilibrium, then spend some time discussing their numerical solution, and finally present a series of example problems, with an emphasis on verification and validation of the solver.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06220",
        "abstract url": "https://arxiv.org/abs/2412.06220",
        "title": "Discrete-Time Distribution Steering using Monte Carlo Tree Search",
        "rating": "-10",
        "keywords": [],
        "abstract": "Optimal control problems with state distribution constraints have attracted interest for their expressivity, but solutions rely on linear approximations. We approach the problem of driving the state of a dynamical system in distribution from a sequential decision-making perspective. We formulate the optimal control problem as an appropriate Markov decision process (MDP), where the actions correspond to the state-feedback control policies. We then solve the MDP using Monte Carlo tree search (MCTS). This renders our method suitable for any dynamics model. A key component of our approach is a novel, easy to compute, distance metric in the distribution space that allows our algorithm to guide the distribution of the state. We experimentally test our algorithm under both linear and nonlinear dynamics.",
        "subjects": [
            "eess.SY",
            "cs.RO"
        ],
        "comment": "Submitted to the IEEE Robotics and Automation Letters for possible publication"
    },
    {
        "paper id": "2412.06222",
        "abstract url": "https://arxiv.org/abs/2412.06222",
        "title": "Blotto on the Ballot: A Ballot Stuffing Blotto Game",
        "rating": "-10",
        "keywords": [],
        "abstract": "We consider the following Colonel Blotto game between parties $P_1$ and $P_A.$ $P_1$ deploys a non negative number of troops across $J$ battlefields, while $P_A$ chooses $K,$ $K < J,$ battlefields to remove all of $P_1$'s troops from the chosen battlefields. $P_1$ has the objective of maximizing the number of surviving troops while $P_A$ wants to minimize it. Drawing an analogy with ballot stuffing by a party contesting an election and the countermeasures by the Election Commission to negate that, we call this the Ballot Stuffing Game. For this zero-sum resource allocation game, we obtain the set of Nash equilibria as a solution to a convex combinatorial optimization problem. We analyze this optimization problem and obtain insights into the several non trivial features of the equilibrium behavior. These features in turn allows to describe the structure of the solutions and efficient algorithms to obtain then. The model is described as ballot stuffing game in a plebiscite but has applications in security and auditing games. The results are extended to a parliamentary election model. Numerical examples illustrate applications of the game.",
        "subjects": [
            "cs.GT",
            "econ.TH"
        ],
        "comment": "11 pages, 5 figures"
    },
    {
        "paper id": "2412.06223",
        "abstract url": "https://arxiv.org/abs/2412.06223",
        "title": "On low-power error-correcting cooling codes with large distances",
        "rating": "-10",
        "keywords": [],
        "abstract": "A low-power error-correcting cooling (LPECC) code was introduced as a coding scheme for communication over a bus by Chee et al. to control the peak temperature, the average power consumption of on-chip buses, and error-correction for the transmitted information, simultaneously. Specifically, an $(n, t, w, e)$-LPECC code is a coding scheme over $n$ wires that avoids state transitions on the $t$ hottest wires and allows at most $w$ state transitions in each transmission, and can correct up to $e$ transmission errors. In this paper, we study the maximum possible size of an $(n, t, w, e)$-LPECC code, denoted by $C(n,t,w,e)$. When $w=e+2$ is large, we establish a general upper bound $C(n,t,w,w-2)\\leq \\lfloor \\binom{n+1}{2}/\\binom{w+t}{2}\\rfloor$; when $w=e+2=3$, we prove $C(n,t,3,1) \\leq \\lfloor \\frac{n(n+1)}{6(t+1)}\\rfloor$. Both bounds are tight for large $n$ satisfying some divisibility conditions. Previously, tight bounds were known only for $w=e+2=3,4$ and $t\\leq 2$. In general, when $w=e+d$ is large for a constant $d$, we determine the asymptotic value of $C(n,t,w,w-d)\\sim \\binom{n}{d}/\\binom{w+t}{d}$ as $n$ goes to infinity, which can be extended to $q$-ary codes.",
        "subjects": [
            "cs.IT",
            "math.CO"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06238",
        "abstract url": "https://arxiv.org/abs/2412.06238",
        "title": "My 10-Day App Crash Course in China: An Autoethnography",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper presents an autoethnography of my recent trip to China, during which I engaged in using various apps and discovered the cultural and social norms embedded in everyday mobile app use. Navigating between Western and Chinese cultures, my experience was simultaneously exhilarating, embarrassing, and bewildering. Through this autoethnography, I aim to illustrate how I adjusted to Chinese technological norms, usage patterns, and interactions during my initial stay, and to offer observations on the technosocial differences related to smartphone apps in both cultures. Using descriptions and summative analyses, I identified four meaningful themes: 1) smartphones as the backbone for modern living in China, 2) smartphone attachment, 3) the superapps, and 4) the intricate web of Chinese technosocial norms governing everyday usage. Taken together, these findings highlight how cultural and societal differences shape app design and user experiences, consequently influencing how travelers and expatriates adapt to an increasingly digitalized world.",
        "subjects": [
            "cs.HC"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06241",
        "abstract url": "https://arxiv.org/abs/2412.06241",
        "title": "Plagiarism Detection Using Machine Learning",
        "rating": "-10",
        "keywords": [],
        "abstract": "Plagiarism is an act of using someone else's work without proper acknowledgment, and this sin is seen to cut across various arenas including the academy, publishing, and other similar arenas. The traditional methods of plagiarism detection through keyword matching and review by humans usually fail to cope with increasingly sophisticated techniques used to mask copy pasted content. This paper aims to introduce a plagiarism detection approach based on machine learning that utilizes natural language processing and complex classification algorithms toward efficient detection of similarities between the documents. The developed model has the capability to detect both exact and paraphrased plagiarism accurately using advanced feature extraction techniques with supervised learning algorithms. We adapted and tested our model on an extensive text sample dataset. And we demonstrated some promising results about precision, recall, and detection accuracy. These outcomes showed that applying machine learning techniques can significantly enhance the functionalities of plagiarism detection systems and improve traditional ones with robust scalability. Future work would include enlargement of the dataset and fine-tuning of the model toward more complicated cases of disguised plagiarism.",
        "subjects": [
            "cs.ET",
            "cs.CC"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06269",
        "abstract url": "https://arxiv.org/abs/2412.06269",
        "title": "Schema Evolution in Interactive Programming Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Many improvements to programming have come from shortening feedback loops, for example with Integrated Development Environments, Unit Testing, Live Programming, and Distributed Version Control. A barrier to feedback that deserves greater attention is Schema Evolution. When requirements on the shape of data change then existing data must be migrated into the new shape, and existing code must be modified to suit. Currently these adaptations are often performed manually, or with ad hoc scripts. Manual schema evolution not only delays feedback but since it occurs outside the purview of version control tools it also interrupts collaboration. Schema evolution has long been studied in databases. We observe that the problem also occurs in non-database contexts that have been less studied. We present a suite of challenge problems exemplifying this range of contexts, including traditional database programming as well as live front-end programming, model-driven development, and collaboration in computational documents. We systematize these various contexts by defining a set of layers and dimensions of schema evolution. We offer these challenge problems to ground future research on the general problem of schema evolution in interactive programming systems and to serve as a basis for evaluating the results of that research. We hope that better support for schema evolution will make programming more live and collaboration more fluid.",
        "subjects": [
            "cs.PL"
        ],
        "comment": "arXiv admin note: text overlap with arXiv:2309.11406"
    },
    {
        "paper id": "2412.06274",
        "abstract url": "https://arxiv.org/abs/2412.06274",
        "title": "Does Task Complexity Moderate the Benefits of Liveness? A Controlled Experiment",
        "rating": "-10",
        "keywords": [],
        "abstract": "Live programming features can be found in a range of programming environments, from individual prototypes to widely used environments. While liveness is generally considered a useful property, there is little empirical evidence on when and how liveness can be beneficial. Even though there are few experimental studies, their results are largely inconclusive. We reviewed existing experiments and related studies to gather a collection of potential effects of liveness and moderating factors. Based on this collection, we concluded that **task complexity** and **prior experience addressing liveness** are potentially essential factors neglected in previous experiments. To fill this gap, we devised and conducted a controlled experiment (N = 37) testing the hypothesis that task complexity moderates the effects of live introspection tools on participants? debugging efficiency, given participants with prior experience with liveness. Our results do not support the hypothesis that task complexity moderates the effect of live introspection tools. This non-significant moderation effect might result from the low number of participants, as the data suggests a moderation effect. The results also show that in our experiment setting, live introspection tools significantly reduced the time participants took to debug the tasks. For researchers interested in the effects of liveness, our findings suggest that studies on liveness should make conscious choices about task complexity and participants' prior experience with liveness. For designers of programming environments, the results of our experiment are a step toward understanding when and how programming tools should be live to become more helpful to programmers.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06283",
        "abstract url": "https://arxiv.org/abs/2412.06283",
        "title": "A lower bound on the state complexity of transforming two-way nondeterministic finite automata to unambiguous finite automata",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper establishes a lower bound on the number of states necessary in the worst case to simulate an $n$-state two-way nondeterministic finite automaton (2NFA) by a one-way unambiguous finite automaton (UFA). It is proved that for every $n$, there is a language recognized by an $n$-state 2NFA that requires a UFA with at least $\\sum_{k=1}^{n} (k - 1)! \\cdot k! \\cdot \\mathrm{stirling2}(n, k) \\cdot \\mathrm{stirling2}(n+1, k)$ = $\u03a9\\big( n^{2n+2} / e^{2n} \\big)$ states, where $\\mathrm{stirling2}(n, k)$ denotes Stirling's numbers of the second kind. This result is proved by estimating the rank of a certain matrix, which is constructed for the universal language for $n$-state 2NFAs, and describes every possible behaviour of these automata during their computation.",
        "subjects": [
            "cs.FL"
        ],
        "comment": "31 pages, 7 figures"
    },
    {
        "paper id": "2412.06290",
        "abstract url": "https://arxiv.org/abs/2412.06290",
        "title": "The Hybrid ROA: A Flexible and Scalable Encoding Scheme for Route Origin Authorization",
        "rating": "-10",
        "keywords": [],
        "abstract": "On top of the Resource Public Key Infrastructure (RPKI), the Route Origin Authorization (ROA) creates a cryptographically verifiable binding of an autonomous system to a set of IP prefixes it is authorized to originate. By their design, ROAs can protect the inter-domain routing system against prefix and sub-prefix hijacks. However, it is hard for the state-of-the-art approach, the maxLength-based ROA encoding scheme, to guarantee security and scalability at the same time when facing various authorization scenarios. To this end, we propose a novel bitmap-based encoding scheme for ROAs to provide flexible and controllable compression. Furthermore, the hybrid ROA encoding scheme (h-ROA) is proposed, which encodes ROAs based on maxLength and bitmap jointly. This approach ensures strong security, provides flexibility and significantly improves system scalability, enabling it to effectively handle various authorization patterns. According to the performance evaluation with real-world data sets, h-ROA outperforms the state-of-the-art approach $1.99 \\sim 3.28$ times in terms of the encoding speed, and it can reduce the cost of a router to synchronize all validated ROA payloads by $43.9\\% \\sim 56.6\\%$.",
        "subjects": [
            "cs.CR"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06294",
        "abstract url": "https://arxiv.org/abs/2412.06294",
        "title": "Beyond pip install: Evaluating LLM Agents for the Automated Installation of Python Projects",
        "rating": "-10",
        "keywords": [],
        "abstract": "Many works have recently proposed the use of Large Language Model (LLM) based agents for performing `repository level' tasks, loosely defined as a set of tasks whose scopes are greater than a single file. This has led to speculation that the orchestration of these repository-level tasks could lead to software engineering agents capable of performing almost independently of human intervention. However, of the suite of tasks that would need to be performed by this autonomous software engineering agent, we argue that one important task is missing, which is to fulfil project level dependency by installing other repositories. To investigate the feasibility of this repository level installation task, we introduce a benchmark of of repository installation tasks curated from 40 open source Python projects, which includes a ground truth installation process for each target repository. Further, we propose Installamatic, an agent which aims to perform and verify the installation of a given repository by searching for relevant instructions from documentation in the repository. Empirical experiments reveal that that 55% of the studied repositories can be automatically installed by our agent at least one out of ten times. Through further analysis, we identify the common causes for our agent's inability to install a repository, discuss the challenges faced in the design and implementation of such an agent and consider the implications that such an agent could have for developers.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06321",
        "abstract url": "https://arxiv.org/abs/2412.06321",
        "title": "A Flexible Template for Edge Generative AI with High-Accuracy Accelerated Softmax & GELU",
        "rating": "-10",
        "keywords": [],
        "abstract": "Transformer-based generative Artificial Intelligence (GenAI) models achieve remarkable results in a wide range of fields, including natural language processing, computer vision, and audio processing. However, this comes at the cost of increased complexity and the need of sophisticated non-linearities such as softmax and GELU. Even if Transformers are computationally dominated by matrix multiplications (MatMul), these non-linearities can become a performance bottleneck, especially if dedicated hardware is used to accelerate MatMul operators. In this work, we introduce a GenAI BFloat16 Transformer acceleration template based on a heterogeneous tightly-coupled cluster containing 256KiB of shared SRAM, 8 general-purpose RISC-V cores, a 24x8 systolic array MatMul accelerator, and a novel accelerator for Transformer softmax and GELU non-linearities: SoftEx. SoftEx introduces an approximate exponentiation algorithm balancing efficiency (121x speedup over glibc's implementation) with accuracy (mean relative error of 0.14%). In 12nm technology, SoftEx occupies 0.039 mm$^2$, only 3.22% of the cluster, which achieves an operating frequency of 1.12 GHz. Compared to optimized software running on the RISC-V cores, SoftEx achieves significant improvements, accelerating softmax and GELU computations by up to 10.8x and 5.11x, respectively, while reducing their energy consumption by up to 10.8x and 5.29x. These enhancements translate into a 1.58x increase in throughput (310 GOPS at 0.8V) and a 1.42x improvement in energy efficiency (1.34 TOPS/W at 0.55V) on end-to-end ViT inference workloads.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06361",
        "abstract url": "https://arxiv.org/abs/2412.06361",
        "title": "weberknecht -- a One-Sided Crossing Minimization solver",
        "rating": "-10",
        "keywords": [],
        "abstract": "We describe the implementation of the exact solver weberknecht and the heuristic solver weberknecht_h for the One-Sided Crossing Minimization problem.",
        "subjects": [
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06373",
        "abstract url": "https://arxiv.org/abs/2412.06373",
        "title": "Noise Covariances Identification by MDM: Weighting, Recursion, and Implementation",
        "rating": "-10",
        "keywords": [],
        "abstract": "The problem of noise covariance matrix identification of stochastic linear time-varying state-space models is addressed. The measurement difference method (MDM) is generalized to time-varying dimensions of the measurement and control. Three MDM identification techniques that differ in weighting used in the underlying least squares method are proposed. The techniques differ in estimate quality and computational complexity. In addition, recursive forms are designed for two techniques. The performance of the proposed techniques is analyzed using two numerical examples. The implementation of techniques is enclosed with the paper.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Accepted for IFAC SYSID24"
    },
    {
        "paper id": "2412.06376",
        "abstract url": "https://arxiv.org/abs/2412.06376",
        "title": "Data-Augmented Numerical Integration in State Prediction: Rule Selection",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper deals with the state prediction of nonlinear stochastic dynamic systems. The emphasis is laid on a solution to the integral Chapman-Kolmogorov equation by a deterministic-integration-rule-based point-mass method. A novel concept of reliable data-augmented, i.e., mathematics- and data-informed, integration rule is developed to enhance the point-mass state predictor, where the trained neural network (representing data contribution) is used for the selection of the best integration rule from a set of available rules (representing mathematics contribution). The proposed approach combining the best properties of the standard mathematics-informed and novel data-informed rules is thoroughly discussed.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "Submitted to IFAC SYSID24"
    },
    {
        "paper id": "2412.06391",
        "abstract url": "https://arxiv.org/abs/2412.06391",
        "title": "Owi: Performant Parallel Symbolic Execution Made Easy, an Application to WebAssembly",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this paper, we present the design of Owi, a symbolic interpreter for WebAssembly written in OCaml, and how we used it to create a state-of-the-art tool to find bugs in programs combining C and Rust code. WebAssembly (Wasm) is a binary format for executable programs. Originally intended for web applications, Wasm is also considered a serious alternative for server-side runtimes and embedded systems due to its performance and security benefits. Despite its security guarantees and sandboxing capabilities, Wasm code is still vulnerable to buffer overflows and memory leaks, which can lead to exploits on production software. To help prevent those, different techniques can be used, including symbolic execution. Owi is built around a modular, monadic interpreter capable of both normal and symbolic execution of Wasm programs. Monads have been identified as a way to write modular interpreters since 1995 and this strategy has allowed us to build a robust and performant symbolic execution tool which our evaluation shows to be the best currently available for Wasm. Moreover, because WebAssembly is a compilation target for multiple languages (such as Rust and C), Owi can be used to find bugs in C and Rust code, as well as in codebases mixing the two. We demonstrate this flexibility through illustrative examples and evaluate its scalability via comprehensive experiments using the 2024 Test-Comp benchmarks. Results show that Owi achieves comparable performance to state-of-the-art tools like KLEE and Symbiotic, and exhibits advantages in specific scenarios where KLEE's approximations could lead to false negatives.",
        "subjects": [
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06401",
        "abstract url": "https://arxiv.org/abs/2412.06401",
        "title": "Memory-Based Control with Event-Triggered Protocol for interval type-2 fuzzy network system under fading channel",
        "rating": "-10",
        "keywords": [],
        "abstract": "To address the challenges in networked environments and control problems associated with complex nonlinear uncertain systems, this paper investigates the design of a membership-function-dependent (MFD) memory output-feedback (MOF) controller for interval type-2 (IT2) fuzzy systems under fading channels, leveraging a memory dynamic event-triggering mechanism (MDETM). To conserve communication resources, MDETM reduces the frequency of data transmission. For mitigating design conservatism, a MOF controller is employed. A stochastic process models the fading channel, accounting for phenomena such as reflection, refraction, and diffraction that occur during data packet transmission through networks. An actuator failure model addresses potential faults and inaccuracies in practical applications. Considering the impacts of channel fading and actuator failures, the non-parallel distributed compensation (non-PDC) strategy enhances the robustness and anti-interference capability of the MDETM MOF controller. By fully exploiting membership function information, novel MFD control design results ensure mean-square exponential stability and $\\mathscr H_{\\infty}$ performance $\u03b3$ for the closed-loop system. Simulation studies validate the effectiveness of the proposed approach.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06407",
        "abstract url": "https://arxiv.org/abs/2412.06407",
        "title": "Normal Nested Answer Set Programs: Syntactics, Semantics and Logical Calculi",
        "rating": "-10",
        "keywords": [],
        "abstract": "Nested answer set programming (NASP; Lifschitz et al., 1999) generalizes answer set programming (ASP) by admitting nested expressions in rule bodies and heads, and thus, NASP aims at exploiting program succinctness. Yet, although NASP expressiveness is undoubtedly superior to ASP one, the former's reasoning capabilities remain unexplored. This reality seems subsequent to the next existing wide-ranging gap: normal nested programs (NNPs) are not known, or in other words, the nested normal-disjunctive boundary is unidentified thus far. Such an unfavorable situation is yet antagonistic to that of ASP as its normal programs (NPs) have been vital for propelling ASP. We will fill such a gap by defining the NNPs, their semantics and their associated nested logical calculi. Besides, while the unique known way to compute nested programs is unfold them back, we propose to do so in their original form. Firstly, we give the syntax of NNPs. For that, we initially define the positive-Horn nested-expressions and then an NNP rule as one whose head (resp. body) is a positive-Horn (resp. general) nested-expression. Secondly, we set up the semantics of NNPs by lifting to the nesting level, classical NP notions including: answer set, minimal and least model, closedness, supported-ness, immediate consequence operator and program consistency. We besides show that NNP restricted to ASP coincides with NP. Thirdly, we introduce nested logical calculi, concretely, nested unit-resolution and nested hyper unit-resolution, proving that they recover unit-resolution and hyper unit-resolution in the ASP setting. We also show how both nested logical calculi allow to process the least model of not-free NNP programs. To end, we demonstrate that computing answer sets of (resp. not-free) NNP programs is (resp. P-complete) NP-complete.",
        "subjects": [
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06421",
        "abstract url": "https://arxiv.org/abs/2412.06421",
        "title": "Physical Layer Security in AmBC-NOMA Networks with Random Eavesdroppers",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this work, we investigate the physical layer security (PLS) of ambient backscatter communication non-orthogonal multiple access (AmBC-NOMA) networks where non-colluding eavesdroppers (Eves) are randomly distributed. In the proposed system, a base station (BS) transmits a superimposed signal to a typical NOMA user pair, while a backscatter device~(BD) simultaneously transmits its unique signal by reflecting and modulating the BS's signal. Meanwhile, Eves passively attempt to wiretap the ongoing transmissions. Notably, the number and locations of Eves are unknown, posing a substantial security threat to the system. To address this challenge, the BS injects artificial noise (AN) to mislead the Eves, and a protected zone is employed to create an Eve-exclusion area around the BS. Theoretical expressions for outage probability (OP) and intercept probability (IP) are provided to evaluate the system's reliability-security trade-off. Asymptotic behavior at high signal-to-noise ratio (SNR) is further explored, including the derivation of diversity orders for the OP. Numerical results validate the analytical findings through extensive simulations, demonstrating that both the AN injection and protected zone can effectively enhance PLS. Furthermore, analysis and insights of different key parameters, including transmit SNR, reflection efficiency at the BD, power allocation coefficient, power fraction allocated to desired signal, Eve-exclusion area radius, Eve distribution density, and backscattered AN cancellation efficiency, on OP and IP are also provided.",
        "subjects": [
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06431",
        "abstract url": "https://arxiv.org/abs/2412.06431",
        "title": "A Program Instrumentation Framework for Automatic Verification",
        "rating": "-10",
        "keywords": [],
        "abstract": "In deductive verification and software model checking, dealing with certain specification language constructs can be problematic when the back-end solver is not sufficiently powerful or lacks the required theories. One way to deal with this is to transform, for verification purposes, the program to an equivalent one not using the problematic constructs, and to reason about this equivalent program instead. In this article, we propose program instrumentation as a unifying verification paradigm that subsumes various existing ad-hoc approaches, has a clear formal correctness criterion, can be applied automatically, and can transfer back witnesses and counterexamples. We illustrate our approach on the automated verification of programs that involve quantification and aggregation operations over arrays, such as the maximum value or sum of the elements in a given segment of the array, which are known to be difficult to reason about automatically. We implement our approach in the MonoCera tool, which is tailored to the verification of programs with aggregation, and evaluate it on example programs, including SV-COMP programs.",
        "subjects": [
            "cs.LO"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2306.00004"
    },
    {
        "paper id": "2412.06444",
        "abstract url": "https://arxiv.org/abs/2412.06444",
        "title": "The Complexity of Tullock Contests",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper investigates the algorithmic complexity of computing the pure Nash Equilibrium (PNE) in Tullock contests. A key aspect of this analysis lies in the elasticity parameter $r_i$, which dictates whether a contestant $i$'s cost function is convex, concave, or neither. Our primary contribution is the identification of how the domains of $r_i$ govern the computational complexity of solving Tullock contests. Specifically, we establish the following results: - Efficient Algorithms for $r_i \\notin (1,2]$: When no contestant's elasticity parameter $r_i$ lies within the interval $(1,2]$, we show that an efficient algorithm can be designed to compute the pure Nash Equilibrium. - Hardness Results for $r_i \\in (1,2]$: When many $r_i$ values fall within the interval $(1,2]$, we prove that determining the existence of a pure Nash Equilibrium cannot be achieved in polynomial time, assuming the Exponential Time Hypothesis (ETH). - Approximation Algorithms for $r_i \\in (1,2]$: In cases where many $r_i$ values fall within the interval $(1,2]$, we propose a Fully Polynomial-Time Approximation Scheme (FPTAS) to compute an $\u03b5$-approximate PNE, provided an exact PNE exists. All our algorithms are implemented efficiently to handle large-scale instances. Computational experiments validate their effectiveness, even under challenging scenarios with complex elasticity distributions.",
        "subjects": [
            "cs.GT"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06468",
        "abstract url": "https://arxiv.org/abs/2412.06468",
        "title": "How many continuous measurements are needed to learn a vector?",
        "rating": "-10",
        "keywords": [],
        "abstract": "One can recover vectors from $\\mathbb{R}^m$ with arbitrary precision, using only $\\lceil \\log_2(m+1)\\rceil +1$ continuous measurements that are chosen adaptively. This surprising result is explained and discussed, and we present applications to infinite-dimensional approximation problems.",
        "subjects": [
            "math.NA",
            "cs.CC",
            "cs.IT"
        ],
        "comment": "12 pages"
    },
    {
        "paper id": "2412.06482",
        "abstract url": "https://arxiv.org/abs/2412.06482",
        "title": "A Cardinality-Constrained Approach to Combinatorial Bilevel Congestion Pricing",
        "rating": "-10",
        "keywords": [],
        "abstract": "Combinatorial bilevel congestion pricing (CBCP), a variant of the discrete network design problem, seeks to minimize the total travel time experienced by all travelers in a road network, by strategically selecting toll locations and determining the corresponding charges. Conventional wisdom suggests that these problems are intractable since they have to be formulated and solved with a significant number of integer variables. Here, we devise a scalable local algorithm for the CBCP problem that guarantees convergence to a Kuhn-Tucker-Karush point. Our approach is novel in that it eliminates the use of integer variables altogether, instead introducing a cardinality constraint that limits the number of toll locations to a user-specified upper bound. The resulting bilevel program with the cardinality constraint is then transformed into a block-separable, single-level optimization problem that can be solved efficiently after penalization and decomposition. We are able to apply the algorithm to solve, in about 20 minutes, a CBCP instance with up to 3,000 links, of which hundreds can be tolled. To the best of our knowledge, no existing algorithm can solve CBCP problems at such a scale while providing any assurance of convergence.",
        "subjects": [
            "math.OC",
            "cs.GT",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06485",
        "abstract url": "https://arxiv.org/abs/2412.06485",
        "title": "Fourier-enhanced reduced-order surrogate modeling for uncertainty quantification in electric machine design",
        "rating": "-10",
        "keywords": [],
        "abstract": "This work proposes a data-driven surrogate modeling framework for cost-effectively inferring the torque of a permanent magnet synchronous machine under geometric design variations. The framework is separated into a reduced-order modeling and an inference part. Given a dataset of torque signals, each corresponding to a different set of design parameters, torque dimension is first reduced by post-processing a discrete Fourier transform and keeping a reduced number of frequency components. This allows to take advantage of torque periodicity and preserve physical information contained in the frequency components. Next, a response surface model is computed by means of machine learning regression, which maps the design parameters to the reduced frequency components. The response surface models of choice are polynomial chaos expansions, feedforward neural networks, and Gaussian processes. Torque inference is performed by evaluating the response surface model for new design parameters and then inverting the dimension reduction. Numerical results show that the resulting surrogate models lead to sufficiently accurate torque predictions for previously unseen design configurations. The framework is found to be significantly advantageous compared to approximating the original (not reduced) torque signal directly, as well as slightly advantageous compared to using principal component analysis for dimension reduction. The combination of discrete Fourier transform-based dimension reduction with Gaussian process-based response surfaces yields the best-in-class surrogate model for this use case. The surrogate models replace the original, high-fidelity model in Monte Carlo-based uncertainty quantification studies, where they provide accurate torque statistics estimates at significantly reduced computational cost.",
        "subjects": [
            "cs.CE"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06509",
        "abstract url": "https://arxiv.org/abs/2412.06509",
        "title": "Reasoning about Strategic Abilities in Stochastic Multi-agent Systems",
        "rating": "-10",
        "keywords": [],
        "abstract": "Reasoning about strategic abilities is key to AI systems comprising multiple agents, which provide a unified framework for formalizing various problems in game theory, social choice theory, etc. In this work, we propose a probabilistic extension of the alternating-time $\u03bc$-calculus (AMC), named PAMC, for reasoning about the strategic abilities of agents in stochastic multi-agent systems. We show that PAMC subsumes two existing logics AMC and P$\u03bc$TL (a probabilistic extension of the modal $\u03bc$-calculus), but is incomparable with the probabilistic alternating-time temporal logic (PATL). We study the problems of model checking and satisfiability checking for PAMC. We first give a model checking algorithm by leveraging algorithms for solving normal-form games and AMC model checking. We establish that the model checking problem of PAMC remains in UP$\\cap$co-UP, the same complexity class as the model checking problem for AMC and P$\u03bc$TL. We also provide a new reduction from the satisfiability problem of PAMC to solving parity games, by which we obtain an EXPTIME decision procedure, as well as the small model property which allows us to construct a model for each satisfiable PAMC formula. Satisfiability in PAMC has the same complexity as in the modal $\u03bc$-calculus, unlike PCTL and PATL whose satisfiability checking problems remain open. We have implemented both the model checking and satisfiability checking algorithms as open-source tools. Experimental results are reported, showcasing the practical applications and effectiveness of our approaches.",
        "subjects": [
            "cs.LO",
            "cs.FL",
            "cs.GT"
        ],
        "comment": "40 pages"
    },
    {
        "paper id": "2412.06542",
        "abstract url": "https://arxiv.org/abs/2412.06542",
        "title": "Sequential Printed MLP Circuits for Super TinyML Multi-Sensory Applications",
        "rating": "-10",
        "keywords": [],
        "abstract": "Super-TinyML aims to optimize machine learning models for deployment on ultra-low-power application domains such as wearable technologies and implants. Such domains also require conformality, flexibility, and non-toxicity which traditional silicon-based systems cannot fulfill. Printed Electronics (PE) offers not only these characteristics, but also cost-effective and on-demand fabrication. However, Neural Networks (NN) with hundreds of features -- often necessary for target applications -- have not been feasible in PE because of its restrictions such as limited device count due to its large feature sizes. In contrast to the state of the art using fully parallel architectures and limited to smaller classifiers, in this work we implement a super-TinyML architecture for bespoke (application-specific) NNs that surpasses the previous limits of state of the art and enables NNs with large number of parameters. With the introduction of super-TinyML into PE technology, we address the area and power limitations through resource sharing with multi-cycle operation and neuron approximation. This enables, for the first time, the implementation of NNs with up to $35.9\\times$ more features and $65.4\\times$ more coefficients than the state of the art solutions.",
        "subjects": [
            "cs.AR"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06552",
        "abstract url": "https://arxiv.org/abs/2412.06552",
        "title": "Direct Sums for Parity Decision Trees",
        "rating": "-10",
        "keywords": [],
        "abstract": "Direct sum theorems state that the cost of solving $k$ instances of a problem is at least $\u03a9(k)$ times the cost of solving a single instance. We prove the first such results in the randomised parity decision tree model. We show that a direct sum theorem holds whenever (1) the lower bound for parity decision trees is proved using the discrepancy method; or (2) the lower bound is proved relative to a product distribution.",
        "subjects": [
            "cs.CC"
        ],
        "comment": "37 pages"
    },
    {
        "paper id": "2412.06564",
        "abstract url": "https://arxiv.org/abs/2412.06564",
        "title": "Applications and Implications of Large Language Models in Qualitative Analysis: A New Frontier for Empirical Software Engineering",
        "rating": "-10",
        "keywords": [],
        "abstract": "The use of large language models (LLMs) for qualitative analysis is gaining attention in various fields, including software engineering, where qualitative methods are essential for understanding human and social factors. This study aimed to investigate how LLMs are currently used in qualitative analysis and their potential applications in software engineering research, focusing on the benefits, limitations, and practices associated with their use. A systematic mapping study was conducted, analyzing 21 relevant studies to explore reported uses of LLMs for qualitative analysis. The findings indicate that LLMs are primarily used for tasks such as coding, thematic analysis, and data categorization, offering benefits like increased efficiency and support for new researchers. However, limitations such as output variability, challenges in capturing nuanced perspectives, and ethical concerns related to privacy and transparency were also identified. The study emphasizes the need for structured strategies and guidelines to optimize LLM use in qualitative research within software engineering, enhancing their effectiveness while addressing ethical considerations. While LLMs show promise in supporting qualitative analysis, human expertise remains crucial for interpreting data, and ongoing exploration of best practices will be vital for their successful integration into empirical software engineering research.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06567",
        "abstract url": "https://arxiv.org/abs/2412.06567",
        "title": "Integrating Positionality Statements in Empirical Software Engineering Research",
        "rating": "-10",
        "keywords": [],
        "abstract": "Positionality statements are a reflective practice established in fields such as social sciences, where they enhance transparency, reflexivity, and ethical integrity by acknowledging how researchers identities, experiences, and perspectives may shape their work. This study aimed to investigate the understanding, usage, and potential value of positionality statements in software engineering research, particularly in studies focused on diversity and inclusion.",
        "subjects": [
            "cs.SE"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06603",
        "abstract url": "https://arxiv.org/abs/2412.06603",
        "title": "Examining the Use and Impact of an AI Code Assistant on Developer Productivity and Experience in the Enterprise",
        "rating": "-10",
        "keywords": [],
        "abstract": "AI assistants are being created to help software engineers conduct a variety of coding-related tasks, such as writing, documenting, and testing code. We describe the use of the watsonx Code Assistant (WCA), an LLM-powered coding assistant deployed internally within IBM. Through surveys of two user cohorts (N=669) and unmoderated usability testing (N=15), we examined developers' experiences with WCA and its impact on their productivity. We learned about their motivations for using (or not using) WCA, we examined their expectations of its speed and quality, and we identified new considerations regarding ownership of and responsibility for generated code. Our case study characterizes the impact of an LLM-powered assistant on developers' perceptions of productivity and it shows that although such tools do often provide net productivity increases, these benefits may not always be experienced by all users.",
        "subjects": [
            "cs.HC",
            "cs.SE"
        ],
        "comment": "21 pages, 3 figures. To be published in CHI EA 2025"
    },
    {
        "paper id": "2412.06637",
        "abstract url": "https://arxiv.org/abs/2412.06637",
        "title": "FREYJA: Efficient Join Discovery in Data Lakes",
        "rating": "-10",
        "keywords": [],
        "abstract": "Data lakes are massive repositories of raw and heterogeneous data, designed to meet the requirements of modern data storage. Nonetheless, this same philosophy increases the complexity of performing discovery tasks to find relevant data for subsequent processing. As a response to these growing challenges, we present FREYJA, a modern data discovery system capable of effectively exploring data lakes, aimed at finding candidates to perform joins and increase the number of attributes for downstream tasks. More precisely, we want to compute rankings that sort potential joins by their relevance. Modern mechanisms apply advanced table representation learning (TRL) techniques to yield accurate joins. Yet, this incurs high computational costs when dealing with elevated volumes of data. In contrast to the state-of-the-art, we adopt a novel notion of join quality tailored to data lakes, which leverages syntactic measurements while achieving accuracy comparable to that of TRL approaches. To obtain this metric in a scalable manner we train a general purpose predictive model. Predictions are based, rather than on large-scale datasets, on data profiles, succinct representations that capture the underlying characteristics of the data. Our experiments show that our system, FREYJA, matches the results of the state-of-the-art whilst reducing the execution times by several orders of magnitude.",
        "subjects": [
            "cs.DB"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06652",
        "abstract url": "https://arxiv.org/abs/2412.06652",
        "title": "Institutional Shifts in Contribution to Indian Research Output during the last two decades",
        "rating": "-10",
        "keywords": [],
        "abstract": "In the past few decades, India has emerged as a major knowledge producer, with research output being contributed by a diverse set of institutions ranging from centrally funded to state funded, and from public funded to private funded institutions. A significant change has been witnessed in Indian institutional actors during the last two decades, with various new private universities being set up and several new IITs, NITs, IISERs being established. Therefore, it is important to identify whether the composition of the list of the top 100 research output producing institutions of India has changed significantly during the recent two decades. This study attempted to analyse the changes during the two 10-year periods (2004-13 and 2014-23). The institutions which retain their position within top 100 during both periods are identified, along with the change in their positions. Similarly, institutions that were there in top 100 list during first time period (2004-13) and go out of top 100 list during second time period (2014-23) are also identified. In the same line, the new entrant institutions in the top 100 list during second time period (2014-23) are identified too. The results obtained indicate towards an institutional shift in the contribution to Indian research output.",
        "subjects": [
            "cs.DL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06658",
        "abstract url": "https://arxiv.org/abs/2412.06658",
        "title": "A signal discovery step in interstellar communication",
        "rating": "-10",
        "keywords": [],
        "abstract": "Prior work using synchronized, geographically spaced radio telescopes, and a radio interferometer, suggests that narrow-bandwidth polarized pulse pair measurements repeatedly falsify a noise-cause hypothesis, given a prior celestial direction of interest. A four-step method was proposed, tested, and reported, using interferometer phase measurements, to seek common celestial directions among pulse pair components, during 92 days of observation. In the work reported here, the proposed four-step signal discovery method is simplified to have a single step. A 123.8 day interferometer experiment provides measurement evidence supporting a hypothesis that the prior direction of interest, and a second direction of interest, are associated with celestial coordinates. Each pointing direction measures statistical power at greater than six standard deviations, with some indications of associated interferometer-induced Right Ascension aliasing. Explanations are proposed and discussed.",
        "subjects": [
            "eess.SP",
            "astro-ph.IM"
        ],
        "comment": "13 pages, 21 figures"
    },
    {
        "paper id": "2412.06662",
        "abstract url": "https://arxiv.org/abs/2412.06662",
        "title": "Stochastic LQR Design With Disturbance Preview",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper considers the discrete-time, stochastic LQR problem with $p$ steps of disturbance preview information where $p$ is finite. We first derive the solution for this problem on a finite horizon with linear, time-varying dynamics and time-varying costs. Next, we derive the solution on the infinite horizon with linear, time-invariant dynamics and time-invariant costs. Our proofs rely on the well-known principle of optimality. We provide an independent proof for the principle of optimality that relies only on nested information structure. Finally, we show that the finite preview controller converges to the optimal noncausal controller as the preview horizon $p$ tends to infinity. We also provide a simple example to illustrate both the finite and infinite horizon results.",
        "subjects": [
            "math.OC",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06667",
        "abstract url": "https://arxiv.org/abs/2412.06667",
        "title": "Design Multiband Monopole and Microstrip Patch Antennas using High Frequency Structure Simulator",
        "rating": "-10",
        "keywords": [],
        "abstract": "This paper describes the design, construction and testing of a dual-band monopole microstrip patch antenna to operate in the GSM and DCS bands. For compactness the monopole antenna is mounted on top of a FR4 substrate and is designed as a planar structure which can be wrapped into a box like structure. The performance characteristics of the wrapped dual band monopole antenna are simulated on Ansoft HFSS before fabrication and testing on a Network Analyzer. The simulated HFSS return loss of the antenna shows broad agreement with experiment over the frequency range from 500MHz to 4GHz. Radiation patterns are generated in each band of operation of the antenna. The effect of feedline impedance matching on the performance of the monopole antenna is discussed.",
        "subjects": [
            "eess.SP",
            "physics.app-ph"
        ],
        "comment": "6 pages, 10 figures"
    },
    {
        "paper id": "2412.06683",
        "abstract url": "https://arxiv.org/abs/2412.06683",
        "title": "A Decoupled Channel Estimation Method for Beyond Diagonal RIS",
        "rating": "-10",
        "keywords": [],
        "abstract": "Beyond diagonal reconfigurable intelligent surface (BD-RIS) is a new architecture for RIS where elements are interconnected to provide more wave manipulation flexibility than traditional single connected RIS, enhancing data rate and coverage. However, channel estimation for BD-RIS is challenging due to the more complex multiple-connection structure involving their scattering elements. To address this issue, this paper proposes a decoupled channel estimation method for BD-RIS that yields separate estimates of the involved channels to enhance the accuracy of the overall combined channel by capitalizing on its Kronecker structure. Starting from a least squares estimate of the combined channel and by properly reshaping the resulting filtered signal, the proposed algorithm resorts to a Khatri-Rao Factorization (KRF) method that teases out the individual channels based on simple rank-one matrix approximation steps. Numerical results show that the proposed decoupled channel estimation yields more accurate channel estimates than the classical least squares scheme.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06702",
        "abstract url": "https://arxiv.org/abs/2412.06702",
        "title": "CHOICE: Coordinated Human-Object Interaction in Cluttered Environments for Pick-and-Place Actions",
        "rating": "-10",
        "keywords": [],
        "abstract": "Animating human-scene interactions such as pick-and-place tasks in cluttered, complex layouts is a challenging task, with objects of a wide variation of geometries and articulation under scenarios with various obstacles. The main difficulty lies in the sparsity of the motion data compared to the wide variation of the objects and environments as well as the poor availability of transition motions between different tasks, increasing the complexity of the generalization to arbitrary conditions. To cope with this issue, we develop a system that tackles the interaction synthesis problem as a hierarchical goal-driven task. Firstly, we develop a bimanual scheduler that plans a set of keyframes for simultaneously controlling the two hands to efficiently achieve the pick-and-place task from an abstract goal signal such as the target object selected by the user. Next, we develop a neural implicit planner that generates guidance hand trajectories under diverse object shape/types and obstacle layouts. Finally, we propose a linear dynamic model for our DeepPhase controller that incorporates a Kalman filter to enable smooth transitions in the frequency domain, resulting in a more realistic and effective multi-objective control of the character.Our system can produce a wide range of natural pick-and-place movements with respect to the geometry of objects, the articulation of containers and the layout of the objects in the scene.",
        "subjects": [
            "cs.GR",
            "cs.RO"
        ],
        "comment": "19 pages, 14 figures"
    },
    {
        "paper id": "2412.06706",
        "abstract url": "https://arxiv.org/abs/2412.06706",
        "title": "Asynchronous Agents with Perfect Recall: Model Reductions, Knowledge-Based Construction, and Model Checking for Coalitional Strategies",
        "rating": "-10",
        "keywords": [],
        "abstract": "Model checking of strategic abilities for agents with memory is a notoriously hard problem, and very few attempts have been made to tackle it. In this paper, we present two important steps towards this goal. First, we take the partial-order reduction scheme that was recently proved to preserve individual and coalitional abilities of memoryless agents, and show that it also works for agents with memory. Secondly, we take the Knowledge-Based Subset Construction, that was recently studied for synchronous concurrent games, and adapt it to preserve abilities of memoryful agents in asynchronous MAS. On the way, we also propose a new execution semantics for strategies in asynchronous MAS, that combines elements of Concurrent Game Structures and Interleaved Interpreted Systems in a natural and intuitive way.",
        "subjects": [
            "cs.MA",
            "cs.LO"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06713",
        "abstract url": "https://arxiv.org/abs/2412.06713",
        "title": "A Tensor-Structured Approach to Dynamic Channel Prediction for Massive MIMO Systems with Temporal Non-Stationarity",
        "rating": "-10",
        "keywords": [],
        "abstract": "In moderate- to high-mobility scenarios, channel state information (CSI) varies rapidly and becomes temporally non-stationary, leading to significant performance degradation in channel reciprocity-dependent massive multiple-input multiple-output (MIMO) transmission. To address this challenge, we propose a tensor-structured approach to dynamic channel prediction (TS-DCP) for massive MIMO systems with temporal non-stationarity, leveraging dual-timescale and cross-domain correlations. Specifically, due to the inherent spatial consistency, non-stationary channels on long-timescales are treated as stationary on short-timescales, decoupling complicated correlations into more tractable dual-timescale ones. To exploit such property, we frame the pilot symbols, capturing short-timescale correlations within frames by Doppler domain modeling and long-timescale correlations across frames by Markov/autoregressive processes. Based on this, we develop the tensor-structured signal model in the spatial-frequency-temporal domain, incorporating correlated angle-delay-Doppler domain channels and Vandermonde-structured factor matrices. Furthermore, we model cross-domain correlations within each frame, arising from clustered scatterer distributions, using tensor-structured upgradations of Markov processes and coupled Gaussian distributions. Following these probabilistic models, we formulate the TS-DCP as the variational free energy (VFE) minimization problem, designing trial belief structures through online approximation and the Bethe method. This yields the online TS-DCP algorithm derived from a dual-layer VFE optimization process, where both outer and inner layers leverage the multilinear structure of channels to reduce computational complexity significantly. Numerical simulations demonstrate the significant superiority of the proposed algorithm over benchmarks in terms of channel prediction performance.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "This work has been submitted to the IEEE for possible publication"
    },
    {
        "paper id": "2412.06725",
        "abstract url": "https://arxiv.org/abs/2412.06725",
        "title": "Harmonic Mean Density Fusion in Distributed Tracking: Performance and Comparison",
        "rating": "-10",
        "keywords": [],
        "abstract": "A distributed sensor fusion architecture is preferred in a real target-tracking scenario as compared to a centralized scheme since it provides many practical advantages in terms of computation load, communication bandwidth, fault-tolerance, and scalability. In multi-sensor target-tracking literature, such systems are better known by the pseudonym - track fusion, since processed tracks are fused instead of raw measurements. A fundamental problem, however, in such systems is the presence of unknown correlations between the tracks, which renders a standard Kalman filter (naive fusion) useless. A widely accepted solution is covariance intersection (CI) which provides near-optimal estimates but at the cost of a conservative covariance. Thus, the estimates are pessimistic, which might result in a delayed error convergence. Also, fusion of Gaussian mixture densities is an active area of research where standard methods of track fusion cannot be used. In this article, harmonic mean density (HMD) based fusion is discussed, which seems to handle both of these issues. We present insights on HMD fusion and prove that the method is a result of minimizing average Pearson divergence. This article also provides an alternative and easy implementation based on an importance-sampling-like method without the requirement of a proposal density. Similarity of HMD with inverse covariance intersection is an interesting find, and has been discussed in detail. Results based on a real-world multi-target multi-sensor scenario show that the proposed approach converges quickly than existing track fusion algorithms while also being consistent, as evident from the normalized estimation-error squared (NEES) plots.",
        "subjects": [
            "eess.SP",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06730",
        "abstract url": "https://arxiv.org/abs/2412.06730",
        "title": "A subgradient splitting algorithm for optimization on nonpositively curved metric spaces",
        "rating": "-10",
        "keywords": [],
        "abstract": "Many of the primal ingredients of convex optimization extend naturally from Euclidean to Hadamard spaces $\\unicode{x2014}$ nonpositively curved metric spaces like Euclidean, Hilbert, and hyperbolic spaces, metric trees, and more general CAT(0) cubical complexes. Linear structure, however, and the duality theory it supports are absent. Nonetheless, we introduce a new type of subgradient for convex functions on Hadamard spaces, based on Busemann functions. This notion supports a splitting subgradient method with guaranteed complexity bounds. In particular, the algorithm solves $p$-mean problems in general Hadamard spaces: we illustrate by computing medians in BHV tree space.",
        "subjects": [
            "math.OC",
            "cs.CG",
            "cs.DS",
            "math.NA"
        ],
        "comment": "41 pages, 5 figures"
    },
    {
        "paper id": "2412.06735",
        "abstract url": "https://arxiv.org/abs/2412.06735",
        "title": "Partially Observed Optimal Stochastic Control: Regularity, Optimality, Approximations, and Learning",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this review/tutorial article, we present recent progress on optimal control of partially observed Markov Decision Processes (POMDPs). We first present regularity and continuity conditions for POMDPs and their belief-MDP reductions, where these constitute weak Feller and Wasserstein regularity and controlled filter stability. These are then utilized to arrive at existence results on optimal policies for both discounted and average cost problems, and regularity of value functions. Then, we study rigorous approximation results involving quantization based finite model approximations as well as finite window approximations under controlled filter stability. Finally, we present several recent reinforcement learning theoretic results which rigorously establish convergence to near optimality under both criteria.",
        "subjects": [
            "math.OC",
            "eess.SY"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06754",
        "abstract url": "https://arxiv.org/abs/2412.06754",
        "title": "Probability and Angelic Nondeterminism with Multiset Semantics",
        "rating": "-10",
        "keywords": [],
        "abstract": "We introduce a version of probabilistic Kleene algebra with angelic nondeterminism and a corresponding class of automata. Our approach implements semantics via distributions over multisets in order to overcome theoretical barriers arising from the lack of a distributive law between the powerset and Giry monads. We produce a full Kleene theorem and a coalgebraic theory, as well as both operational and denotational semantics and equational reasoning principles.",
        "subjects": [
            "cs.LO",
            "cs.FL",
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.06757",
        "abstract url": "https://arxiv.org/abs/2412.06757",
        "title": "Why Do Developers Engage with ChatGPT in Issue-Tracker? Investigating Usage and Reliance on ChatGPT-Generated Code",
        "rating": "-10",
        "keywords": [],
        "abstract": "Large language models (LLMs) like ChatGPT have shown the potential to assist developers with coding and debugging tasks. However, their role in collaborative issue resolution is underexplored. In this study, we analyzed 1,152 Developer-ChatGPT conversations across 1,012 issues in GitHub to examine the diverse usage of ChatGPT and reliance on its generated code. Our contributions are fourfold. First, we manually analyzed 289 conversations to understand ChatGPT's usage in the GitHub Issues. Our analysis revealed that ChatGPT is primarily utilized for ideation, whereas its usage for validation (e.g., code documentation accuracy) is minimal. Second, we applied BERTopic modeling to identify key areas of engagement on the entire dataset. We found that backend issues (e.g., API management) dominate conversations, while testing is surprisingly less covered. Third, we utilized the CPD clone detection tool to check if the code generated by ChatGPT was used to address issues. Our findings revealed that ChatGPT-generated code was used as-is to resolve only 5.83\\% of the issues. Fourth, we estimated sentiment using a RoBERTa-based sentiment analysis model to determine developers' satisfaction with different usages and engagement areas. We found positive sentiment (i.e., high satisfaction) about using ChatGPT for refactoring and addressing data analytics (e.g., categorizing table data) issues. On the contrary, we observed negative sentiment when using ChatGPT to debug issues and address automation tasks (e.g., GUI interactions). Our findings show the unmet needs and growing dissatisfaction among developers. Researchers and ChatGPT developers should focus on developing task-specific solutions that help resolve diverse issues, improving user satisfaction and problem-solving efficiency in software development.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Accepted in SANER 2025"
    },
    {
        "paper id": "2412.06917",
        "abstract url": "https://arxiv.org/abs/2412.06917",
        "title": "Haptics in Micro- and Nano-Manipulation",
        "rating": "-10",
        "keywords": [],
        "abstract": "One of the motivations for the development of wirelessly guided untethered magnetic devices (UMDs), such as microrobots and nanorobots, is the continuous demand to manipulate, sort, and assemble micro-objects with high level of accuracy and dexterity. UMDs can function as microgrippers or manipulators and move micro-objects with or without direct contact. In this case, the UMDs can be directly teleoperated by an operator using haptic tele-manipulation systems. The aim of this chapter is threefold: first, to provide a mathematical framework to design a scaled bilateral tele-manipulation system to achieve wireless actuation of micro-objects using magnetically-guided UMDs; second, to demonstrate closed-loop stability based on absolute stability theory; third, to provide experimental case studies performed on haptic devices to manipulate microrobots and assemble micro-objects. In this chapter, we are concerned with some fundamental concepts of electromagnetics and low-Reynolds number hydrodynamics to understand the stability and performance of haptic devices in micro- and nano-manipulation applications.",
        "subjects": [
            "cs.RO"
        ],
        "comment": "16 Pages, 7 Figures"
    },
    {
        "paper id": "2412.06948",
        "abstract url": "https://arxiv.org/abs/2412.06948",
        "title": "Opportunities and Security Risks of Technical Leverage: A Replication Study on the NPM Ecosystem",
        "rating": "-10",
        "keywords": [],
        "abstract": "To comply with high productivity demands, software developers reuse free open-source software (FOSS) code to avoid reinventing the wheel when incorporating software features. The reliance on FOSS reuse has been shown to improve productivity and the quality of delivered software; however, reusing FOSS comes at the risk of exposing software projects to public vulnerabilities. Massacci and Pashchenko have explored this trade-off in the Java ecosystem through the lens of technical leverage: the ratio of code borrowed from FOSS over the code developed by project maintainers. In this paper, we replicate the work of Massacci and Pashchenko and we expand the analysis to include level-1 transitive dependencies to study technical leverage in the fastest-growing NPM ecosystem. We investigated 14,042 NPM library releases and found that both opportunities and risks of technical leverage are magnified in the NPM ecosystem. Small-medium libraries leverage 2.5x more code from FOSS than their code, while large libraries leverage only 3\\% of FOSS code in their projects. Our models indicate that technical leverage shortens the release cycle for small-medium libraries. However, the risk of vulnerability exposure is 4-7x higher for libraries with high technical leverage. We also expanded our replication study to include the first level of transitive dependencies, and show that the results still hold, albeit with significant changes in the magnitude of both opportunities and risks of technical leverage. Our results indicate the extremes of opportunities and risks in NPM, where high technical leverage enables fast releases but comes at the cost of security risks.",
        "subjects": [
            "cs.SE"
        ],
        "comment": "Manuscript accepted for publication in Empirical Software Engineering (EMSE)"
    },
    {
        "paper id": "2412.06994",
        "abstract url": "https://arxiv.org/abs/2412.06994",
        "title": "Phaedrus: Exploring Dynamic Application Behavior with Lightweight Generative Models and Large-Language Models",
        "rating": "-10",
        "keywords": [],
        "abstract": "Application profiling is an indispensable technique for many software development tasks, such as code optimization and memory management, where optimization decisions are tailored to specific program profiles. Unfortunately, modern applications codebases exhibit highly variant behavior across different inputs, creating challenges for conventional profiling approaches that rely on a single execution instance. In this paper, we propose \\textbf{Phaedrus}, a new \\textit{compiler-assisted deep learning framework} designed to predict dynamic program behaviors across varied execution scenarios, specifically focusing on dynamic function call prediction. Traditional profile-guided optimization methods struggle with the input-dependent variability of modern applications, where profiling on different inputs yields divergent application behaviors. To address this, Phaedrus proposes two new approaches: \\textit{Application Profile Generalization}, which uses generative models trained on compressed and augmented \\textit{Whole Program Path} (WPP) profiles to predict application behavior under unseen inputs, and \\textit{Application Behavior Synthesis}, a profile-less approach where Large Language Models (LLMs) directly infer dynamic functions based on source code \\& static compiler analysis, bypassing the need for traditional profiling. Our experiments show that \\textit{Phaedrus} can achieve upto $10^7X$ reduction in WPP profile sizes, can predict dynamic hot functions that cover upto 85-99\\% of the execution time, along with an average of \\textbf{13.46\\%} (upto \\textbf{65\\%}) reduction in application binary size reduction, without profiles.",
        "subjects": [
            "cs.SE",
            "cs.PL"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07015",
        "abstract url": "https://arxiv.org/abs/2412.07015",
        "title": "Accurate Performance Modeling And Uncertainty Analysis of Lossy Compression in Scientific Applications",
        "rating": "-10",
        "keywords": [],
        "abstract": "Scientific applications typically generate large volumes of floating-point data, making lossy compression one of the most effective methods for data reduction, thereby lowering storage requirements and improving performance in large-scale applications. However, variations in compression time can significantly impact overall performance improvement, due to inaccurate scheduling, workload imbalances, etc. Existing approaches rely on empirical methods to predict the compression performance, which often lack interpretability and suffer from limitations in accuracy and generalizability. In this paper, we propose surrogate models for predicting the compression time of prediction-based lossy compression and provide a detailed analysis of the factors influencing time variability with uncertainty analysis. Our evaluation shows that our solution can accuratly predict the compression time with 5% average error across six scientific datasets. It also provides accurate 95% confidence interval, which is essential for time-sensitive scheduling and applications.",
        "subjects": [
            "cs.PF"
        ],
        "comment": "10 pages, 5 figures"
    },
    {
        "paper id": "2412.07040",
        "abstract url": "https://arxiv.org/abs/2412.07040",
        "title": "Beyond Idle Channels: Unlocking Idle Space with Signal Alignment in Massive MIMO Cognitive Radio Networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "Cognitive radio networks (CRNs) have traditionally focused on utilizing idle channels to enhance spectrum efficiency. However, as wireless networks grow denser, channel-centric strategies face increasing limitations. This paper introduces a paradigm shift by exploring the underutilized potential of idle spatial dimensions, termed idle space, in co-channel transmissions. By integrating massive multiple-input multiple-output (MIMO) systems with signal alignment techniques, we enable secondary users to transmit without causing interference to primary users by aligning their signals within the null spaces of primary receivers. We propose a comprehensive framework that synergizes spatial spectrum sensing, signal alignment, and resource allocation, specifically designed for secondary users in CRNs. Theoretical analyses and extensive simulations validate the framework, demonstrating substantial gains in spectrum efficiency, throughput, and interference mitigation. The results show that the proposed approach not only ensures interference-free coexistence with primary users but also unlocks untapped spatial resources for secondary transmissions.",
        "subjects": [
            "eess.SP"
        ],
        "comment": "9 pages, 7 figures"
    },
    {
        "paper id": "2412.07045",
        "abstract url": "https://arxiv.org/abs/2412.07045",
        "title": "Identifying the Barriers to Human-Centered Design in the Workplace: Perspectives from UX Professionals",
        "rating": "-10",
        "keywords": [],
        "abstract": "Human-centered design, a theoretical ideal, is sometimes compromised in industry practice. Technology firms juggle competing priorities, such as adopting new technologies and generating shareholder returns, which may conflict with human-centered design values. This study sought to identify the types of workplace situations that present barriers for human-centered design, going beyond the views and behaviors of individual professionals. Q methodology was used to analyze the experiences of 14 UX professionals based in the United States. Five factors were identified, representing workplace situations in which human-centered design is inhibited, despite the involvement of UX professionals: Single-Minded Arrogance, Competing Visions, Moving Fast and Breaking Things, Pragmatically Getting By, and Sidestepping Responsibility. Underpinning these five factors are the dimensions of speed and clarity of vision. This paper demonstrates connections between the literature on UX ethics and human-centered design practice, and its findings point toward opportunities for education and intervention to better enable human-centered and ethical design in practice.",
        "subjects": [
            "cs.HC"
        ],
        "comment": "Forthcoming in Proceedings of the ACM on Human-Computer Interaction, and to be presented at CSCW 2025"
    },
    {
        "paper id": "2412.07046",
        "abstract url": "https://arxiv.org/abs/2412.07046",
        "title": "NP-hardness and a PTAS for the Euclidean Steiner Line Problem",
        "rating": "-10",
        "keywords": [],
        "abstract": "The Euclidean Steiner Tree Problem (EST) seeks a minimum-cost tree interconnecting a given set of terminal points in the Euclidean plane, allowing the use of additional intersection points. In this paper, we consider two variants that include an additional straight line $\u03b3$ with zero cost, which must be incorporated into the tree. In the Euclidean Steiner fixed Line Problem (ESfL), this line is given as input and can be treated as a terminal. In contrast, the Euclidean Steiner Line Problem (ESL) requires determining the optimal location of $\u03b3$. Despite recent advances, including heuristics and a 1.214-approximation algorithm for both problems, a formal proof of NP-hardness has remained open. In this work, we close this gap by proving that both the ESL and ESfL are NP-hard. Additionally, we prove that both problems admit a polynomial-time approximation scheme (PTAS), by demonstrating that approximation algorithms for the EST can be adapted to the ESL and ESfL with appropriate modifications. Specifically, we show ESfL$\\leq_{\\text{PTAS}}$EST and ESL$\\leq_{\\text{PTAS}}$EST, i.e., provide a PTAS reduction to the EST.",
        "subjects": [
            "cs.CG",
            "cs.DS"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07059",
        "abstract url": "https://arxiv.org/abs/2412.07059",
        "title": "Optimal Routing and Link Configuration for Covert Heterogeneous Wireless Networks",
        "rating": "-10",
        "keywords": [],
        "abstract": "Nodes in contemporary radio networks often have multiple interfaces available for communication: WiFi, cellular, LoRa, Zigbee, etc. This motivates understanding both link and network configuration when multiple communication modalities with vastly different capabilities are available to each node. In conjunction, covertness or the hiding of radio communications is often a significant concern in both commercial and military wireless networks. We consider the optimal routing problem in wireless networks when nodes have multiple interfaces available and intend to hide the presence of the transmission from attentive and capable adversaries. We first consider the maximization of the route capacity given an end-to-end covertness constraint against a single adversary and we find a polynomial-time algorithm for optimal route selection and link configuration. We further provide optimal polynomial-time algorithms for two important extensions: (i) statistical uncertainty during optimization about the channel state information for channels from system nodes to the adversary; and, (ii) maintaining covertness against multiple adversaries. Numerical results are included to demonstrate the gains of employing heterogeneous radio resources and to compare the performance of the proposed approach versus alternatives.",
        "subjects": [
            "eess.SP",
            "cs.NI"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07065",
        "abstract url": "https://arxiv.org/abs/2412.07065",
        "title": "Haptic Stylus vs. Handheld Controllers: A Comparative Study for Surface Visualization Interactions",
        "rating": "-10",
        "keywords": [],
        "abstract": "Surface visualizations are essential in analyzing three-dimensional spatiotemporal phenomena. Given its ability to provide enhanced spatial perception and scene maneuverability, virtual reality (VR) is an essential medium for surface visualization and interaction tasks. Such tasks primarily rely on visual cues that require an unoccluded view of the surface region under consideration. Haptic force feedback is a tangible interaction modality that alleviates the reliance on visual-only cues by allowing a direct physical sensation of the surface. In this paper, we evaluate the use of a force-based haptic stylus compared to handheld VR controllers via a between-subjects user study involving fundamental interaction tasks performed on surface visualizations. Keeping a consistent visual design across both modalities, our study incorporates tasks that require the localization of the highest, lowest, and random points on surfaces; and tasks that focus on brushing curves on surfaces with varying complexity and occlusion levels. Our findings show that participants took longer to brush curves using the haptic modality but could draw smoother curves compared to the handheld controllers. In contrast, haptics was faster in point localization, but the accuracy depended on the visual cues and occlusions associated with the tasks. Finally, we discuss participant feedback on using haptic force feedback as a tangible input modality and share takeaways to help outline design strategies for using haptics-based tangible inputs for surface visualization and interaction tasks.",
        "subjects": [
            "cs.HC",
            "cs.GR"
        ],
        "comment": "arXiv admin note: substantial text overlap with arXiv:2408.04031"
    },
    {
        "paper id": "2412.07074",
        "abstract url": "https://arxiv.org/abs/2412.07074",
        "title": "Channel Spreading Function-Inspired Channel Transfer Function Estimation for OFDM Systems with High-Mobility",
        "rating": "-10",
        "keywords": [],
        "abstract": "In this letter, we propose a novel channel transfer function (CTF) estimation approach for orthogonal frequency division multiplexing (OFDM) systems in high-mobility scenarios, that leverages the stationary properties of the delay-Doppler domain channel spreading function (CSF). First, we develop a CSF estimation model for OFDM systems that relies solely on discrete pilot symbols in the time-frequency (TF) domain, positioned at predefined resource elements. We then present theorems to elucidate the relationship between CSF compactness and pilot spacing in the TF domain for accurate CSF acquisition. Based on the estimated CSF, we finally estimate the CTF for data symbols. Numerical results show that, in high-mobility scenarios, the proposed approach outperforms traditional interpolation-based methods and closely matches the optimal estimator in terms of estimation accuracy. This work may pave the way for CSF estimation in commercial OFDM systems, benefiting high-mobility communications, integrated sensing and communications, and related applications.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    },
    {
        "paper id": "2412.07086",
        "abstract url": "https://arxiv.org/abs/2412.07086",
        "title": "A Fixed Point Iteration Technique for Proving Correctness of Slicing for Probabilistic Programs",
        "rating": "-10",
        "keywords": [],
        "abstract": "When proving the correctness of a method for slicing probabilistic programs, it was previously discovered by the authors that for a fixed point iteration to work one needs a non-standard starting point for the iteration. This paper presents and explores this technique in a general setting; it states the lemmas that must be established to use the technique to prove the correctness of a program transformation, and sketches how to apply the technique to slicing of probabilistic programs.",
        "subjects": [
            "cs.PL"
        ],
        "comment": "To be published by Springer in Festschrift for Alan Mycroft"
    },
    {
        "paper id": "2412.07173",
        "abstract url": "https://arxiv.org/abs/2412.07173",
        "title": "Semantic Communications for Digital Signals via Carrier Images",
        "rating": "-10",
        "keywords": [],
        "abstract": "Most of current semantic communication (SemCom) frameworks focus on the image transmission, which, however, do not address the problem on how to deliver digital signals without any semantic features. This paper proposes a novel SemCom approach to transmit digital signals by using the image as the carrier signal. Specifically, the proposed approach encodes the digital signal as a binary stream and maps it to mask locations on an image. This allows binary data to be visually represented, enabling the use of existing model, pre-trained Masked Autoencoders (MAE), which are optimized for masked image reconstruction, as the SemCom encoder and decoder. Since MAE can both process and recover masked images, this approach allows for the joint transmission of digital signals and images without additional overhead. In addition, considering the mask tokens transmission encoded by the MAE still faces extra costs, we design a sparse encoding module at the transmitter to encode the mask tokens into a sparse matrix, and it can be recovered at the receiver. Thus, this approach simply needs to transmit the latent representations of the unmasked patches and a sparse matrix, which further reduce the transmission overhead compared with the original MAE encoder. Simulation results show that the approach maintains reliable transmission of digital signals and images even in a high mask ratio of transmitted images.",
        "subjects": [
            "eess.SP"
        ],
        "comment": null
    }
]